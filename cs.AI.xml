<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#32593;&#32476;Point-NN&#12290;&#23427;&#22312;&#21508;&#31181;&#19977;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#25110;&#35757;&#32451;&#65292;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#26500;&#24314;&#21442;&#25968;&#21270;&#32593;&#32476;&#21644;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#19977;&#32500;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2303.08134</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#21270;&#32593;&#32476;&#22312;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#21442;&#25968;&#19981;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis. (arXiv:2303.08134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#32593;&#32476;Point-NN&#12290;&#23427;&#22312;&#21508;&#31181;&#19977;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#25110;&#35757;&#32451;&#65292;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#26500;&#24314;&#21442;&#25968;&#21270;&#32593;&#32476;&#21644;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#19977;&#32500;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#32593;&#32476;&#8212;&#8212;Point-NN&#12290;&#35813;&#32593;&#32476;&#20165;&#30001;&#19981;&#21487;&#23398;&#20064;&#32452;&#20214;&#32452;&#25104;&#65292;&#21253;&#25324;&#26368;&#36828;&#28857;&#37319;&#26679;&#65288;FPS&#65289;&#12289;K&#36817;&#37051;&#65288;k-NN&#65289;&#21644;&#21152;&#26435;&#24179;&#22343;&#27744;&#21270;&#31561;&#25805;&#20316;&#21450;&#19977;&#35282;&#20989;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#22312;&#21508;&#31181;&#19977;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#25110;&#35757;&#32451;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#31181;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#12290;&#39318;&#20808;&#65292;Point-NN&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20854;&#19978;&#31616;&#21333;&#25554;&#20837;&#32447;&#24615;&#23618;&#26469;&#26500;&#24314;&#21442;&#25968;&#21270;&#32593;&#32476;&#12290;&#22312;&#38750;&#21442;&#25968;&#22522;&#30784;&#19978;&#65292;&#24471;&#21040;&#30340;Point-PN&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#65292;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;Point-NN&#21487;&#20197;&#34987;&#35270;&#20026;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#19977;&#32500;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;Point-NN&#25429;&#33719;&#20114;&#34917;&#30340;&#20960;&#20309;&#30693;&#35782;&#65292;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#23545;&#19981;&#21516;&#19977;&#32500;&#22522;&#20934;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21457;&#26356;&#22810;&#23545;&#38750;&#32447;&#24615;&#25805;&#20316;&#21644;&#20960;&#20309;&#30693;&#35782;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.08133</link><description>&lt;p&gt;
MeshDiffusion&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MeshDiffusion: Score-based Generative 3D Mesh Modeling. (arXiv:2303.08133v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#29289;&#20307;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#21644;&#29289;&#29702;&#20223;&#30495;&#31561;&#22810;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#30456;&#27604;&#20110;&#20307;&#32032;&#21644;&#28857;&#20113;&#31561;&#20854;&#20182;3D&#34920;&#31034;&#65292;&#32593;&#26684;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#20248;&#36234;&#65292;&#22240;&#20026;(1)&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#20219;&#24847;&#22320;&#25805;&#32437;&#24418;&#29366;&#20197;&#20379;&#37325;&#26032;&#29031;&#26126;&#21644;&#20223;&#30495;&#65292;(2)&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;&#29616;&#20195;&#22270;&#24418;&#27969;&#27700;&#32447;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#27969;&#27700;&#32447;&#22823;&#22810;&#25968;&#38024;&#23545;&#32593;&#26684;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#20197;&#24448;&#21487;&#25193;&#23637;&#30340;3D&#32593;&#26684;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#21518;&#22788;&#29702;&#65292;&#24182;&#19988;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#36807;&#20110;&#24179;&#28369;&#25110;&#22024;&#26434;&#30340;&#34920;&#38754;&#65292;&#32570;&#20047;&#31934;&#32454;&#30340;&#20960;&#20309;&#32454;&#33410;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;3D&#32593;&#26684;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#21464;&#24418;&#22235;&#38754;&#20307;&#32593;&#26684;&#26469;&#34920;&#31034;&#32593;&#26684;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#32593;&#26684;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PiMAE&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#26041;&#38754;&#20419;&#36827;&#28857;&#20113;&#21644;RGB&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21253;&#25324;&#21033;&#29992;&#25237;&#24433;&#27169;&#22359;&#20114;&#34917;&#22320;&#23545;&#40784;&#20004;&#31181;&#27169;&#24577;&#30340;&#25513;&#34109;&#21644;&#21487;&#35265;&#20196;&#29260;&#12289;&#21033;&#29992;&#20004;&#20010;&#25903;&#36335;&#30340;MAE&#31649;&#36947;&#21644;&#20849;&#20139;&#35299;&#30721;&#22120;&#20419;&#36827;&#25513;&#34109;&#20196;&#29260;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#35774;&#35745;&#20102;&#36328;&#27169;&#24577;&#37325;&#26500;&#27169;&#22359;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.08129</link><description>&lt;p&gt;
PiMAE: &#28857;&#20113;&#21644;&#22270;&#20687;&#20132;&#20114;&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection. (arXiv:2303.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PiMAE&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#26041;&#38754;&#20419;&#36827;&#28857;&#20113;&#21644;RGB&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21253;&#25324;&#21033;&#29992;&#25237;&#24433;&#27169;&#22359;&#20114;&#34917;&#22320;&#23545;&#40784;&#20004;&#31181;&#27169;&#24577;&#30340;&#25513;&#34109;&#21644;&#21487;&#35265;&#20196;&#29260;&#12289;&#21033;&#29992;&#20004;&#20010;&#25903;&#36335;&#30340;MAE&#31649;&#36947;&#21644;&#20849;&#20139;&#35299;&#30721;&#22120;&#20419;&#36827;&#25513;&#34109;&#20196;&#29260;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#35774;&#35745;&#20102;&#36328;&#27169;&#24577;&#37325;&#26500;&#27169;&#22359;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#22312;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#29420;&#31435;&#35774;&#32622;&#20013;&#23398;&#20064;&#24378;&#30340;&#35270;&#35273;&#34920;&#24449;&#65292;&#22312;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#28857;&#20113;&#21644;RGB&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#20004;&#31181;&#27169;&#24577;&#24120;&#24120;&#22312;&#29616;&#23454;&#20013;&#21516;&#26102;&#21576;&#29616;&#65292;&#24182;&#25506;&#32034;&#20854;&#26377;&#24847;&#20041;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#24037;&#20316;&#20013;&#36328;&#27169;&#24577;&#21327;&#21516;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PiMAE&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#26041;&#38754;&#20419;&#36827;&#19977;&#32500;&#21644;&#20108;&#32500;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27880;&#24847;&#21040;&#20004;&#20010;&#28304;&#20043;&#38388;&#25513;&#34109;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21033;&#29992;&#25237;&#24433;&#27169;&#22359;&#26469;&#20114;&#34917;&#22320;&#23545;&#40784;&#20004;&#31181;&#27169;&#24577;&#30340;&#25513;&#34109;&#21644;&#21487;&#35265;&#20196;&#29260;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20004;&#20010;&#25903;&#36335;&#30340;MAE&#31649;&#36947;&#21644;&#19968;&#20010;&#26032;&#22411;&#30340;&#20849;&#20139;&#35299;&#30721;&#22120;&#26469;&#20419;&#36827;&#25513;&#34109;&#20196;&#29260;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#36328;&#27169;&#24577;&#37325;&#26500;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders learn strong visual representations and achieve state-of-the-art results in several independent modalities, yet very few works have addressed their capabilities in multi-modality settings. In this work, we focus on point cloud and RGB image data, two modalities that are often presented together in the real world, and explore their meaningful interactions. To improve upon the cross-modal synergy in existing works, we propose PiMAE, a self-supervised pre-training framework that promotes 3D and 2D interaction through three aspects. Specifically, we first notice the importance of masking strategies between the two sources and utilize a projection module to complementarily align the mask and visible tokens of the two modalities. Then, we utilize a well-crafted two-branch MAE pipeline with a novel shared decoder to promote cross-modality interaction in the mask tokens. Finally, we design a unique cross-modal reconstruction module to enhance representation learning for bot
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#25968;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#27979;&#35797;&#26597;&#35810;&#19978;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#24182;&#27809;&#26377;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20840;&#28436;&#31034;ICL&#12290;</title><link>http://arxiv.org/abs/2303.08119</link><description>&lt;p&gt;
&#19968;&#20154;&#29420;&#33310;&#22909;&#36824;&#26159;&#20154;&#22810;&#38393;&#24515;&#65311;&#19981;&#21516;&#28436;&#31034;&#27425;&#25968;&#19979;&#30340;&#19978;&#19979;&#25991;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations. (arXiv:2303.08119v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#25968;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#27979;&#35797;&#26597;&#35810;&#19978;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#24182;&#27809;&#26377;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20840;&#28436;&#31034;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#20102;&#19968;&#20123;&#36755;&#20837;&#36755;&#20986;&#28436;&#31034;&#65288;demos&#65289;&#24182;&#32473;&#20986;&#26356;&#22810;&#28436;&#31034;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65288;&#8220;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#8221;&#65289;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#27979;&#35797;&#26597;&#35810;&#19978;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#26469;&#36827;&#34892;ICL&#30340;&#20219;&#21153;~\cite{wei2022chain}&#12290;&#24778;&#20154;&#22320;&#65292;&#24403;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#23545;&#20110;&#27599;&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#25105;&#20204;&#23558;&#28436;&#31034;&#20998;&#31867;&#20026;&#8220;&#27491;&#30830;&#28436;&#31034;&#8221;&#21644;&#8220;&#38169;&#35823;&#28436;&#31034;&#8221;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24191;&#27867;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#20559;&#24046;&#65306;&#22823;&#22810;&#25968;&#27979;&#35797;&#26597;&#35810;&#30340;&#22823;&#22810;&#25968;&#28436;&#31034;&#37117;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#35299;&#37322;&#20102;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#28436;&#31034;&#26102;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#65288;&#24102;&#21644;&#19981;&#24102;CoT&#65289;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#37319;&#29992;&#30340;&#20840;&#28436;&#31034;ICL&#65292;&#34920;&#26126;&#28436;&#31034;&#25968;&#37327;&#24182;&#19981;&#24635;&#26159;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps ("chain of thoughts (CoT)") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into "correct demos" leading to the correct answer, and "wrong demos" resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#39046;&#22495;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#20986;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#24456;&#22823;&#24433;&#21709;&#65292;&#39046;&#22495;&#36890;&#29992;&#24615;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#28304;&#22495;&#20013;&#23398;&#20064;&#27169;&#22411;&#24182;&#36866;&#24212;&#26032;&#30340;&#39046;&#22495;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08106</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;&#65306;&#27010;&#24565;&#65292;&#29616;&#29366;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization in Machine Learning Models for Wireless Communications: Concepts, State-of-the-Art, and Open Issues. (arXiv:2303.08106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#39046;&#22495;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#20986;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#24456;&#22823;&#24433;&#21709;&#65292;&#39046;&#22495;&#36890;&#29992;&#24615;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#28304;&#22495;&#20013;&#23398;&#20064;&#27169;&#22411;&#24182;&#36866;&#24212;&#26032;&#30340;&#39046;&#22495;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#19968;&#31181;&#28508;&#22312;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#65292;&#24212;&#29992;ML&#25216;&#26415;&#35299;&#20915;&#26080;&#32447;&#20256;&#36755;&#38142;&#30340;&#19981;&#21516;&#23618;&#27425;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#24212;&#29992;&#37117;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20551;&#23450;&#28304;&#65288;&#35757;&#32451;&#65289;&#21644;&#30446;&#26631;&#65288;&#27979;&#35797;&#65289;&#25968;&#25454;&#29420;&#31435;&#19988;&#38543;&#26426;&#20998;&#24067;&#30456;&#21516;&#65288;i.i.d.&#65289;&#12290;&#36825;&#20010;&#20551;&#35774;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#22240;&#20026;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#25110;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39046;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#21644;&#29420;&#29305;&#30340;&#28304;&#22495;/&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;OOD&#30456;&#20851;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;&#21463;&#21040;&#23545;&#26080;&#32447;&#24212;&#29992;&#20013;DG&#38656;&#27714;&#30340;&#37325;&#35270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36890;&#29992;&#24615;&#30340;&#27010;&#24565;&#21644;&#20854;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven machine learning (ML) is promoted as one potential technology to be used in next-generations wireless systems. This led to a large body of research work that applies ML techniques to solve problems in different layers of the wireless transmission link. However, most of these applications rely on supervised learning which assumes that the source (training) and target (test) data are independent and identically distributed (i.i.d). This assumption is often violated in the real world due to domain or distribution shifts between the source and the target data. Thus, it is important to ensure that these algorithms generalize to out-of-distribution (OOD) data. In this context, domain generalization (DG) tackles the OOD-related issues by learning models on different and distinct source domains/datasets with generalization capabilities to unseen new domains without additional finetuning. Motivated by the importance of DG requirements for wireless applications, we present a comprehe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#22270;&#20687;&#24341;&#23548;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#21160;&#30830;&#23450;&#33107;&#39592;&#22797;&#20301;&#26041;&#21521;&#65292;&#36827;&#32780;&#25552;&#39640;&#39592;&#25240;&#25163;&#26415;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25913;&#21892;&#24739;&#32773;&#30340;&#25163;&#26415;&#25928;&#26524;&#65292;&#38477;&#20302;&#24739;&#32773;&#21457;&#29983;&#39592;&#24615;&#20851;&#33410;&#28814;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2303.08105</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#36381;&#39592;&#39592;&#25240;&#20462;&#22797;&#30340;&#22270;&#20687;&#24341;&#23548;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Guidance for Robot-Assisted Ankle Fracture Repair. (arXiv:2303.08105v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#22270;&#20687;&#24341;&#23548;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#21160;&#30830;&#23450;&#33107;&#39592;&#22797;&#20301;&#26041;&#21521;&#65292;&#36827;&#32780;&#25552;&#39640;&#39592;&#25240;&#25163;&#26415;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25913;&#21892;&#24739;&#32773;&#30340;&#25163;&#26415;&#25928;&#26524;&#65292;&#38477;&#20302;&#24739;&#32773;&#21457;&#29983;&#39592;&#24615;&#20851;&#33410;&#28814;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#28041;&#21450;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#33107;&#39592;&#22797;&#20301;&#30340;&#22270;&#20687;&#24341;&#23548;&#26694;&#26550;&#65292;&#24182;&#26088;&#22312;&#20135;&#29983;&#21644;&#35777;&#26126;&#33258;&#21160;&#30830;&#23450;&#33107;&#39592;&#22797;&#20301;&#26041;&#21521;&#30340;&#36719;&#20214;&#30340;&#27491;&#30830;&#21151;&#33021;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20943;&#21387;&#31243;&#24207;&#65292;&#20174;&#32780;&#20943;&#23569;&#25163;&#26415;&#26102;&#38388;&#21644;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#29702;&#24819;&#30340;&#26368;&#32456;&#33107;&#39592;&#20301;&#32622;&#35823;&#24046;&#20943;&#23567;&#12289;&#38887;&#24102;&#32467;&#26500;&#24674;&#22797;&#24471;&#21040;&#25913;&#21892;&#20197;&#21450;&#21019;&#20260;&#21518;&#39592;&#24615;&#20851;&#33410;&#28814;&#21457;&#29983;&#29575;&#38477;&#20302;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project concerns developing and validating an image guidance framework for application to a robotic-assisted fibular reduction in ankle fracture surgery. The aim is to produce and demonstrate proper functioning of software for automatic determination of directions for fibular repositioning with the ultimate goal of application to a robotic reduction procedure that can reduce the time and complexity of the procedure as well as provide the benefits of reduced error in ideal final fibular position, improved syndesmosis restoration and reduced incidence of post-traumatic osteoarthritis. The focus of this product will be developing and testing the image guidance software, from the input of preoperative images through the steps of automated segmentation and registration until the output of a final transformation that can be used as instructions to a robot on how to reposition the fibula, but will not involve developing or implementing the hardware of the robot itself.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32500;&#22810;&#21033;&#20122;&#20122;&#39532;&#36874;&#26893;&#29289;&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#19968;&#31181;&#32500;&#22810;&#21033;&#20122;&#20122;&#39532;&#36874;&#20248;&#21270;&#65288;VAO&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21344;&#39046;&#26356;&#22810;&#34920;&#38754;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08070</link><description>&lt;p&gt;
&#32500;&#22810;&#21033;&#20122;&#20122;&#39532;&#36874;&#20248;&#21270;&#65288;VAO&#65289;:&#21463;&#24040;&#22411;&#30561;&#33714;&#26893;&#29289;&#21551;&#31034;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Victoria Amazonica Optimization (VAO): An Algorithm Inspired by the Giant Water Lily Plant. (arXiv:2303.08070v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08070
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32500;&#22810;&#21033;&#20122;&#20122;&#39532;&#36874;&#26893;&#29289;&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#19968;&#31181;&#32500;&#22810;&#21033;&#20122;&#20122;&#39532;&#36874;&#20248;&#21270;&#65288;VAO&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21344;&#39046;&#26356;&#22810;&#34920;&#38754;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#22810;&#21033;&#20122;&#20122;&#39532;&#36874;&#26893;&#29289;&#65292;&#36890;&#24120;&#31216;&#20026;&#24040;&#22411;&#30561;&#33714;&#65292;&#25317;&#26377;&#19990;&#30028;&#19978;&#30452;&#24452;&#26368;&#22823;&#30340;&#28418;&#28014;&#29699;&#24418;&#21494;&#29255;&#65292;&#26368;&#22823;&#30452;&#24452;&#21487;&#36798;3&#31859;&#12290;&#23427;&#36890;&#36807;&#21050;&#30340;&#21147;&#37327;&#23637;&#24320;&#21494;&#23376;&#65292;&#22312;&#20854;&#19979;&#38754;&#24418;&#25104;&#22823;&#29255;&#38452;&#24433;&#65292;&#25212;&#26432;&#38656;&#35201;&#38451;&#20809;&#30340;&#20219;&#20309;&#26893;&#29289;&#12290;&#36825;&#20123;&#27700;&#20013;&#38712;&#20027;&#29992;&#23427;&#20204;&#24378;&#22823;&#30340;&#21050;&#36843;&#20351;&#24444;&#27492;&#21319;&#33267;&#27700;&#38754;&#24182;&#22686;&#24378;&#23427;&#20204;&#21344;&#25454;&#27700;&#38754;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#23427;&#20204;&#22312;&#27744;&#22616;&#25110;&#30406;&#22320;&#20013;&#30340;&#25193;&#25955;&#65292;&#26089;&#26399;&#29983;&#38271;&#30340;&#21494;&#23376;&#26377;&#26356;&#22810;&#30340;&#29983;&#38271;&#31354;&#38388;&#65292;&#27599;&#29255;&#21494;&#23376;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22823;&#23567;&#12290;&#23427;&#30340;&#33457;&#26159;&#38544;&#34109;&#38596;&#24615;&#30340;&#65292;&#24403;&#23427;&#20204;&#24320;&#25918;&#26102;&#65292;Cyclocephala&#30002;&#34411;&#36127;&#36131;&#25480;&#31881;&#36807;&#31243;&#65292;&#34987;&#22899;&#33457;&#30340;&#27668;&#21619;&#25152;&#21560;&#24341;&#12290;&#30002;&#34411;&#36827;&#20837;&#33457;&#21518;&#65292;&#34987;&#33457;&#31881;&#35206;&#30422;&#65292;&#23558;&#20854;&#36716;&#31227;&#21040;&#21478;&#19968;&#26421;&#33457;&#19978;&#36827;&#34892;&#21463;&#31934;&#12290;&#30002;&#34411;&#31163;&#24320;&#21518;&#65292;&#33457;&#21464;&#25104;&#20102;&#38596;&#33457;&#65292;&#39068;&#33394;&#20174;&#30333;&#33394;&#21464;&#25104;&#20102;&#31881;&#33394;&#12290;&#38596;&#33457;&#27515;&#20129;&#24182;&#27785;&#20837;&#27700;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Victoria Amazonica plant, often known as the Giant Water Lily, has the largest floating spherical leaf in the world, with a maximum leaf diameter of 3 meters. It spreads its leaves by the force of its spines and creates a large shadow underneath, killing any plants that require sunlight. These water tyrants use their formidable spines to compel each other to the surface and increase their strength to grab more space from the surface. As they spread throughout the pond or basin, with the earliest-growing leaves having more room to grow, each leaf gains a unique size. Its flowers are transsexual and when they bloom, Cyclocephala beetles are responsible for the pollination process, being attracted to the scent of the female flower. After entering the flower, the beetle becomes covered with pollen and transfers it to another flower for fertilization. After the beetle leaves, the flower turns into a male and changes color from white to pink. The male flower dies and sinks into the water
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26041;&#27861;IEA-GAN&#65292;&#36890;&#36807;&#20135;&#29983;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08046</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#33258;&#30417;&#30563;&#20851;&#31995;&#25512;&#29702;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and Self-Supervised Relational Reasoning. (arXiv:2303.08046v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26041;&#27861;IEA-GAN&#65292;&#36890;&#36807;&#20135;&#29983;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#65292;&#27169;&#25311;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#19968;&#30452;&#26159;&#19968;&#20010;&#23384;&#20648;&#25104;&#26412;&#39640;&#12289;&#35745;&#31639;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20351;&#36825;&#20010;&#36807;&#31243;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#65292;&#20294;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#27169;&#25311;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20102;&#20107;&#20214;&#20869;&#30456;&#20851;&#21644;&#32454;&#31890;&#24230;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65288;IEA-GAN&#65289;&#65292;&#34701;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22411;&#12290;IEA-GAN&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;&#65292;&#36817;&#20284;&#20110;&#25506;&#27979;&#22120;&#27169;&#25311;&#20013;&#8220;&#20107;&#20214;&#8221;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;IEA-GAN&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IEA-GAN&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating high-resolution detector responses is a storage-costly and computationally intensive process that has long been challenging in particle physics. Despite the ability of deep generative models to make this process more cost-efficient, ultra-high-resolution detector simulation still proves to be difficult as it contains correlated and fine-grained mutual information within an event. To overcome these limitations, we propose Intra-Event Aware GAN (IEA-GAN), a novel fusion of Self-Supervised Learning and Generative Adversarial Networks. IEA-GAN presents a Relational Reasoning Module that approximates the concept of an ''event'' in detector simulation, allowing for the generation of correlated layer-dependent contextualized images for high-resolution detector responses with a proper relational inductive bias. IEA-GAN also introduces a new intra-event aware loss and a Uniformity loss, resulting in significant enhancements to image fidelity and diversity. We demonstrate IEA-GAN's ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;2022 N2C2&#20020;&#24202;&#25361;&#25112;&#36187;&#20013;&#20851;&#20110;&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#24182;&#35780;&#20272;&#33258;&#21160;&#39044;&#27979;&#22240;&#26524;&#30456;&#20851;&#27010;&#24565;&#30340;NLP&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.08038</link><description>&lt;p&gt;
&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#65306;2022 N2C2Track 3&#20849;&#20139;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Progress Note Understanding -- Assessment and Plan Reasoning: Overview of the 2022 N2C2 Track 3 Shared Task. (arXiv:2303.08038v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;2022 N2C2&#20020;&#24202;&#25361;&#25112;&#36187;&#20013;&#20851;&#20110;&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#24182;&#35780;&#20272;&#33258;&#21160;&#39044;&#27979;&#22240;&#26524;&#30456;&#20851;&#27010;&#24565;&#30340;NLP&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#26085;&#36827;&#23637;&#31508;&#35760;&#26159;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#24120;&#35265;&#30340;&#31867;&#22411;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#22312;&#20854;&#20013;&#35760;&#24405;&#24739;&#32773;&#30340;&#27599;&#26085;&#36827;&#23637;&#21644;&#27835;&#30103;&#35745;&#21010;&#12290; EHR&#26088;&#22312;&#35760;&#24405;&#20026;&#24739;&#32773;&#25552;&#20379;&#30340;&#25152;&#26377;&#25252;&#29702;&#65292;&#20294;&#23427;&#20063;&#20250;&#20351;&#31508;&#35760;&#33192;&#32960;&#24182;&#21253;&#21547;&#20998;&#25955;&#35786;&#26029;&#21644;&#27835;&#30103;&#35745;&#21010;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290; &#22312;EHR&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#29992;&#20110;&#20449;&#24687;&#25552;&#21462;&#12290;&#24456;&#23569;&#26377;&#20219;&#21153;&#20351;&#29992;NLP&#26041;&#27861;&#36827;&#34892;&#19979;&#28216;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;2022&#24180;&#22269;&#23478;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20020;&#24202;&#25361;&#25112;&#36187;&#65288;N2C2&#65289;Track 3&#65306;&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#65292;&#20316;&#20026;&#26032;&#19968;&#22871;&#20219;&#21153;&#30340;&#19968;&#27493;&#12290; &#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#20219;&#21153;&#20391;&#37325;&#20110;&#36827;&#23637;&#31508;&#35760;&#30340;&#26368;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#21253;&#21547;&#20581;&#24247;&#38382;&#39064;&#21644;&#35786;&#26029;&#30340;&#35780;&#20272;&#21644;&#35745;&#21010;&#23376;&#37096;&#20998;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#24182;&#35780;&#20272;&#33258;&#21160;&#39044;&#27979;&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#20013;&#22240;&#26524;&#30456;&#20851;&#27010;&#24565;&#30340;NLP&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily progress notes are common types in the electronic health record (EHR) where healthcare providers document the patient's daily progress and treatment plans. The EHR is designed to document all the care provided to patients, but it also enables note bloat with extraneous information that distracts from the diagnoses and treatment plans. Applications of natural language processing (NLP) in the EHR is a growing field with the majority of methods in information extraction. Few tasks use NLP methods for downstream diagnostic decision support. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3: Progress Note Understanding - Assessment and Plan Reasoning as one step towards a new suite of tasks. The Assessment and Plan Reasoning task focuses on the most critical components of progress notes, Assessment and Plan subsections where health problems and diagnoses are contained. The goal of the task was to develop and evaluate NLP systems that automatically predict causal re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.08035</link><description>&lt;p&gt;
ISimDL: &#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#39537;&#21160;&#30340;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#24378;&#20581;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#31995;&#32479;&#24050;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#38656;&#35201;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#33455;&#29255;&#12290;&#22312;&#32435;&#31859;&#26102;&#20195;&#65292;&#35774;&#22791;&#36234;&#26469;&#36234;&#23481;&#26131;&#21463;&#21040;&#27704;&#20037;&#24615;&#21644;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#27492;&#31867;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#20102;&#35299;&#31070;&#32463;&#21152;&#36895;&#22120;&#33455;&#29255;&#20013;&#30340;&#25925;&#38556;&#22914;&#20309;&#22312;DL&#24212;&#29992;&#32423;&#21035;&#19978;&#34920;&#29616;&#20026;&#38169;&#35823;&#65292;&#20854;&#20013;&#25925;&#38556;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#38169;&#35823;&#12290;&#20351;&#29992;&#25925;&#38556;&#27880;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#36719;&#20214;&#32423;&#21035;&#20462;&#25913;&#31070;&#32463;&#20803;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#25191;&#34892;DL&#31995;&#32479;&#30340;&#38887;&#24615;&#30740;&#31350;&#65292;&#23601;&#22909;&#20687;&#30828;&#20214;&#21463;&#21040;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#19968;&#26679;&#12290;&#29616;&#26377;&#30340;&#25925;&#38556;&#27169;&#22411;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#20998;&#26512;&#26356;&#24555;&#65292;&#20294;&#38656;&#35201;&#35813;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#20801;&#35768;&#36827;&#19968;&#27493;&#20998;&#26512;&#31579;&#36873;&#20986;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ISimDL&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#12290;ISimDL&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#20943;&#23569;&#20102;&#25925;&#38556;&#27880;&#20837;&#20998;&#26512;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#65292;&#21516;&#26102;&#20173;&#30830;&#20445;&#36275;&#22815;&#35206;&#30422;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;ISimDL&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#23427;&#25552;&#20379;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate impo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#28145;&#24230;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#32534;&#30721;&#22120;&#32467;&#26500;&#26356;&#31526;&#21512;&#20844;&#24179;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;SR&#27169;&#22411;&#30340;&#20559;&#24046;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08026</link><description>&lt;p&gt;
&#28145;&#24230;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Bias and Fairness In Deep Speaker Recognition. (arXiv:2303.08026v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#28145;&#24230;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#32534;&#30721;&#22120;&#32467;&#26500;&#26356;&#31526;&#21512;&#20844;&#24179;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;SR&#27169;&#22411;&#30340;&#20559;&#24046;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20351;&#29992;&#35828;&#35805;&#20154;&#35782;&#21035;&#65288;SR&#65289;&#31995;&#32479;&#20316;&#20026;&#35748;&#35777;&#20010;&#20154;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#26041;&#24335;&#30340;&#26234;&#33021;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;SR&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#19977;&#20010;&#27969;&#34892;&#21644;&#30456;&#20851;&#23450;&#20041;&#65288;&#21363;&#32479;&#35745;&#24179;&#31561;&#12289;&#22343;&#34913;&#36180;&#29575;&#21644;&#24179;&#31561;&#26426;&#20250;&#65289;&#30340;&#26368;&#26032;SR&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;5&#31181;&#27969;&#34892;&#30340;&#31070;&#32463;&#26550;&#26500;&#21644;5&#31181;&#24120;&#29992;&#30340;&#20002;&#22833;&#21151;&#33021;&#26469;&#35757;&#32451;SR&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30456;&#23545;&#20110;&#24615;&#21035;&#21644;&#22269;&#31821;&#32452;&#21035;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#35814;&#32454;&#23454;&#39564;&#38416;&#26126;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#26356;&#22797;&#26434;&#30340;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#26356;&#31526;&#21512;&#20844;&#24179;&#30340;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;SR&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ubiquity of smart devices that use speaker recognition (SR) systems as a means of authenticating individuals and personalizing their services, fairness of SR systems has becomes an important point of focus. In this paper we study the notion of fairness in recent SR systems based on 3 popular and relevant definitions, namely Statistical Parity, Equalized Odds, and Equal Opportunity. We examine 5 popular neural architectures and 5 commonly used loss functions in training SR systems, while evaluating their fairness against gender and nationality groups. Our detailed experiments shed light on this concept and demonstrate that more sophisticated encoder architectures better align with the definitions of fairness. Additionally, we find that the choice of loss functions can significantly impact the bias of SR models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;</title><link>http://arxiv.org/abs/2303.08021</link><description>&lt;p&gt;
&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#24456;&#26377;&#21069;&#36884;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#12290;&#20248;&#21270;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30830;&#23450;&#30340;&#36845;&#20195;&#27425;&#25968;&#26469;&#26368;&#22823;&#21270;&#22522;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30142;&#30149;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65306;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#21644;&#34588;&#34562;&#31639;&#27861;&#65292;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;99.63%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;AraBERT&#33719;&#24471;&#20102;88%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08010</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#32423;&#32852;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65306;&#24403;&#28145;&#24230;&#38598;&#25104;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#26377;&#25928;&#26102;
&lt;/p&gt;
&lt;p&gt;
Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31616;&#21333;&#12289;&#21487;&#38752;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37096;&#32626;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65292;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#35745;&#31639;&#24320;&#38144;&#22823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#34920;&#26126;&#23545;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#38598;&#25104;&#21487;&#20197;&#27604;&#22312;&#21516;&#19968;&#26550;&#26500;&#26063;&#20013;&#32553;&#25918;&#21333;&#19968;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36890;&#36807;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#32423;&#32852;&#38598;&#25104;&#25104;&#21592;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#25928;&#29575;&#25552;&#39640;&#25193;&#23637;&#21040;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35768;&#22810;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#37117;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#26032;&#39062;&#35265;&#35299;&#26159;&#20165;&#23558;&#25509;&#36817;&#20108;&#20998;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#20256;&#36882;&#21040;&#21518;&#32493;&#32423;&#32852;&#38454;&#27573;&#12290;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#22312;&#20351;&#29992;&#27604;&#22522;&#32447;&#26356;&#23569;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#19982;&#23436;&#25972;&#38598;&#25104;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#24182;&#22312;&#30495;&#23454;&#24773;&#26223;&#19979;&#27979;&#35797;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#20851;&#38190;&#20107;&#20214;&#21644;&#29983;&#23384;&#26465;&#20214;&#30340;&#19968;&#31181;&#26032;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#26089;&#30340;&#30896;&#25758;&#26816;&#27979;&#26102;&#38388;&#21644;&#36739;&#23569;&#30340;&#35823;&#25253;&#26816;&#27979;&#65292;&#36866;&#29992;&#20110;ADAS&#21644;AD&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08007</link><description>&lt;p&gt;
&#39550;&#39542;&#25903;&#25345;&#30340;&#36830;&#32493;&#39118;&#38505;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Continuous Risk Measures for Driving Support. (arXiv:2303.08007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#24182;&#22312;&#30495;&#23454;&#24773;&#26223;&#19979;&#27979;&#35797;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#20851;&#38190;&#20107;&#20214;&#21644;&#29983;&#23384;&#26465;&#20214;&#30340;&#19968;&#31181;&#26032;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#26089;&#30340;&#30896;&#25758;&#26816;&#27979;&#26102;&#38388;&#21644;&#36739;&#23569;&#30340;&#35823;&#25253;&#26816;&#27979;&#65292;&#36866;&#29992;&#20110;ADAS&#21644;AD&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#24230;&#37327;&#65292;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#21155;&#65292;&#24182;&#22312;&#19968;&#32452;&#30495;&#23454;&#30340;&#32437;&#21521;&#21644;&#20132;&#21449;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23450;&#37327;&#27979;&#35797;&#12290;&#25105;&#20204;&#20174;&#20256;&#32479;&#30340;&#21551;&#21457;&#24335;&#26102;&#38388;&#30456;&#36935;&#65288;TTC&#65289;&#24320;&#22987;&#65292;&#23558;&#20854;&#25193;&#23637;&#21040;2D&#25805;&#20316;&#21644;&#38750;&#30896;&#25758;&#24773;&#20917;&#65292;&#20197;&#24674;&#22797;&#26368;&#25509;&#36817;&#36973;&#36935;&#26102;&#38388;&#65288;TTCE&#65289;&#12290;&#31532;&#20108;&#20010;&#39118;&#38505;&#24230;&#37327;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#24314;&#27169;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#31354;&#38388;&#21344;&#29992;&#27010;&#29575;&#26469;&#35745;&#31639;&#30896;&#25758;&#39118;&#38505;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#31232;&#30095;&#20851;&#38190;&#20107;&#20214;&#21644;&#25152;&#35859;&#30340;&#29983;&#23384;&#26465;&#20214;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#23548;&#33268;&#30340;&#29983;&#23384;&#20998;&#26512;&#26174;&#31034;&#22312;&#30896;&#25758;&#30340;&#26089;&#26399;&#26816;&#27979;&#26102;&#38388;&#21644;&#22312;&#36817;&#30896;&#25758;&#21644;&#38750;&#30896;&#25758;&#24773;&#20917;&#19979;&#36739;&#23569;&#30340;&#35823;&#25253;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#36739;&#26089;&#30340;&#26816;&#27979;&#26102;&#38388;&#65292;&#24182;&#24471;&#21040;&#20854;&#22362;&#23454;&#29702;&#35770;&#22522;&#30784;&#25903;&#25345;&#12290;&#23427;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;TTCE&#21644;&#36866;&#29992;&#20110;ADAS&#21644;AD&#39564;&#35777;&#30340;&#39640;&#26031;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we compare three different model-based risk measures by evaluating their stengths and weaknesses qualitatively and testing them quantitatively on a set of real longitudinal and intersection scenarios. We start with the traditional heuristic Time-To-Collision (TTC), which we extend towards 2D operation and non-crash cases to retrieve the Time-To-Closest-Encounter (TTCE). The second risk measure models position uncertainty with a Gaussian distribution and uses spatial occupancy probabilities for collision risks. We then derive a novel risk measure based on the statistics of sparse critical events and so-called survival conditions. The resulting survival analysis shows to have an earlier detection time of crashes and less false positive detections in near-crash and non-crash cases supported by its solid theoretical grounding. It can be seen as a generalization of TTCE and the Gaussian method which is suitable for the validation of ADAS and AD.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Robust Multi-agent Attention Actor-Critic&#31639;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#22522;&#31449;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20197;&#35299;&#20915;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#34920;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.08003</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#27880;&#24847;&#21147;Actor-Critic&#31639;&#27861;&#29992;&#20110;&#34562;&#31389;&#32593;&#32476;&#30340;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Attention Actor-Critic Algorithm for Load Balancing in Cellular Networks. (arXiv:2303.08003v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Robust Multi-agent Attention Actor-Critic&#31639;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#22522;&#31449;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20197;&#35299;&#20915;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#34920;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34562;&#31389;&#32593;&#32476;&#20013;&#65292;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#20174;&#19968;&#20010;&#22522;&#31449;&#65288;BS&#65289;&#20999;&#25442;&#21040;&#21478;&#19968;&#20010;&#22522;&#31449;&#65292;&#23548;&#33268;&#20102;&#22522;&#31449;&#20043;&#38388;&#36127;&#36733;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#31449;&#21487;&#20197;&#21512;&#20316;&#65292;&#20197;&#23454;&#29616;&#24179;&#31283;&#30340;&#36801;&#31227;&#65288;&#25110;&#20999;&#25442;&#65289;&#24182;&#28385;&#36275;&#29992;&#25143;&#35774;&#22791;&#30340;&#26381;&#21153;&#35201;&#27714;&#12290;&#35813;&#35770;&#25991;&#23558;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Robust Multi-agent Attention Actor-Critic&#65288;Robust-MA3C&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#22522;&#31449;&#65288;&#21363;&#20195;&#29702;&#65289;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#24182;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#33258;&#28982;&#26234;&#33021;&#20307;&#26469;&#27169;&#25311;&#31995;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#40723;&#21169;&#39640;&#24615;&#33021;&#22522;&#31449;&#24110;&#21161;&#20302;&#24615;&#33021;&#22522;&#31449;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26041;&#26696;&#65292;&#21487;&#20197;&#20026;&#27963;&#36291;&#30340;UE&#21644;&#31354;&#38386;&#30340;UE&#25552;&#20379;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29366;&#24577;-of-the-art&#31639;&#27861;&#30456;&#27604;&#65292;Robust-MA3C&#31639;&#27861;&#22312;&#36127;&#36733;&#22343;&#34913;&#12289;&#26381;&#21153;&#36136;&#37327;&#21644;&#36816;&#34892;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cellular networks, User Equipment (UE) handoff from one Base Station (BS) to another, giving rise to the load balancing problem among the BSs. To address this problem, BSs can work collaboratively to deliver a smooth migration (or handoff) and satisfy the UEs' service requirements. This paper formulates the load balancing problem as a Markov game and proposes a Robust Multi-agent Attention Actor-Critic (Robust-MA3C) algorithm that can facilitate collaboration among the BSs (i.e., agents). In particular, to solve the Markov game and find a Nash equilibrium policy, we embrace the idea of adopting a nature agent to model the system uncertainty. Moreover, we utilize the self-attention mechanism, which encourages high-performance BSs to assist low-performance BSs. In addition, we consider two types of schemes, which can facilitate load balancing for both active UEs and idle UEs. We carry out extensive evaluations by simulations, and simulation results illustrate that, compared to the sta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35302;&#21453;&#39304;&#36827;&#34892;&#26410;&#30693;&#29289;&#20307;&#23450;&#20301;&#21644;&#37325;&#26500;&#30340;&#38381;&#29615;&#26041;&#27861;FingerSLAM&#65292;&#36890;&#36807;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#21452;&#37325;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#38381;&#29615;&#26426;&#21046;&#32467;&#21512;&#35302;&#35273;&#21644;&#35270;&#35273;&#30340;&#20004;&#31181;&#24863;&#30693;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#23039;&#24577;&#20272;&#35745;&#30340;&#20248;&#21270;&#65292;&#24182;&#27604;&#21333;&#19968;&#27169;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#31934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2303.07997</link><description>&lt;p&gt;
FingerSLAM: &#21033;&#29992;&#35270;&#35302;&#21453;&#39304;&#36827;&#34892;&#26410;&#30693;&#29289;&#20307;&#23450;&#20301;&#21644;&#37325;&#26500;&#30340;&#38381;&#29615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FingerSLAM: Closed-loop Unknown Object Localization and Reconstruction from Visuo-tactile Feedback. (arXiv:2303.07997v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35302;&#21453;&#39304;&#36827;&#34892;&#26410;&#30693;&#29289;&#20307;&#23450;&#20301;&#21644;&#37325;&#26500;&#30340;&#38381;&#29615;&#26041;&#27861;FingerSLAM&#65292;&#36890;&#36807;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#21452;&#37325;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#38381;&#29615;&#26426;&#21046;&#32467;&#21512;&#35302;&#35273;&#21644;&#35270;&#35273;&#30340;&#20004;&#31181;&#24863;&#30693;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#23039;&#24577;&#20272;&#35745;&#30340;&#20248;&#21270;&#65292;&#24182;&#27604;&#21333;&#19968;&#27169;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21033;&#29992;&#35270;&#35302;&#21453;&#39304;&#36827;&#34892;&#26410;&#30693;&#29289;&#20307;&#30340;6&#33258;&#30001;&#24230;&#23450;&#20301;&#21644;&#19977;&#32500;&#37325;&#26500;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FingerSLAM&#30340;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#38381;&#29615;&#23039;&#24577;&#20272;&#35745;&#22120;&#12290;FingerSLAM&#32467;&#21512;&#20102;&#25163;&#25351;&#39030;&#31471;&#30340;&#23616;&#37096;&#35302;&#35273;&#20256;&#24863;&#21644;&#25163;&#33109;&#22788;&#20840;&#23616;&#35270;&#35273;&#20256;&#24863;&#65292;&#30001;&#20004;&#20010;&#23039;&#24577;&#20272;&#35745;&#22120;&#26500;&#25104;&#65306;&#22810;&#36890;&#36947;&#30340;&#31934;&#32454;&#35302;&#35273;&#23039;&#24577;&#20272;&#35745;&#22120;&#21644;&#21333;&#36890;&#36947;&#30340;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38381;&#29615;&#26426;&#21046;&#65292;&#20027;&#21160;&#21305;&#37197;&#24403;&#21069;&#35270;&#35273;&#19982;&#35302;&#35273;&#22270;&#20687;&#19982;&#20808;&#21069;&#23384;&#20648;&#30340;&#20851;&#38190;&#24103;&#65292;&#20197;&#20943;&#23569;&#32047;&#31215;&#35823;&#24046;&#12290; FingerSLAM&#23558;&#35302;&#35273;&#21644;&#35270;&#35273;&#20004;&#31181;&#24863;&#30693;&#27169;&#24335;&#20197;&#21450;&#38381;&#29615;&#26426;&#21046;&#32435;&#20837;&#22522;&#20110;&#22240;&#23376;&#22270;&#20248;&#21270;&#26694;&#26550;&#30340;&#26041;&#26696;&#20013;&#12290;&#36825;&#31181;&#26041;&#26696;&#33021;&#22815;&#20135;&#29983;&#27604;&#21333;&#19968;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#26356;&#31934;&#30830;&#30340;&#20248;&#21270;&#23039;&#24577;&#20272;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of using visuo-tactile feedback for 6-DoF localization and 3D reconstruction of unknown in-hand objects. We propose FingerSLAM, a closed-loop factor graph-based pose estimator that combines local tactile sensing at finger-tip and global vision sensing from a wrist-mount camera. FingerSLAM is constructed with two constituent pose estimators: a multi-pass refined tactile-based pose estimator that captures movements from detailed local textures, and a single-pass vision-based pose estimator that predicts from a global view of the object. We also design a loop closure mechanism that actively matches current vision and tactile images to previously stored key-frames to reduce accumulated error. FingerSLAM incorporates the two sensing modalities of tactile and vision, as well as the loop closure mechanism with a factor graph-based optimization framework. Such a framework produces an optimized pose estimation solution that is more accurate than the standal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35821;&#20041;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#28151;&#21512;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;&#65292;&#29992;&#20110;Instagram&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07957</link><description>&lt;p&gt;
Instagram&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#33258;&#21160;&#25688;&#35201;&#65306;&#32467;&#21512;&#35821;&#20041;&#21644;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic summarisation of Instagram social network posts Combining semantic and statistical approaches. (arXiv:2303.07957v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35821;&#20041;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#28151;&#21512;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;&#65292;&#29992;&#20110;Instagram&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#25968;&#25454;&#21644;&#25991;&#26412;&#25991;&#20214;&#30340;&#28608;&#22686;&#65292;&#20363;&#22914;&#25991;&#31456;&#12289;&#32593;&#39029;&#12289;&#20070;&#31821;&#12289;&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#31561;&#65292;&#20026;&#25991;&#26412;&#22788;&#29702;&#30340;&#21508;&#20010;&#39046;&#22495;&#24102;&#26469;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#25163;&#21160;&#22788;&#29702;&#21644;&#25688;&#35201;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#23545;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#26159;&#38750;&#24120;&#22256;&#38590;&#12289;&#26114;&#36149;&#12289;&#32791;&#26102;&#21644;&#19981;&#21487;&#33021;&#30340;&#36807;&#31243;&#12290;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#20998;&#20026;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#20004;&#31867;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#20041;&#21644;&#32479;&#35745;&#26041;&#27861;&#26469;&#33258;&#21160;&#25688;&#35201;Instagram&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#32479;&#35745;&#37325;&#35201;&#24615;&#26469;&#25552;&#21462;&#37325;&#35201;&#30340;&#21477;&#23376;&#12290;&#23545;Instagram&#24086;&#23376;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of data and text documents such as articles, web pages, books, social network posts, etc. on the Internet has created a fundamental challenge in various fields of text processing under the title of "automatic text summarisation". Manual processing and summarisation of large volumes of textual data is a very difficult, expensive, time-consuming and impossible process for human users. Text summarisation systems are divided into extractive and abstract categories. In the extractive summarisation method, the final summary of a text document is extracted from the important sentences of the same document without any modification. In this method, it is possible to repeat a series of sentences and to interfere with pronouns. However, in the abstract summarisation method, the final summary of a textual document is extracted from the meaning and significance of the sentences and words of the same document or other documents. Many of the works carried out have used extraction me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07940</link><description>&lt;p&gt;
&#20851;&#20110;&#20135;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Connection between Concept Drift and Uncertainty in Industrial Artificial Intelligence. (arXiv:2303.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#23383;&#23402;&#29983;&#26159;&#24037;&#19994;4.0&#38761;&#21629;&#30340;&#21069;&#27839;&#65292;&#20854;&#25216;&#26415;&#30001;&#29289;&#32852;&#32593;&#21644;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#25903;&#25345;&#12290;&#20174;&#24037;&#19994;&#36164;&#20135;&#20013;&#25910;&#38598;&#30340;&#20449;&#24687;&#20197;&#25345;&#32493;&#30340;&#26041;&#24335;&#20135;&#29983;&#65292;&#20135;&#29983;&#30340;&#25968;&#25454;&#27969;&#24517;&#39035;&#22312;&#20005;&#26684;&#30340;&#26102;&#38388;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#27969;&#36890;&#24120;&#20250;&#21463;&#21040;&#38750;&#24179;&#31283;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#25968;&#25454;&#27969;&#30340;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#27169;&#22411;&#25429;&#33719;&#30340;&#30693;&#35782;&#21487;&#33021;&#20250;&#21464;&#24471;&#36807;&#26102;&#65288;&#23548;&#33268;&#25152;&#35859;&#30340;&#27010;&#24565;&#28418;&#31227;&#25928;&#24212;&#65289;&#12290;&#21450;&#26089;&#21457;&#29616;&#65288;&#28418;&#31227;&#65289;&#30340;&#21464;&#21270;&#23545;&#20110;&#26356;&#26032;&#27169;&#22411;&#30340;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#22330;&#26223;&#20013;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#19982;&#27969;&#25968;&#25454;&#30456;&#20851;&#32852;&#30340;&#22522;&#26412;&#30495;&#30456;&#19981;&#23481;&#26131;&#25214;&#21040;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#20135;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#26469;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-based digital twins are at the leading edge of the Industry 4.0 revolution, which are technologically empowered by the Internet of Things and real-time data analysis. Information collected from industrial assets is produced in a continuous fashion, yielding data streams that must be processed under stringent timing constraints. Such data streams are usually subject to non-stationary phenomena, causing that the data distribution of the streams may change, and thus the knowledge captured by models used for data analysis may become obsolete (leading to the so-called concept drift effect). The early detection of the change (drift) is crucial for updating the model's knowledge, which is challenging especially in scenarios where the ground truth associated to the stream data is not readily available. Among many other techniques, the estimation of the model's confidence has been timidly suggested in a few studies as a criterion for detecting drifts in unsupervised settings. The goal of thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07909</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#24037;&#20316;&#65292;&#26412;&#35843;&#26597;&#20174;&#31616;&#21333;&#20171;&#32461;&#22522;&#26412;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#24320;&#22987;&#65292;&#25509;&#30528;&#26159;&#26465;&#20214;&#25110;&#24341;&#23548;&#22914;&#20309;&#25913;&#36827;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#25991;&#26412;&#24341;&#23548;&#21019;&#24847;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#36804;&#20170;&#20026;&#27490;&#25152;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#21487;&#35299;&#20915;&#23545;&#31216;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07897</link><description>&lt;p&gt;
&#23545;&#31216;&#29615;&#22659;&#19979;&#30340;&#29289;&#20307;&#23450;&#20301;&#22810;&#31890;&#23376;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiparticle Kalman filter for object localization in symmetric environments. (arXiv:2303.07897v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#21487;&#35299;&#20915;&#23545;&#31216;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#29289;&#20307;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;&#22797;&#26434;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#31867;&#21644;&#31890;&#23376;&#28388;&#27874;&#22120;&#31867;&#20004;&#31181;&#35299;&#20915;&#23450;&#20301;&#38382;&#39064;&#30340;&#28388;&#27874;&#22120;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28388;&#27874;&#31639;&#27861;&#65292;&#34701;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#22312;&#23545;&#31216;&#19988;&#26377;&#22122;&#22768;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#22810;&#31890;&#23376;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36825;&#20123;&#29615;&#22659;&#23545;&#20110;&#20004;&#31181;&#32463;&#20856;&#26041;&#27861;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#31890;&#23376;&#28388;&#27874;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#30001;&#20110;&#21482;&#26377;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#26410;&#30693;&#21021;&#22987;&#29366;&#24577;&#65292;&#22240;&#27492;&#25105;&#20204;&#21482;&#32771;&#34385;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;&#20102;&#22914;&#27492;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#20301;&#35823;&#24046;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#22343;&#20248;&#20110;&#31890;&#23376;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study considers the object localization problem and proposes a novel multiparticle Kalman filter to solve it in complex and symmetric environments. Two well-known classes of filtering algorithms to solve the localization problem are Kalman filter-based methods and particle filter-based methods. We consider these classes, demonstrate their complementary properties, and propose a novel filtering algorithm that takes the best from two classes. We evaluate the multiparticle Kalman filter in symmetric and noisy environments. Such environments are especially challenging for both classes of classical methods. We compare the proposed approach with the particle filter since only this method is feasible if the initial state is unknown. In the considered challenging environments, our method outperforms the particle filter in terms of both localization error and runtime.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#22495;TSG&#35774;&#32622;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#30721;&#21387;&#32553;&#20301;&#27969;&#26469;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21644;&#25552;&#39640;&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07863</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#21387;&#32553;&#35270;&#39057;&#30340;&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos. (arXiv:2303.07863v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#22495;TSG&#35774;&#32622;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#30721;&#21387;&#32553;&#20301;&#27969;&#26469;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21644;&#25552;&#39640;&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#26088;&#22312;&#26681;&#25454;&#21477;&#23376;&#26597;&#35810;&#36890;&#36807;&#35821;&#20041;&#23450;&#20301;&#30446;&#26631;&#30636;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#22495;TSG&#65288;Temporal Sentence Grounding&#65289;&#35774;&#32622;&#65292;&#30452;&#25509;&#20351;&#29992;&#21387;&#32553;&#35270;&#39057;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#12290;&#38024;&#23545;&#21407;&#22987;&#35270;&#39057;&#27604;&#29305;&#27969;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#19977;&#25903;&#36335;&#21387;&#32553;&#31354;&#38388;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65288;TCSF&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23450;&#20301;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21387;&#32553;&#20266;&#24433;&#26469;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#32534;&#30721;&#21387;&#32553;&#20301;&#27969;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20808;&#35299;&#30721;&#25972;&#20010;&#24103;&#30340;&#26041;&#27861;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a target moment semantically according to a sentence query. Although previous respectable works have made decent success, they only focus on high-level visual features extracted from the consecutive decoded frames and fail to handle the compressed videos for query modelling, suffering from insufficient representation capability and significant computational complexity during training and testing. In this paper, we pose a new setting, compressed-domain TSG, which directly utilizes compressed videos rather than fully-decompressed frames as the visual input. To handle the raw video bit-stream input, we propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) framework, which extracts and aggregates three kinds of low-level visual features (I-frame, motion vector and residual features) for effective and efficient grounding. Particularly, instead of encoding the whole decoded frames like previous
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.07846</link><description>&lt;p&gt;
&#39640;&#25928;&#29575;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21363;&#36890;&#36807;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#65292;&#24050;&#32463;&#34987;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#24182;&#19981;&#26159;&#39044;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#12290;&#20026;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#29983;&#25104;&#22823;&#37327;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#19981;&#21463;&#21508;&#31181;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#34920;&#26684;&#25968;&#25454;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#30340;&#19981;&#21516;&#25439;&#22351;&#26041;&#27861;&#65292;&#20197;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#21508;&#31181;&#25197;&#26354;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#20351;&#19968;&#20010;&#20449;&#24687;&#37327;&#22823;&#30340;&#29305;&#24449;&#27969;&#24418;&#19982;&#19968;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#22120;&#19982;&#19968;&#20010;&#22797;&#26434;&#30340;&#20998;&#31867;&#22120;&#21327;&#21516;&#24037;&#20316;&#33021;&#22815;&#25552;&#39640;&#29366;&#24577;&#34920;&#24449;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChatGPT&#25552;&#31034;&#27169;&#24335;&#65292;&#29992;&#20110;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12289;&#37325;&#26500;&#12289;&#38656;&#27714;&#35843;&#26597;&#21644;&#36719;&#20214;&#35774;&#35745;&#12290;&#23427;&#25552;&#20379;&#20102;&#36719;&#20214;&#24037;&#31243;&#27169;&#24335;&#30446;&#24405;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#31181;&#25552;&#31034;&#27169;&#24335;&#65292;&#21253;&#25324;&#25913;&#21892;&#38656;&#27714;&#35843;&#26597;&#12289;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#20195;&#30721;&#36136;&#37327;&#12289;&#37325;&#26500;&#21644;&#31995;&#32479;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.07839</link><description>&lt;p&gt;
ChatGPT&#25552;&#31034;&#27169;&#24335;&#29992;&#20110;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12289;&#37325;&#26500;&#12289;&#38656;&#27714;&#35843;&#26597;&#21644;&#36719;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design. (arXiv:2303.07839v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChatGPT&#25552;&#31034;&#27169;&#24335;&#65292;&#29992;&#20110;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12289;&#37325;&#26500;&#12289;&#38656;&#27714;&#35843;&#26597;&#21644;&#36719;&#20214;&#35774;&#35745;&#12290;&#23427;&#25552;&#20379;&#20102;&#36719;&#20214;&#24037;&#31243;&#27169;&#24335;&#30446;&#24405;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#31181;&#25552;&#31034;&#27169;&#24335;&#65292;&#21253;&#25324;&#25913;&#21892;&#38656;&#27714;&#35843;&#26597;&#12289;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#20195;&#30721;&#36136;&#37327;&#12289;&#37325;&#26500;&#21644;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36719;&#20214;&#24037;&#31243;&#30340;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#24335;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;ChatGPT&#33258;&#21160;&#21270;&#24120;&#29992;&#30340;&#36719;&#20214;&#24037;&#31243;&#27963;&#21160;&#20013;&#36935;&#21040;&#30340;&#36890;&#29992;&#38382;&#39064;&#65292;&#20363;&#22914;&#30830;&#20445;&#20195;&#30721;&#19982;&#31532;&#19977;&#26041;&#24211;&#35299;&#32806;&#65292;&#24182;&#22312;&#23454;&#29616;&#20043;&#21069;&#27169;&#25311;Web&#24212;&#29992;&#31243;&#24207;API&#12290;&#26412;&#25991;&#20026;&#20351;&#29992;LLMs&#36827;&#34892;&#36719;&#20214;&#24037;&#31243;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#36719;&#20214;&#24037;&#31243;&#27169;&#24335;&#30446;&#24405;&#65292;&#26681;&#25454;&#23427;&#20204;&#35299;&#20915;&#30340;&#38382;&#39064;&#31867;&#22411;&#20998;&#31867;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#23427;&#25506;&#35752;&#20102;&#20960;&#31181;&#25552;&#31034;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#24050;&#24212;&#29992;&#20110;&#25913;&#21892;&#38656;&#27714;&#35843;&#26597;&#12289;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#20195;&#30721;&#36136;&#37327;&#12289;&#37325;&#26500;&#21644;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and simulating a web application API before it is implemented. This paper provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, refactoring, and system design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;SNN&#27169;&#22411;&#65292;motorSRNN&#65292;&#36890;&#36807;&#25429;&#33719;&#21644;&#22521;&#20859;&#36816;&#21160;&#30382;&#23618;&#31070;&#32463;&#20803;&#30340;&#20313;&#24358;&#35843;&#35856;&#31561;&#29983;&#29289;&#23398;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#29492;&#23376;&#36816;&#21160;&#30382;&#23618;&#33033;&#20914;&#21015;&#30340;&#33391;&#22909;&#20998;&#31867;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#20135;&#29983;&#20102;&#39069;&#22806;&#30340;&#29983;&#29289;&#21151;&#33021;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07830</link><description>&lt;p&gt;
&#30382;&#23618;&#23574;&#23792;&#21015;&#35299;&#30721;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29983;&#29289;&#21151;&#33021;&#30456;&#20284;&#24615;&#23545;&#31070;&#32463;&#35745;&#31639;&#39044;&#27979;&#26377;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Emergent Bio-Functional Similarities in a Cortical-Spike-Train-Decoding Spiking Neural Network Facilitate Predictions of Neural Computation. (arXiv:2303.07830v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;SNN&#27169;&#22411;&#65292;motorSRNN&#65292;&#36890;&#36807;&#25429;&#33719;&#21644;&#22521;&#20859;&#36816;&#21160;&#30382;&#23618;&#31070;&#32463;&#20803;&#30340;&#20313;&#24358;&#35843;&#35856;&#31561;&#29983;&#29289;&#23398;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#29492;&#23376;&#36816;&#21160;&#30382;&#23618;&#33033;&#20914;&#21015;&#30340;&#33391;&#22909;&#20998;&#31867;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#20135;&#29983;&#20102;&#39069;&#22806;&#30340;&#29983;&#29289;&#21151;&#33021;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#26356;&#22909;&#30340;&#29983;&#29289;&#23398;&#21487;&#22609;&#24615;&#65292;&#20294;&#38754;&#21521;&#30446;&#26631;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20998;&#31867;&#29983;&#29289;&#33033;&#20914;&#21015;&#26041;&#38754;&#23578;&#26410;&#23454;&#29616;&#21487;&#24212;&#29992;&#24615;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#24456;&#23569;&#30340;&#29983;&#29289;&#21151;&#33021;&#30456;&#20284;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;motorSRNN&#65292;&#19968;&#31181;&#30001;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#31070;&#32463;&#36816;&#21160;&#30005;&#36335;&#21551;&#21457;&#30340;&#24490;&#29615;SNN&#12290;&#36890;&#36807;&#22312;&#35299;&#30721;&#29492;&#23376;&#30340;&#20027;&#35201;&#36816;&#21160;&#30382;&#23618;&#33033;&#20914;&#21015;&#20013;&#20351;&#29992;motorSRNN&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;motorSRNN&#36890;&#36807;&#25429;&#33719;&#21644;&#22521;&#20859;&#26356;&#22810;&#20313;&#24358;&#35843;&#35856;&#26469;&#19982;&#36755;&#20837;&#36827;&#34892;&#36890;&#20449;&#65292;&#36825;&#26159;&#36816;&#21160;&#30382;&#23618;&#31070;&#32463;&#20803;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#35757;&#32451;&#35825;&#23548;&#30340;&#20313;&#24358;&#35843;&#35856;&#30340;&#22521;&#20859;&#21644;&#25345;&#20037;&#24615;&#20063;&#22312;&#25105;&#20204;&#30340;&#29492;&#23376;&#20013;&#35266;&#23519;&#21040;&#12290;&#27492;&#22806;&#65292;motorSRNN&#22312;&#21333;&#20010;&#31070;&#32463;&#20803;&#12289;&#32676;&#20307;&#21644;&#30005;&#36335;&#27700;&#24179;&#19978;&#20135;&#29983;&#20102;&#39069;&#22806;&#30340;&#29983;&#29289;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#35777;&#26126;&#20102;&#29983;&#29289;&#23398;&#30340;&#32039;&#23494;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its better bio-plausibility, goal-driven spiking neural network (SNN) has not achieved applicable performance for classifying biological spike trains, and showed little bio-functional similarities compared to traditional artificial neural networks. In this study, we proposed the motorSRNN, a recurrent SNN topologically inspired by the neural motor circuit of primates. By employing the motorSRNN in decoding spike trains from the primary motor cortex of monkeys, we achieved a good balance between classification accuracy and energy consumption. The motorSRNN communicated with the input by capturing and cultivating more cosine-tuning, an essential property of neurons in the motor cortex, and maintained its stability during training. Such training-induced cultivation and persistency of cosine-tuning was also observed in our monkeys. Moreover, the motorSRNN produced additional bio-functional similarities at the single-neuron, population, and circuit levels, demonstrating biological a
&lt;/p&gt;</description></item><item><title>ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07811</link><description>&lt;p&gt;
ICICLE: &#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07811
&lt;/p&gt;
&lt;p&gt;
ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#33021;&#22815;&#22686;&#37327;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#20419;&#36827;&#26032;&#26087;&#20219;&#21153;&#20043;&#38388;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#23398;&#20064;&#23545;&#35299;&#37322;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#29702;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#32780;&#25913;&#21464;&#65292;&#23548;&#33268;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#26679;&#26412;&#30340; Interpretable Class-InCremental LEarning (ICICLE) &#26041;&#27861;&#65292;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#28857;&#65306;&#35299;&#37322;&#24615;&#27491;&#21017;&#21270;&#12289;&#20197;&#24494;&#31890;&#31890;&#24230;&#20026;&#22522;&#30784;&#30340;&#21407;&#22411;&#21021;&#22987;&#21270;&#31574;&#30053;&#20197;&#21450;&#38024;&#23545;&#21407;&#22411;&#37096;&#20998;&#30340;&#20219;&#21153;&#26102;&#25928;&#20559;&#24046;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ICICLE&#20943;&#23569;&#20102;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free me
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#25163;&#30340;&#35774;&#35745;&#21644;&#21151;&#33021;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23558;&#24341;&#39046;&#36827;&#20837;&#19968;&#20010;&#26032;&#30340;&#25913;&#36827;&#26102;&#20195;&#12290;</title><link>http://arxiv.org/abs/2303.07807</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#20316;&#65306;&#21069;&#26223;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot Grasping and Manipulation: A Prospective. (arXiv:2303.07807v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07807
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25163;&#30340;&#35774;&#35745;&#21644;&#21151;&#33021;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23558;&#24341;&#39046;&#36827;&#20837;&#19968;&#20010;&#26032;&#30340;&#25913;&#36827;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#31616;&#31616;&#21333;&#21333;&#30340;&#25569;&#25163;&#24517;&#23558;&#26292;&#38706;&#20182;&#20204;&#30340;&#36523;&#20221;&#12290;&#8221;&#36825;&#23601;&#26159;&#23433;&#19996;&#23612;&#183;&#38669;&#26222;&#37329;&#26031;&#22312;&#34394;&#26500;&#35282;&#33394;&#32599;&#20271;&#29305;&#183;&#31119;&#29305;&#21338;&#22763;&#20013;&#24635;&#32467;2016&#24180;&#31185;&#24187;&#30005;&#35270;&#21095;&#12298;&#35199;&#37096;&#19990;&#30028;&#12299;&#20013;&#20027;&#26426;&#30340;&#19968;&#20010;&#32570;&#38519;&#12290;&#22312;&#36825;&#20010;&#25925;&#20107;&#20013;&#65292;&#35199;&#37096;&#19990;&#30028;&#26159;&#19968;&#20010;&#26410;&#26469;&#20027;&#20041;&#20027;&#39064;&#20844;&#22253;&#65292;&#20027;&#26426;&#26159;&#34987;&#35774;&#35745;&#25104;&#19982;&#20154;&#31867;&#23458;&#20154;&#38590;&#20197;&#21306;&#20998;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#65292;&#20294;&#23427;&#20204;&#30340;&#25163;&#21364;&#36824;&#26410;&#34987;&#23436;&#21892;&#12290;&#22312;&#21478;&#19968;&#20010;&#32463;&#20856;&#30340;&#31185;&#24187;&#31995;&#21015;&#20013;&#65292;&#31185;&#23398;&#23478;&#36890;&#36807;&#23545;&#26410;&#26469;&#25163;&#30340;&#36870;&#21521;&#24037;&#31243;&#65292;&#24320;&#21551;&#20102;&#20840;&#21512;&#25104;&#26234;&#33021;Skynet&#30340;&#31192;&#23494;&#12290;&#22312;&#36825;&#20004;&#20010;&#25925;&#20107;&#24773;&#33410;&#20013;&#65292;&#29616;&#23454;&#21551;&#21457;&#20102;&#23567;&#35828;&#20889;&#20316;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#20154;&#35774;&#35745;&#25163;&#21644;&#22797;&#21046;&#24378;&#22823;&#21487;&#38752;&#30340;&#25805;&#20316;&#21160;&#20316;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23558;&#24341;&#39046;&#25105;&#20204;&#36827;&#20837;&#19968;&#20010;&#26032;&#30340;&#25913;&#36827;&#26102;&#20195;&#12290;&#19968;&#20010;&#19990;&#32426;&#21069;&#65292;&#31532;&#19977;&#27425;&#24037;&#19994;&#38761;&#21629;&#23558;&#26426;&#22120;&#20154;&#24102;&#21040;&#20102;&#35013;&#37197;&#32447;&#19978;&#65292;&#27704;&#36828;&#25913;&#21464;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#26041;&#24335;&#12290;&#19979;&#19968;&#20010;&#38761;&#21629;&#24050;&#32463;&#24320;&#22987;&#65292;&#24102;&#32473;&#25105;&#20204;&#30340;&#26159;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
``A simple handshake would give them away''. This is how Anthony Hopkins' fictional character, Dr Robert Ford, summarises a particular flaw of the 2016 science-fiction \emph{Westworld}'s hosts. In the storyline, Westworld is a futuristic theme park and the hosts are autonomous robots engineered to be indistinguishable from the human guests, except for their hands that have not been perfected yet. In another classic science-fiction saga, scientists unlock the secrets of full synthetic intelligence, Skynet, by reverse engineering a futuristic hand.  In both storylines, reality inspires fiction on one crucial point: designing hands and reproducing robust and reliable manipulation actions is one of the biggest challenges in robotics.  Solving this problem would lead us to a new, improved era of autonomy. A century ago, the third industrial revolution brought robots into the assembly lines, changing our way of working forever. The next revolution has already started by bringing us artificia
&lt;/p&gt;</description></item><item><title>OVRL-V2&#26159;&#19968;&#31687;&#20851;&#20110;ImageNav&#21644;ObjectNav&#30340;&#26368;&#26032;&#22522;&#20934;&#27169;&#22411;&#65292;&#30001;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#20214;&#32452;&#25104;&#65292;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#26469;&#28304;&#20110;&#23545;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#32780;&#19988;&#36890;&#29992;&#24615;&#36739;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.07798</link><description>&lt;p&gt;
OVRL-V2&#65306;ImageNav&#21644;ObjectNav&#30340;&#31616;&#21333;&#26368;&#26032;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav. (arXiv:2303.07798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07798
&lt;/p&gt;
&lt;p&gt;
OVRL-V2&#26159;&#19968;&#31687;&#20851;&#20110;ImageNav&#21644;ObjectNav&#30340;&#26368;&#26032;&#22522;&#20934;&#27169;&#22411;&#65292;&#30001;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#20214;&#32452;&#25104;&#65292;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#26469;&#28304;&#20110;&#23545;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#32780;&#19988;&#36890;&#29992;&#24615;&#36739;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#30001;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#20214;&#65288;ViTs&#12289;&#21367;&#31215;&#21644;LSTM&#65289;&#32452;&#25104;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#26144;&#23556;&#25110;&#35745;&#21010;&#27169;&#22359;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22312;ImageNav&#65288;&#8220;&#22312;&lt;&#27492;&#22270;&#29255;&gt;&#20013;&#36827;&#20837;&#20301;&#32622;&#8221;&#65289;&#21644;ObjectNav&#65288;&#8220;&#26597;&#25214;&#26885;&#23376;&#8221;&#65289;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#36825;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#20855;&#26377;&#31616;&#21333;&#24615;&#30340;&#20248;&#28857;&#65292;&#38543;&#30528;&#21487;&#29992;&#35745;&#31639;&#30340;&#27491;&#21521;&#32553;&#25918;&#65292;&#23545;&#22810;&#20010;&#20219;&#21153;&#20855;&#26377;&#22810;&#21151;&#33021;&#24212;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViT&#65289;&#30340;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#34429;&#28982;&#21367;&#31215;&#32593;&#32476;&#30340;&#35757;&#32451;&#26041;&#26696;&#26159;&#25104;&#29087;&#19988;&#31283;&#20581;&#30340;&#65292;&#20294;&#35270;&#35273;&#23548;&#33322;&#30340;ViTs&#30340;&#35757;&#32451;&#26041;&#26696;&#26159;&#20381;&#36182;&#24615;&#30340;&#21644;&#33030;&#24369;&#30340;&#65292;&#23578;&#26410;&#23436;&#20840;&#21457;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36890;&#30340;ViT&#22312;&#35270;&#35273;&#23548;&#33322;&#19978;&#34920;&#29616;&#19981;&#22914;ResNets&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;ViT&#34917;&#19969;&#34920;&#31034;&#19978;&#36816;&#34892;&#30340;&#21387;&#32553;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a single neural network architecture composed of task-agnostic components (ViTs, convolutions, and LSTMs) that achieves state-of-art results on both the ImageNav ("go to location in &lt;this picture&gt;") and ObjectNav ("find a chair") tasks without any task-specific modules like object detection, segmentation, mapping, or planning modules. Such general-purpose methods offer advantages of simplicity in design, positive scaling with available compute, and versatile applicability to multiple tasks. Our work builds upon the recent success of self-supervised learning (SSL) for pre-training vision transformers (ViT). However, while the training recipes for convolutional networks are mature and robust, the recipes for ViTs are contingent and brittle, and in the case of ViTs for visual navigation, yet to be fully discovered. Specifically, we find that vanilla ViTs do not outperform ResNets on visual navigation. We propose the use of a compression layer operating over ViT patch representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#21457;&#29616;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#22522;&#26412;&#25968;&#20540;&#21644;&#31639;&#26415;&#30693;&#35782;&#30340;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#26102;&#20063;&#32463;&#24120;&#26080;&#27861;&#32988;&#20219;&#12290;</title><link>http://arxiv.org/abs/2303.07735</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#33021;&#20570;&#31639;&#26415;&#21527;&#65311;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#26412;&#25968;&#23383;&#25216;&#33021;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models. (arXiv:2303.07735v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#21457;&#29616;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#22522;&#26412;&#25968;&#20540;&#21644;&#31639;&#26415;&#30693;&#35782;&#30340;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#26102;&#20063;&#32463;&#24120;&#26080;&#27861;&#32988;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#33021;&#23637;&#31034;&#22797;&#26434;&#25512;&#29702;&#25216;&#33021;&#30340;&#23398;&#20064;&#27169;&#22411;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#32780;&#25968;&#23398;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#35780;&#20272;&#31185;&#23398;&#36827;&#27493;&#26041;&#21521;&#30340;&#30446;&#26631;&#39046;&#22495;&#20043;&#19968;&#12290;&#36807;&#21435;&#20960;&#24180;&#37324;&#24050;&#32463;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#12289;&#25968;&#20540;&#31215;&#20998;&#21644;&#26032;&#29468;&#24819;&#25110;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#30340;&#21457;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#32489;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25968;&#37327;&#21644;&#31526;&#21495;&#25968;&#23383;&#30340;&#22522;&#26412;&#29702;&#35299;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#23457;&#26680;&#65292;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22312;&#38754;&#23545;&#30456;&#23545;&#31616;&#21333;&#30340;&#27979;&#35797;&#22522;&#26412;&#25968;&#20540;&#21644;&#31639;&#26415;&#30693;&#35782;&#30340;&#20219;&#21153;&#26102;&#65292;&#20063;&#32463;&#24120;&#26080;&#27861;&#32988;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating learning models that can exhibit sophisticated reasoning skills is one of the greatest challenges in deep learning research, and mathematics is rapidly becoming one of the target domains for assessing scientific progress in this direction. In the past few years there has been an explosion of neural network architectures, data sets, and benchmarks specifically designed to tackle mathematical problems, reporting notable success in disparate fields such as automated theorem proving, numerical integration, and discovery of new conjectures or matrix multiplication algorithms. However, despite these impressive achievements it is still unclear whether deep learning models possess an elementary understanding of quantities and symbolic numbers. In this survey we critically examine the recent literature, concluding that even state-of-the-art architectures often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24378;&#24230;&#21463;&#25511;&#30340;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#20197;&#21450;&#20998;&#23618;&#38901;&#24459;&#39044;&#27979;&#22120;&#26469;&#25913;&#21892;&#38899;&#39057;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#20013;&#30340;&#38901;&#24459;&#38382;&#39064;&#65292;&#35299;&#38500;&#20102;&#19968;&#23545;&#22810;&#26144;&#23556;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.07711</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#21644;&#20998;&#23618;&#24314;&#27169;&#25913;&#36827;&#38899;&#39057;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#30340;&#38901;&#24459;
&lt;/p&gt;
&lt;p&gt;
Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis. (arXiv:2303.07711v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24378;&#24230;&#21463;&#25511;&#30340;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#20197;&#21450;&#20998;&#23618;&#38901;&#24459;&#39044;&#27979;&#22120;&#26469;&#25913;&#21892;&#38899;&#39057;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#20013;&#30340;&#38901;&#24459;&#38382;&#39064;&#65292;&#35299;&#38500;&#20102;&#19968;&#23545;&#22810;&#26144;&#23556;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#26088;&#22312;&#23558;&#28304;&#21457;&#35328;&#20154;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#30446;&#26631;&#21457;&#35328;&#20154;&#38899;&#33394;&#30340;&#21512;&#25104;&#35821;&#38899;&#20013;&#12290;&#22312;&#22823;&#22810;&#25968;&#26041;&#27861;&#20013;&#65292;&#21512;&#25104;&#30340;&#32454;&#31890;&#24230;&#38901;&#24459;&#29305;&#24449;&#36890;&#24120;&#34920;&#31034;&#28304;&#21457;&#35328;&#20154;&#30340;&#24179;&#22343;&#39118;&#26684;&#65292;&#31867;&#20284;&#20110;&#19968;&#23545;&#22810;&#38382;&#39064;&#65288;&#21363;&#65292;&#22810;&#20010;&#38901;&#24459;&#21464;&#21270;&#23545;&#24212;&#20110;&#21516;&#19968;&#25991;&#26412;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#24230;&#21463;&#25511;&#30340;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#65292;&#20197;&#35299;&#24320;&#39118;&#26684;&#19982;&#20869;&#23481;&#21644;&#38899;&#33394;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25913;&#21892;&#20840;&#23616;&#39118;&#26684;&#23884;&#20837;&#30340;&#34920;&#31034;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#21487;&#20197;&#32531;&#35299;&#38901;&#24459;&#39044;&#27979;&#20013;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#38901;&#24459;&#39044;&#27979;&#22120;&#26469;&#25913;&#21892;&#38901;&#24459;&#24314;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26131;&#20110;&#39044;&#27979;&#30340;&#28304;&#21457;&#35328;&#20154;&#38901;&#24459;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#26684;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35762;&#35805;&#20154;&#38388;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#26410;&#35266;&#23519;&#21040;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-speaker style transfer in speech synthesis aims at transferring a style from source speaker to synthesized speech of a target speaker's timbre. In most previous methods, the synthesized fine-grained prosody features often represent the source speaker's average style, similar to the one-to-many problem(i.e., multiple prosody variations correspond to the same text). In response to this problem, a strength-controlled semi-supervised style extractor is proposed to disentangle the style from content and timbre, improving the representation and interpretability of the global style embedding, which can alleviate the one-to-many mapping and data imbalance problems in prosody prediction. A hierarchical prosody predictor is proposed to improve prosody modeling. We find that better style transfer can be achieved by using the source speaker's prosody features that are easily predicted. Additionally, a speaker-transfer-wise cycle consistency loss is proposed to assist the model in learning un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07693</link><description>&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Policy Learning for Offline-to-Online Reinforcement Learning. (arXiv:2303.07693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#19968;&#20010;&#29615;&#22659;&#26469;&#25910;&#38598;&#26032;&#40092;&#30340;&#25968;&#25454;&#65292;&#20294;&#24403;&#22312;&#32447;&#20132;&#20114;&#25104;&#26412;&#39640;&#26114;&#26102;&#19981;&#20999;&#23454;&#38469;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#30452;&#25509;&#20174;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24046;&#65292;&#23558;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#31163;&#32447;&#21040;&#22312;&#32447;&#30340;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#20195;&#29702;&#39318;&#20808;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#20877;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26174;&#24335;&#32771;&#34385;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#20102;&#33258;&#36866;&#24212;&#26356;&#26032;&#31574;&#30053;&#65292;&#21363;&#23545;&#31163;&#32447;&#25968;&#25454;&#38598;&#37319;&#29992;&#24754;&#35266;&#26356;&#26032;&#31574;&#30053;&#65292;&#32780;&#23545;&#22312;&#32447;&#25968;&#25454;&#38598;&#37319;&#29992;&#20048;&#35266;/&#36138;&#24515;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24182;&#23454;&#29616;&#20004;&#32773;&#26368;&#20339;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#20122;&#32447;&#24615;&#21518;&#24724;&#30028;&#65292;&#19982;&#26377;&#21516;&#26102;&#35775;&#38382;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#30340;oracle&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#20381;&#23384;&#26631;&#31614;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#25191;&#34892;&#20219;&#21153;&#65292;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#22343;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.07689</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dual-Attention Model for Aspect-Level Sentiment Classification. (arXiv:2303.07689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#20381;&#23384;&#26631;&#31614;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#25191;&#34892;&#20219;&#21153;&#65292;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#22343;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;(DAM)&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#29992;&#20110;&#20154;&#24037;&#35774;&#35745;&#29305;&#24449;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#21644;&#22522;&#20110;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#37117;&#26377;&#19981;&#38169;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#35748;&#20026;&#23427;&#20204;&#37117;&#32570;&#23569;&#19968;&#20010;&#37325;&#35201;&#30340;&#21477;&#27861;&#20449;&#24687;&#65306;&#20381;&#23384;&#26631;&#31614;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20381;&#23384;&#26631;&#31614;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#25191;&#34892;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65306;&#31508;&#35760;&#26412;&#30005;&#33041;&#21644;&#39184;&#21381;&#25968;&#25454;&#38598;&#26469;&#33258;SemEval 2014&#65292;&#26368;&#21518;&#19968;&#20010;&#26159;Twitter&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
I propose a novel dual-attention model(DAM) for aspect-level sentiment classification. Many methods have been proposed, such as support vector machines for artificial design features, long short-term memory networks based on attention mechanisms, and graph neural networks based on dependency parsing. While these methods all have decent performance, I think they all miss one important piece of syntactic information: dependency labels. Based on this idea, this paper proposes a model using dependency labels for the attention mechanism to do this task. We evaluate the proposed approach on three datasets: laptop and restaurant are from SemEval 2014, and the last one is a twitter dataset. Experimental results show that the dual attention model has good performance on all three datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#65288;FPTN&#65289;&#65292;&#23558;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#27839;&#20256;&#24863;&#22120;&#32500;&#24230;&#32780;&#38750;&#26102;&#38388;&#32500;&#24230;&#21010;&#20998;&#20026;&#24207;&#21015;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#23884;&#20837;&#26041;&#24335;&#23558;&#36825;&#20123;&#21521;&#37327;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#21033;&#29992;Transformer&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#39044;&#27979;&#30340;&#20132;&#36890;&#27969;&#37327;&#65292;FPTN&#19981;&#20165;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;&#20960;&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.07685</link><description>&lt;p&gt;
FPTN:&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FPTN: Fast Pure Transformer Network for Traffic Flow Forecasting. (arXiv:2303.07685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#65288;FPTN&#65289;&#65292;&#23558;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#27839;&#20256;&#24863;&#22120;&#32500;&#24230;&#32780;&#38750;&#26102;&#38388;&#32500;&#24230;&#21010;&#20998;&#20026;&#24207;&#21015;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#23884;&#20837;&#26041;&#24335;&#23558;&#36825;&#20123;&#21521;&#37327;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#21033;&#29992;Transformer&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#39044;&#27979;&#30340;&#20132;&#36890;&#27969;&#37327;&#65292;FPTN&#19981;&#20165;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;&#20960;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#35270;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22826;&#22810;&#30340;&#20256;&#24863;&#22120;&#20250;&#23548;&#33268;&#19968;&#20010;&#22823;&#20110;800&#30340;&#21521;&#37327;&#65292;&#36825;&#24456;&#38590;&#22312;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#35774;&#35745;&#20102;&#22797;&#26434;&#30340;&#26426;&#21046;&#26469;&#25429;&#33719;MTS&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#39044;&#27979;&#36895;&#24230;&#32531;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#65288;FPTN&#65289;&#12290;&#39318;&#20808;&#65292;&#23558;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#27839;&#20256;&#24863;&#22120;&#32500;&#24230;&#32780;&#38750;&#26102;&#38388;&#32500;&#24230;&#21010;&#20998;&#20026;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20805;&#20998;&#34920;&#31034;&#22797;&#26434;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#23884;&#20837;&#26041;&#24335;&#23558;&#36825;&#20123;&#21521;&#37327;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#20043;&#21518;&#65292;&#20026;&#20102;&#21516;&#26102;&#25429;&#33719;&#36825;&#20123;&#21521;&#37327;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Transformer&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#39044;&#27979;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FPTN&#19981;&#20165;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36816;&#34892;&#36895;&#24230;&#24555;&#20960;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow forecasting is challenging due to the intricate spatio-temporal correlations in traffic flow data. Existing Transformer-based methods usually treat traffic flow forecasting as multivariate time series (MTS) forecasting. However, too many sensors can cause a vector with a dimension greater than 800, which is difficult to process without information loss. In addition, these methods design complex mechanisms to capture spatial dependencies in MTS, resulting in slow forecasting speed. To solve the abovementioned problems, we propose a Fast Pure Transformer Network (FPTN) in this paper. First, the traffic flow data are divided into sequences along the sensor dimension instead of the time dimension. Then, to adequately represent complex spatio-temporal correlations, Three types of embeddings are proposed for projecting these vectors into a suitable vector space. After that, to capture the complex spatio-temporal correlations simultaneously in these vectors, we utilize Transforme
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20016;&#23500;&#30340;&#38899;&#39057;&#27169;&#22411;&#21453;&#28436;&#65288;FRAMI&#65289;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#29992;&#22768;&#38899;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#29305;&#24449;&#19981;&#21464;&#23545;&#27604;&#25439;&#22833;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#29305;&#24449;&#20016;&#23500;&#30340;Mel&#39057;&#35889;&#22270;&#65292;&#20877;&#21033;&#29992;&#32479;&#35745;&#27719;&#38598;&#23618;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07643</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#20016;&#23500;&#30340;&#38899;&#39057;&#27169;&#22411;&#21453;&#28436;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#20197;&#23454;&#29616;&#36890;&#29992;&#22768;&#38899;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Feature-Rich Audio Model Inversion for Data-Free Knowledge Distillation Towards General Sound Classification. (arXiv:2303.07643v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20016;&#23500;&#30340;&#38899;&#39057;&#27169;&#22411;&#21453;&#28436;&#65288;FRAMI&#65289;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#29992;&#22768;&#38899;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#29305;&#24449;&#19981;&#21464;&#23545;&#27604;&#25439;&#22833;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#29305;&#24449;&#20016;&#23500;&#30340;Mel&#39057;&#35889;&#22270;&#65292;&#20877;&#21033;&#29992;&#32479;&#35745;&#27719;&#38598;&#23618;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#21518;&#12290;&#23613;&#31649;&#25216;&#26415;&#26377;&#30528;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#20854;&#22312;&#38899;&#39057;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#36824;&#19981;&#26159;&#24456;&#22909;&#12290;&#30001;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#65292;&#20854;&#20855;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#24314;&#27169;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#20016;&#23500;&#30340;&#38899;&#39057;&#27169;&#22411;&#21453;&#28436;&#65288;FRAMI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#22768;&#38899;&#20998;&#31867;&#20219;&#21153;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#12290;&#23427;&#39318;&#20808;&#36890;&#36807;&#29305;&#24449;&#19981;&#21464;&#23545;&#27604;&#25439;&#22833;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#29305;&#24449;&#20016;&#23500;&#30340;Mel&#39057;&#35889;&#22270;&#12290;&#28982;&#21518;&#65292;&#22312;&#36825;&#20123;&#29305;&#24449;&#20016;&#23500;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#26102;&#65292;&#20877;&#21033;&#29992;&#32479;&#35745;&#27719;&#38598;&#23618;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#22312;Urbansound8k&#12289;ESC-50&#21644;audioMNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FRAMI&#33021;&#22815;&#29983;&#25104;&#29305;&#24449;&#20016;&#23500;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#37325;&#29992;&#38544;&#34255;&#29366;&#24577;&#21644;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Free Knowledge Distillation (DFKD) has recently attracted growing attention in the academic community, especially with major breakthroughs in computer vision. Despite promising results, the technique has not been well applied to audio and signal processing. Due to the variable duration of audio signals, it has its own unique way of modeling. In this work, we propose feature-rich audio model inversion (FRAMI), a data-free knowledge distillation framework for general sound classification tasks. It first generates high-quality and feature-rich Mel-spectrograms through a feature-invariant contrastive loss. Then, the hidden states before and after the statistics pooling layer are reused when knowledge distillation is performed on these feature-rich samples. Experimental results on the Urbansound8k, ESC-50, and audioMNIST datasets demonstrate that FRAMI can generate feature-rich samples. Meanwhile, the accuracy of the student model is further improved by reusing the hidden state and sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; I$^2$-SDF &#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36319;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#23460;&#20869;&#22330;&#26223;&#30340;&#37325;&#24314;&#21644;&#32534;&#36753;&#65292;&#37319;&#29992;&#27668;&#27873;&#25439;&#22833;&#20989;&#25968;&#21644;&#38169;&#35823;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#26696;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#35299;&#31070;&#32463;&#36752;&#23556;&#22330;&#20026;&#22330;&#26223;&#30340;&#31354;&#38388;&#21464;&#21270;&#26448;&#26009;&#23454;&#29616;&#20102;&#29289;&#29702;&#21644;&#36924;&#30495;&#30340;&#22330;&#26223;&#32534;&#36753;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.07634</link><description>&lt;p&gt;
I$^2$-SDF: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20809;&#32447;&#36861;&#36394;&#23454;&#29616;&#20869;&#37096;&#22330;&#26223;&#37325;&#24314;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs. (arXiv:2303.07634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; I$^2$-SDF &#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36319;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#23460;&#20869;&#22330;&#26223;&#30340;&#37325;&#24314;&#21644;&#32534;&#36753;&#65292;&#37319;&#29992;&#27668;&#27873;&#25439;&#22833;&#20989;&#25968;&#21644;&#38169;&#35823;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#26696;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#35299;&#31070;&#32463;&#36752;&#23556;&#22330;&#20026;&#22330;&#26223;&#30340;&#31354;&#38388;&#21464;&#21270;&#26448;&#26009;&#23454;&#29616;&#20102;&#29289;&#29702;&#21644;&#36924;&#30495;&#30340;&#22330;&#26223;&#32534;&#36753;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; I$^2$-SDF &#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36861;&#36394;&#25216;&#26415;&#23545;&#31070;&#32463;&#32593;&#32476;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#20013;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#29289;&#20307;&#37325;&#24314;&#12289;&#36752;&#23556;&#21644;&#26448;&#36136;&#24674;&#22797;&#12290;&#38024;&#23545;&#23567;&#22411;&#29289;&#20307;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#27668;&#27873;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#35823;&#24046;&#24341;&#23548;&#33258;&#36866;&#24212;&#21462;&#26679;&#26041;&#26696;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#23460;&#20869;&#22330;&#26223;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#36752;&#23556;&#22330;&#20998;&#35299;&#20026;&#22330;&#26223;&#30340;&#31354;&#38388;&#21464;&#21270;&#26448;&#26009;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#34920;&#38754;&#30340;&#21487;&#24494;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36861;&#36394;&#21644;&#21457;&#23556;&#22120;&#35821;&#20041;&#20998;&#21106;&#26469;&#23454;&#29616;&#22522;&#20110;&#29289;&#29702;&#21644;&#36924;&#30495;&#30340;&#22330;&#26223;&#37325;&#29031;&#21644;&#32534;&#36753;&#24212;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23460;&#20869;&#22330;&#26223;&#37325;&#24314;&#12289;&#26032;&#35270;&#35282;&#21512;&#25104;&#21644;&#22330;&#26223;&#32534;&#36753;&#26041;&#38754;&#30340;&#21331;&#36234;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene reconstruction and editing using differentiable Monte Carlo raytracing on neural signed distance fields (SDFs). Our holistic neural SDF-based framework jointly recovers the underlying shapes, incident radiance and materials from multi-view images. We introduce a novel bubble loss for fine-grained small objects and error-guided adaptive sampling scheme to largely improve the reconstruction quality on large-scale indoor scenes. Further, we propose to decompose the neural radiance field into spatially-varying material of the scene as a neural field through surface-based, differentiable Monte Carlo raytracing and emitter semantic segmentations, which enables physically based and photorealistic scene relighting and editing applications. Through a number of qualitative and quantitative experiments, we demonstrate the superior quality of our method on indoor scene reconstruction, novel view synthesis, and scene editin
&lt;/p&gt;</description></item><item><title>RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.07622</link><description>&lt;p&gt;
RE-MOVE&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#21160;&#24577;&#29615;&#22659;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07622
&lt;/p&gt;
&lt;p&gt;
RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#22312;&#23454;&#26102;&#37096;&#32626;&#26399;&#38388;&#36866;&#24212;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RE-MOVE&#65288;&#35831;&#27714;&#24110;&#21161;&#24182;&#31227;&#21160;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#26469;&#35843;&#25972;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#20197;&#36866;&#24212;&#29615;&#22659;&#30340;&#23454;&#26102;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#35831;&#27714;&#21453;&#39304;&#24182;&#22914;&#20309;&#23558;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20013;&#12290;RE-MOVE&#21033;&#29992;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#35831;&#27714;&#20154;&#31867;&#21453;&#39304;&#30340;&#26368;&#20339;&#26102;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#36827;&#34892;&#23454;&#26102;&#36866;&#24212;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21512;&#25104;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#27979;&#35797;&#26102;&#38388;&#21160;&#24577;&#23548;&#33322;&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#23478;&#38271;&#30340;&#20559;&#22909;&#25968;&#25454;&#19982;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#21010;&#20998;&#23567;&#23398;&#30340;&#32771;&#21220;&#36793;&#30028;&#65292;&#20197;&#26368;&#23567;&#21270;&#30333;/&#38750;&#30333;&#20154;&#31181;&#26063;&#20998;&#31163;&#65292;&#21516;&#26102;&#20943;&#36731;&#23545;&#34892;&#31243;&#26102;&#38388;&#21644;&#23398;&#26657;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;14&#65285;&#30340;&#31181;&#26063;&#38548;&#31163;&#29575;&#30456;&#23545;&#38477;&#20302;&#65292;&#38656;&#35201;&#32422;20&#65285;&#30340;&#23398;&#29983;&#36716;&#25442;&#23398;&#26657;&#65292;&#24182;&#20986;&#20046;&#24847;&#26009;&#22320;&#30053;&#24494;&#32553;&#30701;&#20102;&#34892;&#31243;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.07603</link><description>&lt;p&gt;
&#37325;&#21010;&#23567;&#23398;&#32771;&#21220;&#36793;&#30028;&#20197;&#20419;&#36827;&#31181;&#26063;&#21644;&#27665;&#26063;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Redrawing attendance boundaries to promote racial and ethnic diversity in elementary schools. (arXiv:2303.07603v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#23478;&#38271;&#30340;&#20559;&#22909;&#25968;&#25454;&#19982;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#21010;&#20998;&#23567;&#23398;&#30340;&#32771;&#21220;&#36793;&#30028;&#65292;&#20197;&#26368;&#23567;&#21270;&#30333;/&#38750;&#30333;&#20154;&#31181;&#26063;&#20998;&#31163;&#65292;&#21516;&#26102;&#20943;&#36731;&#23545;&#34892;&#31243;&#26102;&#38388;&#21644;&#23398;&#26657;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;14&#65285;&#30340;&#31181;&#26063;&#38548;&#31163;&#29575;&#30456;&#23545;&#38477;&#20302;&#65292;&#38656;&#35201;&#32422;20&#65285;&#30340;&#23398;&#29983;&#36716;&#25442;&#23398;&#26657;&#65292;&#24182;&#20986;&#20046;&#24847;&#26009;&#22320;&#30053;&#24494;&#32553;&#30701;&#20102;&#34892;&#31243;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32654;&#22269;&#23398;&#21306;&#21046;&#23450;&#8220;&#32771;&#21220;&#36793;&#30028;&#8221;&#26469;&#23450;&#20041;&#20998;&#37197;&#23398;&#29983;&#21040;&#23478;&#38468;&#36817;&#23398;&#26657;&#30340;&#21306;&#22495;&#65292;&#24448;&#24448;&#37325;&#28436;&#37051;&#37324;&#31181;&#26063;&#38548;&#31163;&#22312;&#23398;&#26657;&#20013;&#30340;&#29616;&#35937;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#23567;&#23398;&#19978;&#65292;&#24182;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#37325;&#26032;&#21010;&#20998;&#32771;&#21220;&#36793;&#30028;&#26159;&#21542;&#33021;&#22815;&#20943;&#23569;&#23398;&#26657;&#30340;&#31181;&#26063;&#38548;&#31163;&#29616;&#35937;&#65311;&#25105;&#20204;&#23558;&#23478;&#38271;&#30340;&#20559;&#22909;&#25968;&#25454;&#19982;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#27169;&#25311;&#20102;&#35206;&#30422;&#36229;&#36807;3&#30334;&#19975;&#23567;&#23398;&#29983;&#30340;98&#20010;&#32654;&#22269;&#23398;&#21306;&#30340;&#26367;&#20195;&#36793;&#30028;&#65292;&#20197;&#26368;&#23567;&#21270;&#30333;/&#38750;&#30333;&#20154;&#31181;&#26063;&#20998;&#31163;&#65292;&#21516;&#26102;&#20943;&#36731;&#23545;&#34892;&#31243;&#26102;&#38388;&#21644;&#23398;&#26657;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#22312;&#21508;&#20010;&#23398;&#21306;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20013;&#20301;&#25968;14%&#30340;&#31181;&#26063;&#38548;&#31163;&#29575;&#30456;&#23545;&#38477;&#20302;&#65292;&#36825;&#38656;&#35201;&#32422;20%&#30340;&#23398;&#29983;&#36716;&#25442;&#23398;&#26657;&#65292;&#24182;&#20986;&#20046;&#24847;&#26009;&#22320;&#30053;&#24494;&#32553;&#30701;&#20102;&#34892;&#31243;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#20844;&#20849;&#20202;&#34920;&#26495;&#65288;https://www.schooldiversity.org/&#65289;&#65292;&#23637;&#31034;&#36825;&#20123;&#26367;&#20195;&#36793;&#30028;&#65292;&#24182;&#36992;&#35831;&#23398;&#26657;&#33891;&#20107;&#20250;&#21450;&#20854;&#21033;&#30410;&#30456;&#20851;&#32773;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most US school districts draw "attendance boundaries" to define catchment areas that assign students to schools near their homes, often recapitulating neighborhood demographic segregation in schools. Focusing on elementary schools, we ask: how much might we reduce school segregation by redrawing attendance boundaries? Combining parent preference data with methods from combinatorial optimization, we simulate alternative boundaries for 98 US school districts serving over 3 million elementary-aged students, minimizing White/non-White segregation while mitigating changes to travel times and school sizes. Across districts, we observe a median 14% relative decrease in segregation, which we estimate would require approximately 20\% of students to switch schools and, surprisingly, a slight reduction in travel times. We release a public dashboard depicting these alternative boundaries (https://www.schooldiversity.org/) and invite both school boards and their constituents to evaluate their viabi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38024;&#23545;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#22269;&#23478;&#30340;COVID-19&#30123;&#24773;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#23545;COVID-19&#24863;&#26579;&#24773;&#20917;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.07600</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#22269;&#23478;&#30340;COVID-19&#24863;&#26579;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC) Countries using Machine Learning. (arXiv:2303.07600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38024;&#23545;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#22269;&#23478;&#30340;COVID-19&#30123;&#24773;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#23545;COVID-19&#24863;&#26579;&#24773;&#20917;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#33258;&#21435;&#24180;&#39318;&#27425;&#26816;&#27979;&#20197;&#26469;&#65292;&#24050;&#32463;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24863;&#26579;&#20102;&#36229;&#36807;6800&#19975;&#20154;&#12290;&#26102;&#38388;&#24207;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;COVID-19&#30340;&#24863;&#26579;&#24773;&#20917;&#12290;&#26412;&#25991;&#20351;&#29992;Johns Hopkins&#30340;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#38024;&#23545;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#65288;GCC&#65289;&#22269;&#23478;&#24320;&#21457;&#20102;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;2020&#24180;1&#26376;22&#26085;&#33267;2021&#24180;1&#26376;22&#26085;&#20043;&#38388;&#30340;&#19968;&#24180;&#32047;&#35745;COVID-19&#30149;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#24863;&#26579;&#25968;&#25454;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20026;&#25152;&#30740;&#31350;&#30340;&#22269;&#23478;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;COVID-19&#30340;&#24863;&#26579;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 has infected more than 68 million people worldwide since it was first detected about a year ago. Machine learning time series models have been implemented to forecast COVID-19 infections. In this paper, we develop time series models for the Gulf Cooperation Council (GCC) countries using the public COVID-19 dataset from Johns Hopkins. The dataset set includes the one-year cumulative COVID-19 cases between 22/01/2020 to 22/01/2021. We developed different models for the countries under study based on the spatial distribution of the infection data. Our experimental results show that the developed models can forecast COVID-19 infections with high precision.
&lt;/p&gt;</description></item><item><title>AdPE&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20301;&#32622;&#23884;&#20837;&#65292;&#25197;&#26354;&#23616;&#37096;&#32467;&#26500;&#65292;&#24378;&#21046;Transformer&#32534;&#30721;&#22120;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#20855;&#26377;&#24046;&#21035;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07598</link><description>&lt;p&gt;
AdPE&#65306;&#36890;&#36807;MAE+&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+. (arXiv:2303.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07598
&lt;/p&gt;
&lt;p&gt;
AdPE&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20301;&#32622;&#23884;&#20837;&#65292;&#25197;&#26354;&#23616;&#37096;&#32467;&#26500;&#65292;&#24378;&#21046;Transformer&#32534;&#30721;&#22120;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#20855;&#26377;&#24046;&#21035;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#35270;&#35273;Transformer&#26088;&#22312;&#36890;&#36807;&#39044;&#35774;&#20219;&#21153;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#39044;&#20808;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;Masked Image Modeling&#65288;MIM&#65289;&#65292;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;Transformer&#39044;&#27979;&#25513;&#34109;&#34917;&#19969;&#30456;&#23545;&#24212;&#12290;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20934;&#21017;&#26159;&#39044;&#35774;&#20219;&#21153;&#38656;&#35201;&#36275;&#22815;&#38590;&#20197;&#38450;&#27490;Transformer&#23398;&#20064;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#19981;&#36275;&#36947;&#30340;&#20302;&#23618;&#27425;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Adversarial Positional Embedding&#65288;AdPE&#65289;&#26041;&#27861; - &#36890;&#36807;&#25200;&#21160;&#20301;&#32622;&#32534;&#30721;&#26469;&#25197;&#26354;&#23616;&#37096;&#35270;&#35273;&#32467;&#26500;&#65292;&#20197;&#20351;&#24471;&#23398;&#20064;&#30340;Transformer&#19981;&#33021;&#20165;&#20351;&#29992;&#23616;&#37096;&#30456;&#20851;&#30340;&#34917;&#19969;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#34917;&#19969;&#12290;&#25105;&#20204;&#20551;&#35774;&#23427;&#36843;&#20351;Transformer&#32534;&#30721;&#22120;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#20855;&#26377;&#24046;&#21035;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#20854;&#20013;&#23545;&#25239;&#20301;&#32622;&#21487;&#20197;&#27169;&#25311;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning of vision transformers seeks to pretrain an encoder via pretext tasks without labels. Among them is the Masked Image Modeling (MIM) aligned with pretraining of language transformers by predicting masked patches as a pretext task. A criterion in unsupervised pretraining is the pretext task needs to be sufficiently hard to prevent the transformer encoder from learning trivial low-level features not generalizable well to downstream tasks. For this purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It distorts the local visual structures by perturbing the position encodings so that the learned transformer cannot simply use the locally correlated patches to predict the missing ones. We hypothesize that it forces the transformer encoder to learn more discriminative features in a global context with stronger generalizability to downstream tasks. We will consider both absolute and relative positional encodings, where adversarial positions can be im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#32423;&#21035;&#38647;&#36798;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#23884;&#20837;&#24335;&#35745;&#31639;&#30340;&#23454;&#26102;&#37096;&#32626;&#65292;&#36895;&#24230;&#36798;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;100&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.07586</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#21152;&#36895;&#22120;&#30340;&#38647;&#36798;&#24863;&#30693;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Knowledge Distillation for Radar Perception on Embedded Accelerators. (arXiv:2303.07586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#32423;&#21035;&#38647;&#36798;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#23884;&#20837;&#24335;&#35745;&#31639;&#30340;&#23454;&#26102;&#37096;&#32626;&#65292;&#36895;&#24230;&#36798;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35768;&#22810;&#29992;&#20110;&#36947;&#36335;&#23433;&#20840;&#24863;&#30693;&#30340;&#38647;&#36798;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#37117;&#26080;&#27861;&#24456;&#22909;&#22320;&#36816;&#34892;&#22312;&#29992;&#20110;&#27773;&#36710;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#12290;&#30456;&#21453;&#65292;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#19987;&#38376;&#21152;&#36895;&#22120;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#32423;&#21035;&#38647;&#36798;&#24863;&#30693;&#20219;&#21153;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#29992;&#20110;&#38745;&#24577;&#30446;&#26631;&#26816;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#23398;&#29983;&#27169;&#22411;&#12290;&#35813;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#23884;&#20837;&#24335;&#35745;&#31639;&#36827;&#34892;&#23454;&#26102;&#37096;&#32626;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#27604;&#25945;&#24072;&#27169;&#22411;&#24555;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many radar signal processing methodologies are being developed for critical road safety perception tasks. Unfortunately, these signal processing algorithms are often poorly suited to run on embedded hardware accelerators used in automobiles. Conversely, end-to-end machine learning (ML) approaches better exploit the performance gains brought by specialized accelerators. In this paper, we propose a teacher-student knowledge distillation approach for low-level radar perception tasks. We utilize a hybrid model for stationary object detection as a teacher to train an end-to-end ML student model. The student can efficiently harness embedded compute for real-time deployment. We demonstrate that the proposed student model runs at speeds 100x faster than the teacher model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21019;&#32426;&#24405;&#24615;&#33021;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#24182;&#24635;&#32467;&#30456;&#20851;&#25991;&#29486;&#36164;&#26009;&#12290;</title><link>http://arxiv.org/abs/2303.07576</link><description>&lt;p&gt;
NLP &#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in NLP: A Survey. (arXiv:2303.07576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21019;&#32426;&#24405;&#24615;&#33021;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#24182;&#24635;&#32467;&#30456;&#20851;&#25991;&#29486;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26174;&#31034;&#20102;&#21019;&#35760;&#24405;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#27010;&#36848;&#21644;&#25512;&#23548;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#35770;&#65292;&#28982;&#21518;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20174;&#25991;&#26412;&#29983;&#25104;&#12289;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#31561;&#22235;&#20010;&#26041;&#38754;&#36827;&#34892;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#30456;&#20851;&#30340;&#25991;&#29486;&#36164;&#26009;&#65292;&#24182;&#26368;&#32456;&#35760;&#24405;&#20102;&#36825;&#20010;&#20027;&#39064;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#30340;&#32463;&#39564;&#21644;&#24863;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a powerful family of deep generative models, with record-breaking performance in many applications. This paper first gives an overview and derivation of the basic theory of diffusion models, then reviews the research results of diffusion models in the field of natural language processing, from text generation, text-driven image generation and other four aspects, and analyzes and summarizes the relevant literature materials sorted out, and finally records the experience and feelings of this topic literature review research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.07557</link><description>&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;: &#26032;&#30340;&#25361;&#25112;&#12289;&#35270;&#35282;&#21644;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights. (arXiv:2303.07557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#20013;&#65292;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#32456;&#36523;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#36235;&#21183;&#65292;&#23427;&#33021;&#22815;&#28385;&#36275;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#19981;&#26029;&#36866;&#24212;&#26032;&#25361;&#25112;&#24182;&#20445;&#30041;&#36807;&#21435;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20154;&#33268;&#21147;&#20110;&#24314;&#31435;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#30784;&#65292;&#36825;&#19982;&#26356;&#24191;&#27867;&#25506;&#32034;&#30340;&#20998;&#31867;&#35774;&#32622;&#23384;&#22312;&#26412;&#36136;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#12289;&#38416;&#36848;&#21644;&#35752;&#35770;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#65292;&#35797;&#22270;&#20026;&#20854;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#24314;&#31435;&#22522;&#30784;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#24456;&#37325;&#35201;&#65292;&#23450;&#20041;&#20102;&#24212;&#23545;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#35774;&#32622;&#21644;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is of paramount importance in many real-world domains, characterized by evolving behavior. Lifelong learning represents an emerging trend, answering the need for machine learning models that continuously adapt to new challenges in dynamic environments while retaining past knowledge. However, limited efforts are dedicated to building foundations for lifelong anomaly detection, which provides intrinsically different challenges compared to the more widely explored classification setting. In this paper, we face this issue by exploring, motivating, and discussing lifelong anomaly detection, trying to build foundations for its wider adoption. First, we explain why lifelong anomaly detection is relevant, defining challenges and opportunities to design anomaly detection methods that deal with lifelong learning complexities. Second, we characterize learning settings and a scenario generation procedure that enables researchers to experiment with lifelong anomaly detection using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.07551</link><description>&lt;p&gt;
&#21512;&#24182;&#20915;&#31574;Transformer&#65306;&#22810;&#20219;&#21153;&#31574;&#30053;&#24418;&#25104;&#30340;&#26435;&#37325;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#31574;&#30053;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#38598;&#20013;&#30340;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#21019;&#24314;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#12289;&#21333;&#29420;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#21017;&#36825;&#26679;&#20570;&#23601;&#27604;&#36739;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#25110;&#24179;&#22343;&#19981;&#21516;MuJoCo&#36816;&#21160;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;Decision Transformer&#30340;&#23376;&#38598;&#26469;&#36808;&#20986;&#36825;&#20010;&#26041;&#21521;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#24418;&#25104;&#27809;&#26377;&#38598;&#20013;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22312;&#21512;&#24182;&#31574;&#30053;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22914;&#26524;&#25152;&#26377;&#31574;&#30053;&#37117;&#20174;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#24182;&#22312;&#38382;&#39064;&#29305;&#23450;&#30340;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#35843;&#25972;&#22120;&#30340;&#24207;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21442;&#25968;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;1.82&#20493;&#30340;&#23792;&#20540;&#21152;&#36895;&#21644;1.48&#20493;&#30340;&#24179;&#22343;&#21152;&#36895;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.07535</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36335;&#24452;&#35268;&#21010;&#65306;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path Planning using Reinforcement Learning: A Policy Iteration Approach. (arXiv:2303.07535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#35843;&#25972;&#22120;&#30340;&#24207;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21442;&#25968;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;1.82&#20493;&#30340;&#23792;&#20540;&#21152;&#36895;&#21644;1.48&#20493;&#30340;&#24179;&#22343;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23454;&#26102;&#22788;&#29702;&#30340;&#24433;&#21709;&#22312;&#26368;&#36817;&#34987;&#35748;&#35782;&#21040;&#65292;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#39640;&#25928;&#23454;&#29616;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#23613;&#31649;RL&#31639;&#27861;&#20013;&#21033;&#29992;Bellman&#26041;&#31243;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#26159;&#35774;&#35745;&#21442;&#25968;&#30340;&#22823;&#25628;&#32034;&#31354;&#38388;&#20063;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#38416;&#26126;&#19982;&#24378;&#21270;&#23398;&#20064;&#21442;&#25968;&#30456;&#20851;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#31574;&#30053;&#36845;&#20195;&#26041;&#38754;&#12290;&#32771;&#34385;&#21040;&#24494;&#35843;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21442;&#25968;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#35843;&#33410;&#22120;&#30340;&#24207;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#25506;&#32034;&#36825;&#20123;&#21442;&#25968;&#30340;&#36807;&#31243;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;1.82&#20493;&#30340;&#23792;&#20540;&#21152;&#36895;&#65292;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;1.48&#20493;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the impact of real-time processing being realized in the recent past, the need for efficient implementations of reinforcement learning algorithms has been on the rise. Albeit the numerous advantages of Bellman equations utilized in RL algorithms, they are not without the large search space of design parameters.  This research aims to shed light on the design space exploration associated with reinforcement learning parameters, specifically that of Policy Iteration. Given the large computational expenses of fine-tuning the parameters of reinforcement learning algorithms, we propose an auto-tuner-based ordinal regression approach to accelerate the process of exploring these parameters and, in return, accelerate convergence towards an optimal policy. Our approach provides 1.82x peak speedup with an average of 1.48x speedup over the previous state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07522</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#26159;&#19968;&#31181;&#22810;&#24863;&#23448;&#30340;&#20307;&#39564;&#65292;&#20294;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#24863;&#30693;&#26469;&#32472;&#21046;&#21644;&#23548;&#33322;&#20182;&#20204;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;3D&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#12290;&#22312;&#23548;&#33322;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;(&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#25110;&#22320;&#26631;&#30340;&#38899;&#39057;&#29255;&#27573;)&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#28155;&#21152;&#38899;&#39057;&#20449;&#24687;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#28040;&#38500;&#30446;&#26631;&#20301;&#32622;&#30340;&#27495;&#20041;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#22312;&#27169;&#31946;&#22330;&#26223;&#20013;&#25552;&#20379;50%&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#22312;&#25191;&#34892;&#19968;&#31995;&#21015;&#28216;&#25103;&#26102;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#32593;&#32476;&#30340;&#28608;&#27963;&#36275;&#36857;&#21464;&#24471;&#31232;&#30095;&#23548;&#33268;&#26799;&#24230;&#21464;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.07507</link><description>&lt;p&gt;
&#36830;&#32493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;
&lt;/p&gt;
&lt;p&gt;
Loss of Plasticity in Continual Deep Reinforcement Learning. (arXiv:2303.07507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#22312;&#25191;&#34892;&#19968;&#31995;&#21015;&#28216;&#25103;&#26102;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#32593;&#32476;&#30340;&#28608;&#27963;&#36275;&#36857;&#21464;&#24471;&#31232;&#30095;&#23548;&#33268;&#26799;&#24230;&#21464;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#20856;&#22522;&#20110;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#24490;&#29615;&#25191;&#34892;Atari 2600&#28216;&#25103;&#26102;&#65292;&#23427;&#20204;&#22833;&#21435;&#20102;&#23398;&#20064;&#33391;&#22909;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#22312;&#26102;&#38388;&#19978;&#22914;&#20309;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#36275;&#36857;&#21464;&#24471;&#26356;&#21152;&#31232;&#30095;&#65292;&#23548;&#33268;&#26799;&#24230;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn continually is essential in a complex and changing world. In this paper, we characterize the behavior of canonical value-based deep reinforcement learning (RL) approaches under varying degrees of non-stationarity. In particular, we demonstrate that deep RL agents lose their ability to learn good policies when they cycle through a sequence of Atari 2600 games. This phenomenon is alluded to in prior work under various guises -e.g., loss of plasticity, implicit under-parameterization, primacy bias, and capacity loss. We investigate this phenomenon closely at scale and analyze how the weights, gradients, and activations change over time in several experiments with varying dimensions (e.g., similarity between games, number of games, number of frames per game), with some experiments spanning 50 days and 2 billion environment interactions. Our analysis shows that the activation footprint of the network becomes sparser, contributing to the diminishing gradients. We inves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20803;&#23398;&#20064;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07502</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#27861;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#36817;&#26399;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Meta-learning approaches for few-shot learning: A survey of recent advances. (arXiv:2303.07502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20803;&#23398;&#20064;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#23398;&#20064;&#22810;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#19987;&#27880;&#20110;&#21516;&#20998;&#24067;&#39044;&#27979;&#65292;&#23548;&#33268;&#20854;&#22312;&#26032;&#30340;&#26410;&#35265;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#22240;&#26679;&#26412;&#19981;&#36275;&#32780;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24212;&#23569;&#37327;&#26679;&#26412;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65292;&#28982;&#21518;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;(I) &#22522;&#20110;&#24230;&#37327;&#30340;&#12289;(II) &#22522;&#20110;&#35760;&#24518;&#30340;&#12289;(III) &#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its astounding success in learning deeper multi-dimensional data, the performance of deep learning declines on new unseen tasks mainly due to its focus on same-distribution prediction. Moreover, deep learning is notorious for poor generalization from few samples. Meta-learning is a promising approach that addresses these issues by adapting to new tasks with few-shot datasets. This survey first briefly introduces meta-learning and then investigates state-of-the-art meta-learning methods and recent advances in: (I) metric-based, (II) memory-based, (III), and learning-based methods. Finally, current challenges and insights for future researches are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28176;&#36827;&#22320;&#20923;&#32467;&#26377;&#26368;&#39640;&#30456;&#20851;&#24615;&#30340;&#37096;&#20998;&#23618;&#26469;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#25152;&#23398;&#34920;&#31034;&#30340;&#20016;&#23500;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07477</link><description>&lt;p&gt;
&#21033;&#29992;&#28176;&#36827;&#20219;&#21153;&#30456;&#20851;&#24615;&#23618;&#20923;&#32467;&#30340;&#26377;&#25928;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Self-supervised Continual Learning with Progressive Task-correlated Layer Freezing. (arXiv:2303.07477v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28176;&#36827;&#22320;&#20923;&#32467;&#26377;&#26368;&#39640;&#30456;&#20851;&#24615;&#30340;&#37096;&#20998;&#23618;&#26469;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#25152;&#23398;&#34920;&#31034;&#30340;&#20016;&#23500;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#19968;&#20123;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#29615;&#22659;&#19979;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#65292;&#21363;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#65288;SSCL&#65289;&#12290; SSCL&#24050;&#34987;&#35777;&#26126;&#20248;&#20110;&#21463;&#30417;&#30563;&#30340;&#36830;&#32493;&#23398;&#20064;&#65288;SCL&#65289;&#65292;&#22240;&#20026;&#25152;&#23398;&#34920;&#31034;&#26356;&#21152;&#20016;&#23500;&#21644;&#40065;&#26834;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#35774;&#35745;&#26234;&#33021;&#65292;&#21017;SSCL&#30340;&#22521;&#35757;&#22797;&#26434;&#24615;&#21487;&#33021;&#20250;&#30001;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22825;&#28982;&#22521;&#35757;&#25104;&#26412;&#32780;&#21464;&#24471;&#31105;&#27490;&#39640;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#39318;&#20808;&#30740;&#31350;SSCL&#35774;&#32622;&#20013;&#30340;&#20219;&#21153;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#22312;SSL&#23398;&#20064;&#30340;&#32972;&#26223;&#27169;&#22411;&#19979;&#65292;&#20013;&#38388;&#29305;&#24449;&#22312;&#20219;&#21153;&#20043;&#38388;&#39640;&#24230;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#23618;&#20923;&#32467;&#30340;SSCL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#20855;&#26377;&#26368;&#39640;&#30456;&#20851;&#24615;&#30340;&#37096;&#20998;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the success of Self-supervised learning (SSL) in learning visual representations from unlabeled data, a few recent works have studied SSL in the context of continual learning (CL), where multiple tasks are learned sequentially, giving rise to a new paradigm, namely self-supervised continual learning (SSCL). It has been shown that the SSCL outperforms supervised continual learning (SCL) as the learned representations are more informative and robust to catastrophic forgetting. However, if not designed intelligently, the training complexity of SSCL may be prohibitively high due to the inherent training cost of SSL. In this work, by investigating the task correlations in SSCL setup first, we discover an interesting phenomenon that, with the SSL-learned background model, the intermediate features are highly correlated between tasks. Based on this new finding, we propose a new SSCL method with layer-wise freezing which progressively freezes partial layers with the highest correla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#36827;&#34892;&#20102;&#23454;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#21442;&#32771;&#27169;&#22411;&#25991;&#26723;&#19981;&#20840;&#12289;&#38656;&#27714;&#21464;&#21270;&#20197;&#21450;&#23454;&#29616;&#21644;&#27979;&#35797;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#35813;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20010;&#21035;&#24037;&#31243;&#24072;&#21487;&#33021;&#32570;&#20047;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20294;&#22242;&#38431;&#24517;&#39035;&#24212;&#29992;&#36719;&#20214;&#24037;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25165;&#33021;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2303.07476</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Challenges and Practices of Deep Learning Model Reengineering: A Case Study on Computer Vision. (arXiv:2303.07476v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#36827;&#34892;&#20102;&#23454;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#21442;&#32771;&#27169;&#22411;&#25991;&#26723;&#19981;&#20840;&#12289;&#38656;&#27714;&#21464;&#21270;&#20197;&#21450;&#23454;&#29616;&#21644;&#27979;&#35797;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#35813;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20010;&#21035;&#24037;&#31243;&#24072;&#21487;&#33021;&#32570;&#20047;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20294;&#22242;&#38431;&#24517;&#39035;&#24212;&#29992;&#36719;&#20214;&#24037;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25165;&#33021;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24037;&#31243;&#32452;&#32455;&#27491;&#22312;&#37325;&#26032;&#23454;&#29616;&#21644;&#25193;&#23637;&#30740;&#31350;&#30028;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#25551;&#36848;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;-&#37325;&#29992;&#65292;&#20877;&#29616;&#65292;&#35843;&#25972;&#21644;&#22686;&#24378;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;-&#30001;&#20110;&#21442;&#32771;&#27169;&#22411;&#25991;&#26723;&#19981;&#20840;&#12289;&#38656;&#27714;&#21464;&#21270;&#20197;&#21450;&#23454;&#29616;&#21644;&#27979;&#35797;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20010;&#21035;&#24037;&#31243;&#24072;&#21487;&#33021;&#32570;&#20047;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20294;&#22242;&#38431;&#24517;&#39035;&#24212;&#29992;&#36719;&#20214;&#24037;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25165;&#33021;&#25104;&#21151;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20174;&#8220;&#20135;&#21697;&#8221;&#35270;&#35282;&#30740;&#31350;DL&#31995;&#32479;&#65292;&#26080;&#35770;&#24037;&#31243;&#24072;&#30340;&#30446;&#30340;&#22914;&#20309;&#65292;&#37117;&#20250;&#30740;&#31350;&#39033;&#30446;&#20013;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#8220;&#36807;&#31243;&#8221;&#35270;&#35282;&#30340;&#37325;&#26500;&#27963;&#21160;&#19978;&#65292;&#19987;&#27880;&#20110;&#21442;&#19982;&#37325;&#26500;&#36807;&#31243;&#30340;&#24037;&#31243;&#24072;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#30340;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Many engineering organizations are reimplementing and extending deep neural networks from the research community. We describe this process as deep learning model reengineering. Deep learning model reengineering - reusing, reproducing, adapting, and enhancing state-of-the-art deep learning approaches - is challenging for reasons including under-documented reference models, changing requirements, and the cost of implementation and testing. In addition, individual engineers may lack expertise in software engineering, yet teams must apply knowledge of software engineering and deep learning to succeed. Prior work has examined on DL systems from a "product" view, examining defects from projects regardless of the engineers' purpose. Our study is focused on reengineering activities from a "process" view, and focuses on engineers specifically engaged in the reengineering process.  Our goal is to understand the characteristics and challenges of deep learning model reengineering. We conducted a c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23454;&#20307;&#35299;&#26512;&#21644;&#26597;&#35810;&#22238;&#31572;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#29305;&#27530;&#23454;&#20363;&#30340;&#31561;&#20215;&#31867;&#21644;&#20540;&#38598;&#26469;&#23450;&#20041;KB&#30340;&#35821;&#20041;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#20250;&#22833;&#36133;&#30340;&#36861;&#36394;&#36807;&#31243;&#65292;&#29983;&#25104;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.07469</link><description>&lt;p&gt;
&#19968;&#31181;&#23558;&#23454;&#20307;&#28040;&#35299;&#21644;&#26597;&#35810;&#22238;&#31572;&#32467;&#21512;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Combining Entity Resolution and Query Answering in Knowledge Bases. (arXiv:2303.07469v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23454;&#20307;&#35299;&#26512;&#21644;&#26597;&#35810;&#22238;&#31572;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#29305;&#27530;&#23454;&#20363;&#30340;&#31561;&#20215;&#31867;&#21644;&#20540;&#38598;&#26469;&#23450;&#20041;KB&#30340;&#35821;&#20041;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#20250;&#22833;&#36133;&#30340;&#36861;&#36394;&#36807;&#31243;&#65292;&#29983;&#25104;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20307;&#35299;&#26512;&#21644;&#26597;&#35810;&#22238;&#31572;&#19982;&#20855;&#26377;&#20803;&#32452;&#29983;&#25104;&#20381;&#36182;&#20851;&#31995;&#65288;tgds&#65289;&#21644;&#31561;&#24335;&#29983;&#25104;&#20381;&#36182;&#20851;&#31995;&#65288;egds&#65289;&#20316;&#20026;&#35268;&#21017;&#30340;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#29305;&#27530;&#23454;&#20363;&#30340;&#31561;&#20215;&#31867;&#21644;&#20540;&#38598;&#26469;&#23450;&#20041;KB&#30340;&#35821;&#20041;&#12290;&#30452;&#35266;&#22320;&#65292;&#21069;&#32773;&#25910;&#38598;&#34920;&#31034;&#30456;&#21516;&#29616;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#25152;&#26377;&#23454;&#20307;&#65292;&#32780;&#21518;&#32773;&#25910;&#38598;&#23646;&#24615;&#30340;&#25152;&#26377;&#21487;&#36873;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#24182;&#35268;&#36991;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#27492;&#26032;&#26694;&#26550;&#30340;&#36861;&#36394;&#36807;&#31243;&#65292;&#24182;&#20855;&#26377;&#27704;&#36828;&#19981;&#20250;&#22833;&#36133;&#30340;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#36861;&#36394;&#36807;&#31243;&#32456;&#27490;&#26102;&#65292;&#23427;&#20250;&#29983;&#25104;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#33719;&#21462;&#36830;&#25509;&#26597;&#35810;&#30340;&#30830;&#23450;&#24615;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#36861;&#36394;&#19981;&#32456;&#27490;&#26102;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new framework for combining entity resolution and query answering in knowledge bases (KBs) with tuple-generating dependencies (tgds) and equality-generating dependencies (egds) as rules. We define the semantics of the KB in terms of special instances that involve equivalence classes of entities and sets of values. Intuitively, the former collect all entities denoting the same real-world object, while the latter collect all alternative values for an attribute. This approach allows us to both resolve entities and bypass possible inconsistencies in the data. We then design a chase procedure that is tailored to this new framework and has the feature that it never fails; moreover, when the chase procedure terminates, it produces a universal solution, which in turn can be used to obtain the certain answers to conjunctive queries. We finally discuss challenges arising when the chase does not terminate.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32844;&#19994;&#22260;&#26827;&#36873;&#25163;&#30340;&#31227;&#21160;&#20915;&#31574;&#21457;&#29616;&#65292;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#20154;&#31867;&#24320;&#22987;&#20570;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#26032;&#39062;&#30340;&#20915;&#31574;&#26356;&#39057;&#32321;&#22320;&#21457;&#29983;&#65292;&#21487;&#33021;&#24847;&#21619;&#30528;&#36229;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21487;&#20197;&#25913;&#21464;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07462</link><description>&lt;p&gt;
&#36229;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26032;&#22855;&#24615;&#26469;&#25913;&#36827;&#20154;&#31867;&#20915;&#31574;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Superhuman Artificial Intelligence Can Improve Human Decision Making by Increasing Novelty. (arXiv:2303.07462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32844;&#19994;&#22260;&#26827;&#36873;&#25163;&#30340;&#31227;&#21160;&#20915;&#31574;&#21457;&#29616;&#65292;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#20154;&#31867;&#24320;&#22987;&#20570;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#26032;&#39062;&#30340;&#20915;&#31574;&#26356;&#39057;&#32321;&#22320;&#21457;&#29983;&#65292;&#21487;&#33021;&#24847;&#21619;&#30528;&#36229;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21487;&#20197;&#25913;&#21464;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#20915;&#31574;&#65292;&#24182;&#19988;&#26377;&#21738;&#20123;&#26426;&#21046;&#21487;&#29992;&#20110;&#25903;&#25345;&#36825;&#31181;&#24433;&#21709;&#65311;&#25105;&#20204;&#22312;&#19968;&#20010;&#39046;&#22495;&#20013;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#30340;AI&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#36807;&#21435;71&#24180;&#65288;1950-2021&#65289;&#32844;&#19994;&#22260;&#26827;&#36873;&#25163;&#25152;&#20570;&#30340;&#36229;&#36807;580&#19975;&#20010;&#31227;&#21160;&#20915;&#31574;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#36229;&#20154;&#24037;&#26234;&#33021;&#31243;&#24207;&#26469;&#20272;&#35745;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#20915;&#31574;&#36136;&#37327;&#65292;&#29983;&#25104;&#20102;580&#20159;&#20010;&#21453;&#20107;&#23454;&#30340;&#28216;&#25103;&#27169;&#24335;&#65292;&#24182;&#23558;&#23454;&#38469;&#20154;&#31867;&#20915;&#31574;&#30340;&#32988;&#29575;&#19982;&#21453;&#20107;&#23454;&#30340;AI&#20915;&#31574;&#30340;&#32988;&#29575;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#20154;&#31867;&#24320;&#22987;&#20570;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26102;&#38388;&#19978;&#26816;&#26597;&#20102;&#20154;&#31867;&#29609;&#23478;&#30340;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#26032;&#39062;&#30340;&#20915;&#31574;&#65288;&#21363;&#20197;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#31227;&#21160;&#65289;&#26356;&#39057;&#32321;&#22320;&#21457;&#29983;&#65292;&#24182;&#19982;&#26356;&#39640;&#30340;&#20915;&#31574;&#36136;&#37327;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#20154;&#24037;&#26234;&#33021;&#31243;&#24207;&#30340;&#21457;&#23637;&#21487;&#33021;&#20250;&#25913;&#21464;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
How will superhuman artificial intelligence (AI) affect human decision making? And what will be the mechanisms behind this effect? We address these questions in a domain where AI already exceeds human performance, analyzing more than 5.8 million move decisions made by professional Go players over the past 71 years (1950-2021). To address the first question, we use a superhuman AI program to estimate the quality of human decisions across time, generating 58 billion counterfactual game patterns and comparing the win rates of actual human decisions with those of counterfactual AI decisions. We find that humans began to make significantly better decisions following the advent of superhuman AI. We then examine human players' strategies across time and find that novel decisions (i.e., previously unobserved moves) occurred more frequently and became associated with higher decision quality after the advent of superhuman AI. Our findings suggest that the development of superhuman AI programs ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615; Masking over Masking &#31574;&#30053;&#26469;&#22686;&#24378;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#21270;&#33021;&#21147;&#21644;&#20248;&#21270;&#25928;&#29575;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.07457</link><description>&lt;p&gt;
AMOM: &#36866;&#24212;&#24615; Masking over Masking &#29992;&#20110;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AMOM: Adaptive Masking over Masking for Conditional Masked Language Model. (arXiv:2303.07457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615; Masking over Masking &#31574;&#30053;&#26469;&#22686;&#24378;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#21270;&#33021;&#21147;&#21644;&#20248;&#21270;&#25928;&#29575;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#24050;&#32463;&#22312;&#21508;&#31181;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#65292;&#20294;&#26159;&#25512;&#29702;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#21152;&#36895;&#25512;&#29702;&#38454;&#27573;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#20854;&#20013;&#65292;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411; (CMLM) &#26159;&#26368;&#36890;&#29992;&#30340;&#26694;&#26550;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25903;&#25345;&#35768;&#22810;&#19981;&#21516;&#30340;&#24207;&#21015;&#29983;&#25104;&#22330;&#26223;&#65292;&#24182;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36866;&#24212;&#24615; Masking over Masking &#31574;&#30053;&#26469;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#32454;&#21270;&#33021;&#21147;&#24182;&#20351;&#32534;&#30721;&#22120;&#30340;&#20248;&#21270;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#24635;&#20849; \textbf{15} &#20010;&#25968;&#25454;&#38598;&#19978;&#30340; \textbf{3} &#20010;&#19981;&#21516;&#20219;&#21153;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#65289;&#30340;&#23454;&#39564;&#30830;&#35748;&#65306;&#25105;&#20204;&#25552;&#20986;&#30340;&#31616;&#21333;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-to-sequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference efficiency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frameworks, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. In this paper, we further introduce a simple yet effective adaptive masking over masking strategy to enhance the refinement capability of the decoder and make the encoder optimization easier. Experiments on \textbf{3} different tasks (neural machine translation, summarization, and code generation) with \textbf{15} datasets in total confirm that our proposed simple method achieves significant performance improvement ove
&lt;/p&gt;</description></item><item><title>DRISHTI&#26159;&#19968;&#20010;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#21487;&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#23454;&#29992;&#30340;&#23548;&#33322;&#36741;&#21161;&#65292;&#21253;&#25324;&#26816;&#27979;&#21644;&#35782;&#21035;&#29992;&#25143;&#36335;&#24452;&#21450;&#36335;&#24452;&#21069;&#26041;&#38556;&#30861;&#29289;&#65292;&#24182;&#36890;&#36807;&#38899;&#39057;&#36755;&#20986;&#21578;&#30693;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2303.07451</link><description>&lt;p&gt;
DRISHTI&#65306;&#38754;&#21521;&#35270;&#38556;&#20154;&#22763;&#30340;&#35270;&#35273;&#23548;&#33322;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
DRISHTI: Visual Navigation Assistant for Visually Impaired. (arXiv:2303.07451v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07451
&lt;/p&gt;
&lt;p&gt;
DRISHTI&#26159;&#19968;&#20010;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#21487;&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#23454;&#29992;&#30340;&#23548;&#33322;&#36741;&#21161;&#65292;&#21253;&#25324;&#26816;&#27979;&#21644;&#35782;&#21035;&#29992;&#25143;&#36335;&#24452;&#21450;&#36335;&#24452;&#21069;&#26041;&#38556;&#30861;&#29289;&#65292;&#24182;&#36890;&#36807;&#38899;&#39057;&#36755;&#20986;&#21578;&#30693;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#31038;&#20250;&#20013;&#65292;&#29420;&#31435;&#29983;&#27963;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#30450;&#20154;&#32780;&#35328;&#21364;&#21313;&#20998;&#23616;&#38480;&#12290;&#30450;&#20154;&#21644;&#35270;&#35273;&#38556;&#30861;&#32773;&#65288;BVI&#65289;&#38656;&#35201;&#25163;&#21160;&#25903;&#25345;&#26469;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20302;&#25104;&#26412;&#39640;&#24615;&#33021;&#21487;&#31359;&#25140;&#36741;&#21161;&#35774;&#22791;DRISHTI&#65292;&#23427;&#30001;&#25668;&#20687;&#22836;&#27169;&#22359;&#12289;ESP32&#22788;&#29702;&#22120;&#12289;&#34013;&#29273;&#27169;&#22359;&#12289;&#26234;&#33021;&#25163;&#26426;&#21644;&#25196;&#22768;&#22120;&#32452;&#25104;&#65292;&#26088;&#22312;&#20026;BVI&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#23548;&#33322;&#36741;&#21161;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#21644;&#29702;&#35299;&#29992;&#25143;&#36335;&#24452;&#21644;&#36335;&#24452;&#21069;&#26041;&#38556;&#30861;&#29289;&#30340;&#22823;&#33268;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38899;&#39057;&#36755;&#20986;&#21578;&#30693;BVI&#29992;&#25143;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#33258;&#34892;&#33719;&#21462;&#26041;&#21521;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#19968;&#32452;&#35270;&#35273;&#38556;&#30861;&#32773;&#36523;&#19978;&#27979;&#35797;DRISHTI&#30340;&#21407;&#22411;&#26426;&#65292;&#20197;&#23454;&#29616;&#32463;&#27982;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's society, where independent living is becoming increasingly important, it can be extremely constricting for those who are blind. Blind and visually impaired (BVI) people face challenges because they need manual support to prompt information about their environment. In this work, we took our first step towards developing an affordable and high-performing eye wearable assistive device, DRISHTI, to provide visual navigation assistance for BVI people. This system comprises a camera module, ESP32 processor, Bluetooth module, smartphone and speakers. Using artificial intelligence, this system is proposed to detect and understand the nature of the users' path and obstacles ahead of the user in that path and then inform BVI users about it via audio output to enable them to acquire directions by themselves on their journey. This first step discussed in this paper involves establishing a proof-of-concept of achieving the right balance of affordability and performance by testing an init
&lt;/p&gt;</description></item><item><title>AI&#20262;&#29702;&#23398;&#20851;&#27880;&#26426;&#22120;&#20154;&#38450;&#27490;&#20854;&#23545;&#20154;&#31867;&#34892;&#20026;&#19981;&#24403;&#65292;&#20294;&#23436;&#20840;&#24573;&#35270;&#20102;&#26426;&#22120;&#20154;&#38656;&#35201;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#21019;&#36896;&#32773;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07439</link><description>&lt;p&gt;
&#35770;&#24314;&#36896;&#26377;&#24847;&#35782;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
On the ethics of constructing conscious AI. (arXiv:2303.07439v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07439
&lt;/p&gt;
&lt;p&gt;
AI&#20262;&#29702;&#23398;&#20851;&#27880;&#26426;&#22120;&#20154;&#38450;&#27490;&#20854;&#23545;&#20154;&#31867;&#34892;&#20026;&#19981;&#24403;&#65292;&#20294;&#23436;&#20840;&#24573;&#35270;&#20102;&#26426;&#22120;&#20154;&#38656;&#35201;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#21019;&#36896;&#32773;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#29992;&#20027;&#20041;&#30340;&#36716;&#21521;&#20013;&#65292;AI&#20262;&#29702;&#23398;&#36825;&#19968;&#26032;&#30340;&#23398;&#31185;&#34987;&#20154;&#31867;&#23545;&#20854;&#21019;&#36896;&#29289;&#30340;&#24656;&#24807;&#25152;&#25903;&#37197;&#65292;&#36825;&#31181;&#24656;&#24807;&#22312;&#24191;&#27867;&#19988;&#38271;&#20037;&#30021;&#38144;&#30340;&#25991;&#23398;&#20256;&#32479;&#20013;&#26377;&#25152;&#21453;&#26144;&#12290;&#24343;&#20848;&#32943;&#26031;&#22374;&#30340;&#24618;&#29289;&#22312;&#29595;&#20029;&#183;&#38634;&#33713;&#30340;&#23567;&#35828;&#20013;&#21453;&#25239;&#20182;&#30340;&#21019;&#36896;&#32773;&#65307;H.&#33713;&#32500;&#20811;&#30340;1920&#24180;&#30340;&#25103;&#21095;&#20013;&#30340;&#24322;&#31471;&#39764;&#20687;&#22312;&#29378;&#26292;&#20013;&#30340;&#34892;&#21160;&#65307;&#21345;&#38647;&#23572;&#183;&#22855;&#20329;&#20811;&#65288;Karel &#268;apek&#65289;&#30340;&#21467;&#36870;&#26426;&#22120;&#20154;--&#36825;&#20123;&#20197;&#21450;&#20854;&#20182;&#25968;&#30334;&#20010;&#31867;&#20284;&#30340;&#20363;&#23376;&#26159;AI&#20262;&#29702;&#23398;&#20851;&#27880;&#26426;&#22120;&#20154;&#38450;&#27490;&#20854;&#23545;&#20154;&#31867;&#34892;&#20026;&#19981;&#24403;&#30340;&#32972;&#26223;&#12290;&#22312;&#36825;&#19977;&#20010;&#34394;&#26500;&#26696;&#20363;&#20013;&#65288;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#26696;&#20363;&#20013;&#65289;&#65292;&#36825;&#20010;&#21487;&#24604;&#30340;&#20154;&#36896;&#29289;&#8212;&#8212;&#26080;&#24773;&#22320;&#34987;&#21093;&#21066;&#12289;&#34987;&#19968;&#20010;&#26432;&#20154;&#30340;&#26292;&#24466;&#36924;&#21040;&#32477;&#36335;&#12289;&#20026;&#20102;&#33258;&#21355;&#32780;&#34987;&#36843;&#37319;&#21462;&#26292;&#21147;&#8212;&#8212;&#25317;&#26377;&#20854;&#20316;&#32773;&#30340;&#21516;&#24773;&#12290;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#26497;&#23569;&#25968;&#20363;&#22806;&#24773;&#20917;&#19979;&#65292;AI&#20262;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#23436;&#20840;&#26080;&#35270;&#26426;&#22120;&#20154;&#38656;&#35201;&#20445;&#25252;&#20854;&#21019;&#36896;&#32773;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In its pragmatic turn, the new discipline of AI ethics came to be dominated by humanity's collective fear of its creatures, as reflected in an extensive and perennially popular literary tradition. Dr. Frankenstein's monster in the novel by Mary Shelley rising against its creator; the unorthodox golem in H. Leivick's 1920 play going on a rampage; the rebellious robots of Karel \v{C}apek -- these and hundreds of other examples of the genre are the background against which the preoccupation of AI ethics with preventing robots from behaving badly towards people is best understood. In each of these three fictional cases (as well as in many others), the miserable artificial creature -- mercilessly exploited, or cornered by a murderous mob, and driven to violence in self-defense -- has its author's sympathy. In real life, with very few exceptions, things are different: theorists working on the ethics of AI completely ignore the possibility of robots needing protection from their creators. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PO-ST-DIM&#65292;&#25913;&#36827;&#20102;ST-DIM&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.07437</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#21487;&#35266;&#27979;Atari&#28216;&#25103;&#20013;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning in Partially Observable Atari Games. (arXiv:2303.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PO-ST-DIM&#65292;&#25913;&#36827;&#20102;ST-DIM&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#29615;&#22659;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#22312;&#20197;&#21069;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#27604;&#29983;&#25104;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24847;&#35782;&#21040;&#25513;&#34109;&#22270;&#20687;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;&#25513;&#27169;&#20316;&#20026;&#22686;&#24378;&#25216;&#26415;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#21033;&#29992;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20180;&#32454;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#23436;&#20840;&#21487;&#35266;&#23519;&#29615;&#22659;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#21019;&#24314;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#26089;&#26399;&#30340;Atari 2600&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#35780;&#20272;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#28145;&#24230;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;ST-DIM&#65289;&#30340;&#23545;&#27604;&#26041;&#27861;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#19981;&#22914;&#20854;&#30417;&#30563;&#23545;&#24212;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;ST-DIM&#65288;PO-ST-DIM&#65289;&#65292;&#36890;&#36807;&#21512;&#24182;&#37096;&#20998;&#21487;&#35266;&#27979;&#26041;&#26696;&#26469;&#25913;&#36827;&#23545;&#27604;&#26041;&#27861;&#12290;PO-ST-DIM&#20248;&#20110;ST-DIM&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
State representation learning aims to capture latent factors of an environment. Contrastive methods have performed better than generative models in previous state representation learning research. Although some researchers realize the connections between masked image modeling and contrastive representation learning, the effort is focused on using masks as an augmentation technique to represent the latent generative factors better. Partially observable environments in reinforcement learning have not yet been carefully studied using unsupervised state representation learning methods.  In this article, we create an unsupervised state representation learning scheme for partially observable states. We conducted our experiment on a previous Atari 2600 framework designed to evaluate representation learning models. A contrastive method called Spatiotemporal DeepInfomax (ST-DIM) has shown state-of-the-art performance on this benchmark but remains inferior to its supervised counterpart. Our appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#31639;&#27861;&#37197;&#32622;&#20197;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#20010;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20195;&#34920;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#33258;&#21160;&#26816;&#27979;&#12290;&#36825;&#23545;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#31639;&#27861;&#37197;&#32622;&#20855;&#26377;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2303.07434</link><description>&lt;p&gt;
&#22810;&#31639;&#27861;&#37197;&#32622;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering Multiple Algorithm Configurations. (arXiv:2303.07434v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#31639;&#27861;&#37197;&#32622;&#20197;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#20010;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20195;&#34920;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#33258;&#21160;&#26816;&#27979;&#12290;&#36825;&#23545;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#31639;&#27861;&#37197;&#32622;&#20855;&#26377;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#20154;&#20174;&#19994;&#32773;&#32463;&#24120;&#20381;&#36182;&#32463;&#20856;&#30340;&#25163;&#24037;&#35774;&#35745;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#19968;&#32452;&#21253;&#21547;&#20856;&#22411;&#37096;&#32626;&#26465;&#20214;&#30340;&#27880;&#37322;&#31034;&#20363;&#26469;&#20248;&#21270;&#12290;&#36825;&#31181;&#35774;&#32622;&#30340;&#33258;&#21160;&#35843;&#25972;&#31216;&#20026;&#31639;&#27861;&#37197;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#37197;&#32622;&#25193;&#23637;&#21040;&#33258;&#21160;&#21457;&#29616;&#35843;&#25972;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#20010;&#27169;&#24335;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#37197;&#32622;&#27169;&#24335;&#20195;&#34920;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#33258;&#21160;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#24335;&#21457;&#29616;&#26041;&#27861;&#65306;&#21518;&#22788;&#29702;&#26041;&#27861;&#12289;&#22810;&#38454;&#27573;&#26041;&#27861;&#21644;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#31435;&#20307;&#28145;&#24230;&#20272;&#35745;&#12289;&#21487;&#24494;&#20998;&#28210;&#26579;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#35270;&#35273;&#27979;&#36317;&#65289;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#22312;&#21512;&#25104;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#31639;&#27861;&#37197;&#32622;&#20013;&#26816;&#27979;&#21040;&#22810;&#20010;&#27169;&#24335;&#30340;&#26126;&#26174;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practitioners in robotics regularly depend on classic, hand-designed algorithms. Often the performance of these algorithms is tuned across a dataset of annotated examples which represent typical deployment conditions. Automatic tuning of these settings is traditionally known as algorithm configuration. In this work, we extend algorithm configuration to automatically discover multiple modes in the tuning dataset. Unlike prior work, these configuration modes represent multiple dataset instances and are detected automatically during the course of optimization. We propose three methods for mode discovery: a post hoc method, a multi-stage method, and an online algorithm using a multi-armed bandit. Our results characterize these methods on synthetic test functions and in multiple robotics application domains: stereoscopic depth estimation, differentiable rendering, motion planning, and visual odometry. We show the clear benefits of detecting multiple modes in algorithm configuration spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32925;&#33039;&#32593;&#26684;&#37325;&#24314;&#27169;&#22411;&#65292;&#21487;&#23454;&#26102;&#29983;&#25104;&#32925;&#33039;&#19977;&#35282;&#24418;&#24418;&#29366;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#21363;&#26102;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#25512;&#26029;&#32593;&#26684;&#24418;&#29366;&#21644;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2303.07432</link><description>&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#35270;&#35282;&#32925;&#33039;&#32593;&#26684;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
End-to-end Deformable Attention Graph Neural Network for Single-view Liver Mesh Reconstruction. (arXiv:2303.07432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32925;&#33039;&#32593;&#26684;&#37325;&#24314;&#27169;&#22411;&#65292;&#21487;&#23454;&#26102;&#29983;&#25104;&#32925;&#33039;&#19977;&#35282;&#24418;&#24418;&#29366;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#21363;&#26102;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#25512;&#26029;&#32593;&#26684;&#24418;&#29366;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32959;&#30244;&#25918;&#30103;&#26159;&#27835;&#30103;&#30284;&#30151;&#24739;&#32773;&#30340;&#26368;&#24120;&#35265;&#26041;&#24335;&#20043;&#19968;&#65292;&#28982;&#32780;&#65292;&#31934;&#30830;&#30340;&#27835;&#30103;&#20132;&#20184;&#23545;&#26469;&#33258;&#33258;&#30001;&#21628;&#21560;&#30340;&#19981;&#21516;&#36816;&#21160;&#27169;&#24335;&#30340;&#32771;&#34385;&#26159;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26681;&#25454;&#22312;&#26415;&#21069;&#38454;&#27573;&#33719;&#24471;&#30340;&#21442;&#32771;&#20998;&#21106;&#21644;&#27835;&#30103;&#36807;&#31243;&#20013;&#33719;&#21462;&#30340;&#20108;&#32500;MRI&#20896;&#29366;&#20999;&#29255;&#65292;&#23454;&#26102;&#29983;&#25104;&#32925;&#33039;&#30340;&#19977;&#35282;&#24418;&#24418;&#29366;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#65292;&#24182;&#33021;&#25429;&#33719;&#38750;&#27431;&#20960;&#37324;&#24503;&#22495;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21453;&#65292;&#23427;&#23436;&#20840;&#20197;&#32593;&#26684;&#32467;&#26500;&#20135;&#29983;&#24418;&#29366;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#22270;&#20687;&#27491;&#30830;&#25512;&#26029;&#32593;&#26684;&#24418;&#29366;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#21363;&#26102;&#26041;&#27861;&#26469;&#20351;&#32925;&#33039;&#32593;&#26684;&#39030;&#28857;&#19982;&#33719;&#21462;&#30340;&#20108;&#32500;&#22270;&#20687;&#23545;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensity modulated radiotherapy (IMRT) is one of the most common modalities for treating cancer patients. One of the biggest challenges is precise treatment delivery that accounts for varying motion patterns originating from free-breathing. Currently, image-guided solutions for IMRT is limited to 2D guidance due to the complexity of 3D tracking solutions. We propose a novel end-to-end attention graph neural network model that generates in real-time a triangular shape of the liver based on a reference segmentation obtained at the preoperative phase and a 2D MRI coronal slice taken during the treatment. Graph neural networks work directly with graph data and can capture hidden patterns in non-Euclidean domains. Furthermore, contrary to existing methods, it produces the shape entirely in a mesh structure and correctly infers mesh shape and position based on a surrogate image. We define two on-the-fly approaches to make the correspondence of liver mesh vertices with 2D images obtained dur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26497;&#22320;&#36965;&#24863;&#20912;&#23618;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;Polar-VQA&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24378;&#35843;VQA&#22312;&#20912;&#23618;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23545;&#29616;&#26377;VQA&#26041;&#27861;&#22312;Polar-VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#32447;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.07403</link><description>&lt;p&gt;
&#26497;&#22320;-VQA: &#26497;&#22320;&#21306;&#22495;&#36965;&#24863;&#20912;&#23618;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region. (arXiv:2303.07403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26497;&#22320;&#36965;&#24863;&#20912;&#23618;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;Polar-VQA&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24378;&#35843;VQA&#22312;&#20912;&#23618;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23545;&#29616;&#26377;VQA&#26041;&#27861;&#22312;Polar-VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#32447;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20912;&#24029;&#23398;&#23478;&#26469;&#35828;&#65292;&#30740;&#31350;&#26497;&#22320;&#30340;&#20912;&#23618;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#20174;&#20912;&#24029;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20272;&#35745;&#20912;&#23618;&#21402;&#24230;&#65292;&#39044;&#27979;&#26410;&#26469;&#20960;&#24180;&#30340;&#20912;&#31215;&#32047;&#31561;&#65289;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#25506;&#32034;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31185;&#23398;&#23478;&#21487;&#20197;&#36890;&#36807;&#23545;&#22270;&#20687;&#25552;&#38382;&#33719;&#21462;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#26497;&#22320;&#36965;&#24863;&#20912;&#23618;&#22270;&#20687;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;VQA&#25968;&#25454;&#38598;Polar-VQA&#12290;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#22270;&#20687;&#37117;&#26159;&#20351;&#29992;&#22235;&#31181;&#31867;&#22411;&#30340;&#31354;&#20013;&#38647;&#36798;&#25910;&#38598;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20984;&#26174;&#22312;&#20912;&#23618;&#30740;&#31350;&#20013;VQA&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;Polar-VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#29616;&#26377;VQA&#26041;&#27861;&#30340;&#22522;&#32447;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
For glaciologists, studying ice sheets from the polar regions is critical. With the advancement of deep learning techniques, we can now extract high-level information from the ice sheet data (e.g., estimating the ice layer thickness, predicting the ice accumulation for upcoming years, etc.). However, a vision-based conversational deep learning approach has not been explored yet, where scientists can get information by asking questions about images. In this paper, we have introduced the task of Visual Question Answering (VQA) on remote-sensed ice sheet imagery. To study, we have presented a unique VQA dataset, Polar-VQA, in this study. All the images in this dataset were collected using four types of airborne radars. The main objective of this research is to highlight the importance of VQA in the context of ice sheet research and conduct a baseline study of existing VQA approaches on Polar-VQA dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26102;&#24207;&#31354;&#38388;&#32593;&#32476;&#30340;&#30896;&#25758;&#36991;&#20813;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20851;&#32852;&#36755;&#20837;&#22270;&#20687;&#30340;&#21306;&#22495;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#30896;&#25758;&#36991;&#20813;&#12290;</title><link>http://arxiv.org/abs/2303.07352</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#22522;&#20110;&#26102;&#24207;&#31354;&#38388;&#32593;&#32476;&#30340;&#30896;&#25758;&#36991;&#20813;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sequential Spatial Network for Collision Avoidance in Autonomous Driving. (arXiv:2303.07352v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26102;&#24207;&#31354;&#38388;&#32593;&#32476;&#30340;&#30896;&#25758;&#36991;&#20813;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20851;&#32852;&#36755;&#20837;&#22270;&#20687;&#30340;&#21306;&#22495;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#30896;&#25758;&#36991;&#20813;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#30896;&#25758;&#36991;&#20813;&#26041;&#38754;&#65292;&#24050;&#32463;&#24212;&#29992;&#20102;&#22810;&#31181;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#36890;&#36807;&#35843;&#25972;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#36991;&#20813;&#19982;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#30340;&#20132;&#21449;&#25110;&#37325;&#21472;&#65292;&#20197;&#23454;&#29616;&#30896;&#25758;&#36991;&#20813;&#30340;&#30446;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#30001;&#20110;&#22238;&#24402;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#22320;&#20851;&#32852;&#36755;&#20837;&#22270;&#20687;&#30340;&#21306;&#22495;&#29305;&#24449;&#32780;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26102;&#24207;&#31354;&#38388;&#32593;&#32476;&#30340;&#30896;&#25758;&#36991;&#20813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several autonomous driving strategies have been applied to autonomous vehicles, especially in the collision avoidance area. The purpose of collision avoidance is achieved by adjusting the trajectory of autonomous vehicles (AV) to avoid intersection or overlap with the trajectory of surrounding vehicles. A large number of sophisticated vision algorithms have been designed for target inspection, classification, and other tasks, such as ResNet, YOLO, etc., which have achieved excellent performance in vision tasks because of their ability to accurately and quickly capture regional features. However, due to the variability of different tasks, the above models achieve good performance in capturing small regions but are still insufficient in correlating the regional features of the input image with each other. In this paper, we aim to solve this problem and develop an algorithm that takes into account the advantages of CNN in capturing regional features while establishing feature correlation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#39030;&#28857;&#25193;&#25955;&#27169;&#22411;(PVD)&#65292;&#29992;&#20110;&#32479;&#19968;&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#39640;&#32500;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#29983;&#25104;&#39640;&#32500;&#39030;&#28857;&#24207;&#21015;&#23481;&#26131;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#20110;&#39030;&#28857;&#25968;&#37327;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#23545;&#35937;&#36718;&#24275;&#21305;&#37197;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.07216</link><description>&lt;p&gt;
&#24182;&#34892;&#39030;&#28857;&#25193;&#25955;&#29992;&#20110;&#32479;&#19968;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Parallel Vertex Diffusion for Unified Visual Grounding. (arXiv:2303.07216v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#39030;&#28857;&#25193;&#25955;&#27169;&#22411;(PVD)&#65292;&#29992;&#20110;&#32479;&#19968;&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#39640;&#32500;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#29983;&#25104;&#39640;&#32500;&#39030;&#28857;&#24207;&#21015;&#23481;&#26131;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#20110;&#39030;&#28857;&#25968;&#37327;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#23545;&#35937;&#36718;&#24275;&#21305;&#37197;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#30340;&#35270;&#35273;&#23450;&#20301;&#36861;&#27714;&#19968;&#31181;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#25216;&#26415;&#36335;&#32447;&#65292;&#20197;&#21033;&#29992;&#22810;&#20219;&#21153;&#25968;&#25454;&#65292;&#20943;&#23569;&#20219;&#21153;&#29305;&#23450;&#30340;&#35774;&#35745;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#26694;&#21644;&#25513;&#30721;&#20316;&#20026;&#39030;&#28857;&#24207;&#21015;&#21576;&#29616;&#65292;&#20197;&#24314;&#27169;&#24341;&#29992;&#26816;&#27979;&#21644;&#20998;&#21106;&#20316;&#20026;&#33258;&#22238;&#24402;&#39034;&#24207;&#39030;&#28857;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#39034;&#24207;&#29983;&#25104;&#39640;&#32500;&#39030;&#28857;&#24207;&#21015;&#23481;&#26131;&#20986;&#38169;&#65292;&#22240;&#20026;&#24207;&#21015;&#30340;&#19978;&#28216;&#20173;&#20445;&#25345;&#38745;&#24577;&#65292;&#24182;&#19988;&#26080;&#27861;&#22522;&#20110;&#19979;&#28216;&#39030;&#28857;&#20449;&#24687;&#36827;&#34892;&#31934;&#32454;&#21270;&#30340;&#25913;&#36827;&#65292;&#21363;&#20351;&#23384;&#22312;&#37325;&#22823;&#30340;&#20301;&#32622;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#39030;&#28857;&#25968;&#37327;&#26377;&#38480;&#65292;&#23545;&#35937;&#20855;&#26377;&#22797;&#26434;&#36718;&#24275;&#30340;&#36739;&#24046;&#25311;&#21512;&#38480;&#21046;&#20102;&#24615;&#33021;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#25193;&#23637;&#30340;&#24182;&#34892;&#39030;&#28857;&#29983;&#25104;&#33539;&#20363;&#65292;&#21482;&#38656;&#20462;&#25913;&#22122;&#22768;&#32500;&#24230;&#21363;&#21487;&#12290;&#25105;&#20204;&#33539;&#20363;&#30340;&#30452;&#35266;&#23454;&#29616;&#26159;&#24182;&#34892;&#39030;&#28857;&#25193;&#25955; (PVD)&#65292;&#30452;&#25509;&#23558;&#39030;&#28857;&#22352;&#26631;&#35774;&#32622;&#20026;&#29983;&#25104;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unified visual grounding pursues a simple and generic technical route to leverage multi-task data with less task-specific design. The most advanced methods typically present boxes and masks as vertex sequences to model referring detection and segmentation as an autoregressive sequential vertex generation paradigm. However, generating high-dimensional vertex sequences sequentially is error-prone because the upstream of the sequence remains static and cannot be refined based on downstream vertex information, even if there is a significant location gap. Besides, with limited vertexes, the inferior fitting of objects with complex contours restricts the performance upper bound. To deal with this dilemma, we propose a parallel vertex generation paradigm for superior high-dimension scalability with a diffusion model by simply modifying the noise dimension. An intuitive materialization of our paradigm is Parallel Vertex Diffusion (PVD) to directly set vertex coordinates as the generation targe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#20803;&#36827;&#21270;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026; NeuroFS&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27966;&#29983;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#20855;&#26377;&#26368;&#39640;&#25490;&#21517;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.07200</link><description>&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#36827;&#21270;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks. (arXiv:2303.07200v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#20803;&#36827;&#21270;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026; NeuroFS&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27966;&#29983;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#20855;&#26377;&#26368;&#39640;&#25490;&#21517;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#20174;&#25968;&#25454;&#20013;&#36873;&#25321;&#20449;&#24687;&#37327;&#39640;&#30340;&#21464;&#37327;&#23376;&#38598;&#65292;&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#32780;&#19988;&#21487;&#20197;&#20943;&#36731;&#36164;&#28304;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#24212;&#29992;&#20110;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#26102;&#20250;&#21463;&#21040;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#36827;&#21270;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#36164;&#28304;&#26377;&#25928;&#30340;&#30417;&#30563;&#24335;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026; "NeuroFS"&#12290;&#36890;&#36807;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#20462;&#21098;&#26080;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;NeuroFS&#26377;&#25928;&#22320;&#27966;&#29983;&#20986;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#36890;&#36807;&#23545; $11$ &#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20302;&#32500;&#21644;&#39640;&#32500;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; NeuroFS &#22312;&#32771;&#34385;&#21040;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#26102;&#20855;&#26377;&#26368;&#39640;&#30340;&#25490;&#21517;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention on feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named \enquote{NeuroFS}. By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on $11$ low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. The code 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;&#22312;&#36229;&#36234;&#23454;&#26102;&#25110;&#38750;&#24120;&#32531;&#24930;&#30340;&#25805;&#20316;&#20013;&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#8220;&#39044;&#20808;&#25511;&#21046;&#25351;&#20196;&#8221;&#26694;&#26550;&#30340;&#8220;&#33258;&#20027;&#21629;&#20196;&#8221;&#65292;&#26469;&#23454;&#29616;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#30340;&#38382;&#36131;&#21644;&#36131;&#20219;&#25152;&#38656;&#30340;&#28145;&#24605;&#29087;&#34385;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.06813</link><description>&lt;p&gt;
&#24847;&#20041;&#28145;&#36828;&#30340;&#20154;&#31867;&#25351;&#20196;&#65306;&#20316;&#20026;&#23454;&#29616;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#30340;&#26041;&#27861;&#30340;&#39640;&#32423;&#25511;&#21046;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Meaningful human command: Advance control directives as a method to enable moral and legal responsibility for autonomous weapons systems. (arXiv:2303.06813v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;&#22312;&#36229;&#36234;&#23454;&#26102;&#25110;&#38750;&#24120;&#32531;&#24930;&#30340;&#25805;&#20316;&#20013;&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#8220;&#39044;&#20808;&#25511;&#21046;&#25351;&#20196;&#8221;&#26694;&#26550;&#30340;&#8220;&#33258;&#20027;&#21629;&#20196;&#8221;&#65292;&#26469;&#23454;&#29616;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#30340;&#38382;&#36131;&#21644;&#36131;&#20219;&#25152;&#38656;&#30340;&#28145;&#24605;&#29087;&#34385;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#25112;&#20105;&#30340;&#36895;&#24230;&#27491;&#22312;&#21152;&#24555;&#65292;&#24120;&#35268;&#37096;&#38431;&#19982;&#22823;&#35268;&#27169;&#20351;&#29992;&#33258;&#20027;&#31995;&#32479;&#21644;&#20154;&#26426;&#38598;&#25104;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#20154;&#31867;&#22914;&#20309;&#30830;&#20445;&#22312;&#27491;&#24120;&#26102;&#38388;&#21442;&#25968;&#20043;&#22806;&#36816;&#34892;&#30340;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#12290;&#26412;&#31456;&#32771;&#34385;&#20102;&#20154;&#31867;&#26159;&#21542;&#21487;&#20197;&#31449;&#22312;&#23454;&#26102;&#20043;&#22806;&#65292;&#24182;&#36890;&#36807;&#20808;&#24314;&#31435;&#21512;&#21516;&#25480;&#26435;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#21160;&#65292;&#22312;&#26410;&#26469;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26159;&#22312;&#36229;&#36234;&#23454;&#26102;&#25110;&#38750;&#24120;&#32531;&#24930;&#30340;&#25805;&#20316;&#20013;&#65292;&#20154;&#31867;&#30340;&#24847;&#35782;&#21644;&#38598;&#20013;&#21147;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#30693;&#24773;&#12290;&#22312;&#8220;&#39044;&#20808;&#21307;&#30103;&#27861;&#24459;&#20808;&#20363;&#8221;&#20013;&#25214;&#21040;&#30340;&#32463;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#8220;&#39044;&#20808;&#25511;&#21046;&#25351;&#20196;&#8221;&#65288;ACD&#65289;&#21487;&#20197;&#23454;&#29616;&#27494;&#22120;&#31995;&#32479;&#30340;&#38382;&#36131;&#21644;&#36131;&#20219;&#25152;&#38656;&#30340;&#32791;&#26102;&#12289;&#28145;&#24605;&#29087;&#34385;&#30340;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#20027;&#21629;&#20196;&#8221;&#30340;&#26500;&#24819;&#65292;&#24182;&#36890;&#36807;ACD&#30340;&#26500;&#24314;&#21644;&#27861;&#24459;&#20262;&#29702;&#26694;&#26550;&#36827;&#34892;&#25903;&#25745;&#21644;&#21512;&#27861;&#21270;&#12290;&#36825;&#23558;&#20351;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#30340;&#25509;&#21463;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#24314;&#31435;&#24847;&#20041;&#28145;&#36828;&#30340;&#20154;&#31867;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
21st Century war is increasing in speed, with conventional forces combined with massed use of autonomous systems and human-machine integration. However, a significant challenge is how humans can ensure moral and legal responsibility for systems operating outside of normal temporal parameters. This chapter considers whether humans can stand outside of real time and authorise actions for autonomous systems by the prior establishment of a contract, for actions to occur in a future context particularly in faster than real time or in very slow operations where human consciousness and concentration could not remain well informed. The medical legal precdent found in 'advance care directives' suggests how the time-consuming, deliberative process required for accountability and responsibility of weapons systems may be achievable outside real time captured in an 'advance control driective' (ACD). The chapter proposes 'autonomy command' scaffolded and legitimised through the construction of ACD a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#20844;&#24335;&#21644;&#24320;&#25918;&#24335;&#35268;&#21010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#24110;&#21161;&#35299;&#20915;&#33258;&#21160;&#35843;&#25972;&#35268;&#21010;&#20869;&#37096;&#21442;&#25968;&#30340;&#25361;&#25112;&#21644;&#32570;&#20047;&#32479;&#19968;&#38382;&#39064;&#23450;&#20041;&#21644;&#36719;&#20214;&#26694;&#26550;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06768</link><description>&lt;p&gt;
&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#65306;&#20844;&#24335;&#21644;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Planner Optimization Problem: Formulations and Frameworks. (arXiv:2303.06768v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#20844;&#24335;&#21644;&#24320;&#25918;&#24335;&#35268;&#21010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#24110;&#21161;&#35299;&#20915;&#33258;&#21160;&#35843;&#25972;&#35268;&#21010;&#20869;&#37096;&#21442;&#25968;&#30340;&#25361;&#25112;&#21644;&#32570;&#20047;&#32479;&#19968;&#38382;&#39064;&#23450;&#20041;&#21644;&#36719;&#20214;&#26694;&#26550;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#35268;&#21010;&#30340;&#20869;&#37096;&#21442;&#25968;&#23545;&#20110;&#26368;&#22823;&#21270;&#35268;&#21010;&#22120;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#35843;&#25972;&#38024;&#23545;&#38382;&#39064;&#23454;&#20363;&#30340;&#20869;&#37096;&#21442;&#25968;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#38598;&#20013;&#20110;&#23398;&#20064;&#35268;&#21010;&#21442;&#25968;&#29983;&#25104;&#22120;&#65292;&#20294;&#32570;&#20047;&#19968;&#33268;&#30340;&#38382;&#39064;&#23450;&#20041;&#21644;&#36719;&#20214;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#65288;POP&#65289;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#25918;&#24335;&#35268;&#21010;&#20248;&#21270;&#26694;&#26550;&#65288;OPOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#21487;&#37325;&#29992;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying internal parameters for planning is crucial to maximizing the performance of a planner. However, automatically tuning internal parameters which are conditioned on the problem instance is especially challenging. A recent line of work focuses on learning planning parameter generators, but lack a consistent problem definition and software framework. This work proposes the unified planner optimization problem (POP) formulation, along with the Open Planner Optimization Framework (OPOF), a highly extensible software framework to specify and to solve these problems in a reusable manner.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21322;&#33258;&#20027;&#20195;&#29702;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#22312;&#23545;&#20219;&#21153;&#25104;&#21151;&#32467;&#26524;&#30340;&#20449;&#24515;&#20302;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#65292;&#26377;&#25928;&#38477;&#20302;&#19987;&#23478;&#35843;&#29992;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06710</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20026;&#20154;-&#26426;&#21327;&#21516;&#26426;&#22120;&#20154;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning. (arXiv:2303.06710v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21322;&#33258;&#20027;&#20195;&#29702;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#22312;&#23545;&#20219;&#21153;&#25104;&#21151;&#32467;&#26524;&#30340;&#20449;&#24515;&#20302;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#65292;&#26377;&#25928;&#38477;&#20302;&#19987;&#23478;&#35843;&#29992;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;-&#26426;&#21327;&#21516;&#33539;&#24335;&#20013;&#65292;&#26426;&#22120;&#20154;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#22823;&#37096;&#20998;&#26102;&#38388;&#20869;&#33258;&#20027;&#23436;&#25104;&#20219;&#21153;&#65292;&#24403;&#38656;&#35201;&#24110;&#21161;&#26102;&#65292;&#21487;&#20197;&#21521;&#22806;&#37096;&#19987;&#23478;&#23547;&#27714;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#20851;&#38190;&#26159;&#35201;&#30693;&#36947;&#20309;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#65306;&#22826;&#23569;&#30340;&#35831;&#27714;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#29359;&#38169;&#65292;&#20294;&#22826;&#22810;&#30340;&#35831;&#27714;&#20250;&#20351;&#19987;&#23478;&#36807;&#36733;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21322;&#33258;&#20027;&#20195;&#29702;&#22312;&#23545;&#20219;&#21153;&#25104;&#21151;&#32467;&#26524;&#30340;&#20449;&#24515;&#20302;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#12290;&#32622;&#20449;&#24230;&#26159;&#36890;&#36807;&#20272;&#35745;&#24403;&#21069;&#29366;&#24577;&#30340;&#22238;&#25253;&#26041;&#24046;&#26469;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20272;&#35745;&#21487;&#20197;&#36890;&#36807;&#31867;&#20284;&#36125;&#23572;&#26364;&#36882;&#24402;&#30340;&#35757;&#32451;&#36807;&#31243;&#36880;&#27493;&#25913;&#36827;&#12290;&#22312;&#20855;&#26377;&#23436;&#20840;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29366;&#24577;&#20449;&#24687;&#30340;&#31163;&#25955;&#23548;&#33322;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#21033;&#29992;&#26377;&#38480;&#30340;&#19987;&#23478;&#35843;&#29992;&#39044;&#31639;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#35775;&#38382;&#19987;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#65292;&#40723;&#21169;&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2303.06430</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#25991;&#26412;&#21327;&#20316;&#20219;&#21153;&#20013;&#20132;&#20114;&#35774;&#35745;&#31354;&#38388;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks. (arXiv:2303.06430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#65292;&#40723;&#21169;&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a spectrum of content generation tasks and their corresponding human-AI interaction patterns, encouraging the research community to focus on more complex and interdependent tasks that require greater levels of human involvement.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26410;&#26469;&#20197;&#21450;&#20154;&#31867;&#22914;&#20309;&#19982;LLMs&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#65306;1&#65289;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#22266;&#23450;&#33539;&#22260;&#20869;&#23481;&#31574;&#21010;&#20219;&#21153;&#65292;2&#65289;&#20855;&#26377;&#31934;&#30830;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#29420;&#31435;&#21019;&#24847;&#20219;&#21153;&#65292;&#20197;&#21450;3&#65289;&#20855;&#26377;&#36845;&#20195;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#22797;&#26434;&#19988;&#30456;&#20114;&#20381;&#36182;&#30340;&#21019;&#24847;&#20219;&#21153;&#12290;&#25105;&#20204;&#40723;&#21169;&#29983;&#25104;AI&#21644;HCI&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs. In this paper, we present a spectrum of content generation tasks and their corresponding human-AI interaction patterns. These tasks include: 1) fixed-scope content curation tasks with minimal human-AI interactions, 2) independent creative tasks with precise human-AI interactions, and 3) complex and interdependent creative tasks with iterative human-AI interactions. We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
&lt;/p&gt;</description></item><item><title>HiNet&#26159;&#19968;&#31181;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#32593;&#32476;&#65292;&#20855;&#26377;&#23618;&#27425;&#20449;&#24687;&#25552;&#21462;&#21644;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#65292;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06095</link><description>&lt;p&gt;
HiNet: &#19968;&#31181;&#20855;&#26377;&#23618;&#27425;&#20449;&#24687;&#25552;&#21462;&#30340;&#26032;&#22411;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HiNet: Novel Multi-Scenario &amp; Multi-Task Learning with Hierarchical Information Extraction. (arXiv:2303.06095v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06095
&lt;/p&gt;
&lt;p&gt;
HiNet&#26159;&#19968;&#31181;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#32593;&#32476;&#65292;&#20855;&#26377;&#23618;&#27425;&#20449;&#24687;&#25552;&#21462;&#21644;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#65292;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#30340;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#31181;&#26377;&#25928;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;&#26159;&#22312;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#22810;&#22330;&#26223;&#36801;&#31227;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20197;&#23558;&#25152;&#26377;&#20449;&#24687;&#25237;&#24433;&#21040;&#21516;&#19968;&#29305;&#24449;&#31354;&#38388;&#20026;&#30446;&#26631;&#30340;MoE&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22330;&#26223;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#25512;&#33616;&#30340;Hierarchical information extraction Network&#65288;HiNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22522;&#20110;&#20174;&#31895;&#21040;&#32454;&#30340;&#30693;&#35782;&#36716;&#31227;&#26041;&#26696;&#23454;&#29616;&#20998;&#23618;&#25552;&#21462;&#12290;&#20998;&#23618;&#32593;&#32476;&#30340;&#22810;&#20010;&#25552;&#21462;&#23618;&#20351;&#27169;&#22411;&#33021;&#22815;&#22686;&#24378;&#36328;&#22330;&#26223;&#20256;&#36882;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#22330;&#26223;&#21644;&#20219;&#21153;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#20219;&#21153;&#30456;&#20851;&#24615;&#21644;&#22330;&#26223;&#29420;&#29305;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;HiNet&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-scenario &amp; multi-task learning has been widely applied to many recommendation systems in industrial applications, wherein an effective and practical approach is to carry out multi-scenario transfer learning on the basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based method, which aims to project all information in the same feature space, cannot effectively deal with the complex relationships inherent among various scenarios and tasks, resulting in unsatisfactory performance. To tackle the problem, we propose a Hierarchical information extraction Network (HiNet) for multi-scenario and multi-task recommendation, which achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme. The multiple extraction layers of the hierarchical network enable the model to enhance the capability of transferring valuable information across scenarios while preserving specific features of scenarios and tasks. Furthermore, a novel scenario-aware attentive netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdaOFUL&#21644;VARA&#20004;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#26377;&#38480;&#26041;&#24046;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;AdaOFUL&#20855;&#26377;&#29366;&#24577;-of-the-art&#30340;&#36951;&#25022;&#30028;&#65292;VARA&#36798;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.05606</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37325;&#23614;&#22870;&#21169;&#26041;&#24046;&#24863;&#30693;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards. (arXiv:2303.05606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdaOFUL&#21644;VARA&#20004;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#26377;&#38480;&#26041;&#24046;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;AdaOFUL&#20855;&#26377;&#29366;&#24577;-of-the-art&#30340;&#36951;&#25022;&#30028;&#65292;VARA&#36798;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;AdaOFUL&#21644;VARA&#65292;&#29992;&#20110;&#22312;&#20165;&#23384;&#22312;&#26377;&#38480;&#26041;&#24046;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#12290;&#23545;&#20110;&#32447;&#24615;&#38543;&#26426;&#36172;&#24466;&#65292;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#33258;&#36866;&#24212;Huber&#22238;&#24402;&#24182;&#25552;&#20986;AdaOFUL&#26469;&#35299;&#20915;&#37325;&#23614;&#22870;&#21169;&#38382;&#39064;&#12290;AdaOFUL&#36798;&#21040;&#20102;&#29366;&#24577;-of-the-art&#30340;&#36951;&#25022;&#30028;&#65292;&#21363;$ \widetilde{O}\big&#65288;d\big(\sum_{t=1}^T\nu_{t}^2\big)^{1/2}+d\big)$&#65292;&#20854;&#20013;$\nu_{t}^2$&#26159;&#31532;$t$&#36718;&#22870;&#21169;&#35266;&#27979;&#21040;&#30340;&#26465;&#20214;&#26041;&#24046;&#65292;$d$&#26159;&#29305;&#24449;&#32500;&#24230;&#65292;$\widetilde{O}&#65288;\cdot&#65289;$ &#38544;&#34255;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;&#22312;AdaOFUL&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VARA&#29992;&#20110;&#32447;&#24615;MDP&#65292;&#23427;&#36798;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#65292;&#21363; $ \widetilde{O}(d\sqrt{HG^*K})$&#12290;&#36825;&#37324;&#65292;$H$&#26159;&#20107;&#20214;&#30340;&#38271;&#24230;&#65292;$K$&#26159;&#20107;&#20214;&#30340;&#25968;&#37327;&#65292;$G^*$&#26159;&#36739;&#23567;&#30340;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#37327;&#65292;&#24403;&#22312;&#20854;&#20182;&#23454;&#20363;&#30456;&#20851;&#37327;&#34987;&#38480;&#21046;&#26102;&#65292;&#23427;&#21487;&#20197;&#34987;&#36793;&#30028;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two algorithms, AdaOFUL and VARA, for online sequential decision-making in the presence of heavy-tailed rewards with only finite variances. For linear stochastic bandits, we address the issue of heavy-tailed rewards by modifying the adaptive Huber regression and proposing AdaOFUL. AdaOFUL achieves a state-of-the-art regret bound of $\widetilde{O}\big(d\big(\sum_{t=1}^T \nu_{t}^2\big)^{1/2}+d\big)$ as if the rewards were uniformly bounded, where $\nu_{t}^2$ is the observed conditional variance of the reward at round $t$, $d$ is the feature dimension, and $\widetilde{O}(\cdot)$ hides logarithmic dependence. Building upon AdaOFUL, we propose VARA for linear MDPs, which achieves a tighter variance-aware regret bound of $\widetilde{O}(d\sqrt{HG^*K})$. Here, $H$ is the length of episodes, $K$ is the number of episodes, and $G^*$ is a smaller instance-dependent quantity that can be bounded by other instance-dependent quantities when additional structural conditions on the 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepSeaProbLog&#30340;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#23558;&#28145;&#24230;&#27010;&#29575;&#32534;&#31243;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#65292;&#25903;&#25345;&#22312;&#36923;&#36753;&#32422;&#26463;&#26465;&#20214;&#19979;&#25512;&#26029;&#21644;&#23398;&#20064;&#31163;&#25955;&#21644;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.04660</link><description>&lt;p&gt;
&#31163;&#25955;-&#36830;&#32493;&#22495;&#20013;&#30340;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Probabilistic Logic Programming in Discrete-Continuous Domains. (arXiv:2303.04660v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04660
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepSeaProbLog&#30340;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#23558;&#28145;&#24230;&#27010;&#29575;&#32534;&#31243;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#65292;&#25903;&#25345;&#22312;&#36923;&#36753;&#32422;&#26463;&#26465;&#20214;&#19979;&#25512;&#26029;&#21644;&#23398;&#20064;&#31163;&#25955;&#21644;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#36923;&#36753;&#24418;&#24335;&#30340;&#31526;&#21495;&#32972;&#26223;&#30693;&#35782;&#12290;NeSy&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#33539;&#22260;&#20869;&#26377;&#21161;&#20110;&#23398;&#20064;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#25512;&#26029;&#38750;&#20998;&#24067;&#25968;&#25454;&#12290;&#27010;&#29575;NeSy&#30528;&#37325;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#36923;&#36753;&#21644;&#27010;&#29575;&#35770;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#36824;&#20801;&#35768;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#23398;&#20064;&#12290;&#24403;&#21069;&#27010;&#29575;NeSy&#31995;&#32479;&#65288;&#22914;DeepProbLog&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#23616;&#38480;&#20110;&#26377;&#38480;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28145;&#24230;&#27010;&#29575;&#32534;&#31243;&#65288;DPP&#65289;&#22312;&#24314;&#27169;&#21644;&#20248;&#21270;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepSeaProbLog&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#23558;DPP&#25216;&#26415;&#32435;&#20837;NeSy&#20013;&#12290;&#36825;&#26679;&#20570;&#30340;&#32467;&#26524;&#26159;&#22312;&#36923;&#36753;&#32422;&#26463;&#26465;&#20214;&#19979;&#25903;&#25345;&#31163;&#25955;&#21644;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#30340;&#25512;&#26029;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;1&#65289;DeepSeaProbLog&#30340;&#35821;&#20041;&#65292;2&#65289;&#23427;&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;3&#65289;&#23427;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;4&#65289;&#19968;&#32452;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#39046;&#22495;&#30340;&#24314;&#27169;&#12289;&#25512;&#26029;&#21644;&#23398;&#20064;&#26041;&#38754;&#30456;&#23545;&#20110;&#24403;&#21069;NeSy&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepS
&lt;/p&gt;</description></item><item><title>Dish-TS&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31995;&#25968;&#32593;&#32476;&#65288;CONET&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#20998;&#24067;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#23558;Lookback&#31383;&#21475;&#20316;&#20026;&#36755;&#20837;&#31354;&#38388;&#65292;Horizon&#31383;&#21475;&#20316;&#20026;&#36755;&#20986;&#31354;&#38388;&#65292;&#23558;&#20998;&#24067;&#20559;&#31227;&#24635;&#32467;&#20026;&#20869;&#37096;&#31354;&#38388;&#20559;&#31227;&#21644;&#19981;&#21516;&#31354;&#38388;&#20559;&#31227;&#20004;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.14829</link><description>&lt;p&gt;
Dish-TS: &#19968;&#31181;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#36890;&#29992;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting. (arXiv:2302.14829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14829
&lt;/p&gt;
&lt;p&gt;
Dish-TS&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31995;&#25968;&#32593;&#32476;&#65288;CONET&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#20998;&#24067;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#23558;Lookback&#31383;&#21475;&#20316;&#20026;&#36755;&#20837;&#31354;&#38388;&#65292;Horizon&#31383;&#21475;&#20316;&#20026;&#36755;&#20986;&#31354;&#38388;&#65292;&#23558;&#20998;&#24067;&#20559;&#31227;&#24635;&#32467;&#20026;&#20869;&#37096;&#31354;&#38388;&#20559;&#31227;&#21644;&#19981;&#21516;&#31354;&#38388;&#20559;&#31227;&#20004;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#25351;&#30340;&#26159;&#26102;&#38388;&#24207;&#21015;&#22312;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23427;&#24456;&#22823;&#31243;&#24230;&#19978;&#38459;&#30861;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#20559;&#31227;&#30340;&#30740;&#31350;&#22823;&#22810;&#23616;&#38480;&#20110;&#20998;&#24067;&#37327;&#21270;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24573;&#35270;&#20102;Lookback&#21644;Horizon&#20043;&#38388;&#30340;&#28508;&#22312;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#24635;&#32467;&#20026;&#20004;&#31867;&#12290;&#23558;Lookback&#31383;&#21475;&#35270;&#20026;&#36755;&#20837;&#31354;&#38388;&#65292;Horizon&#31383;&#21475;&#35270;&#20026;&#36755;&#20986;&#31354;&#38388;&#65292;&#23384;&#22312;(i)&#20869;&#37096;&#31354;&#38388;&#20559;&#31227;&#65292;&#21363;&#22312;&#36755;&#20837;&#31354;&#38388;&#20869;&#30340;&#20998;&#24067;&#38543;&#26102;&#38388;&#20445;&#25345;&#20559;&#31227;&#65292;&#20197;&#21450;(ii)&#19981;&#21516;&#31354;&#38388;&#20559;&#31227;&#65292;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Dish-TS&#30340;&#36890;&#29992;&#31070;&#32463;&#27169;&#22411;&#26469;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20272;&#35745;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31995;&#25968;&#32593;&#32476;&#65288;CONET&#65289;&#65292;&#23427;&#21487;&#20197;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26144;&#23556;&#36755;&#20837;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution shift in Time Series Forecasting (TSF), indicating series distribution changes over time, largely hinders the performance of TSF models. Existing works towards distribution shift in time series are mostly limited in the quantification of distribution and, more importantly, overlook the potential shift between lookback and horizon windows. To address above challenges, we systematically summarize the distribution shift in TSF into two categories. Regarding lookback windows as input-space and horizon windows as output-space, there exist (i) intra-space shift, that the distribution within the input-space keeps shifted over time, and (ii) inter-space shift, that the distribution is shifted between input-space and output-space. Then we introduce, Dish-TS, a general neural paradigm for alleviating distribution shift in TSF. Specifically, for better distribution estimation, we propose the coefficient net (CONET), which can be any neural architectures, to map input sequences in
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#23567;&#30340;&#29702;&#35770;&#20219;&#21153;&#25913;&#21160;&#19978;&#23481;&#26131;&#22833;&#36133;&#65292;&#34920;&#26126;&#22312;&#30452;&#35273;&#24515;&#29702;&#23398;&#27169;&#22411;&#35780;&#20272;&#20013;&#38656;&#35201;&#25345;&#24576;&#30097;&#24577;&#24230;&#65292;&#19988;&#22833;&#36133;&#26696;&#20363;&#24212;&#34987;&#37325;&#35270;&#12290;</title><link>http://arxiv.org/abs/2302.08399</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; Theory-of-Mind &#20219;&#21153;&#30340;&#24494;&#23567;&#25913;&#21464;&#19978;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08399
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#23567;&#30340;&#29702;&#35770;&#20219;&#21153;&#25913;&#21160;&#19978;&#23481;&#26131;&#22833;&#36133;&#65292;&#34920;&#26126;&#22312;&#30452;&#35273;&#24515;&#29702;&#23398;&#27169;&#22411;&#35780;&#20272;&#20013;&#38656;&#35201;&#25345;&#24576;&#30097;&#24577;&#24230;&#65292;&#19988;&#22833;&#36133;&#26696;&#20363;&#24212;&#34987;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#35273;&#24515;&#29702;&#23398;&#26159;&#24120;&#35782;&#25512;&#29702;&#30340;&#25903;&#26609;&#12290;&#22312;&#26426;&#22120;&#26234;&#33021;&#20013;&#22797;&#21046;&#36825;&#31181;&#25512;&#29702;&#26159;&#36808;&#21521;&#31867;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#22522;&#30707;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#39033;&#20219;&#21153;&#21644;&#22522;&#20934;&#29992;&#20110;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#25512;&#29702;&#65292;&#29305;&#21035;&#20851;&#27880;&#24515;&#28789;&#29702;&#35770;&#20219;&#21153;&#20013;&#30340;&#20449;&#24565;&#24402;&#23646;&#12290;&#36825;&#20123;&#20219;&#21153;&#26082;&#26377;&#25104;&#21151;&#26696;&#20363;&#20063;&#26377;&#22833;&#36133;&#26696;&#20363;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;&#26368;&#36817;&#22768;&#31216;&#30340;&#25104;&#21151;&#26696;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#32500;&#25345;ToM&#21407;&#21017;&#30340;&#23567;&#24133;&#21464;&#21270;&#20351;&#32467;&#26524;&#22823;&#30456;&#24452;&#24237;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#30452;&#35273;&#24515;&#29702;&#23398;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#38646;&#20551;&#35774;&#24212;&#35813;&#25345;&#24576;&#30097;&#24577;&#24230;&#65292;&#24182;&#19988;&#31163;&#32676;&#25925;&#38556;&#26696;&#20363;&#24212;&#35813;&#36229;&#36807;&#24179;&#22343;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#24378;&#22823;&#30340;LLM&#65288;Large-Large Models&#65289;&#22312;&#29702;&#35299;&#24515;&#29702;&#23398;&#20219;&#21153;&#19978;&#21487;&#33021;&#21462;&#24471;&#30340;&#26410;&#26469;&#25104;&#21151;&#23545;&#20154;&#31867;ToM&#20219;&#21153;&#24847;&#21619;&#30528;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#19968;&#20123;&#38750;&#32463;&#20856;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#30005;&#36335;&#65292;&#24182;&#36890;&#36807;&#37327;&#23376;&#27010;&#29575;&#12289;&#35282;&#24230;&#21644;&#23376;&#31354;&#38388;&#31561;&#26041;&#24335;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#21644;&#23427;&#20204;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#21644;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.03012</link><description>&lt;p&gt;
&#29992;&#20110;&#35748;&#30693;&#20915;&#31574;&#30340;&#37327;&#23376;&#30005;&#36335;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Components for Cognitive Decision-Making. (arXiv:2302.03012v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19968;&#20123;&#38750;&#32463;&#20856;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#30005;&#36335;&#65292;&#24182;&#36890;&#36807;&#37327;&#23376;&#27010;&#29575;&#12289;&#35282;&#24230;&#21644;&#23376;&#31354;&#38388;&#31561;&#26041;&#24335;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#21644;&#23427;&#20204;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#34920;&#26126;&#65292;&#19968;&#20123;&#38750;&#32463;&#20856;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#20316;&#20026;&#30005;&#36335;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#12290;&#33258;20&#19990;&#32426;60&#24180;&#20195;&#20197;&#26469;&#65292;&#35768;&#22810;&#35266;&#23519;&#21040;&#30340;&#35748;&#30693;&#34892;&#20026;&#24050;&#34987;&#35777;&#26126;&#36829;&#21453;&#20102;&#22522;&#20110;&#32463;&#20856;&#27010;&#29575;&#21644;&#38598;&#21512;&#35770;&#30340;&#35268;&#21017;&#12290;&#20363;&#22914;&#65292;&#38382;&#39064;&#25552;&#20986;&#30340;&#39034;&#24207;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#26159;&#21542;&#22238;&#31572;&#8220;&#26159;&#8221;&#25110;&#8220;&#21542;&#8221;&#65292;&#22240;&#27492;&#22238;&#31572;&#20004;&#20010;&#38382;&#39064;&#8220;&#26159;&#8221;&#30340;&#20154;&#21475;&#19981;&#33021;&#34987;&#24314;&#27169;&#20026;&#20004;&#20010;&#22266;&#23450;&#38598;&#21512;&#30340;&#20132;&#38598;&#12290;&#28982;&#32780;&#65292;&#23427;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19981;&#21516;&#39034;&#24207;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#25237;&#24433;&#12290;&#36825;&#21644;&#20854;&#20182;&#31034;&#20363;&#24050;&#32463;&#25104;&#21151;&#22320;&#20351;&#29992;&#20102;&#37327;&#23376;&#27010;&#29575;&#26469;&#25551;&#36848;&#65292;&#36825;&#20381;&#36182;&#20110;&#27604;&#36739;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#35282;&#24230;&#65292;&#32780;&#19981;&#26159;&#23376;&#38598;&#20043;&#38388;&#30340;&#20307;&#31215;&#12290;&#29616;&#22312;&#22312;2020&#24180;&#21021;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#24050;&#32463;&#36798;&#21040;&#20102;&#19968;&#23450;&#27700;&#24179;&#65292;&#19968;&#20123;&#37327;&#23376;&#35748;&#30693;&#27169;&#22411;&#21487;&#20197;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#24182;&#30740;&#31350;&#65292;&#23558;&#24515;&#29702;&#29366;&#24577;&#34920;&#31034;&#20026;qubit&#23492;&#23384;&#22120;&#20013;&#30340;&#29366;&#24577;&#65292;&#35748;&#30693;&#25805;&#20316;&#21644;&#20915;&#31574;&#25805;&#20316;&#36890;&#36807;&#37327;&#23376;&#38376;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates that some nonclassical models of human decision-making can be run successfully as circuits on quantum computers. Since the 1960s, many observed cognitive behaviors have been shown to violate rules based on classical probability and set theory. For example, the order in which questions are posed affects whether participants answer 'yes' or 'no', so the population that answers `yes' to both questions cannot be modeled as the intersection of two fixed sets. It can however be modeled as a sequence of projections carried out in different orders. This and other examples have been described successfully using quantum probability, which relies on comparing angles between subspaces rather than volumes between subsets. Now in the early 2020s, quantum computers have reached the point where some of these quantum cognitive models can be implemented and investigated on quantum hardware, representing the mental states in qubit registers, and the cognitive operations and decisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2302.02601</link><description>&lt;p&gt;
&#23398;&#20064;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#36229;&#36234;&#38142;&#25509;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#24050;&#30693;&#20107;&#23454;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#20165;&#32771;&#34385;&#23454;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#32771;&#34385;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26356;&#39640;&#32423;&#30340;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;$\langle T_1$, PrerequisiteFor, $T_2\rangle$&#65292;&#20854;&#20013;PrerequisiteFor&#26159;&#26356;&#39640;&#32423;&#21035;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#30001;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;BiVE&#36890;&#36807;&#32771;&#34385;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#19977;&#20803;&#32452;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#22686;&#24378;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#65292;&#21517;&#20026;STGN&#65292;&#29992;&#20110;&#20869;&#23481;&#27969;&#34892;&#24230;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#34920;&#38754;&#25299;&#25169;&#32467;&#26500;&#32972;&#21518;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#26102;&#38388;&#32858;&#21512;&#26426;&#21046;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;STGN&#22312;&#20869;&#23481;&#27969;&#34892;&#24230;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#26399;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.12355</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#22686;&#24378;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#29992;&#20110;&#20869;&#23481;&#27969;&#34892;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantics-enhanced Temporal Graph Networks for Content Popularity Prediction. (arXiv:2301.12355v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#22686;&#24378;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#65292;&#21517;&#20026;STGN&#65292;&#29992;&#20110;&#20869;&#23481;&#27969;&#34892;&#24230;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#34920;&#38754;&#25299;&#25169;&#32467;&#26500;&#32972;&#21518;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#26102;&#38388;&#32858;&#21512;&#26426;&#21046;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;STGN&#22312;&#20869;&#23481;&#27969;&#34892;&#24230;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#26399;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#35270;&#39057;&#27969;&#26381;&#21153;&#21644;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20363;&#22914;GPT&#65292;&#24847;&#21619;&#30528;&#20114;&#32852;&#32593;&#27969;&#37327;&#30340;&#24040;&#22823;&#22686;&#38271;&#12290;&#20026;&#20102;&#32531;&#35299;&#27969;&#37327;&#21387;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20855;&#26377;&#32593;&#32476;&#20013;&#23384;&#20648;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#22312;&#38752;&#36817;&#29992;&#25143;&#30340;&#35774;&#22791;&#19978;&#32531;&#23384;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#30456;&#24212;&#22320;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#32531;&#23384;&#21033;&#29992;&#29575;&#65292;&#24517;&#39035;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#27969;&#34892;&#24230;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#20351;&#29992;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;(DGNN)&#27169;&#22411;&#39044;&#27979;&#27969;&#34892;&#24230;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;DGNN&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#22788;&#29702;&#22823;&#37096;&#20998;&#29992;&#25143;&#22788;&#20110;&#38750;&#27963;&#21160;&#29366;&#24577;&#30340;&#31232;&#30095;&#25968;&#25454;&#38598;&#26102;&#30340;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#38761;&#24615;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#65292;&#31216;&#20026;&#35821;&#20041;&#22686;&#24378;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;(STGN)&#65292;&#23427;&#23558;&#39069;&#22806;&#30340;&#35821;&#20041;&#20449;&#24687;&#38468;&#21152;&#21040;&#29992;&#25143; - &#20869;&#23481;&#20108;&#20998;&#22270;&#20013;&#65292;&#24182;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#34920;&#38754;&#25299;&#25169;&#32467;&#26500;&#32972;&#21518;&#30340;&#38544;&#21547;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#24182;&#32467;&#21512;&#29992;&#25143;&#20559;&#22909;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;STGN&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#20869;&#23481;&#27969;&#34892;&#24230;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36828;&#26399;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surging demand for high-definition video streaming services and large neural network models (e.g., Generative Pre-trained Transformer, GPT) implies a tremendous explosion of Internet traffic. To mitigate the traffic pressure, architectures with in-network storage have been proposed to cache popular contents at devices in closer proximity to users. Correspondingly, in order to maximize caching utilization, it becomes essential to devise an effective popularity prediction method. In that regard, predicting popularity with dynamic graph neural network (DGNN) models achieve remarkable performance. However, DGNN models still suffer from tackling sparse datasets where most users are inactive. Therefore, we propose a reformative temporal graph network, named semantics-enhanced temporal graph network (STGN), which attaches extra semantic information into the user-content bipartite graph and could better leverage implicit relationships behind the superficial topology structure. On top of th
&lt;/p&gt;</description></item><item><title>SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;</title><link>http://arxiv.org/abs/2301.07074</link><description>&lt;p&gt;
SegViz&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07074
&lt;/p&gt;
&lt;p&gt;
SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22810;&#20010;&#19979;&#28216;&#20020;&#24202;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#25163;&#21160;&#27880;&#37322;&#26159;&#32791;&#26102;&#30340;&#12289;&#38656;&#35201;&#39640;&#25216;&#33021;&#30340;&#12289;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;3D&#22270;&#20687;&#12290;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#22810;&#20010;&#32452;&#30340;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#30693;&#35782;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegViz&#65292;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#23558;SegViz&#30340;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#38598;&#20013;&#32858;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#30340;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#23545;1&#20159;&#27963;&#36291;&#29992;&#25143;&#30340;&#26381;&#21153;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.00503</link><description>&lt;p&gt;
&#25903;&#20184;&#23453;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#30340;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#30340;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#23545;1&#20159;&#27963;&#36291;&#29992;&#25143;&#30340;&#26381;&#21153;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#30340;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#24050;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#37096;&#32626;&#65292;&#20026;&#36229;&#36807;1&#20159;&#27963;&#36291;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;AlipayKG&#65292;&#29992;&#20110;&#26174;&#24335;&#22320;&#25551;&#36848;&#29992;&#25143;&#24847;&#22270;&#30340;&#31163;&#32447;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#12289;&#20016;&#23500;&#30340;&#20869;&#23481;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#19987;&#23478;&#35268;&#21017;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#25512;&#26029;&#22312;&#32447;&#29992;&#25143;&#30340;&#19979;&#19968;&#27493;&#24847;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.
&lt;/p&gt;</description></item><item><title>SceneRF&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#32467;&#21512;NeRF&#30340;&#36752;&#23556;&#22330;&#25216;&#26415;&#65292;&#26080;&#38656;&#28145;&#24230;&#30417;&#30563;&#65292;&#21482;&#38656;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#22330;&#26223;&#65292;&#33021;&#22815;&#29983;&#25104;&#26032;&#30340;&#28145;&#24230;&#35270;&#22270;&#24182;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#65292;&#24615;&#33021;&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.02501</link><description>&lt;p&gt;
SceneRF: &#21033;&#29992;&#36752;&#23556;&#22330;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;3D&#22330;&#26223;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields. (arXiv:2212.02501v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02501
&lt;/p&gt;
&lt;p&gt;
SceneRF&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#32467;&#21512;NeRF&#30340;&#36752;&#23556;&#22330;&#25216;&#26415;&#65292;&#26080;&#38656;&#28145;&#24230;&#30417;&#30563;&#65292;&#21482;&#38656;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#22330;&#26223;&#65292;&#33021;&#22815;&#29983;&#25104;&#26032;&#30340;&#28145;&#24230;&#35270;&#22270;&#24182;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#65292;&#24615;&#33021;&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#30340;&#30740;&#31350;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#28145;&#24230;&#30417;&#30563;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#23569;&#23545;&#26114;&#36149;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SceneRF&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#22330;&#26223;&#37325;&#24314;&#26041;&#27861;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;(NeRF)&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#19968;&#20010;&#36752;&#23556;&#22330;&#65292;&#24182;&#37319;&#29992;&#20102;&#26174;&#24335;&#28145;&#24230;&#20248;&#21270;&#21644;&#26032;&#39062;&#30340;&#27010;&#29575;&#37319;&#26679;&#31574;&#30053;&#26469;&#26377;&#25928;&#22788;&#29702;&#22823;&#22330;&#26223;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#21482;&#38656;&#36755;&#20837;&#21333;&#20010;&#22270;&#20687;&#21363;&#21487;&#29983;&#25104;&#26032;&#30340;&#28145;&#24230;&#35270;&#22270;&#65292;&#24182;&#23558;&#20854;&#34701;&#21512;&#22312;&#19968;&#36215;&#20197;&#33719;&#24471;3D&#22330;&#26223;&#37325;&#24314;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#23460;&#20869;BundleFusion&#21644;&#23460;&#22806;SemanticKITTI&#22330;&#26223;&#19979;&#65292;&#24615;&#33021;&#20248;&#20110;&#26368;&#36817;&#30340;&#25152;&#26377;&#22522;&#32447;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#26032;&#35270;&#35282;&#28145;&#24230;&#21512;&#25104;&#21644;&#22330;&#26223;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://astra-vision.github.io/SceneRF&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D reconstruction from 2D image was extensively studied, training with depth supervision. To relax the dependence to costly-acquired datasets, we propose SceneRF, a self-supervised monocular scene reconstruction method using only posed image sequences for training. Fueled by the recent progress in neural radiance fields (NeRF) we optimize a radiance field though with explicit depth optimization and a novel probabilistic sampling strategy to efficiently handle large scenes. At inference, a single input image suffices to hallucinate novel depth views which are fused together to obtain 3D scene reconstruction. Thorough experiments demonstrate that we outperform all recent baselines for novel depth views synthesis and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI. Our code is available at https://astra-vision.github.io/SceneRF.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14769</link><description>&lt;p&gt;
&#21521;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65311;&#24314;&#31435;&#25308;&#21344;&#24237;&#40065;&#26834;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#36890;&#36807;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#65288;&#21363;&#19981;&#21516;&#29615;&#22659;&#65289;&#20013;&#20445;&#25345;&#25968;&#25454;&#26469;&#20445;&#25252;&#20010;&#20154;&#35270;&#35273;&#29615;&#22659;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#19979;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#23545;&#26381;&#21153;&#22120;&#26159;&#19981;&#21487;&#35775;&#38382;&#30340;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#36731;&#26131;&#22320;&#27745;&#26579;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#22312;&#19981;&#34987;&#36890;&#30693;&#30340;&#24773;&#20917;&#19979;&#22312;&#20195;&#29702;&#20154;&#20013;&#24314;&#31435;&#21518;&#38376;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#20250;&#23545;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#21361;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#21518;&#38376;&#25805;&#32437;&#20195;&#29702;&#20154;&#36827;&#34892;&#23548;&#33322;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#20013;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#38656;&#35201;&#36319;&#38543;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#23460;&#20869;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21363;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#65292;&#20854;&#20013;&#24694;&#24847;&#23458;&#25143;&#31471;&#36890;&#36807;&#25805;&#32437;&#26412;&#22320;&#36712;&#36857;&#25968;&#25454;&#26469;&#21521;&#20840;&#23616;&#27169;&#22411;&#26893;&#20837;&#21518;&#38376;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NAW&#21487;&#20197;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#32780;&#19988;&#24615;&#33021;&#19979;&#38477;&#24494;&#19981;&#36275;&#36947;&#12290;&#20026;&#20102;&#38450;&#27490;NAW&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#27010;&#24565;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#22312;VLN&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Resu
&lt;/p&gt;</description></item><item><title>Grad-StyleSpeech&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#21457;&#35328;&#20154;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;&#65292;&#20934;&#30830;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#39118;&#26684;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09383</link><description>&lt;p&gt;
Grad-StyleSpeech: &#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#21457;&#35328;&#20154;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Grad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with Diffusion Models. (arXiv:2211.09383v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09383
&lt;/p&gt;
&lt;p&gt;
Grad-StyleSpeech&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#21457;&#35328;&#20154;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;&#65292;&#20934;&#30830;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#39118;&#26684;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#20219;&#24847;&#21457;&#35328;&#20154;&#33258;&#36866;&#24212;TTS&#26041;&#27861;&#30001;&#20110;&#22312;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#39118;&#26684;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#19981;&#20339;&#32780;&#34920;&#29616;&#20986;&#19981;&#23613;&#20154;&#24847;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Grad-StyleSpeech&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#20960;&#31186;&#38047;&#30340;&#21442;&#32771;&#35821;&#38899;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#38750;&#24120;&#33258;&#28982;&#19988;&#19982;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#26497;&#20854;&#30456;&#20284;&#30340;&#35821;&#38899;&#12290;Grad-StyleSpeech&#22312;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#36817;&#30340;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;TTS&#22522;&#32447;&#12290;&#22768;&#38899;&#26679;&#20363;&#21487;&#22312;https://nardien.github.io/grad-stylespeech-demo&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a significant progress in Text-To-Speech (TTS) synthesis technology in recent years, thanks to the advancement in neural generative modeling. However, existing methods on any-speaker adaptive TTS have achieved unsatisfactory performance, due to their suboptimal accuracy in mimicking the target speakers' styles. In this work, we present Grad-StyleSpeech, which is an any-speaker adaptive TTS framework that is based on a diffusion model that can generate highly natural speech with extremely high similarity to target speakers' voice, given a few seconds of reference speech. Grad-StyleSpeech significantly outperforms recent speaker-adaptive TTS baselines on English benchmarks. Audio samples are available at https://nardien.github.io/grad-stylespeech-demo.
&lt;/p&gt;</description></item><item><title>PhaseAug &#26159;&#19968;&#31181;&#21487;&#24494;&#30340;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#26059;&#36716;&#27599;&#20010;&#39057;&#29575;&#31665;&#30340;&#30456;&#20301;&#65292;&#27169;&#25311;&#19968;&#23545;&#22810;&#26144;&#23556;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35757;&#32451;&#20013;&#20986;&#29616;&#21608;&#26399;&#24615;&#20266;&#24433;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#26080;&#38656;&#20219;&#20309;&#26550;&#26500;&#20462;&#25913;&#23601;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.04610</link><description>&lt;p&gt;
PhaseAug: &#19968;&#31181;&#21487;&#24494;&#30340;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29992;&#20110;&#27169;&#25311;&#19968;&#23545;&#22810;&#26144;&#23556;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
PhaseAug: A Differentiable Augmentation for Speech Synthesis to Simulate One-to-Many Mapping. (arXiv:2211.04610v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04610
&lt;/p&gt;
&lt;p&gt;
PhaseAug &#26159;&#19968;&#31181;&#21487;&#24494;&#30340;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#26059;&#36716;&#27599;&#20010;&#39057;&#29575;&#31665;&#30340;&#30456;&#20301;&#65292;&#27169;&#25311;&#19968;&#23545;&#22810;&#26144;&#23556;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35757;&#32451;&#20013;&#20986;&#29616;&#21608;&#26399;&#24615;&#20266;&#24433;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#26080;&#38656;&#20219;&#20309;&#26550;&#26500;&#20462;&#25913;&#23601;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22522;&#30784;&#22768;&#30721;&#22120;&#34987;&#35757;&#32451;&#25104;&#20174;&#23545;&#24212;&#30340;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#30830;&#20999;&#30340;&#35821;&#38899;&#27874;&#24418;&#65292;&#32780;&#19981;&#32771;&#34385;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#19968;&#23545;&#22810;&#20851;&#31995;&#12290;&#36825;&#31181;&#20256;&#32479;&#35757;&#32451;&#20250;&#23548;&#33268;&#29983;&#25104;&#35821;&#38899;&#20449;&#21495;&#20013;&#20986;&#29616;&#21608;&#26399;&#24615;&#20266;&#24433;&#65292;&#20351;&#24471;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#37117;&#21457;&#29983;&#20102;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PhaseAug&#8212;&#8212;&#31532;&#19968;&#31181;&#19981;&#21516;iable &#30340;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;&#36890;&#36807;&#26059;&#36716;&#27599;&#20010;&#39057;&#29575;&#31665;&#30340;&#30456;&#20301;&#65292;&#27169;&#25311;&#19968;&#23545;&#22810;&#26144;&#23556;&#20851;&#31995;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#20219;&#20309;&#26550;&#26500;&#20462;&#25913;&#23601;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312; https://github.com/mindslab-ai/phaseaug &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous generative adversarial network (GAN)-based neural vocoders are trained to reconstruct the exact ground truth waveform from the paired mel-spectrogram and do not consider the one-to-many relationship of speech synthesis. This conventional training causes overfitting for both the discriminators and the generator, leading to the periodicity artifacts in the generated audio signal. In this work, we present PhaseAug, the first differentiable augmentation for speech synthesis that rotates the phase of each frequency bin to simulate one-to-many mapping. With our proposed method, we outperform baselines without any architecture modification. Code and audio samples will be available at https://github.com/mindslab-ai/phaseaug.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;(ARD)&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#20013;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#20851;&#31995;&#20197;&#21450;&#26631;&#35760;&#26032;&#20851;&#31995;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24120;&#35268;&#21644;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.04215</link><description>&lt;p&gt;
&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;&#65306;&#36890;&#21521;&#36890;&#29992;&#21644;&#26631;&#31614;&#24863;&#30693;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Active Relation Discovery: Towards General and Label-aware Open Relation Extraction. (arXiv:2211.04215v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;(ARD)&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#20013;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#20851;&#31995;&#20197;&#21450;&#26631;&#35760;&#26032;&#20851;&#31995;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24120;&#35268;&#21644;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;(OpenRE)&#26088;&#22312;&#21457;&#29616;&#24320;&#25918;&#39046;&#22495;&#30340;&#26032;&#20851;&#31995;&#12290;&#20197;&#21069;&#30340;OpenRE&#26041;&#27861;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;(1)&#26080;&#27861;&#20805;&#20998;&#21306;&#20998;&#24050;&#30693;&#20851;&#31995;&#21644;&#26032;&#20851;&#31995;&#12290;&#24403;&#23558;&#24120;&#35268;&#27979;&#35797;&#35774;&#32622;&#25193;&#23637;&#21040;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#26102;&#65292;&#20854;&#20013;&#27979;&#35797;&#25968;&#25454;&#21487;&#33021;&#20063;&#26469;&#33258;&#24050;&#30693;&#31867;&#21035;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;(2)&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#24517;&#39035;&#36827;&#34892;&#20108;&#27425;&#26631;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20026;&#26032;&#20851;&#31995;&#26631;&#35760;&#20154;&#31867;&#21487;&#35835;&#21644;&#26377;&#24847;&#20041;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;(ARD)&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20851;&#31995;&#24322;&#24120;&#20540;&#26816;&#27979;&#26469;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#20851;&#31995;&#65292;&#24182;&#28041;&#21450;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#26631;&#35760;&#26032;&#20851;&#31995;&#12290;&#23545;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;ARD&#22312;&#24120;&#35268;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Relation Extraction (OpenRE) aims to discover novel relations from open domains. Previous OpenRE methods mainly suffer from two problems: (1) Insufficient capacity to discriminate between known and novel relations. When extending conventional test settings to a more general setting where test data might also come from seen classes, existing approaches have a significant performance decline. (2) Secondary labeling must be performed before practical application. Existing methods cannot label human-readable and meaningful types for novel relations, which is urgently required by the downstream tasks. To address these issues, we propose the Active Relation Discovery (ARD) framework, which utilizes relational outlier detection for discriminating known and novel relations and involves active learning for labeling novel relations. Extensive experiments on three real-world datasets show that ARD significantly outperforms previous state-of-the-art methods on both conventional and our propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25216;&#33021;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#23618;&#27425;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.14685</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#30340;&#28436;&#31034;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Demonstrations with Latent Space Priors. (arXiv:2210.14685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25216;&#33021;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#23618;&#27425;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#29366;&#24577;&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20855;&#26377;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#25216;&#33021;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#26469;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#20174;&#19968;&#20010;&#23398;&#20064;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;&#24320;&#22987;&#65292;&#25105;&#20204;&#20998;&#21035;&#35757;&#32451;&#28436;&#31034;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#20302;&#23618;&#31574;&#30053;&#12290;&#24207;&#21015;&#27169;&#22411;&#24418;&#25104;&#20102;&#28508;&#22312;&#31354;&#38388;&#23545;&#21512;&#29702;&#30340;&#28436;&#31034;&#34892;&#20026;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#21152;&#36895;&#39640;&#23618;&#27425;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#20165;&#29366;&#24577;&#30340;&#36816;&#21160;&#25429;&#25417;&#28436;&#31034;&#20013;&#33719;&#21462;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#31181;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#36716;&#31227;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#22312;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#26041;&#38754;&#37117;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#20855;&#26377;&#22797;&#26434;&#12289;&#27169;&#25311;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#25361;&#25112;&#24615;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations provide insight into relevant state or action space regions, bearing great potential to boost the efficiency and practicality of reinforcement learning agents. In this work, we propose to leverage demonstration datasets by combining skill learning and sequence modeling. Starting with a learned joint latent space, we separately train a generative model of demonstration sequences and an accompanying low-level policy. The sequence model forms a latent space prior over plausible demonstration behaviors to accelerate learning of high-level policies. We show how to acquire such priors from state-only motion capture demonstrations and explore several methods for integrating them into policy learning on transfer tasks. Our experimental results confirm that latent space priors provide significant gains in learning speed and final performance. We benchmark our approach on a set of challenging sparse-reward environments with a complex, simulated humanoid, and on offline RL benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.02390</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#30340;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#39640;&#25928;&#30340;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#23398;&#20064;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25552;&#31034;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#35270;&#20026;&#21487;&#35757;&#32451;&#30340;&#65292;&#21516;&#26102;&#20923;&#32467;&#20854;&#20313;&#37096;&#20998;&#65292;&#24182;&#20248;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#24050;&#30693;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#24433;&#21709;&#20102;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#27491;&#21017;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#21046;&#23450;&#20026;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20943;&#23569;&#23545;&#24050;&#35265;&#25552;&#31034;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#25552;&#31034;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20197;&#27010;&#29575;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20316;&#20026;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#25105;&#20204;&#30340;&#25552;&#35758;&#19982;&#22522;&#20110;&#22270;&#20687;&#26080;&#26465;&#20214;&#25110;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26412;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;&#24182;&#34892;Safety Shield&#27169;&#22359;&#65292;&#21487;&#24212;&#29992;&#20110;&#21253;&#21547;&#38750;&#36830;&#25509;&#30340;&#21361;&#38505;&#36710;&#36742;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#36830;&#25509;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#12289;&#21512;&#20316;&#31574;&#30053;&#23398;&#20064;&#12289;&#22270;&#21367;&#31215;&#32593;&#32476;-Transformer&#20316;&#20026;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#20197;&#21450;&#22522;&#20110;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#23433;&#20840;&#23631;&#38556;&#27169;&#22359;&#31561;&#21327;&#35843;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#39640;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.02300</link><description>&lt;p&gt;
&#21160;&#24577;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#36830;&#25509;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26102;&#31354;&#24863;&#30693;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of Connected Autonomous Vehicles in Challenging Scenarios. (arXiv:2210.02300v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;&#24182;&#34892;Safety Shield&#27169;&#22359;&#65292;&#21487;&#24212;&#29992;&#20110;&#21253;&#21547;&#38750;&#36830;&#25509;&#30340;&#21361;&#38505;&#36710;&#36742;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#36830;&#25509;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#12289;&#21512;&#20316;&#31574;&#30053;&#23398;&#20064;&#12289;&#22270;&#21367;&#31215;&#32593;&#32476;-Transformer&#20316;&#20026;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#20197;&#21450;&#22522;&#20110;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#23433;&#20840;&#23631;&#38556;&#27169;&#22359;&#31561;&#21327;&#35843;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#39640;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#25216;&#26415;&#20351;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(CAVs)&#21327;&#20316;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#20849;&#20139;&#20449;&#24687;&#26469;&#25552;&#39640;CAV&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24182;&#34892;&#23433;&#20840;&#23631;&#38556;&#30340;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#21253;&#21547;&#38750;&#36830;&#25509;&#30340;&#21361;&#38505;&#36710;&#36742;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;CAV&#12290;&#25152;&#25552;&#20986;&#30340;MARL&#30340;&#21327;&#35843;&#26426;&#21046;&#21253;&#25324;&#20449;&#24687;&#20849;&#20139;&#21644;&#21512;&#20316;&#31574;&#30053;&#23398;&#20064;&#65292;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)-Transformer&#20316;&#20026;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#65292;&#22686;&#24378;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;&#22522;&#20110;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;(CBF)-&#23433;&#20840;&#26816;&#26597;&#30340;&#23433;&#20840;&#23631;&#38556;&#27169;&#22359;&#20445;&#25252;&#26234;&#33021;&#20307;&#19981;&#37319;&#21462;&#19981;&#23433;&#20840;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;(CMAA2C)&#31639;&#27861;&#65292;&#20026;CAVs&#35757;&#32451;&#23433;&#20840;&#21644;&#21512;&#20316;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#22797;&#26434;&#24773;&#20917;&#21644;&#23454;&#29616;&#39640;&#23433;&#20840;&#24615;&#33021;&#32780;&#21516;&#26102;&#20445;&#25345;&#39640;&#20132;&#36890;&#25928;&#29575;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication technologies enable coordination among connected and autonomous vehicles (CAVs). However, it remains unclear how to utilize shared information to improve the safety and efficiency of the CAV system in dynamic and complicated driving scenarios. In this work, we propose a framework of constrained multi-agent reinforcement learning (MARL) with a parallel Safety Shield for CAVs in challenging driving scenarios that includes unconnected hazard vehicles. The coordination mechanisms of the proposed MARL include information sharing and cooperative policy learning, with Graph Convolutional Network (GCN)-Transformer as a spatial-temporal encoder that enhances the agent's environment awareness. The Safety Shield module with Control Barrier Functions (CBF)-based safety checking protects the agents from taking unsafe actions. We design a constrained multi-agent advantage actor-critic (CMAA2C) algorithm to train safe and cooperative policies for CAVs. With the experiment deployed in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14609</link><description>&lt;p&gt;
&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#33719;&#24471;&#20808;&#36827;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#21464;&#24471;&#26114;&#36149;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#22823;&#22411;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#20449;&#24687;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#21442;&#25968;&#30340;&#32500;&#24230;&#36890;&#24120;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#21442;&#25968;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#38590;&#20197;&#21305;&#37197;&#65292;&#38477;&#20302;&#20102;&#33976;&#39311;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#20462;&#21098;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#21152;&#31283;&#20581;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#39044;&#27979;&#30495;&#23454;&#30340;3D&#21333;&#20010;&#32454;&#32990;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#20010;&#32454;&#32990;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20174;2D&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#36924;&#30495;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;3D&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2208.14125</link><description>&lt;p&gt;
&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;3D&#32454;&#32990;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images. (arXiv:2208.14125v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#39044;&#27979;&#30495;&#23454;&#30340;3D&#21333;&#20010;&#32454;&#32990;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#20010;&#32454;&#32990;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20174;2D&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#36924;&#30495;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;3D&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#21512;&#25104;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;DISPR&#65292;&#29992;&#20110;&#35299;&#20915;&#20174;&#20108;&#32500;&#21333;&#20010;&#32454;&#32990;&#26174;&#24494;&#22270;&#20687;&#39044;&#27979;&#19977;&#32500;&#32454;&#32990;&#24418;&#29366;&#30340;&#21453;&#38382;&#39064;&#12290;&#21033;&#29992;&#20108;&#32500;&#26174;&#24494;&#22270;&#20687;&#20316;&#20026;&#20808;&#39564;&#26465;&#20214;&#65292;DISPR&#34987;&#35843;&#25972;&#20026;&#39044;&#27979;&#36924;&#30495;&#30340;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#12290;&#25105;&#20204;&#20174;&#20845;&#20010;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#20013;&#25552;&#21462;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#23637;&#31034;&#20102;DISPR&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#22312;&#22522;&#20110;&#29305;&#24449;&#30340;&#21333;&#20010;&#32454;&#32990;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23558;DISPR&#39044;&#27979;&#30340;&#29305;&#24449;&#28155;&#21152;&#21040;&#19977;&#20010;&#23569;&#25968;&#31867;&#20013;&#65292;&#23558;&#23439;F1&#24471;&#20998;&#20174;$F1_{macro}=55.2\pm4.6\%$&#25552;&#39640;&#21040;$F1_{macro}=72.2\pm4.9\%$&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#21453;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#65292;&#24182;&#19988;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#20174;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#36924;&#30495;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;3D&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a special type of generative model, capable of synthesising new data from a learnt distribution. We introduce DISPR, a diffusion-based model for solving the inverse problem of three-dimensional (3D) cell shape prediction from two-dimensional (2D) single cell microscopy images. Using the 2D microscopy image as a prior, DISPR is conditioned to predict realistic 3D shape reconstructions. To showcase the applicability of DISPR as a data augmentation tool in a feature-based single cell classification task, we extract morphological features from the red blood cells grouped into six highly imbalanced classes. Adding features from the DISPR predictions to the three minority classes improved the macro F1 score from $F1_\text{macro} = 55.2 \pm 4.6\%$ to $F1_\text{macro} = 72.2 \pm 4.9\%$. We thus demonstrate that diffusion models can be successfully applied to inverse biomedical problems, and that they learn to reconstruct 3D shapes with realistic morphological features from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BoNesis&#36719;&#20214;&#22312;&#26631;&#35760;&#21644;&#28304;&#26631;&#35760;&#20877;&#32534;&#31243;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#25200;&#21160;&#22266;&#23450;&#28857;&#21644;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;&#65292;&#36890;&#36807;&#22312;BNs&#21644;&#38598;&#21512;&#20013;&#36827;&#34892;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.13307</link><description>&lt;p&gt;
BoNesis&#36719;&#20214;&#22312;&#26368;&#22823;&#21253;&#23481;&#24067;&#23572;&#32593;&#32476;&#21644;&#38598;&#21512;&#30340;&#26631;&#35760;&#21644;&#28304;&#26631;&#35760;&#20877;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Marker and source-marker reprogramming of Most Permissive Boolean networks and ensembles with BoNesis. (arXiv:2207.13307v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BoNesis&#36719;&#20214;&#22312;&#26631;&#35760;&#21644;&#28304;&#26631;&#35760;&#20877;&#32534;&#31243;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#25200;&#21160;&#22266;&#23450;&#28857;&#21644;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;&#65292;&#36890;&#36807;&#22312;BNs&#21644;&#38598;&#21512;&#20013;&#36827;&#34892;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#32593;&#32476;&#65288;BNs&#65289;&#26159;&#24212;&#29992;&#20110;&#32454;&#32990;&#34892;&#20026;&#24314;&#27169;&#30340;&#31163;&#25955;&#21160;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;BoNesis&#36719;&#20214;&#24443;&#24213;&#35782;&#21035;&#24378;&#21046;&#20854;&#22266;&#23450;&#28857;&#21644;&#21560;&#24341;&#23376;&#20855;&#26377;&#23646;&#24615;&#30340;&#25200;&#21160;&#32452;&#21512;&#12290;&#25105;&#20204;&#32771;&#34385;&#26631;&#35760;&#23646;&#24615;&#65292;&#23427;&#25351;&#23450;&#26576;&#20123;&#32452;&#20214;&#22266;&#23450;&#20026;&#29305;&#23450;&#20540;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#35760;&#20877;&#32534;&#31243;&#38382;&#39064;&#30340;4&#20010;&#21464;&#20307;&#65306;&#22266;&#23450;&#28857;&#12289;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;&#12289;&#21450;&#20854;&#21487;&#20174;&#32473;&#23450;&#21021;&#22987;&#37197;&#32622;&#21040;&#36798;&#30340;&#20855;&#26377;&#26368;&#23485;&#23481;&#26356;&#26032;&#27169;&#24335;&#30340;&#22266;&#23450;&#28857;&#21644;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;&#30340;&#20877;&#32534;&#31243;&#12290;&#25200;&#21160;&#30001;&#23558;&#19968;&#32452;&#32452;&#20214;&#22266;&#23450;&#20026;&#22266;&#23450;&#20540;&#32452;&#25104;&#12290;&#23427;&#20204;&#21487;&#20197;&#30772;&#22351;&#21644;&#21019;&#24314;&#26032;&#30340;&#21560;&#24341;&#23376;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#38480;&#65292;&#24182;&#21033;&#29992;BoNesis Python&#26694;&#26550;&#23454;&#29616;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20877;&#32534;&#31243;&#38382;&#39064;&#25552;&#21319;&#21040;BNs&#30340;&#38598;&#21512;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean networks (BNs) are discrete dynamical systems with applications to the modeling of cellular behaviors. In this paper, we demonstrate how the software BoNesis can be employed to exhaustively identify combinations of perturbations which enforce properties on their fixed points and attractors. We consider marker properties, which specify that some components are fixed to a specific value. We study 4 variants of the marker reprogramming problem: the reprogramming of fixed points, of minimal trap spaces, and of fixed points and minimal trap spaces reachable from a given initial configuration with the most permissive update mode. The perturbations consist of fixing a set of components to a fixed value. They can destroy and create new attractors. In each case, we give an upper bound on their theoretical computational complexity, and give an implementation of the resolution using the BoNesis Python framework. Finally, we lift the reprogramming problems to ensembles of BNs, as supported
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APOD&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#20943;&#23569;&#31639;&#27861;&#20559;&#35265;&#65292;&#35813;&#26694;&#26550;&#23558;&#27495;&#35270;&#24809;&#32602;&#19982;&#20027;&#21160;&#23454;&#20363;&#36873;&#25321;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.10018</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#27880;&#37322;&#20943;&#23569;&#31639;&#27861;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Algorithmic Bias with Limited Annotations. (arXiv:2207.10018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APOD&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#20943;&#23569;&#31639;&#27861;&#20559;&#35265;&#65292;&#35813;&#26694;&#26550;&#23558;&#27495;&#35270;&#24809;&#32602;&#19982;&#20027;&#21160;&#23454;&#20363;&#36873;&#25321;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#24314;&#27169;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#23454;&#20363;&#30340;&#25935;&#24863;&#23646;&#24615;&#37117;&#26159;&#23436;&#20840;&#21487;&#29992;&#30340;&#65292;&#20294;&#30001;&#20110;&#33719;&#21462;&#25935;&#24863;&#20449;&#24687;&#30340;&#39640;&#25104;&#26412;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#26159;&#36825;&#26679;&#12290;&#24403;&#25935;&#24863;&#23646;&#24615;&#26410;&#20844;&#24320;&#25110;&#26080;&#27861;&#33719;&#24471;&#26102;&#65292;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#20197;&#20943;&#36731;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#25935;&#24863;&#32452;&#20043;&#38388;&#30340;&#20559;&#26012;&#20998;&#24067;&#20250;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#27880;&#37322;&#23376;&#38598;&#30340;&#20559;&#26012;&#24615;&#65292;&#36825;&#23548;&#33268;&#38750;&#26368;&#20248;&#30340;&#20559;&#24046;&#20943;&#36731;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Active Penalization Of Discrimination (APOD)&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#25351;&#23548;&#26377;&#38480;&#27880;&#37322;&#26368;&#22823;&#38480;&#24230;&#22320;&#28040;&#38500;&#31639;&#27861;&#20559;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;APOD&#23558;&#27495;&#35270;&#24809;&#32602;&#19982;&#20027;&#21160;&#23454;&#20363;&#36873;&#25321;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#38480;&#21046;&#31639;&#27861;&#20559;&#35265;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;APOD&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#27880;&#37322;&#25968;&#37327;&#26126;&#26174;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work on fairness modeling commonly assumes that sensitive attributes for all instances are fully available, which may not be true in many real-world applications due to the high cost of acquiring sensitive information. When sensitive attributes are not disclosed or available, it is needed to manually annotate a small part of the training data to mitigate bias. However, the skewed distribution across different sensitive groups preserves the skewness of the original dataset in the annotated subset, which leads to non-optimal bias mitigation. To tackle this challenge, we propose Active Penalization Of Discrimination (APOD), an interactive framework to guide the limited annotations towards maximally eliminating the effect of algorithmic bias. The proposed APOD integrates discrimination penalization with active instance selection to efficiently utilize the limited annotation budget, and it is theoretically proved to be capable of bounding the algorithmic bias. According to the eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26102;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;softmax&#20449;&#24687;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#35780;&#20215;&#26631;&#20934;&#38656;&#26681;&#25454;&#20219;&#21153;&#35268;&#23450;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2207.07506</link><description>&lt;p&gt;
&#25903;&#25345;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;softmax&#20449;&#24687;&#25299;&#23637;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26102;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;softmax&#20449;&#24687;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#35780;&#20215;&#26631;&#20934;&#38656;&#26681;&#25454;&#20219;&#21153;&#35268;&#23450;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#26816;&#27979;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26159;&#19968;&#39033;&#27491;&#22312;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30740;&#31350;&#20851;&#27880;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#20219;&#21153;&#20013;&#21333;&#29420;&#35780;&#20272;&#30340;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#20854;&#22312;&#32852;&#21512;&#19979;&#28216;&#20219;&#21153;&#26102;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26102;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;(SCOD)&#38382;&#39064;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#21160;&#26426;&#22312;&#20110;&#25298;&#32477;&#23427;&#20204;&#65292;&#20174;&#32780;&#38477;&#20302;&#23427;&#20204;&#23545;&#39044;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#35268;&#23450;&#19979;&#65292;&#29616;&#26377;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#21644;&#21482;&#22312;OOD&#26816;&#27979;&#26102;&#35780;&#20272;&#26102;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20102;&#19981;&#21516;&#30340;&#24615;&#33021;&#12290;&#22240;&#20026;&#22914;&#26524;ID&#25968;&#25454;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#23558;ID&#25968;&#25454;&#19982;OOD&#25968;&#25454;&#28151;&#28102;&#23601;&#19981;&#20877;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;ID&#25968;&#25454;&#20013;&#27491;&#30830;&#39044;&#27979;&#21644;&#38169;&#35823;&#39044;&#27979;&#20043;&#38388;&#30340;&#28151;&#28102;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;softmax&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22240;&#26524;&#27169;&#22411;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#23545;&#20110;&#20108;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22270;&#65292;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.07883</link><description>&lt;p&gt;
&#22240;&#26524;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Pure Exploration of Causal Bandits. (arXiv:2206.07883v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22240;&#26524;&#27169;&#22411;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#23545;&#20110;&#20108;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22270;&#65292;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#26159;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#20219;&#21153;&#65306;&#22312;&#32473;&#23450;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#22240;&#26524;&#25512;&#26029;&#20998;&#24067;&#30340;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#21464;&#37327;&#26469;&#24178;&#39044;&#25110;&#19981;&#24178;&#39044;&#65292;&#24182;&#35266;&#23519;&#25152;&#26377;&#38543;&#26426;&#21464;&#37327;&#30340;&#38543;&#26426;&#32467;&#26524;&#65292;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#23569;&#30340;&#20351;&#29992;&#36718;&#25968;&#65292;&#20197;&#27010;&#29575;&#33267;&#23569;&#20026;$1-\delta$&#65292;&#36755;&#20986;&#33021;&#32473;&#20104;&#22870;&#21169;&#21464;&#37327;$Y$&#26368;&#20339;&#65288;&#25110;&#36817;&#20046;&#26368;&#20339;&#65289;&#26399;&#26395;&#32467;&#26524;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#20854;&#20013;$\delta$&#26159;&#32473;&#23450;&#30340;&#32622;&#20449;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#31867;&#22411;&#22240;&#26524;&#27169;&#22411;&#8212;&#8212;&#20108;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;BGLM&#65289; &#21644;&#19968;&#33324;&#22270;&#30340;&#31532;&#19968;&#31181;&#38388;&#38548;&#20381;&#36182;&#24615;&#21644;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#32431;&#25506;&#32034;&#31639;&#27861;&#12290;&#23545;&#20110;BGLM&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#39318;&#27425;&#19987;&#38376;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#35774;&#35745;&#30340;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32780;&#25152;&#26377;&#29616;&#26377;&#30340;&#19968;&#33324;&#22270;&#31639;&#27861;&#37117;&#20855;&#26377;&#25351;&#25968;&#22797;&#26434;&#24230;&#21040;&#22270;&#22823;&#23567;&#25110;&#19968;&#20123;&#19981;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#23545;&#20110;&#19968;&#33324;&#22270;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20165;&#23545;&#25968;&#20110;&#22270;&#22823;&#23567;&#21644;&#26410;&#30693;&#22240;&#26524;&#25928;&#24212;&#30340;&#25968;&#37327;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25269;&#24481;&#38544;&#34255;&#24615;&#28151;&#28102;&#22240;&#32032;&#21644;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#19979;&#30028;&#65292;&#19982;&#19978;&#30028;&#30456;&#21305;&#37197;&#65292;&#34920;&#26126;&#20102;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combinatorial pure exploration of causal bandits is the following online learning task: given a causal graph with unknown causal inference distributions, in each round we choose a subset of variables to intervene or do no intervention, and observe the random outcomes of all random variables, with the goal that using as few rounds as possible, we can output an intervention that gives the best (or almost best) expected outcome on the reward variable $Y$ with probability at least $1-\delta$, where $\delta$ is a given confidence level. We provide the first gap-dependent and fully adaptive pure exploration algorithms on two types of causal models -- the binary generalized linear model (BGLM) and general graphs. For BGLM, our algorithm is the first to be designed specifically for this setting and achieves polynomial sample complexity, while all existing algorithms for general graphs have either sample complexity exponential to the graph size or some unreasonable assumptions. For general 
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#24456;&#38590;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#24418;&#24335;&#65292;&#22240;&#20026;&#36890;&#24120;&#37319;&#29992;&#30340;&#21028;&#26029;&#21551;&#21457;&#24335;&#31639;&#27861;&#20986;&#29616;&#20102;&#19968;&#20123;&#32570;&#38519;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20026;&#22797;&#26434;&#30340;&#35821;&#35328;&#20998;&#26512;&#24037;&#20855;&#21644;&#25945;&#32946;&#26469;&#25552;&#39640;&#20154;&#20204;&#30340;&#21028;&#26029;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.07271</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#20110;AI&#35821;&#35328;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#23384;&#22312;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Human heuristics for AI-generated language are flawed. (arXiv:2206.07271v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07271
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24456;&#38590;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#24418;&#24335;&#65292;&#22240;&#20026;&#36890;&#24120;&#37319;&#29992;&#30340;&#21028;&#26029;&#21551;&#21457;&#24335;&#31639;&#27861;&#20986;&#29616;&#20102;&#19968;&#20123;&#32570;&#38519;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20026;&#22797;&#26434;&#30340;&#35821;&#35328;&#20998;&#26512;&#24037;&#20855;&#21644;&#25945;&#32946;&#26469;&#25552;&#39640;&#20154;&#20204;&#30340;&#21028;&#26029;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35821;&#35328;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#20154;&#31867;&#20132;&#27969;&#30456;&#20114;&#34701;&#21512;&#12290;&#22312;&#32842;&#22825;&#12289;&#37038;&#20214;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#65292;AI&#31995;&#32479;&#24314;&#35758;&#21333;&#35789;&#12289;&#23436;&#25104;&#21477;&#23376;&#25110;&#20135;&#29983;&#25972;&#20010;&#23545;&#35805;&#12290;&#20154;&#20204;&#36890;&#24120;&#26080;&#27861;&#37492;&#21035;AI&#20135;&#29983;&#30340;&#35821;&#35328;&#65292;&#32780;&#23558;&#20854;&#35270;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#35821;&#35328;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#26032;&#24418;&#24335;&#27450;&#39575;&#21644;&#25805;&#32437;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#20998;&#36776;AI&#29983;&#25104;&#30340;&#26368;&#20026;&#20010;&#20154;&#21270;&#21644;&#37325;&#35201;&#30340;&#35821;&#35328;&#24418;&#24335;&#20043;&#19968;&#8212;&#8212;&#21475;&#22836;&#33258;&#25105;&#34920;&#36848;&#12290;&#22312;&#20845;&#20010;&#23454;&#39564;&#20013;&#65292;&#21442;&#19982;&#32773;&#65288;N=4,600&#65289;&#26080;&#27861;&#22312;&#19987;&#19994;&#12289;&#37202;&#24215;&#20197;&#21450;&#32422;&#20250;&#24773;&#22659;&#20013;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#25152;&#29983;&#25104;&#30340;&#33258;&#25105;&#34920;&#36848;&#12290;&#35821;&#35328;&#29305;&#24449;&#30340;&#35745;&#31639;&#20998;&#26512;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#20110;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#21028;&#26029;&#23384;&#22312;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32570;&#38519;&#65292;&#20363;&#22914;&#23558;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#12289;&#32553;&#30053;&#35789;&#20351;&#29992;&#25110;&#23478;&#24237;&#35805;&#39064;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#35821;&#35328;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#65292;&#24314;&#35758;&#20351;&#29992;&#26356;&#20026;&#22797;&#26434;&#30340;&#35821;&#35328;&#20998;&#26512;&#24037;&#20855;&#21644;&#25945;&#32946;&#26469;&#25913;&#21892;&#20154;&#31867;&#30340;&#21028;&#26029;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human communication is increasingly intermixed with language generated by AI. Across chat, email, and social media, AI systems suggest words, complete sentences, or produce entire conversations. AI-generated language is often not identified as such but presented as language written by humans, raising concerns about novel forms of deception and manipulation. Here, we study how humans discern whether verbal self-presentations, one of the most personal and consequential forms of language, were generated by AI. In six experiments, participants (N = 4,600) were unable to detect self-presentations generated by state-of-the-art AI language models in professional, hospitality, and dating contexts. A computational analysis of language features shows that human judgments of AI-generated language are hindered by intuitive but flawed heuristics such as associating first-person pronouns, use of contractions, or family topics with human-written language. We experimentally demonstrate that these heur
&lt;/p&gt;</description></item><item><title>Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.10852</link><description>&lt;p&gt;
Relphormer&#65306;&#20851;&#31995;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10852
&lt;/p&gt;
&lt;p&gt;
Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#25366;&#25496;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;remarkable&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#20013;&#24182;&#27809;&#26377;&#21462;&#24471;&#24456;&#22909;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#24179;&#31227;&#36317;&#31163;&#27169;&#22411;&#25903;&#37197;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#38656;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#38590;&#20197;&#25429;&#25417;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#24322;&#26500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;Transformer&#21464;&#20307;&#65292;&#21517;&#20026;Relphormer&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Triple2Seq&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#37319;&#26679;&#19978;&#19979;&#25991;&#21270;&#30340;&#23376;&#22270;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#23545;&#20851;&#31995;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20445;&#25345;&#23454;&#20307;&#21644;&#20851;&#31995;&#20869;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#34109;&#24335;&#30693;&#35782;&#24314;&#27169;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
&lt;/p&gt;</description></item><item><title>AIGenC &#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#36896;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#24565;&#31354;&#38388;&#21644;&#20998;&#23618;&#32467;&#26500;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#36890;&#29992;&#24615;&#21644;&#21019;&#26032;&#33021;&#21147;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.09738</link><description>&lt;p&gt;
AIGenC&#65306;&#36890;&#36807;&#21019;&#36896;&#21147;&#36827;&#34892; AI &#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
AIGenC: AI generalisation via creativity. (arXiv:2205.09738v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09738
&lt;/p&gt;
&lt;p&gt;
AIGenC &#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#36896;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#24565;&#31354;&#38388;&#21644;&#20998;&#23618;&#32467;&#26500;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#36890;&#29992;&#24615;&#21644;&#21019;&#26032;&#33021;&#21147;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#21019;&#36896;&#21147;&#30340;&#35748;&#30693;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65288;AIGenC&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#24517;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#12289;&#20351;&#29992;&#21644;&#29983;&#25104;&#21487;&#36716;&#31227;&#30340;&#34920;&#24449;&#12290;&#19982;&#26426;&#22120;&#34920;&#24449;&#23398;&#20064;&#19981;&#21516;&#30340;&#26159;&#65292;&#29983;&#29289;&#34920;&#24449;&#21253;&#25324;&#20851;&#31995;&#21644;&#32852;&#24819;&#20449;&#24687;&#65292;&#23884;&#20837;&#20102;&#20016;&#23500;&#21644;&#32467;&#26500;&#21270;&#30340;&#27010;&#24565;&#31354;&#38388;&#12290;AIGenC &#27169;&#22411;&#37319;&#29992;&#20998;&#23618;&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#20855;&#26377;&#19981;&#21516;&#32452;&#20214;&#33719;&#21462;&#30340;&#21508;&#31181;&#32423;&#21035;&#21644;&#31867;&#22411;&#30340;&#34920;&#24449;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#37096;&#20998;&#8212;&#8212;&#27010;&#24565;&#22788;&#29702;&#65292;&#20174;&#24863;&#23448;&#36755;&#20837;&#20013;&#25552;&#21462;&#23545;&#35937;&#21644;&#25903;&#37197;&#22240;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#32534;&#30721;&#25104;&#27010;&#24565;&#31354;&#38388;&#12290;&#29983;&#25104;&#30340;&#34920;&#24449;&#23384;&#20648;&#22312;&#21452;&#37325;&#35760;&#24518;&#31995;&#32479;&#20013;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30446;&#26631;&#23548;&#21521;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#20016;&#23500;&#24230;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#25277;&#35937;&#12290;&#21478;&#22806;&#20004;&#20010;&#32452;&#20214;&#24182;&#34892;&#24037;&#20316;&#65292;&#26816;&#27979;&#21644;&#24674;&#22797;&#23384;&#20648;&#34920;&#24449;&#20013;&#30340;&#30456;&#20851;&#27010;&#24565;&#21644;&#21019;&#36896;&#36830;&#25509;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#29983;&#25104;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#24182;&#22312;&#26032;&#22330;&#26223;&#20013;&#24212;&#29992;&#23427;&#20204;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AIGenC &#22312;&#21508;&#31181;&#36890;&#29992;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by cognitive theories of creativity, this paper introduces a computational model (AIGenC) that lays down the necessary components to enable artificial agents to learn, use and generate transferable representations. Unlike machine representation learning, which relies exclusively on raw sensory data, biological representations incorporate relational and associative information that embeds rich and structured concept spaces. The AIGenC model poses a hierarchical graph architecture with various levels and types of representations procured by different components. The first component, Concept Processing, extracts objects and affordances from sensory input and encodes them into a concept space. The resulting representations are stored in a dual memory system and enriched with goal-directed and temporal information acquired through reinforcement learning, creating a higher-level of abstraction. Two additional components work in parallel to detect and recover relevant concepts and cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#24555;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#12290;</title><link>http://arxiv.org/abs/2202.02113</link><description>&lt;p&gt;
&#20174;&#21306;&#20998;&#21040;&#29983;&#25104;&#65306;&#22522;&#20110;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#24555;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#35299;&#20915;&#20102;&#25193;&#23637;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20316;GenKGC&#30340;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20197;&#21069;&#20855;&#26377;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#65292;&#20379;&#30740;&#31350;&#30446;&#30340;&#20351;&#29992;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/GenKGC&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KGAN&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#32593;&#32476;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#21644;&#19978;&#19979;&#25991;&#12289;&#21477;&#27861;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#25429;&#33719;&#24773;&#24863;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22810;&#35270;&#35282;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2201.04831</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#32593;&#32476;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis. (arXiv:2201.04831v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KGAN&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#32593;&#32476;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#21644;&#19978;&#19979;&#25991;&#12289;&#21477;&#27861;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#25429;&#33719;&#24773;&#24863;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22810;&#35270;&#35282;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#39033;&#32454;&#31890;&#24230;&#20219;&#21153;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#38271;&#21477;&#23376;&#24182;&#33719;&#21462;&#20934;&#30830;&#30340;&#26041;&#38754;&#29305;&#23450;&#20449;&#24687;&#65292;&#36890;&#24120;&#38656;&#35201;&#35821;&#35328;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#22797;&#26434;&#21644;&#20302;&#25928;&#30340;&#26041;&#27861;&#26469;&#21253;&#21547;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#30452;&#25509;&#25628;&#32034;&#22270;&#24418;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#30693;&#35782;&#21644;&#35821;&#35328;&#20449;&#24687;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#22686;&#24378;&#32593;&#32476;KGAN&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#26126;&#30830;&#30340;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;KGAN&#20174;&#22810;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#33719;&#24773;&#24863;&#29305;&#24449;&#34920;&#31034;&#65292;&#21363;&#22522;&#20110;&#19978;&#19979;&#25991;&#12289;&#21477;&#27861;&#21644;&#30693;&#35782;&#30340;&#12290;&#39318;&#20808;&#65292;KGAN&#24182;&#34892;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#21477;&#27861;&#34920;&#31034;&#65292;&#20197;&#20805;&#20998;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;KGAN&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#19978;&#19979;&#25991;&#21644;&#21477;&#27861;&#20449;&#24687;&#30456;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment analysis. To better comprehend long complicated sentences and obtain accurate aspect-specific information, linguistic and commonsense knowledge are generally required in this task. However, most current methods employ complicated and inefficient approaches to incorporate external knowledge, e.g., directly searching the graph nodes. Additionally, the complementarity between external knowledge and linguistic information has not been thoroughly studied. To this end, we propose a knowledge graph augmented network KGAN, which aims to effectively incorporate external knowledge with explicitly syntactic and contextual information. In particular, KGAN captures the sentiment feature representations from multiple different perspectives, i.e., context-, syntaxand knowledge-based. First, KGAN learns the contextual and syntactic representations in parallel to fully extract the semantic features. Then, KGAN integrates the k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25512;&#24191;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#23454;&#36341;&#20013;&#30340;&#36947;&#24503;&#21407;&#21017;&#21644;&#35268;&#33539;&#25351;&#21335;&#65292;&#24182;&#20351;&#20154;&#24037;&#26234;&#33021;&#24471;&#21040;&#36947;&#24503;&#21644;&#36127;&#36131;&#20219;&#30340;&#21019;&#36896;&#12290;</title><link>http://arxiv.org/abs/2112.11208</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#23433;&#20840;&#65306;&#21019;&#24314;&#8220;&#33391;&#22909;&#8221;&#27169;&#22411;&#30340;&#23454;&#29992;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Ethics and Safety: practical tools for creating "good" models. (arXiv:2112.11208v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25512;&#24191;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#23454;&#36341;&#20013;&#30340;&#36947;&#24503;&#21407;&#21017;&#21644;&#35268;&#33539;&#25351;&#21335;&#65292;&#24182;&#20351;&#20154;&#24037;&#26234;&#33021;&#24471;&#21040;&#36947;&#24503;&#21644;&#36127;&#36131;&#20219;&#30340;&#21019;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI Robotics Ethics Society&#65288;AIRES&#65289;&#26159;&#30001;Aaron Hui&#20110;2018&#24180;&#21019;&#31435;&#30340;&#38750;&#33829;&#21033;&#32452;&#32455;&#65292;&#26088;&#22312;&#25512;&#24191;&#20154;&#24037;&#26234;&#33021;&#30340;&#36947;&#24503;&#23454;&#26045;&#21644;&#30417;&#31649;&#30340;&#24847;&#35782;&#21644;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22312;AI&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#20013;&#24314;&#31435;&#36947;&#24503;&#21407;&#21017;&#21644;&#35268;&#33539;&#25351;&#21335;&#30340;&#25552;&#26696;&#65292;&#22635;&#34917;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The AI Robotics Ethics Society (AIRES) is a non-profit organization founded in 2018 by Aaron Hui to promote awareness and the importance of ethical implementation and regulation of AI. AIRES is now an organization with chapters at universities such as UCLA (Los Angeles), USC (University of Southern California), Caltech (California Institute of Technology), Stanford University, Cornell University, Brown University, and the Pontifical Catholic University of Rio Grande do Sul (Brazil). AIRES at PUCRS is the first international chapter of AIRES, and as such, we are committed to promoting and enhancing the AIRES Mission. Our mission is to focus on educating the AI leaders of tomorrow in ethical principles to ensure that AI is created ethically and responsibly. As there are still few proposals for how we should implement ethical principles and normative guidelines in the practice of AI system development, the goal of this work is to try to bridge this gap between discourse and praxis. Betwee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2110.10325</link><description>&lt;p&gt;
&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#21450;&#20854;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32467;&#21512;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#35825;&#23548;&#23398;&#20064;&#65292;&#22312;&#21457;&#26126;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;OSAMTL&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#21463;&#35825;&#23548;&#23398;&#20064;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#19968;&#31181;&#24179;&#34913;&#30340;&#26041;&#24335;&#31616;&#21333;&#22320;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21333;&#20010;&#22024;&#26434;&#26631;&#31614;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;&#20294;&#26159;&#65292;OSAMTL&#19981;&#36866;&#29992;&#20110;&#25552;&#20379;&#22810;&#31181;&#22024;&#26434;&#26679;&#26412;&#65288;DiNS&#65289;&#30340;&#23398;&#20064;&#20219;&#21153;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;DiNS&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#25193;&#23637;&#21407;&#22987;&#30340;OSAMTL&#20197;&#22788;&#29702;DiNS&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#23558;OSAMTL-DiNS&#24212;&#29992;&#20110;MHWSIA&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the effectiveness of the combination of machine learning and logical reasoning, including data-driven logical reasoning, knowledge driven machine learning and abductive learning, in inventing advanced artificial intelligence technologies. One-step abductive multi-target learning (OSAMTL), an approach inspired by abductive learning, via simply combining machine learning and logical reasoning in a one-step balanced way, has as well shown its effectiveness in handling complex noisy labels of a single noisy sample in medical histopathology whole slide image analysis (MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are provided for a learning task. In this paper, giving definition of DiNS, we propose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show 
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21517;&#20026;Broad Ensemble Learning System (BELS)&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#21152;&#39640;&#25928;&#12289;&#31283;&#23450;&#22320;&#26356;&#26032;&#27169;&#22411;&#65292;&#25552;&#39640;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.03540</link><description>&lt;p&gt;
&#19968;&#31181;&#24212;&#23545;&#25968;&#25454;&#27969;&#20998;&#31867;&#27010;&#24565;&#28418;&#31227;&#30340;&#24191;&#20041;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Broad Ensemble Learning System for Drifting Stream Classification. (arXiv:2110.03540v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03540
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21517;&#20026;Broad Ensemble Learning System (BELS)&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#21152;&#39640;&#25928;&#12289;&#31283;&#23450;&#22320;&#26356;&#26032;&#27169;&#22411;&#65292;&#25552;&#39640;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#65292;&#20998;&#31867;&#27169;&#22411;&#24517;&#39035;&#26377;&#25928;&#22320;&#22788;&#29702;&#27010;&#24565;&#28418;&#31227;&#12290;&#38598;&#25104;&#26041;&#27861;&#26159;&#20026;&#27492;&#30446;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#22823;&#22359;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#35201;&#20040;&#36880;&#20010;&#23398;&#20064;&#25968;&#25454;&#12290;&#21069;&#32773;&#21487;&#33021;&#20250;&#38169;&#36807;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#21464;&#21270;&#65292;&#21518;&#32773;&#21017;&#21487;&#33021;&#21463;&#21040;&#25928;&#29575;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;Broad Learning System (BLS)&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#27599;&#27425;&#26356;&#26032;&#26102;&#20351;&#29992;&#23567;&#22359;&#25968;&#25454;&#12290;BLS&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32467;&#26500;&#65292;&#26368;&#36817;&#34987;&#24320;&#21457;&#29992;&#20110;&#22686;&#37327;&#23398;&#20064;&#12290;&#23613;&#31649;&#23427;&#36895;&#24230;&#24456;&#24555;&#65292;&#20294;&#23427;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#22359;&#26469;&#36827;&#34892;&#26377;&#25928;&#26356;&#26032;&#65292;&#32780;&#19988;&#19981;&#33021;&#22788;&#29702;&#25968;&#25454;&#27969;&#20013;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;Broad Ensemble Learning System (BELS)&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26356;&#26032;&#26041;&#24335;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
In a data stream environment, classification models must handle concept drift efficiently and effectively. Ensemble methods are widely used for this purpose; however, the ones available in the literature either use a large data chunk to update the model or learn the data one by one. In the former, the model may miss the changes in the data distribution, and in the latter, the model may suffer from inefficiency and instability. To address these issues, we introduce a novel ensemble approach based on the Broad Learning System (BLS), where mini chunks are used at each update. BLS is an effective lightweight neural architecture recently developed for incremental learning. Although it is fast, it requires huge data chunks for effective updates, and is unable to handle dynamic changes observed in data streams. Our proposed approach named Broad Ensemble Learning System (BELS) uses a novel updating method that significantly improves best-in-class model accuracy. It employs an ensemble of outpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32473;&#27169;&#22411;&#31532;&#20108;&#27425;&#12289;&#31532;&#19977;&#27425;&#29978;&#33267;&#31532;k&#27425;&#24605;&#32771;&#26426;&#20250;&#30340;&#24605;&#36335;&#27969;&#32593;&#32476;&#65292;&#20854;&#21033;&#29992;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#21644;&#26799;&#24230;&#26356;&#26032;&#33021;&#22815;&#32416;&#27491;&#33258;&#36523;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2107.12220</link><description>&lt;p&gt;
&#24605;&#36335;&#27969;&#32593;&#32476;&#65306;&#20174;&#21333;&#19968;&#39044;&#27979;&#21040;&#27169;&#22411;&#24605;&#36335;&#30340;&#20018;&#32852;
&lt;/p&gt;
&lt;p&gt;
Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32473;&#27169;&#22411;&#31532;&#20108;&#27425;&#12289;&#31532;&#19977;&#27425;&#29978;&#33267;&#31532;k&#27425;&#24605;&#32771;&#26426;&#20250;&#30340;&#24605;&#36335;&#27969;&#32593;&#32476;&#65292;&#20854;&#21033;&#29992;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#21644;&#26799;&#24230;&#26356;&#26032;&#33021;&#22815;&#32416;&#27491;&#33258;&#36523;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#36890;&#24120;&#20250;&#21019;&#24314;&#19968;&#31995;&#21015;&#24605;&#36335;&#65288;&#28041;&#21450;&#30452;&#35273;&#20915;&#31574;&#12289;&#21453;&#24605;&#12289;&#38169;&#35823;&#26356;&#27491;&#31561;&#65289;&#20197;&#36798;&#25104;&#20915;&#23450;&#12290;&#20294;&#26159;&#65292;&#22914;&#20170;&#30340;&#27169;&#22411;&#22823;&#22810;&#34987;&#35757;&#32451;&#20026;&#23558;&#36755;&#20837;&#26144;&#23556;&#21040;&#21333;&#19968;&#19988;&#22266;&#23450;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35753;&#27169;&#22411;&#26377;&#31532;&#20108;&#12289;&#31532;&#19977;&#21644;&#31532; k &#27425;&#24605;&#32771;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#20174;&#40657;&#26684;&#23572;&#30340;&#36777;&#35777;&#27861;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#24605;&#36335;&#27969;&#30340;&#27010;&#24565;&#65292;&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#22522;&#20110;&#27491;&#30830;&#24615;&#39044;&#27979;&#30340;&#26799;&#24230;&#25191;&#34892;&#36845;&#20195;&#39044;&#27979;&#26356;&#26032;&#12290;&#25105;&#20204;&#20197;&#38382;&#31572;&#20026;&#20363;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#65288;i&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32416;&#27491;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#65288;ii&#65289;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24605;&#36335;&#27969;&#30340;&#35821;&#20041;&#26657;&#39564;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans solve complex problems, they typically create a sequence of ideas (involving an intuitive decision, reflection, error correction, etc.) in order to reach a conclusive decision. Contrary to this, today's models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. Taking inspiration from Hegel's dialectics, we propose the concept of a thought flow which creates a sequence of predictions. We present a self-correction mechanism that is trained to estimate the model's correctness and performs iterative prediction updates based on the correctness prediction's gradient. We introduce our method at the example of question answering and conduct extensive experiments that demonstrate (i) our method's ability to correct its own predictions and (ii) its potential to notably improve model performances. In addition, we conduct a qualitative analysis of thought flow cor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21382;&#21490;&#32463;&#39564;&#30340;&#22870;&#36175;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/1902.06239</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#36175;&#35774;&#35745;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#26032;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
A new Potential-Based Reward Shaping for Reinforcement Learning Agent. (arXiv:1902.06239v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21382;&#21490;&#32463;&#39564;&#30340;&#22870;&#36175;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#35774;&#35745;&#65288;PBRS&#65289;&#26159;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#21033;&#29992;&#39069;&#22806;&#30693;&#35782;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#20854;&#20013;&#65292;&#20256;&#36882;&#23398;&#20064;&#20013;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#26159;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#25910;&#38598;&#21040;&#30340;&#30693;&#35782;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#32463;&#39564;&#30340;&#22870;&#36175;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#23398;&#20064;&#32463;&#39564;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potential-based reward shaping (PBRS) is a particular category of machine learning methods which aims to improve the learning speed of a reinforcement learning agent by extracting and utilizing extra knowledge while performing a task. There are two steps in the process of transfer learning: extracting knowledge from previously learned tasks and transferring that knowledge to use it in a target task. The latter step is well discussed in the literature with various methods being proposed for it, while the former has been explored less. With this in mind, the type of knowledge that is transmitted is very important and can lead to considerable improvement. Among the literature of both the transfer learning and the potential-based reward shaping, a subject that has never been addressed is the knowledge gathered during the learning process itself. In this paper, we presented a novel potential-based reward shaping method that attempted to extract knowledge from the learning process. The propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#20316;&#20026;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/1512.07430</link><description>&lt;p&gt;
FOLE ERA&#65306;&#22522;&#30784;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
The ERA of FOLE: Foundation. (arXiv:1512.07430v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1512.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#20316;&#20026;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#65288;Kent 2013&#65289;&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#12290;&#26412;&#20307;&#23450;&#20041;&#20102;&#29992;&#20110;&#20026;&#35805;&#35821;&#31038;&#21306;&#24314;&#27169;&#30693;&#35782;&#36164;&#28304;&#30340;&#21407;&#35821;&#65288;Gruber 2009&#65289;&#12290;&#36825;&#20123;&#21407;&#35821;&#21253;&#25324;&#31867;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#65292;&#30001;&#23454;&#20307;-&#20851;&#31995;-&#23646;&#24615;&#65288;ERA&#65289;&#25968;&#25454;&#27169;&#22411;&#65288;Chen 1976&#65289;&#34920;&#31034;&#12290;&#26412;&#25991;&#26159;&#19977;&#31687;&#35770;&#25991;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#23427;&#22312;FOLE&#30340;&#31532;&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the representation of ontologies in the first-order logical environment FOLE (Kent 2013). An ontology defines the primitives with which to model the knowledge resources for a community of discourse (Gruber 2009). These primitives, consisting of classes, relationships and properties, are represented by the entity-relationship-attribute ERA data model (Chen 1976). An ontology uses formal axioms to constrain the interpretation of these primitives. In short, an ontology specifies a logical theory. This paper is the first in a series of three papers that provide a rigorous mathematical representation for the ERA data model in particular, and ontologies in general, within the first-order logical environment FOLE. The first two papers show how FOLE represents the formalism and semantics of (many-sorted) first-order logic in a classification form corresponding to ideas discussed in the Information Flow Framework (IFF). In particular, this first paper provides a foundation 
&lt;/p&gt;</description></item></channel></rss>