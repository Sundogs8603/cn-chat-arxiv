<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#65292;&#36890;&#36807;&#37322;&#25918;&#20854;&#21019;&#36896;&#28508;&#21147;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;LLMs&#20998;&#20026;&#39046;&#23548;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#39046;&#23548;&#32773;&#25552;&#20379;&#22810;&#31181;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#25191;&#34892;&#32773;&#26681;&#25454;&#39046;&#23548;&#32773;&#30340;&#25351;&#24341;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2311.00694</link><description>&lt;p&gt;
&#35299;&#25918;&#21019;&#36896;&#21147;&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#20197;&#25913;&#36827;&#25361;&#25112;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#65292;&#36890;&#36807;&#37322;&#25918;&#20854;&#21019;&#36896;&#28508;&#21147;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;LLMs&#20998;&#20026;&#39046;&#23548;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#39046;&#23548;&#32773;&#25552;&#20379;&#22810;&#31181;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#25191;&#34892;&#32773;&#26681;&#25454;&#39046;&#23548;&#32773;&#30340;&#25351;&#24341;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#38382;&#39064;&#20013;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#26679;&#25110;&#25628;&#32034;&#35814;&#32454;&#21644;&#20302;&#32423;&#30340;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25506;&#32034;&#33021;&#21147;&#19978;&#20173;&#28982;&#26377;&#38480;&#65292;&#20351;&#24471;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24222;&#22823;&#30340;&#35299;&#31354;&#38388;&#20013;&#24456;&#38590;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLMs&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#37322;&#25918;LLMs&#25506;&#32034;&#22810;&#26679;&#21270;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#30340;&#21019;&#36896;&#28508;&#21147;&#12290;&#35813;&#31574;&#30053;&#21253;&#25324;&#19968;&#20010;&#26377;&#36828;&#35265;&#30340;&#39046;&#23548;&#32773;&#65292;&#25552;&#20986;&#22810;&#31181;&#22810;&#26679;&#30340;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#26377;&#19968;&#20010;&#25191;&#34892;&#32773;&#65292;&#26681;&#25454;&#27599;&#20010;&#39640;&#32423;&#25351;&#20196;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#25191;&#34892;&#32773;&#23558;&#39046;&#23548;&#32773;&#30340;&#27599;&#20010;&#25351;&#20196;&#20316;&#20026;&#25351;&#21335;&#65292;&#24182;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20026;&#27599;&#20010;&#39046;&#23548;&#32773;&#30340;&#25552;&#35758;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19786</link><description>&lt;p&gt;
&#20174;&#22806;&#37096;&#21040;Swap&#36951;&#25022;2.0&#65306;&#38024;&#23545;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#32422;&#21270;&#21644;&#26080;&#30693;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#21040;&#22806;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#34892;&#20026;&#31354;&#38388;&#30340;&#26377;&#38480;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23384;&#22312;&#26576;&#20010;&#20551;&#35774;&#31867;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#65292;&#23601;&#24517;&#28982;&#23384;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#26080;Swap&#36951;&#25022;&#31639;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#21487;&#20197;&#20445;&#35777;&#22312;$\log(N)^{O(1/\epsilon)}$&#36718;&#21518;&#65292;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(N)$&#30340;&#24773;&#20917;&#19979;&#65292;Swap&#36951;&#25022;&#34987;&#38480;&#23450;&#20026;$\epsilon$&#65292;&#32780;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#38656;&#35201;$O(N/\epsilon^2)$&#36718;&#21644;&#33267;&#23569;$\Omega(N^2)$&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20276;&#38543;&#30528;&#19968;&#20010;&#30456;&#20851;&#30340;&#19979;&#30028;&#65292;&#19982;[BM07]&#19981;&#21516;&#65292;&#36825;&#20010;&#19979;&#30028;&#36866;&#29992;&#20110;&#26080;&#30693;&#21644;$\ell_1$-&#21463;&#38480;&#30340;&#23545;&#25163;&#21644;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#19979;&#30028;&#30340;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07992</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#20869;&#30740;&#31350;&#27969;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#23792;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#27700;&#25991;&#23398;&#23478;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#23792;&#20540;&#27169;&#24335;&#24322;&#24120;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#25110;&#33258;&#28982;&#29616;&#35937;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#23384;&#22312;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#22522;&#20934;&#21644;&#36873;&#25321;&#26368;&#36866;&#21512;&#32473;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#21512;&#25104;&#30340;&#23792;&#20540;&#27169;&#24335;&#27880;&#20837;&#21040;&#21512;&#25104;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#33258;&#21160;&#21270;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#12290;&#35813;&#26426;&#21046;&#20174;&#20116;&#31181;&#36873;&#25321;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#26550;&#26500;&#21644;&#35757;&#32451;&#21442;&#25968;&#30340;&#20248;&#21270;&#27169;&#22411;&#23454;&#20363;&#65292;&#21363;&#26102;&#24207;&#21367;&#31215;&#32593;&#32476;&#65288;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated machine learning framework designed to assist hydrologists in detecting anomalies in time series data generated by sensors in a research watershed in the northeastern United States critical zone. The framework specifically focuses on identifying peak-pattern anomalies, which may arise from sensor malfunctions or natural phenomena. However, the use of classification methods for anomaly detection poses challenges, such as the requirement for labeled data as ground truth and the selection of the most suitable deep learning model for the given task and dataset. To address these challenges, our framework generates labeled datasets by injecting synthetic peak patterns into synthetically generated time series data and incorporates an automated hyperparameter optimization mechanism. This mechanism generates an optimized model instance with the best architectural and training parameters from a pool of five selected models, namely Temporal Convolutional Network (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.04673</link><description>&lt;p&gt;
SSL-Auth:&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning. (arXiv:2308.04673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20026;&#39044;&#35757;&#32451;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#36825;&#20123;&#32534;&#30721;&#22120;&#24120;&#34987;&#29992;&#20316;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20854;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12289;&#23545;&#25239;&#25915;&#20987;&#31561;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#39564;&#35777;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23436;&#25972;&#24615;&#30340;&#26041;&#26696;&#26469;&#20445;&#25252;&#29992;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) which leverages unlabeled datasets for pre-training powerful encoders has achieved significant success in recent years. These encoders are commonly used as feature extractors for various downstream tasks, requiring substantial data and computing resources for their training process. With the deployment of pre-trained encoders in commercial use, protecting the intellectual property of model owners and ensuring the trustworthiness of the models becomes crucial. Recent research has shown that encoders are threatened by backdoor attacks, adversarial attacks, etc. Therefore, a scheme to verify the integrity of pre-trained encoders is needed to protect users. In this paper, we propose SSL-Auth, the first fragile watermarking scheme for verifying the integrity of encoders without compromising model performance. Our method utilizes selected key samples as watermark information and trains a verification network to reconstruct the watermark information, thereby ver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#19968;&#20010;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#27169;&#22411;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20250;&#31185;&#23398;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36716;&#21270;&#36825;&#20123;&#26500;&#36896;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13912</link><description>&lt;p&gt;
&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31038;&#20250;&#23458;&#35266;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Embedding Democratic Values into Social Media AIs via Societal Objective Functions. (arXiv:2307.13912v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#19968;&#20010;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#27169;&#22411;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20250;&#31185;&#23398;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36716;&#21270;&#36825;&#20123;&#26500;&#36896;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20351;&#20854;&#32771;&#34385;&#21040;&#27665;&#20027;&#20215;&#20540;&#35266;&#65292;&#22914;&#20943;&#23569;&#20826;&#27966;&#25932;&#24847;&#65292;&#20316;&#20026;&#20854;&#30446;&#26631;&#20989;&#25968;&#26469;&#25490;&#21517;&#25105;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#27969;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#24050;&#24314;&#31435;&#12289;&#32463;&#23457;&#26597;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31038;&#20250;&#23458;&#35266;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#36825;&#19968;&#25919;&#27835;&#31185;&#23398;&#26500;&#36896;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#25105;&#20204;&#32570;&#20047;&#21487;&#35266;&#23519;&#30340;&#32467;&#26524;&#26469;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#31038;&#20250;&#31185;&#23398;&#24050;&#32463;&#24320;&#21457;&#20102;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#29992;&#20110;&#36825;&#20123;&#26500;&#36896;&#30340;&#32763;&#35793;&#65292;&#20854;&#31934;&#30830;&#24615;&#20415;&#20110;&#23558;&#20854;&#36716;&#21270;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35814;&#32454;&#25552;&#31034;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#27665;&#20027;&#24577;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#23459;&#20256;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#31243;&#24230;&#65292;&#24182;&#22312;&#19977;&#20010;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#36825;&#20010;&#27665;&#20027;&#24577;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.09477</link><description>&lt;p&gt;
&#36827;&#23637;&#20013;&#30340;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#26159;&#34913;&#37327;&#65288;&#32463;&#39564;&#65289;&#25968;&#25454;&#20013;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#20351;&#29992;&#23545;&#35937;&#30340;&#25968;&#23383;&#23646;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21457;&#23637;&#20986;&#30340;&#24207;&#21015;&#26041;&#27861;&#25968;&#37327;&#30456;&#23545;&#36739;&#23569;&#12290;&#36896;&#25104;&#36825;&#19968;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#19978;&#20010;&#19990;&#32426;&#65292;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#26080;&#27861;&#28385;&#36275;&#24207;&#21015;&#35745;&#31639;&#25152;&#38656;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#26469;&#35828;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#23545;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#36807;&#20110;&#25968;&#23398;&#20005;&#35880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23558;&#35752;&#35770;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#8220;&#35745;&#31639;&#8221;&#24207;&#21015;&#32467;&#26500;&#65288;&#19968;&#31867;&#29305;&#23450;&#30340;&#26377;&#21521;&#22270;&#65289;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20174;&#20854;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#24314;&#31435;&#20026;&#19968;&#39033;&#20840;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;&#38500;&#20102;&#19982;&#20854;&#20182;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#30340;&#20132;&#21449;&#20114;&#34917;&#22806;&#65292;&#24191;&#27867;&#30340;&#23398;&#31185;&#39046;&#22495;&#20063;&#23558;&#21463;&#30410;&#20110;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order is one of the main instruments to measure the relationship between objects in (empirical) data. However, compared to methods that use numerical properties of objects, the amount of ordinal methods developed is rather small. One reason for this is the limited availability of computational resources in the last century that would have been required for ordinal computations. Another reason -- particularly important for this line of research -- is that order-based methods are often seen as too mathematically rigorous for applying them to real-world data. In this paper, we will therefore discuss different means for measuring and 'calculating' with ordinal structures -- a specific class of directed graphs -- and show how to infer knowledge from them. Our aim is to establish Ordinal Data Science as a fundamentally new research agenda. Besides cross-fertilization with other cornerstone machine learning and knowledge representation methods, a broad range of disciplines will benefit from t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21010;&#20998;&#12289;&#35780;&#20272;&#21644;&#32454;&#21270;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#12290;&#36890;&#36807;&#20998;&#35299;&#22797;&#26434;&#30340;&#25552;&#31034;&#24182;&#20351;&#29992;VQA&#27169;&#22411;&#36827;&#34892;&#27979;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#23545;&#40784;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.04749</link><description>&lt;p&gt;
&#21010;&#20998;&#12289;&#35780;&#20272;&#21644;&#32454;&#21270;&#65306;&#36890;&#36807;&#36845;&#20195;VQA&#21453;&#39304;&#35780;&#20272;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback. (arXiv:2307.04749v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21010;&#20998;&#12289;&#35780;&#20272;&#21644;&#32454;&#21270;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#12290;&#36890;&#36807;&#20998;&#35299;&#22797;&#26434;&#30340;&#25552;&#31034;&#24182;&#20351;&#29992;VQA&#27169;&#22411;&#36827;&#34892;&#27979;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#23545;&#40784;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#20986;&#29616;&#65292;&#20197;&#25991;&#26412;&#20026;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#33879;&#24615;&#65292;&#20294;&#26159;&#38543;&#30528;&#25991;&#26412;&#36755;&#20837;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#20173;&#21487;&#33021;&#26080;&#27861;&#29983;&#25104;&#20934;&#30830;&#20256;&#36798;&#32473;&#23450;&#25552;&#31034;&#35821;&#20041;&#30340;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;&#36825;&#31181;&#19981;&#23545;&#40784;&#24448;&#24448;&#34987;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26410;&#33021;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#20998;&#35299;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#35299;&#23545;&#40784;&#20998;&#25968;&#65292;&#23427;&#23558;&#22797;&#26434;&#25552;&#31034;&#20998;&#35299;&#20026;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#26029;&#35328;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;VQA&#27169;&#22411;&#26469;&#27979;&#37327;&#27599;&#20010;&#26029;&#35328;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#23558;&#19981;&#21516;&#26029;&#35328;&#30340;&#23545;&#40784;&#20998;&#25968;&#21512;&#24182;&#21518;&#65292;&#24471;&#21040;&#26368;&#32456;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimenta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03761</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#30001;&#24037;&#19994;&#29289;&#32852;&#32593; (IIoT) &#30417;&#25511;&#30340;&#31995;&#32479;&#36890;&#36807;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#29983;&#25104;&#22823;&#37327;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015; (MTS) &#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#21161;&#20110;&#26465;&#20214;&#30417;&#25511;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#20063;&#32473;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#24322;&#24120;&#21644;&#32972;&#26223;&#24322;&#24120;&#65292;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#38598;&#20307;&#24322;&#24120;&#30340;&#19968;&#31181;&#24120;&#35265;&#21464;&#31181;&#26159;&#24322;&#24120;&#38598;&#20307;&#34892;&#20026;&#30001;&#31995;&#32479;&#20869;&#37096;&#30340;&#30456;&#20114;&#20851;&#31995;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#24322;&#24120;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#36807;&#28909;&#65289;&#12289;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#30340;&#19981;&#27491;&#30830;&#25805;&#20316;&#35774;&#32622;&#25110;&#31995;&#32479;&#32423;&#25925;&#38556;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DyGATAD&#65288;&#19968;&#31181;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65289;&#65292;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14468</link><description>&lt;p&gt;
&#36890;&#29992;&#26694;&#26550;&#19979;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#30740;&#31350;&#36890;&#29992;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19979;&#23545;&#20004;&#20010;&#36866;&#24212;&#24615;&#32422;&#26463;&#36827;&#34892;&#20102;&#39318;&#27425;&#25506;&#32034;&#65306;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#21644;&#25209;&#27425;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31216;&#20026;Eluder Condition&#31867;&#30340;&#36890;&#29992;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#22312;EC&#31867;&#21035;&#19978;&#23454;&#29616;&#20102;&#22823;&#32422;$ \widetilde{\mathcal{O}}(\log K)$&#30340;&#20999;&#25442;&#20195;&#20215;&#21644;$\widetilde{\mathcal{O}}(\sqrt{K})$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#23545;&#20110;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;$B$&#20010;&#25209;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#22823;&#32422;$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#31687;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#27169;&#22411;&#65292;&#22914;&#34920;&#26684;MDP (Bai et al. 2019; Zhang et al. 2020)&#12289;&#32447;&#24615;MDP (Wang et al. 2021; Gao et al. 2021)&#12289;&#20302;Eluder&#32500;&#24230;MDP (Kong et al. 2021; Gao et al. 2021)&#12289;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#31867;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\widetilde{\mathcal{O}}(\log K) $ switching cost with a $\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09597</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#35825;&#39575;&#65288;Clickbait&#65289;&#20250;&#36890;&#36807;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#29978;&#33267;&#24341;&#20154;&#20837;&#32988;&#30340;&#26631;&#39064;&#26469;&#35825;&#23548;&#29992;&#25143;&#36827;&#34892;&#28857;&#20987;&#65292;&#20960;&#20046;&#28183;&#36879;&#21040;&#25152;&#26377;&#22312;&#32447;&#20869;&#23481;&#21457;&#24067;&#32773;&#65292;&#22914;&#26032;&#38395;&#38376;&#25143;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;NLP&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;LLM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#31995;&#32479;&#36824;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;LLM&#22312;&#22810;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21644;&#24494;&#35843;PLM&#26041;&#27861;&#30456;&#27604;&#65292;LLM&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20154;&#31867;&#30452;&#35273;&#19981;&#21516;&#65292;&#23454;&#39564;&#34920;&#26126;LLM&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#22270;&#30340;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#19982;&#25968;&#25454;&#26679;&#26412;&#26080;&#32541;&#38598;&#25104;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18731</link><description>&lt;p&gt;
&#22522;&#20110;&#35748;&#30693;&#22270;&#30340;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hybrid Representation Learning via Epistemic Graph. (arXiv:2305.18731v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#22270;&#30340;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#19982;&#25968;&#25454;&#26679;&#26412;&#26080;&#32541;&#38598;&#25104;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23494;&#38598;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#36890;&#24120;&#25191;&#34892;&#28151;&#21512;&#23398;&#20064;&#65292;&#20363;&#22914;&#33258;&#21457;&#22320;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#29992;&#20110;&#36328;&#39046;&#22495;&#35782;&#21035;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#19982;&#25968;&#25454;&#26679;&#26412;&#38598;&#25104;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#36825;&#31181;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#28145;&#24230;&#29305;&#24449;&#65288;&#20174;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#65289;&#22312;&#32500;&#24230;&#21644;&#30693;&#35782;&#31890;&#24230;&#19978;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#22270;&#23618;&#65288;EGLayer&#65289;&#65292;&#20197;&#23454;&#29616;&#28151;&#21512;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#28145;&#24230;&#29305;&#24449;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#20043;&#38388;&#26356;&#26377;&#25928;&#22320;&#20132;&#25442;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;EGLayer&#23558;&#28145;&#24230;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#65292;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#20256;&#25773;&#21040;&#28145;&#24230;&#29305;&#24449;&#19978;&#12290;&#36825;&#31181;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep models have achieved remarkable success in many vision tasks. Unfortunately, their performance largely depends on intensive training samples. In contrast, human beings typically perform hybrid learning, e.g., spontaneously integrating structured knowledge for cross-domain recognition or on a much smaller amount of data samples for few-shot learning. Thus it is very attractive to extend hybrid learning for the computer vision tasks by seamlessly integrating structured knowledge with data samples to achieve more effective representation learning. However, such a hybrid learning approach remains a great challenge due to the huge gap between the structured knowledge and the deep features (learned from data samples) on both dimensions and knowledge granularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is developed to enable hybrid learning, such that the information can be exchanged more effectively between the deep features and a structured knowledge gra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;TheoremQA&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#22312;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#65292;&#32463;&#36807;&#27979;&#35797;&#65292;GPT-4&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12524</link><description>&lt;p&gt;
TheoremQA&#65306;&#19968;&#31181;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;TheoremQA&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#22312;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#65292;&#32463;&#36807;&#27979;&#35797;&#65292;GPT-4&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;LLMs&#22914;GPT-4&#21644;PaLM-2&#22312;&#35299;&#20915;&#20687;GSM8K&#36825;&#26679;&#30340;&#22522;&#26412;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35299;&#20915;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65288;&#21363;&#23450;&#29702;&#65289;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TheoremQA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;AI&#27169;&#22411;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;TheoremQA&#30001;&#39046;&#22495;&#19987;&#23478;&#31574;&#21010;&#65292;&#21253;&#21547;&#26469;&#33258;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#30005;&#27668;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#20197;&#21450;&#37329;&#34701;&#23398;&#30340;800&#20010;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#28085;&#30422;350&#20010;&#23450;&#29702;&#65288;&#20363;&#22914;&#27888;&#21202;&#23450;&#29702;&#12289;&#25289;&#26684;&#26391;&#26085;&#23450;&#29702;&#12289;&#21704;&#22827;&#26364;&#32534;&#30721;&#12289;&#37327;&#23376;&#23450;&#29702;&#12289;&#24377;&#24615;&#23450;&#29702;&#31561;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;16&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#20195;&#30721;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;Chain-of-Thoughts&#21644;Program-of-Thoughts&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#26159;&#26080;&#19982;&#20262;&#27604;&#30340;&#65292;&#20351;&#29992;Program-of-Thoughts&#25552;&#31034;&#31574;&#30053;&#26102;&#20934;&#30830;&#29575;&#36798;51%&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#36828;&#36828;&#33853;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, EE&amp;CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Progra
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38754;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#26102;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.05403</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38754;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#26102;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30693;&#35782;&#24211;&#26159;&#30693;&#35782;&#20013;&#24515;&#30340;AI&#30340;&#22522;&#30707;&#12290;&#35768;&#22810;&#30693;&#35782;&#24211;&#26159;&#20174;Web&#26469;&#28304;&#23454;&#29992;&#20027;&#20041;&#26500;&#24314;&#30340;&#65292;&#22240;&#27492;&#36828;&#38750;&#23436;&#25972;&#12290;&#36825;&#32473;&#20869;&#23481;&#30340;&#28040;&#36153;&#21644;&#31649;&#29702;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22914;&#20309;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#30693;&#35782;&#24211;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#65288;i&#65289;&#37096;&#20998;&#23553;&#38381;&#19990;&#30028;&#35821;&#20041;&#19979;&#30340;&#30693;&#35782;&#34920;&#31034;&#21644;&#26597;&#35810;&#30340;&#36923;&#36753;&#22522;&#30784;&#65307;&#65288;ii&#65289;&#36890;&#36807;&#32479;&#35745;&#27169;&#24335;&#20272;&#35745;&#27492;&#20449;&#24687;&#65307;&#65288;iii&#65289;&#20174;&#30693;&#35782;&#24211;&#21644;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#20110;&#21484;&#22238;&#29575;&#30340;&#20449;&#24687;&#65307;&#65288;iv&#65289;&#36776;&#21035;&#26377;&#36259;&#30340;&#21542;&#23450;&#35821;&#21477;&#65307;&#20197;&#21450;&#65288;v&#65289;&#30456;&#23545;&#21484;&#22238;&#29575;&#30340;&#23485;&#26494;&#27010;&#24565;&#12290;&#26412;&#35843;&#26597;&#38024;&#23545;&#20004;&#31867;&#21463;&#20247;&#65306;&#65288;1&#65289;&#23547;&#27714;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30693;&#35782;&#25351;&#21335;&#30340;&#20174;&#19994;&#32773;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26088;&#22312;&#25512;&#36827;&#30693;&#35782;&#24211;&#31649;&#29702;&#12289;&#36136;&#37327;&#35780;&#20272;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric AI. Many of them are constructed pragmatically from Web sources, and are thus far from complete. This poses challenges for the consumption as well as the curation of their content. While several surveys target the problem of completing incomplete KBs, the first problem is arguably to know whether and where the KB is incomplete in the first place, and to which degree.  In this survey we discuss how knowledge about completeness, recall, and negation in KBs can be expressed, extracted, and inferred. We cover (i) the logical foundations of knowledge representation and querying under partial closed-world semantics; (ii) the estimation of this information via statistical patterns; (iii) the extraction of information about recall from KBs and text; (iv) the identification of interesting negative statements; and (v) relaxed notions of relative recall.  This survey is targeted at two types of audiences: (1) practitione
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.10629</link><description>&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10629
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#25968;&#25454;&#20016;&#23500;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#26041;&#38754;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29978;&#33267;&#21487;&#20197;&#23398;&#20064;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#20197;&#20415;&#26377;&#25928;&#22320;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#32454;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#25968;&#25454;&#26377;&#38480;&#65307;&#65288;ii&#65289;&#27169;&#22411;&#24320;&#21457;&#25104;&#26412;&#21463;&#38480;&#65307;&#65288;iii&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#20415;&#26377;&#25928;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#27010;&#24565;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#36890;&#36807;&#20174;&#28304;&#39046;&#22495;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#19968;&#20010;&#31934;&#24515;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#32454;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#21487;&#20197;&#24046;&#24322;&#24040;&#22823;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use c
&lt;/p&gt;</description></item></channel></rss>