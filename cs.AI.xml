<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#25968;&#25454;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20934;&#65292;&#19968;&#20010;&#35757;&#32451;-free&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#19982;&#26816;&#32034;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#25581;&#31034;&#29983;&#25104;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13697</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20934;&#27979;&#35797;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Analyzing Generative Data for Visual Recognition. (arXiv:2307.13697v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13697
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#25968;&#25454;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20934;&#65292;&#19968;&#20010;&#35757;&#32451;-free&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#19982;&#26816;&#32034;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#25581;&#31034;&#29983;&#25104;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#25193;&#22823;&#20102;&#23427;&#20204;&#20316;&#20026;&#26377;&#25928;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;&#65292;&#20027;&#35201;&#27604;&#36739;&#20102;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#65288;&#22914;&#29983;&#25104;&#25968;&#25454;&#12289;&#26816;&#32034;&#25968;&#25454;&#12289;&#21407;&#22987;&#25968;&#25454;&#65289;&#30340;&#33539;&#20363;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;1) GenBench&#26500;&#24314;&#65306;&#25105;&#20204;&#35774;&#35745;&#20102;GenBench&#65292;&#19968;&#20010;&#21253;&#21547;22&#20010;&#25968;&#25454;&#38598;&#21644;2548&#20010;&#31867;&#21035;&#30340;&#24191;&#27867;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;2) CLER&#20998;&#25968;&#65306;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;FID&#12289;CLIP&#20998;&#25968;&#65289;&#19982;&#19979;&#28216;&#35782;&#21035;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#36275;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLER&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#25351;&#31034;&#35782;&#21035;&#20219;&#21153;&#35757;&#32451;&#20043;&#21069;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;3) &#26032;&#30340;&#22522;&#20934;&#32447;&#65306;&#23558;&#29983;&#25104;&#25968;&#25454;&#19982;&#26469;&#33258;&#30456;&#21516;&#22806;&#37096;&#27744;&#30340;&#26816;&#32034;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#26377;&#21161;&#20110;&#38416;&#26126;&#29983;&#25104;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;4) &#22806;&#37096;&#30693;&#35782;&#27880;&#20837;&#65306;&#36890;&#36807;&#27880;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#29983;&#25104;&#25968;&#25454;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition. This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\ie generative \vs retrieval \vs original).  Our key contributions are: \textbf{1) GenBench Construction:} We devise \textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks. \textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\eg, FID, CLIP score) with downstream recognition performance, we propose \textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training. \textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data. \textbf{4) External Knowledge Injection:} By 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;text2fabric&#65292;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#25551;&#36848;&#19982;&#38754;&#26009;&#26448;&#26009;&#30456;&#20851;&#32852;&#65292;&#26088;&#22312;&#25552;&#39640;&#26448;&#26009;&#25551;&#36848;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20204;&#22914;&#20309;&#25551;&#36848;&#38754;&#26009;&#30340;&#32039;&#20945;&#35789;&#27719;&#12289;&#23646;&#24615;&#38598;&#21512;&#21644;&#20851;&#38190;&#32467;&#26500;&#21487;&#20197;&#25351;&#23548;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#26448;&#26009;&#65292;&#24182;&#19988;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#19987;&#38376;&#21270;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.13681</link><description>&lt;p&gt;
&#38754;&#26009;&#30340;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
The Visual Language of Fabrics. (arXiv:2307.13681v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;text2fabric&#65292;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#25551;&#36848;&#19982;&#38754;&#26009;&#26448;&#26009;&#30456;&#20851;&#32852;&#65292;&#26088;&#22312;&#25552;&#39640;&#26448;&#26009;&#25551;&#36848;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20204;&#22914;&#20309;&#25551;&#36848;&#38754;&#26009;&#30340;&#32039;&#20945;&#35789;&#27719;&#12289;&#23646;&#24615;&#38598;&#21512;&#21644;&#20851;&#38190;&#32467;&#26500;&#21487;&#20197;&#25351;&#23548;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#26448;&#26009;&#65292;&#24182;&#19988;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#19987;&#38376;&#21270;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;text2fabric&#65292;&#19968;&#20010;&#23558;&#33258;&#30001;&#25991;&#26412;&#25551;&#36848;&#19982;&#21508;&#31181;&#38754;&#26009;&#26448;&#26009;&#20851;&#32852;&#36215;&#26469;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;15,000&#20010;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#19982;3,000&#20010;&#30456;&#24212;&#30340;&#38754;&#26009;&#26448;&#26009;&#22270;&#20687;&#30456;&#20851;&#32852;&#12290;&#20256;&#32479;&#19978;&#65292;&#26448;&#26009;&#25551;&#36848;&#20197;&#26631;&#31614;/&#20851;&#38190;&#23383;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#23545;&#36866;&#24403;&#35789;&#27719;&#30340;&#24050;&#26377;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#23548;&#33268;&#20102;&#19968;&#31181;&#34987;&#20999;&#21106;&#30340;&#25551;&#36848;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#26469;&#25551;&#36848;&#26448;&#26009;&#22806;&#35266;&#30340;&#26356;&#36866;&#24403;&#26041;&#24335;&#65292;&#23558;&#38754;&#26009;&#20316;&#20026;&#38750;&#19987;&#23478;&#32463;&#24120;&#22788;&#29702;&#30340;&#24120;&#35265;&#29289;&#21697;&#20351;&#29992;&#26696;&#20363;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20174;&#25551;&#36848;&#20013;&#24471;&#20986;&#30340;&#32039;&#20945;&#35789;&#27719;&#12289;&#23646;&#24615;&#38598;&#21512;&#21644;&#20851;&#38190;&#32467;&#26500;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#29702;&#35299;&#20154;&#20204;&#22914;&#20309;&#25551;&#36848;&#38754;&#26009;&#65292;&#24182;&#20026;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#26448;&#26009;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#20351;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLI&#65289;&#19987;&#38376;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce text2fabric, a novel dataset that links free-text descriptions to various fabric materials. The dataset comprises 15,000 natural language descriptions associated to 3,000 corresponding images of fabric materials. Traditionally, material descriptions come in the form of tags/keywords, which limits their expressivity, induces pre-existing knowledge of the appropriate vocabulary, and ultimately leads to a chopped description system. Therefore, we study the use of free-text as a more appropriate way to describe material appearance, taking the use case of fabrics as a common item that non-experts may often deal with. Based on the analysis of the dataset, we identify a compact lexicon, set of attributes and key structure that emerge from the descriptions. This allows us to accurately understand how people describe fabrics and draw directions for generalization to other types of materials. We also show that our dataset enables specializing large vision-language models such as CLI
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13658</link><description>&lt;p&gt;
&#20851;&#20110;AI&#38382;&#36131;&#25919;&#31574;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#20316;&#20986;&#30340;&#22238;&#24212;&#12290;&#22312;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#20851;&#38190;&#21477;&#23376;&#26411;&#23614;&#65292;&#25552;&#20379;&#20102;&#35201;&#27714;&#35780;&#35770;&#30340;&#38382;&#39064;&#32534;&#21495;&#30340;&#19978;&#26631;&#12290;&#35813;&#30333;&#30382;&#20070;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
&lt;/p&gt;</description></item><item><title>QuickQual&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24182;&#22312;EyeQ&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35777;&#26126;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#22270;&#20687;&#30340;&#36890;&#29992;&#24863;&#30693;&#29305;&#24449;&#26469;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2307.13646</link><description>&lt;p&gt;
QuickQual: &#20351;&#29992;&#29616;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36731;&#37327;&#12289;&#20415;&#25463;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
QuickQual: Lightweight, convenient retinal image quality scoring with off-the-shelf pretrained models. (arXiv:2307.13646v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13646
&lt;/p&gt;
&lt;p&gt;
QuickQual&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24182;&#22312;EyeQ&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35777;&#26126;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#22270;&#20687;&#30340;&#36890;&#29992;&#24863;&#30693;&#29305;&#24449;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#36136;&#37327;&#23545;&#20110;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#37117;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20294;&#26159;&#35782;&#21035;&#20302;&#36136;&#37327;&#22270;&#20687;&#21487;&#33021;&#32791;&#26102;&#19988;&#20027;&#35266;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;MCFNet&#65292;&#30001;&#19977;&#20010;&#22312;&#19981;&#21516;&#39068;&#33394;&#31354;&#38388;&#20013;&#36816;&#34892;&#30340;Densenet121&#20027;&#24178;&#32452;&#25104;&#12290;MCFNet&#21644;&#21516;&#19968;&#20316;&#32773;&#21457;&#24067;&#30340;EyeQ&#25968;&#25454;&#38598;&#23545;&#20110;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#36827;&#27493;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QuickQual&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#29616;&#25104;&#30340;ImageNet&#39044;&#35757;&#32451;&#30340;Densenet121&#20027;&#24178;&#21644;&#19968;&#20010;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12290;QuickQual&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#20026;EyeQ&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65288;&#20934;&#30830;&#29575;&#65306;88.50% vs MCFNet&#30340;88.00%&#65307;AUC&#65306;0.9687 vs 0.9588&#65289;&#12290;&#36825;&#34920;&#26126;&#65292;&#35270;&#32593;&#33180;&#22270;&#20687;&#36136;&#37327;&#35780;&#20998;&#21487;&#20197;&#21033;&#29992;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#24863;&#30693;&#29305;&#24449;&#26469;&#35299;&#20915;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#22823;&#37327;&#30524;&#24213;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image quality remains a key problem for both traditional and deep learning (DL)-based approaches to retinal image analysis, but identifying poor quality images can be time consuming and subjective. Thus, automated methods for retinal image quality scoring (RIQS) are needed. The current state-of-the-art is MCFNet, composed of three Densenet121 backbones each operating in a different colour space. MCFNet, and the EyeQ dataset released by the same authors, was a huge step forward for RIQS. We present QuickQual, a simple approach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained Densenet121 backbone plus a Support Vector Machine (SVM). QuickQual performs very well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00% for MCFNet; AUC: 0.9687 vs 0.9588). This suggests that RIQS can be solved with generic perceptual features learned on natural images, as opposed to requiring DL models trained on large amounts of fundus images. Additionally, we propose a Fixed Pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.13642</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#33258;&#20027;&#25511;&#21046;&#22120;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#37117;&#21487;&#33021;&#19981;&#23433;&#20840;&#12290;&#33021;&#22815;&#23450;&#37327;&#22320;&#30830;&#23450;&#20309;&#26102;&#20250;&#21457;&#29983;&#36825;&#20123;&#19981;&#23433;&#20840;&#24773;&#20917;&#23545;&#20110;&#21450;&#26102;&#24341;&#20837;&#20154;&#31867;&#30417;&#30563;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#36135;&#36816;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20195;&#29702;&#30340;&#24773;&#20917;&#30340;&#30495;&#27491;&#20851;&#38190;&#24615;&#21487;&#20197;&#34987;&#31283;&#20581;&#22320;&#23450;&#20041;&#20026;&#22312;&#19968;&#20123;&#38543;&#26426;&#21160;&#20316;&#19979;&#22870;&#21169;&#30340;&#24179;&#22343;&#20943;&#23569;&#12290;&#21487;&#20197;&#23558;&#23454;&#26102;&#21487;&#35745;&#31639;&#30340;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#65288;&#21363;&#65292;&#26080;&#38656;&#23454;&#38469;&#27169;&#25311;&#38543;&#26426;&#21160;&#20316;&#30340;&#24433;&#21709;&#65289;&#19982;&#30495;&#27491;&#30340;&#20851;&#38190;&#24615;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20195;&#29702;&#25351;&#26631;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#65292;&#23558;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#30452;&#25509;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;Atari&#29615;&#22659;&#20013;&#36890;&#36807;APE-X&#21644;A3C&#30340;&#23398;&#20064;&#31574;&#30053;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#30340;&#20943;&#23567;&#12290;&#23558;&#23433;&#20840;&#36793;&#30028;&#25972;&#21512;&#21040;&#30417;&#25511;&#31243;&#24207;&#20013;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monit
&lt;/p&gt;</description></item><item><title>GPT-3&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#34920;&#29616;&#26377;&#38480;&#65292;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#26469;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13617</link><description>&lt;p&gt;
GPT-3&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#37329;&#34701;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13617
&lt;/p&gt;
&lt;p&gt;
GPT-3&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#34920;&#29616;&#26377;&#38480;&#65292;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#26469;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20998;&#26512;&#26159;&#35780;&#20272;&#20844;&#21496;&#19994;&#32489;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20174;&#19994;&#32773;&#36890;&#36807;&#28145;&#20837;&#30340;&#37327;&#21270;&#20998;&#26512;&#22238;&#31572;&#37329;&#34701;&#38382;&#39064;&#65292;&#20174;&#32780;&#20570;&#20986;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#37329;&#34701;&#38382;&#31572;&#26159;&#19968;&#20010;&#38656;&#35201;&#23545;&#25968;&#23383;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#30340;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25512;&#29702;&#33021;&#21147;&#22914;&#20309;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#38656;&#35201;&#19968;&#20010;&#26816;&#32034;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25910;&#38598;&#19982;&#37329;&#34701;&#38382;&#39064;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#37329;&#34701;&#31243;&#24207;&#21644;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-3&#20165;&#20165;&#36890;&#36807;&#23569;&#37327;&#31034;&#20363;&#23601;&#23454;&#29616;&#20102;&#24191;&#27867;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;GPT-3&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#21457;&#29616;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#20173;&#28982;&#26159;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of finan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;(DIQNN)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#24341;&#20837;&#36793;&#30028;&#26469;&#21051;&#30011;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23558;&#36793;&#30028;&#25972;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21152;&#36895;&#20102;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.13609</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dendritic Integration Based Quadratic Neural Networks Outperform Traditional Aritificial Ones. (arXiv:2307.13609v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13609
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;(DIQNN)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#24341;&#20837;&#36793;&#30028;&#26469;&#21051;&#30011;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23558;&#36793;&#30028;&#25972;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21152;&#36895;&#20102;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#29305;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20197;&#22686;&#24378;&#35745;&#31639;&#33021;&#21147;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26368;&#36817;&#21457;&#29616;&#30340;&#26641;&#31361;&#36981;&#24490;&#20108;&#27425;&#32508;&#21512;&#35268;&#21017;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;(DIQNN)&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#38477;&#20302;DIQNN&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20302;&#31209;DIQNN&#65292;&#21457;&#29616;&#20854;&#21487;&#20197;&#20445;&#25345;&#21407;&#22987;DIQNN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#30028;&#26469;&#21051;&#30011;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#36825;&#20010;&#36793;&#30028;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#21333;&#35843;&#22686;&#21152;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27867;&#21270;&#35823;&#24046;&#19982;&#36793;&#30028;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20010;&#36793;&#30028;&#25972;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#21518;&#65292;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#25913;&#21464;&#30830;&#23454;&#21152;&#36895;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities poses a formidable challenge in the field of machine learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rules for synaptic inputs, we propose a novel ANN model, Dendritic Integration-Based Quadratic Neural Network (DIQNN). This model shows superior performance over traditional ANNs in a variety of classification tasks. To reduce the computational cost of DIQNN, we introduce the Low-Rank DIQNN, while we find it can retain the performance of the original DIQNN. We further propose a margin to characterize the generalization error and theoretically prove this margin will increase monotonically during training. And we show the consistency between generalization and our margin using numerical experiments. Finally, by integrating this margin into the loss function, the change of test accuracy is indeed accelerated. Our work cont
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RenderSelect&#30340;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;&#65292;&#29992;&#20110;&#21457;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#31526;&#21512;&#21151;&#33021;&#35201;&#27714;&#30340;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13604</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#21457;&#29616;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Cloud Render Farm Services Discovery Using NLP And Ontology Based Knowledge Graph. (arXiv:2307.13604v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13604
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RenderSelect&#30340;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;&#65292;&#29992;&#20110;&#21457;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#31526;&#21512;&#21151;&#33021;&#35201;&#27714;&#30340;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#26159;&#38024;&#23545;&#21160;&#30011;&#39046;&#22495;&#30340;&#29305;&#23450;&#20113;&#26381;&#21153;&#24179;&#21488;&#65288;PaaS&#65289;&#31867;&#22411;&#30340;&#20113;&#26381;&#21153;&#65292;&#25552;&#20379;&#23436;&#25972;&#30340;&#24179;&#21488;&#26469;&#28210;&#26579;&#21160;&#30011;&#25991;&#20214;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#25104;&#26412;&#25928;&#30410;&#19988;&#31526;&#21512;&#21151;&#33021;&#35201;&#27714;&#65288;&#22914;&#21160;&#30011;&#36719;&#20214;&#12289;&#25152;&#38656;&#25554;&#20214;&#31561;&#65289;&#30340;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#30340;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;RenderSelect&#65292;&#29992;&#20110;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#12290;&#20113;&#28210;&#26579;&#20892;&#22330;&#26412;&#20307;&#35821;&#20041;&#19978;&#23450;&#20041;&#20102;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37319;&#29992;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#31639;&#27861;&#65292;&#21253;&#25324;&#27010;&#24565;&#30456;&#20284;&#24615;&#25512;&#29702;&#12289;&#31561;&#20215;&#25512;&#29702;&#21644;&#25968;&#20540;&#30456;&#20284;&#24615;&#25512;&#29702;&#65292;&#26469;&#30830;&#23450;&#20113;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21363;a&#65289;&#21033;&#29992;&#26412;&#20307;&#24110;&#21161;&#65292;b&#65289;
&lt;/p&gt;
&lt;p&gt;
Cloud render farm services are the animation domain specific cloud services Platform-as-a-Service (PaaS) type of cloud services that provides a complete platform to render the animation files. However, identifying the render farm services that is cost effective and also matches the functional requirements that changes for almost every project like the animation software, plug-ins required etc., is a challenge. This research work proposes an ontology-based service discovery engine named RenderSelect for the cloud render farm services. The cloud render farm ontology semantically defines the relationship among the cloud render farm services. The knowledge-based reasoning algorithms namely, the Concept similarity reasoning, Equivalent reasoning and the Numerical similarity reasoning have been applied to determine the similarity among the cloud services. The service discovery engine was evaluated for finding the services under three different scenarios namely a) with help of the ontology, b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#20013;&#22635;&#34917;&#20102;&#35299;&#37322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.13582</link><description>&lt;p&gt;
&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#20013;&#30340;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks. (arXiv:2307.13582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#20013;&#22635;&#34917;&#20102;&#35299;&#37322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26377;&#20960;&#20010;&#20154;&#25552;&#20513;&#35770;&#35777;&#24615;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#35770;&#35777;&#26694;&#26550;&#65288;AFs&#65289;&#30340;&#25512;&#29702;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#34429;&#28982;&#20851;&#20110;&#29992;&#36777;&#35770;/&#20105;&#35770;/&#23545;&#35805;&#30340;&#25193;&#23637;&#35821;&#20041;&#31934;&#31070;&#23450;&#24615;&#22320;&#35299;&#37322;AFs&#30340;&#25512;&#29702;&#32467;&#26524;&#30340;&#30740;&#31350;&#25104;&#26524;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#28176;&#36827;&#35821;&#20041;&#19979;&#35299;&#37322;AFs&#30340;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#21364;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#65292;&#23613;&#31649;&#22312;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#24402;&#22240;&#31934;&#31070;&#24341;&#20837;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#30340;&#32972;&#26223;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#32780;&#29305;&#24449;&#24402;&#22240;&#21017;&#29992;&#20110;&#30830;&#23450;&#29305;&#24449;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of \emph{extension-based semantics}, explaining the quantitative reasoning outcomes of AFs under \emph{gradual semantics} has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of \emph{Argument Attribution Explanations (AAEs)} by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards \emph{topic argument}s 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#23384;&#20998;&#26512;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#36830;&#25509;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#26080;&#38656;&#25968;&#20540;&#31215;&#20998;&#30340;&#36890;&#29992;&#25311;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13579</link><description>&lt;p&gt;
&#22312;&#36890;&#29992;&#25311;&#21512;&#22120;&#26102;&#20195;&#37325;&#26032;&#35299;&#35835;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reinterpreting survival analysis in the universal approximator age. (arXiv:2307.13579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#23384;&#20998;&#26512;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#36830;&#25509;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#26080;&#38656;&#25968;&#20540;&#31215;&#20998;&#30340;&#36890;&#29992;&#25311;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#32479;&#35745;&#23398;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#32463;&#20856;&#32479;&#35745;&#39046;&#22495;&#24050;&#32463;&#25509;&#21463;&#20102;&#28145;&#24230;&#23398;&#20064;&#65292;&#20294;&#26159;&#29983;&#23384;&#20998;&#26512;&#30452;&#21040;&#26368;&#36817;&#25165;&#24341;&#36215;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#20123;&#27880;&#24847;&#12290;&#36825;&#19968;&#26368;&#36817;&#30340;&#21457;&#23637;&#21487;&#33021;&#37096;&#20998;&#21463;&#21040;COVID-19&#22823;&#27969;&#34892;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#29983;&#23384;&#20998;&#26512;&#28508;&#21147;&#25152;&#38656;&#30340;&#24037;&#20855;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#23384;&#20998;&#26512;&#19982;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25216;&#26415;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#38656;&#25968;&#20540;&#31215;&#20998;&#20135;&#29983;&#29983;&#23384;&#26354;&#32447;&#30340;&#36890;&#29992;&#25311;&#21512;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13566</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Imperfect XAI on Human-AI Decision-Making. (arXiv:2307.13566v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#25913;&#36827;&#21508;&#31181;&#21512;&#20316;&#24037;&#20316;&#29615;&#22659;&#19979;&#30340;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#20915;&#31574;&#32773;&#19982;&#19981;&#23436;&#32654;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21512;&#36866;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#20415;&#35774;&#35745;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#21327;&#20316;&#24037;&#20855;&#12290;&#19968;&#20123;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#24076;&#26395;&#25913;&#21892;&#20915;&#31574;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#20316;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#20027;&#35201;&#20851;&#27880;&#38169;&#35823;&#30340;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#25215;&#35748;&#21363;&#20351;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#27491;&#30830;&#65292;&#35299;&#37322;&#20063;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#19981;&#23436;&#32654;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24378;&#22823;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#28041;&#21450;136&#21517;&#21442;&#19982;&#32773;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making beha
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39764;&#26041;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;PDDL&#35821;&#35328;&#65292;&#20351;&#20854;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepCubeA&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#39764;&#26041;&#38382;&#39064;&#65292;&#20294;&#21482;&#26377;18&#65285;&#26159;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13552</link><description>&lt;p&gt;
&#29992;&#39046;&#22495;&#26080;&#20851;&#35268;&#21010;&#22120;&#21644;&#26631;&#20934;&#34920;&#31034;&#35299;&#20915;&#39764;&#26041;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Solving the Rubik's Cube with Domain-Independent Planners Using Standard Representations. (arXiv:2307.13552v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39764;&#26041;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;PDDL&#35821;&#35328;&#65292;&#20351;&#20854;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepCubeA&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#39764;&#26041;&#38382;&#39064;&#65292;&#20294;&#21482;&#26377;18&#65285;&#26159;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39764;&#26041;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#65292;&#24050;&#32463;&#28608;&#21457;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#39640;&#25928;&#30340;&#26367;&#20195;&#34920;&#31034;&#21644;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#39764;&#26041;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#27969;&#34892;&#30340;PDDL&#35821;&#35328;&#65292;&#20351;&#24471;&#39046;&#22495;&#23545;PDDL&#35268;&#21010;&#22120;&#12289;&#31454;&#36187;&#21644;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#26356;&#21152;&#21487;&#35775;&#38382;&#21644;&#26131;&#35835;&#12290;&#28982;&#21518;&#25105;&#20204;&#27604;&#36739;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19968;&#20010;&#21487;&#27604;&#36739;&#30340;&#23454;&#39564;&#20013;&#65292;DeepCubeA&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#21482;&#26377;18&#65285;&#26159;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rubik's Cube (RC) is a well-known and computationally challenging puzzle that has motivated AI researchers to explore efficient alternative representations and problem-solving methods. The ideal situation for planning here is that a problem be solved optimally and efficiently represented in a standard notation using a general-purpose solver and heuristics. The fastest solver today for RC is DeepCubeA with a custom representation, and another approach is with Scorpion planner with State-Action-Space+ (SAS+) representation. In this paper, we present the first RC representation in the popular PDDL language so that the domain becomes more accessible to PDDL planners, competitions, and knowledge engineering tools, and is more human-readable. We then bridge across existing approaches and compare performance. We find that in one comparable experiment, DeepCubeA solves all problems with varying complexities, albeit only 18\% are optimal plans. For the same problem set, Scorpion with SAS+ repre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35268;&#21010;&#26412;&#20307;&#34920;&#31034;&#21644;&#21033;&#29992;&#35268;&#21010;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#33258;&#21160;&#35268;&#21010;&#30340;&#24615;&#33021;&#25928;&#29575;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#21010;&#26412;&#20307;&#65292;&#24182;&#21033;&#29992;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#20307;&#33021;&#22815;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#20351;&#29992;&#20174;&#26412;&#20307;&#20013;&#25552;&#21462;&#30340;&#23439;&#35266;&#32422;&#26463;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13549</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24615;&#33021;&#25928;&#29575;&#30340;&#35268;&#21010;&#26412;&#20307;&#34920;&#31034;&#21644;&#21033;&#29992;&#35268;&#21010;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency. (arXiv:2307.13549v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35268;&#21010;&#26412;&#20307;&#34920;&#31034;&#21644;&#21033;&#29992;&#35268;&#21010;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#33258;&#21160;&#35268;&#21010;&#30340;&#24615;&#33021;&#25928;&#29575;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#21010;&#26412;&#20307;&#65292;&#24182;&#21033;&#29992;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#20307;&#33021;&#22815;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#20351;&#29992;&#20174;&#26412;&#20307;&#20013;&#25552;&#21462;&#30340;&#23439;&#35266;&#32422;&#26463;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#33258;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#21010;&#26412;&#20307;&#65292;&#21033;&#29992;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#65288;IPC&#65289;&#30340;&#35268;&#21010;&#39046;&#22495;&#21644;&#35268;&#21010;&#22120;&#30340;&#25968;&#25454;&#65292;&#22312;&#20004;&#20010;&#20351;&#29992;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26412;&#20307;&#33021;&#22815;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#36890;&#36807;&#20174;&#35268;&#21010;&#26412;&#20307;&#20013;&#25552;&#21462;&#30340;&#23439;&#35266;&#32422;&#26463;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies are known for their ability to organize rich metadata, support the identification of novel insights via semantic queries, and promote reuse. In this paper, we consider the problem of automated planning, where the objective is to find a sequence of actions that will move an agent from an initial state of the world to a desired goal state. We hypothesize that given a large number of available planners and diverse planning domains; they carry essential information that can be leveraged to identify suitable planners and improve their performance for a domain. We use data on planning domains and planners from the International Planning Competition (IPC) to construct a planning ontology and demonstrate via experiments in two use cases that the ontology can lead to the selection of promising planners and improving their performance using macros - a form of action ordering constraints extracted from planning ontology. We also make the planning ontology and associated resources avail
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#20840;&#23616;&#20114;&#21160;&#24615;&#21644;&#27963;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13541</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;:&#19968;&#39033;&#20840;&#38754;&#30340;&#22238;&#39038;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Group Activity Recognition in Computer Vision: A Comprehensive Review, Challenges, and Future Perspectives. (arXiv:2307.13541v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#20840;&#23616;&#20114;&#21160;&#24615;&#21644;&#27963;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#36890;&#36807;&#32676;&#20307;&#20851;&#31995;&#35782;&#21035;&#27963;&#21160;&#22312;&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#22312;&#35270;&#39057;&#20998;&#26512;&#12289;&#30417;&#25511;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#29702;&#35299;&#31038;&#20132;&#27963;&#21160;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#35813;&#27169;&#22411;&#30340;&#20851;&#38190;&#33021;&#21147;&#21253;&#25324;&#39640;&#25928;&#22320;&#24314;&#27169;&#22330;&#26223;&#20013;&#30340;&#20998;&#23618;&#20851;&#31995;&#65292;&#20197;&#21450;&#20934;&#30830;&#22320;&#25552;&#21462;&#32676;&#20307;&#30340;&#29420;&#29305;&#26102;&#31354;&#29305;&#24449;&#12290;&#37492;&#20110;&#36825;&#31181;&#25216;&#26415;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#32676;&#20307;&#27963;&#21160;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;&#24403;&#21069;&#30340;&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#20840;&#23616;&#20114;&#21160;&#24615;&#21644;&#27963;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#21644;&#21508;&#31181;&#32676;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#22522;&#20110;&#31354;&#38388;&#32467;&#26500;&#12289;&#25551;&#36848;&#31526;&#12289;&#38750;&#28145;&#24230;&#23398;&#20064;&#12289;&#20998;&#23618;&#32467;&#26500;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group activity recognition is a hot topic in computer vision. Recognizing activities through group relationships plays a vital role in group activity recognition. It holds practical implications in various scenarios, such as video analysis, surveillance, automatic driving, and understanding social activities. The model's key capabilities encompass efficiently modeling hierarchical relationships within a scene and accurately extracting distinctive spatiotemporal features from groups. Given this technology's extensive applicability, identifying group activities has garnered significant research attention. This work examines the current progress in technology for recognizing group activities, with a specific focus on global interactivity and activities. Firstly, we comprehensively review the pertinent literature and various group activity recognition approaches, from traditional methodologies to the latest methods based on spatial structure, descriptors, non-deep learning, hierarchical re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#24341;&#23548;&#30340;&#22810;&#31890;&#24230;&#21442;&#32771;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#21442;&#32771;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#20013;&#30340;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#20998;&#21106;&#32534;&#30721;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#32454;&#33410;&#20248;&#21270;&#25513;&#33180;&#65292;&#20197;&#21450;&#22312;&#39057;&#35889;&#22495;&#20013;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#26368;&#32456;&#65292;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#23545;&#35937;R-VOS&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#20998;&#21106;&#35270;&#39057;&#20013;&#22810;&#20010;&#21442;&#32771;&#23545;&#35937;&#30340;&#21151;&#33021;&#65292;&#20351;&#24471;R-VOS&#26356;&#24555;&#36895;&#12289;&#26356;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13537</link><description>&lt;p&gt;
&#20809;&#35889;&#24341;&#23548;&#30340;&#22810;&#31890;&#24230;&#21442;&#32771;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Spectrum-guided Multi-granularity Referring Video Object Segmentation. (arXiv:2307.13537v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#24341;&#23548;&#30340;&#22810;&#31890;&#24230;&#21442;&#32771;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#21442;&#32771;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#20013;&#30340;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#20998;&#21106;&#32534;&#30721;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#32454;&#33410;&#20248;&#21270;&#25513;&#33180;&#65292;&#20197;&#21450;&#22312;&#39057;&#35889;&#22495;&#20013;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#26368;&#32456;&#65292;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#23545;&#35937;R-VOS&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#20998;&#21106;&#35270;&#39057;&#20013;&#22810;&#20010;&#21442;&#32771;&#23545;&#35937;&#30340;&#21151;&#33021;&#65292;&#20351;&#24471;R-VOS&#26356;&#24555;&#36895;&#12289;&#26356;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#21442;&#32771;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;R-VOS&#65289;&#25216;&#26415;&#20174;&#32534;&#30721;&#65288;&#20302;&#20998;&#36776;&#29575;&#65289;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20013;&#25552;&#21462;&#26465;&#20214;&#26680;&#65292;&#29992;&#20110;&#20998;&#21106;&#35299;&#30721;&#21518;&#30340;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#29305;&#24449;&#28418;&#31227;&#65292;&#20351;&#24471;&#20998;&#21106;&#26680;&#22312;&#27491;&#21521;&#35745;&#31639;&#36807;&#31243;&#20013;&#38590;&#20197;&#23519;&#35273;&#12290;&#36825;&#23545;&#20998;&#21106;&#26680;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#28418;&#31227;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#24341;&#23548;&#30340;&#22810;&#31890;&#24230;&#65288;SgMg&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#32534;&#30721;&#29305;&#24449;&#36827;&#34892;&#30452;&#25509;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#35270;&#35273;&#32454;&#33410;&#36827;&#19968;&#27493;&#20248;&#21270;&#25513;&#33180;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20809;&#35889;&#24341;&#23548;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#65288;SCF&#65289;&#65292;&#22312;&#39057;&#35889;&#22495;&#20013;&#25191;&#34892;&#24103;&#20869;&#20840;&#23616;&#20132;&#20114;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;SgMg&#25193;&#23637;&#21040;&#25191;&#34892;&#22810;&#23545;&#35937;R-VOS&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#21516;&#26102;&#20998;&#21106;&#35270;&#39057;&#20013;&#30340;&#22810;&#20010;&#21442;&#32771;&#23545;&#35937;&#12290;&#36825;&#19981;&#20165;&#20351;R-VOS&#21464;&#24471;&#26356;&#24555;&#65292;&#32780;&#19988;&#26356;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current referring video object segmentation (R-VOS) techniques extract conditional kernels from encoded (low-resolution) vision-language features to segment the decoded high-resolution features. We discovered that this causes significant feature drift, which the segmentation kernels struggle to perceive during the forward computation. This negatively affects the ability of segmentation kernels. To address the drift problem, we propose a Spectrum-guided Multi-granularity (SgMg) approach, which performs direct segmentation on the encoded features and employs visual details to further optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion (SCF) to perform intra-frame global interactions in the spectral domain for effective multimodal representation. Finally, we extend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video. This not only makes R-VOS faster, but also more practical. Extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65288;RmLR&#65289;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#26412;&#30693;&#35782;&#22686;&#24378;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#65292;&#36890;&#36807;&#20877;&#25366;&#25496;&#31574;&#30053;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#30340;&#21477;&#23376;&#21644;&#35789;&#32423;&#23545;&#40784;&#20197;&#21450;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#26469;&#35299;&#20915;&#22810;&#23545;&#22810;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13529</link><description>&lt;p&gt;
Re-mine, Learn and Reason: &#25506;&#32034;&#35821;&#35328;&#24341;&#23548;&#19979;&#36328;&#27169;&#24577;&#35821;&#20041;&#30456;&#20851;&#24615;&#30340;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection. (arXiv:2307.13529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65288;RmLR&#65289;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#26412;&#30693;&#35782;&#22686;&#24378;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#65292;&#36890;&#36807;&#20877;&#25366;&#25496;&#31574;&#30053;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#30340;&#21477;&#23376;&#21644;&#35789;&#32423;&#23545;&#40784;&#20197;&#21450;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#26469;&#35299;&#20915;&#22810;&#23545;&#22810;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#38656;&#35201;&#35270;&#35273;&#27169;&#22411;&#35299;&#20915;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#39044;&#27979;HOI&#19977;&#20803;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65288;RmLR&#65289;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#26412;&#30693;&#35782;&#26469;&#22686;&#24378;HOI&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#20102;&#20004;&#38454;&#27573;HOI&#26816;&#27979;&#22120;&#20013;&#20132;&#20114;&#20449;&#24687;&#30340;&#25439;&#22833;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20877;&#25366;&#25496;&#31574;&#30053;&#26469;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#21477;&#23376;&#21644;&#35789;&#32423;&#23545;&#40784;&#20197;&#21450;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#22810;&#20010;&#20132;&#20114;&#21644;&#22810;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#22810;&#23545;&#22810;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#20123;&#31574;&#30053;&#20943;&#36731;&#20102;&#22810;&#20010;&#20132;&#20114;&#23548;&#33268;&#30340;&#21305;&#37197;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interact
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22411;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13528</link><description>&lt;p&gt;
FacTool&#65306;&#29983;&#25104;AI&#20013;&#30340;&#20107;&#23454;&#24615;&#26816;&#27979; &#8212;&#8212; &#19968;&#31181;&#20026;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#22330;&#26223;&#21152;&#24378;&#30340;&#24037;&#20855;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13528
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22411;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#26041;&#20415;&#20102;&#39640;&#36136;&#37327;&#25991;&#26412;&#30340;&#21512;&#25104;&#65292;&#20294;&#20063;&#22312;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20197;&#19979;&#38382;&#39064;&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65306;&#65288;1&#65289;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#30001;&#29983;&#25104;&#27169;&#22411;&#22788;&#29702;&#26102;&#65292;&#23384;&#22312;&#30528;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#39118;&#38505;&#65307;&#65288;2&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#24456;&#38271;&#65292;&#32570;&#20047;&#28165;&#26224;&#23450;&#20041;&#30340;&#32454;&#31890;&#24230;&#20010;&#20307;&#20107;&#23454;&#65307;&#65288;3&#65289;&#22312;&#20107;&#23454;&#26816;&#26597;&#36807;&#31243;&#20013;&#32570;&#20047;&#26126;&#30830;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65288;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#65289;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method.
&lt;/p&gt;</description></item><item><title>Zshot&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26368;&#26032;ZSL&#26041;&#27861;&#65292;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.13497</link><description>&lt;p&gt;
Zshot&#65306;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction. (arXiv:2307.13497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13497
&lt;/p&gt;
&lt;p&gt;
Zshot&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26368;&#26032;ZSL&#26041;&#27861;&#65292;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#20219;&#21153;&#28041;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#25110;&#20851;&#31995;&#12290;&#30001;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;ZSL&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#36817;&#24180;&#26469;&#24212;&#29992;&#33539;&#22260;&#24050;&#22823;&#24133;&#22686;&#38271;&#12290;&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#26032;&#30340;&#26041;&#27861;&#65292;ZSL&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#12290;&#30740;&#31350;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#19968;&#20010;&#20840;&#38754;&#25903;&#25345;&#26368;&#26032;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#21457;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;ZSL&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Zshot&#30340;&#21019;&#26032;ZSL&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#27604;&#36739;&#19981;&#21516;&#30340;&#26368;&#26032;ZSL&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25903;&#25345;&#24037;&#19994;&#30028;&#30340;&#26694;&#26550;&#65292;&#20855;&#22791;&#26131;&#29992;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readi
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22522;&#20110;&#36807;&#31243;&#30340;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20803;&#27169;&#24314;&#26041;&#27861;&#29992;&#20110;&#39532;&#38083;&#34223;&#20135;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27169;&#25311;&#24212;&#29992;&#21644;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20013;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13466</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#36827;&#34892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integrating processed-based models and machine learning for crop yield prediction. (arXiv:2307.13466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22522;&#20110;&#36807;&#31243;&#30340;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20803;&#27169;&#24314;&#26041;&#27861;&#29992;&#20110;&#39532;&#38083;&#34223;&#20135;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27169;&#25311;&#24212;&#29992;&#21644;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20013;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#29702;&#35770;&#39537;&#21160;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26657;&#20934;&#26412;&#22320;&#29615;&#22659;&#26041;&#38754;&#24448;&#24448;&#36739;&#20026;&#22256;&#38590;&#65292;&#25110;&#32773;&#20351;&#29992;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#28151;&#21512;&#20803;&#27169;&#24314;&#26041;&#27861;&#30740;&#31350;&#20102;&#39532;&#38083;&#34223;&#20135;&#37327;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#65288;&#39044;&#65289;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#28982;&#21518;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#27169;&#25311;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#20803;&#27169;&#24314;&#26041;&#27861;&#27604;&#32431;&#31929;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#26469;&#33258;&#30000;&#38388;&#35797;&#39564;&#65288;n=303&#65289;&#21644;&#21830;&#19994;&#30000;&#22320;&#65288;n=77&#65289;&#30340;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20013;&#65292;&#20803;&#27169;&#24314;&#26041;&#27861;&#30456;&#23545;&#20110;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#21518;&#32773;&#20013;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#34920;&#29616;&#37117;&#19981;&#22914;&#30001;&#39046;&#22495;&#19987;&#23478;&#35774;&#35745;&#30340;&#25163;&#21160;&#36873;&#25321;&#29305;&#24449;&#38598;&#21644;&#19987;&#38376;&#39044;&#22788;&#29702;&#30340;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#35299;&#38145;&#35270;&#35273;&#23186;&#20307;&#24773;&#24863;&#19990;&#30028;&#30340;&#31185;&#23398;&#12289;&#30740;&#31350;&#21644;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#23545;&#35270;&#35273;&#23186;&#20307;&#20013;&#24773;&#24863;&#30340;&#33258;&#21160;&#29702;&#35299;&#30340;&#25361;&#25112;&#19982;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.13463</link><description>&lt;p&gt;
&#35299;&#38145;&#35270;&#35273;&#23186;&#20307;&#30340;&#24773;&#24863;&#19990;&#30028;&#65306;&#29702;&#35299;&#24773;&#24863;&#30340;&#31185;&#23398;&#12289;&#30740;&#31350;&#21644;&#24433;&#21709;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion. (arXiv:2307.13463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#35299;&#38145;&#35270;&#35273;&#23186;&#20307;&#24773;&#24863;&#19990;&#30028;&#30340;&#31185;&#23398;&#12289;&#30740;&#31350;&#21644;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#23545;&#35270;&#35273;&#23186;&#20307;&#20013;&#24773;&#24863;&#30340;&#33258;&#21160;&#29702;&#35299;&#30340;&#25361;&#25112;&#19982;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#24773;&#24863;&#26234;&#33021;&#25216;&#26415;&#30340;&#20986;&#29616;&#27491;&#22312;&#38761;&#26032;&#35745;&#31639;&#26426;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#23545;&#20154;&#31867;&#34892;&#20026;&#30340;&#27807;&#36890;&#21644;&#29702;&#35299;&#30340;&#26032;&#27700;&#24179;&#65292;&#36825;&#26366;&#32463;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20294;&#23545;&#35270;&#35273;&#23186;&#20307;&#20013;&#24341;&#21457;&#25110;&#34920;&#36798;&#30340;&#24773;&#24863;&#36827;&#34892;&#33258;&#21160;&#29702;&#35299;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#36825;&#19968;&#22256;&#22659;&#28304;&#20110;&#8220;&#24773;&#24863;&#8221;&#32570;&#20047;&#26222;&#36941;&#25509;&#21463;&#30340;&#23450;&#20041;&#65292;&#21152;&#19978;&#24773;&#24863;&#30340;&#20027;&#35266;&#24615;&#21644;&#24494;&#22937;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#12289;&#22810;&#23398;&#31185;&#30340;&#35270;&#35273;&#23186;&#20307;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#27010;&#36848;&#65292;&#20511;&#37492;&#20102;&#24515;&#29702;&#23398;&#12289;&#24037;&#31243;&#23398;&#21644;&#33402;&#26415;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#24773;&#24863;&#30340;&#24515;&#29702;&#23398;&#22522;&#30784;&#21644;&#20174;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#29702;&#35299;&#24773;&#24863;&#30340;&#35745;&#31639;&#21407;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#21644;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of "emotion", coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#30456;&#23545;&#35770;&#37327;&#23376;&#22330;&#35770;&#21644;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;Lieb-Robinson&#30028;&#38480;&#65292;&#25209;&#21028;&#24615;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#24555;&#36895;&#37327;&#23376;&#23384;&#20648;&#22120;&#30340;&#20869;&#22312;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#22312;&#28151;&#21512;&#37327;&#23376;&#22768;&#23398;&#31995;&#32479;&#20013;&#65292;QRAM&#21487;&#20197;&#23481;&#32435;&#26368;&#22810;O(10^7)&#20010;&#36923;&#36753;&#27604;&#29305;&#30340;&#19968;&#32500;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.13460</link><description>&lt;p&gt;
&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#20869;&#23384;&#30340;&#22522;&#26412;&#22240;&#26524;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Fundamental causal bounds of quantum random access memories. (arXiv:2307.13460v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#30456;&#23545;&#35770;&#37327;&#23376;&#22330;&#35770;&#21644;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;Lieb-Robinson&#30028;&#38480;&#65292;&#25209;&#21028;&#24615;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#24555;&#36895;&#37327;&#23376;&#23384;&#20648;&#22120;&#30340;&#20869;&#22312;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#22312;&#28151;&#21512;&#37327;&#23376;&#22768;&#23398;&#31995;&#32479;&#20013;&#65292;QRAM&#21487;&#20197;&#23481;&#32435;&#26368;&#22810;O(10^7)&#20010;&#36923;&#36753;&#27604;&#29305;&#30340;&#19968;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35774;&#22791;&#24212;&#36981;&#23432;&#37327;&#23376;&#29289;&#29702;&#21407;&#21017;&#12290;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#20869;&#23384;&#65288;QRAM&#65289;&#26159;&#35768;&#22810;&#37325;&#35201;&#37327;&#23376;&#31639;&#27861;&#65288;&#22914;&#32447;&#24615;&#20195;&#25968;&#12289;&#25968;&#25454;&#25628;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#65289;&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#32473;&#23450;N&#20010;&#37327;&#23376;&#27604;&#29305;&#26102;&#65292;&#21487;&#20197;&#20197;O(log N)&#30340;&#30005;&#36335;&#28145;&#24230;&#22788;&#29702;O(N)&#30340;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22823;&#37327;&#37327;&#23376;&#27604;&#29305;&#30340;&#30456;&#20114;&#20316;&#29992;&#23616;&#37096;&#30340;&#37327;&#23376;&#26448;&#26009;&#26102;&#65292;&#36825;&#19968;&#20027;&#24352;&#20284;&#20046;&#36829;&#21453;&#20102;&#30456;&#23545;&#35770;&#21407;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#24555;&#36895;&#37327;&#23376;&#23384;&#20648;&#22120;&#30340;&#20869;&#22312;&#30028;&#38480;&#65292;&#21033;&#29992;&#30456;&#23545;&#35770;&#37327;&#23376;&#22330;&#35770;&#21644;Lieb-Robinson&#30028;&#38480;&#22312;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#28151;&#21512;&#37327;&#23376;&#22768;&#23398;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#30828;&#20214;&#35774;&#35745;&#30340;QRAM&#12290;&#20551;&#35774;&#26102;&#38047;&#21608;&#26399;&#32422;&#20026;10^{-3}&#31186;&#65292;&#26684;&#23376;&#38388;&#36317;&#32422;&#20026;1&#24494;&#31859;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;QRAM&#21487;&#20197;&#23481;&#32435;&#26368;&#22810;O(10^7)&#20010;&#36923;&#36753;&#27604;&#29305;&#30340;&#19968;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#21407;&#21019;MCTS&#21464;&#31181;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21508;&#33258;&#30340;&#36335;&#24452;&#36741;&#21161;&#26234;&#33021;&#20307;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#65292;&#24182;&#20801;&#35768;&#23427;&#20204;&#26681;&#25454;&#38656;&#35201;&#31163;&#24320;&#36335;&#24452;&#36991;&#20813;&#30896;&#25758;&#12290;</title><link>http://arxiv.org/abs/2307.13453</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65306;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results. (arXiv:2307.13453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#21407;&#21019;MCTS&#21464;&#31181;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21508;&#33258;&#30340;&#36335;&#24452;&#36741;&#21161;&#26234;&#33021;&#20307;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#65292;&#24182;&#20801;&#35768;&#23427;&#20204;&#26681;&#25454;&#38656;&#35201;&#31163;&#24320;&#36335;&#24452;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24403;&#19968;&#32452;&#26234;&#33021;&#20307;&#34987;&#38480;&#21046;&#22312;&#19968;&#20010;&#22270;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#34987;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#36215;&#22987;&#21644;&#30446;&#26631;&#39030;&#28857;&#65292;&#20219;&#21153;&#26159;&#25214;&#21040;&#19968;&#32452;&#26080;&#30896;&#25758;&#36335;&#24452;&#65288;&#27599;&#20010;&#26234;&#33021;&#20307;&#19968;&#20010;&#36335;&#24452;&#65289;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#36798;&#21040;&#20854;&#30456;&#24212;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#34429;&#28982;MCTS&#24050;&#34987;&#35777;&#26126;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22914;&#23545;&#24328;&#28216;&#25103;&#65288;&#20363;&#22914;&#65292;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#31561;&#65289; &#30340;&#23545;&#24328;&#65292;&#21457;&#29616;&#26356;&#24555;&#30340;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#31561;&#65292;&#20294;&#20854;&#22312;&#27492;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#21407;&#21019;&#21464;&#31181;MCTS&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#22914;&#20309;&#35745;&#31639;&#25351;&#23548; MCTS &#30340;&#22870;&#21169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#33258;&#30340;&#36335;&#24452;&#26469;&#36741;&#21161;&#26234;&#33021;&#20307;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#34892;&#20026;&#65292;&#24182;&#20801;&#35768;&#23427;&#20204;&#22312;&#38656;&#35201;&#26102;&#31163;&#24320;&#36335;&#24452;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we study a well-known and challenging problem of Multi-agent Pathfinding, when a set of agents is confined to a graph, each agent is assigned a unique start and goal vertices and the task is to find a set of collision-free paths (one for each agent) such that each agent reaches its respective goal. We investigate how to utilize Monte-Carlo Tree Search (MCTS) to solve the problem. Although MCTS was shown to demonstrate superior performance in a wide range of problems like playing antagonistic games (e.g. Go, Chess etc.), discovering faster matrix multiplication algorithms etc., its application to the problem at hand was not well studied before. To this end we introduce an original variant of MCTS, tailored to multi-agent pathfinding. The crux of our approach is how the reward, that guides MCTS, is computed. Specifically, we use individual paths to assist the agents with the the goal-reaching behavior, while leaving them freedom to get off the track if it is needed to avoid 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20026;&#21464;&#25442;&#22120;(BeTrans)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#30340;&#38750;&#38745;&#24577;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39034;&#24207;&#25968;&#25454;&#26469;&#36866;&#24212;&#26032;&#30340;&#38750;&#38745;&#24577;&#30340;&#20154;&#31867;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;BeTrans&#22312;&#21327;&#20316;&#29615;&#22659;&#20013;&#25928;&#26524;&#26174;&#33879;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#24555;&#22320;&#36866;&#24212;&#20102;&#38750;&#38745;&#24577;&#30340;&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.13447</link><description>&lt;p&gt;
&#19968;&#20010;&#34892;&#20026;&#21464;&#25442;&#22120;&#29992;&#20110;&#26426;&#22120;&#20154;&#19982;&#38750;&#38745;&#27490;&#20154;&#31867;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
A behavioural transformer for effective collaboration between a robot and a non-stationary human. (arXiv:2307.13447v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20026;&#21464;&#25442;&#22120;(BeTrans)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#30340;&#38750;&#38745;&#24577;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39034;&#24207;&#25968;&#25454;&#26469;&#36866;&#24212;&#26032;&#30340;&#38750;&#38745;&#24577;&#30340;&#20154;&#31867;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;BeTrans&#22312;&#21327;&#20316;&#29615;&#22659;&#20013;&#25928;&#26524;&#26174;&#33879;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#24555;&#22320;&#36866;&#24212;&#20102;&#38750;&#38745;&#24577;&#30340;&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20154;&#31867;&#34892;&#20026;&#30340;&#38750;&#38745;&#27490;&#24615;&#65292;&#30001;&#20110;&#20854;&#34892;&#20026;&#30340;&#21464;&#21270;&#25152;&#20135;&#29983;&#30340;&#38750;&#38745;&#27490;&#24615;&#20250;&#25913;&#21464;&#29615;&#22659;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#25506;&#32034;&#26426;&#22120;&#20154;&#22914;&#20309;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#24182;&#22240;&#27492;&#35299;&#20915;&#38750;&#38745;&#27490;&#24615;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#34892;&#20026;&#21464;&#25442;&#22120;(BeTrans)&#12290;BeTrans&#26159;&#19968;&#20010;&#26465;&#20214;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#38750;&#38745;&#24577;&#34892;&#20026;&#30340;&#26032;&#30340;&#20154;&#31867;&#20195;&#29702;&#65292;&#22240;&#20026;&#23427;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;BeTrans&#22312;&#27169;&#25311;&#30340;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#20559;&#24046;&#30340;&#20154;&#31867;&#20195;&#29702;&#20013;&#65292;&#22312;&#21327;&#20316;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#21407;&#22987;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#30340;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;BeTrans&#19982;&#27169;&#25311;&#30340;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21327;&#20316;&#65292;&#24182;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#24555;&#22320;&#36866;&#24212;&#20102;&#38750;&#38745;&#24577;&#30340;&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#29616;&#35937;&#65292;&#21363;&#22810;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#23384;&#22312;&#23545;&#26576;&#20123;&#35821;&#35328;&#30340;&#30828;&#32534;&#30721;&#20542;&#21521;&#65292;&#24573;&#35270;&#20102;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#35821;&#35328;&#31038;&#21306;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;AI&#25216;&#26415;&#35206;&#30422;&#21040;&#8220;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#8221;&#12290;</title><link>http://arxiv.org/abs/2307.13405</link><description>&lt;p&gt;
&#36208;&#21521;&#32553;&#23567;&#25968;&#23383;&#35821;&#35328;&#40511;&#27807;&#30340;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Bridging the Digital Language Divide. (arXiv:2307.13405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#29616;&#35937;&#65292;&#21363;&#22810;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#23384;&#22312;&#23545;&#26576;&#20123;&#35821;&#35328;&#30340;&#30828;&#32534;&#30721;&#20542;&#21521;&#65292;&#24573;&#35270;&#20102;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#35821;&#35328;&#31038;&#21306;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;AI&#25216;&#26415;&#35206;&#30422;&#21040;&#8220;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#21069;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35821;&#35328;&#25216;&#26415;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#12289;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12289;&#22810;&#35821;&#35328;&#23383;&#20856;&#21644;&#35821;&#26009;&#24211;&#65292;&#20027;&#35201;&#20851;&#27880;&#20840;&#29699;2-3%&#30340;&#26368;&#24120;&#29992;&#35821;&#35328;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;AI&#25216;&#26415;&#25193;&#22823;&#21040;&#8220;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#8221;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#30446;&#26631;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#30340;&#29616;&#35937;&#30340;&#20851;&#27880;&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24448;&#24448;&#34920;&#29616;&#20986;&#23545;&#26576;&#20123;&#35821;&#35328;&#30340;&#30828;&#32534;&#30721;&#20542;&#21521;&#65292;&#36825;&#24448;&#24448;&#26159;&#26080;&#24847;&#35782;&#21644;&#38544;&#34255;&#30340;&#12290;&#21363;&#20351;&#22312;&#31867;&#20284;&#30340;&#27979;&#35797;&#26465;&#20214;&#19979;&#65292;&#35821;&#35328;&#20559;&#35265;&#20063;&#20250;&#23548;&#33268;&#19981;&#21516;&#35821;&#35328;&#30340;&#24615;&#33021;&#19981;&#22343;&#34913;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20855;&#26377;&#20559;&#35265;&#30340;&#25216;&#26415;&#24448;&#24448;&#26159;&#30001;&#20110;&#30740;&#21457;&#26041;&#27861;&#35770;&#27809;&#26377;&#23545;&#25152;&#34920;&#31034;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#24688;&#24403;&#22788;&#29702;&#32780;&#20135;&#29983;&#30340;&#65292;&#29978;&#33267;&#20250;&#22240;&#24573;&#35270;&#22810;&#26679;&#24615;&#23453;&#36149;&#30340;&#26041;&#38754;&#20197;&#21450;&#35821;&#35328;&#31038;&#21306;&#30340;&#38656;&#27714;&#32780;&#24341;&#36215;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a well-known fact that current AI-based language technology -- language models, machine translation systems, multilingual dictionaries and corpora -focuses on the world's 2-3% most widely spoken languages. Recent research efforts have attempted to expand the coverage of AI technology to `under-resourced languages.' The goal of our paper is to bring attention to a phenomenon that we call linguistic bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. Linguistic bias is manifested in uneven per-language performance even in the case of similar test conditions. We show that biased technology is often the result of research and development methodologies that do not do justice to the complexity of the languages being represented, and that can even become ethically problematic as they disregard valuable aspects of diversity as well as the needs of the language communities the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25191;&#34892;&#20195;&#30721;&#21363;&#21487;&#39044;&#27979;&#20195;&#30721;&#35206;&#30422;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#20219;&#21153;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#20195;&#30721;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13383</link><description>&lt;p&gt;
&#26080;&#38656;&#25191;&#34892;&#39044;&#27979;&#20195;&#30721;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
Predicting Code Coverage without Execution. (arXiv:2307.13383v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13383
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25191;&#34892;&#20195;&#30721;&#21363;&#21487;&#39044;&#27979;&#20195;&#30721;&#35206;&#30422;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#20219;&#21153;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#20195;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35206;&#30422;&#29575;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#34913;&#37327;&#31243;&#24207;&#20803;&#32032;&#25191;&#34892;&#24773;&#20917;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;&#35821;&#21477;&#25110;&#20998;&#25903;&#30340;&#25191;&#34892;&#24773;&#20917;&#12290;&#35745;&#31639;&#20195;&#30721;&#35206;&#30422;&#29575;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#65292;&#38656;&#35201;&#26500;&#24314;&#21644;&#25191;&#34892;&#20195;&#30721;&#65292;&#24182;&#19988;&#36824;&#38656;&#35201;&#39069;&#22806;&#30340;&#24320;&#38144;&#36827;&#34892;&#20202;&#22120;&#21270;&#12290;&#27492;&#22806;&#65292;&#35745;&#31639;&#20219;&#20309;&#20195;&#30721;&#29255;&#27573;&#30340;&#35206;&#30422;&#29575;&#38656;&#35201;&#25972;&#20010;&#31243;&#24207;&#30340;&#19978;&#19979;&#25991;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20998;&#25674;&#36825;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#21487;&#20197;&#38477;&#20302;&#20195;&#30721;&#35206;&#30422;&#29575;&#30340;&#25104;&#26412;&#65292;&#21482;&#38656;&#35201;&#28304;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#32780;&#20195;&#30721;&#35206;&#30422;&#29575;&#39044;&#27979;&#20219;&#21153;&#21487;&#20197;&#25104;&#20026;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#20195;&#30721;&#33021;&#21147;&#30340;&#26032;&#39062;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#30721;&#35206;&#30422;&#29575;&#39044;&#27979;&#30340;&#26032;&#39062;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#32473;&#23450;&#27979;&#35797;&#29992;&#20363;&#21644;&#36755;&#20837;&#30340;&#21738;&#20123;&#26041;&#27861;&#34892;&#34987;&#25191;&#34892;&#26469;&#24418;&#24335;&#21270;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#20195;&#30721;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;COVERAGEEVAL&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25191;&#34892;&#27979;&#35797;&#21644;&#20195;&#30721;&#26469;&#33719;&#21462;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code. We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13365</link><description>&lt;p&gt;
&#29992;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#23558;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#37327;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#29616;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#20195;&#12290;&#26080;&#35770;&#36825;&#20123;&#27169;&#22411;&#33258;&#36523;&#30340;&#23481;&#37327;&#21644;&#32467;&#26500;&#22914;&#20309;&#65292;&#37117;&#23384;&#22312;&#23545;LLMs&#20855;&#26377;&#26356;&#38271;&#26356;&#22797;&#26434;&#19978;&#19979;&#25991;&#30340;&#22686;&#24378;&#29702;&#35299;&#30340;&#38656;&#27714;&#65292;&#32780;&#27169;&#22411;&#36890;&#24120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20854;&#29702;&#35299;&#33021;&#21147;&#33539;&#22260;&#30340;&#21477;&#23376;&#24207;&#21015;&#26102;&#20250;&#36935;&#21040;&#19978;&#38480;&#65292;&#23548;&#33268;&#20135;&#29983;&#31163;&#39064;&#25110;&#28151;&#20081;&#30340;&#22238;&#31572;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#20851;&#27880;&#8220;&#20026;&#20160;&#20040;&#27169;&#22411;&#26080;&#27861;&#33258;&#34892;&#24357;&#34917;&#25110;&#22686;&#24378;&#33258;&#24049;&#30340;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#24615;&#36136;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#26368;&#23567;&#21270;&#39069;&#22806;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#21033;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#27880;&#24847;&#21147;&#19982;CNN&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#21463;&#20219;&#21153;&#21644;&#22270;&#20687;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#20219;&#21153;&#30340;&#24847;&#22270;&#21644;&#22270;&#20687;&#30340;&#29305;&#24449;&#37117;&#23545;&#20108;&#32773;&#30340;&#30456;&#20284;&#24615;&#26377;&#25152;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13345</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#26159;&#21542;&#20851;&#27880;&#30456;&#20284;&#21306;&#22495;&#65306;&#20219;&#21153;&#21644;&#22270;&#20687;&#31867;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type. (arXiv:2307.13345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#27880;&#24847;&#21147;&#19982;CNN&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#21463;&#20219;&#21153;&#21644;&#22270;&#20687;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#20219;&#21153;&#30340;&#24847;&#22270;&#21644;&#22270;&#20687;&#30340;&#29305;&#24449;&#37117;&#23545;&#20108;&#32773;&#30340;&#30456;&#20284;&#24615;&#26377;&#25152;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#24378;&#22823;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20294;&#26159;&#20160;&#20040;&#22240;&#32032;&#20915;&#23450;&#23427;&#20204;&#26159;&#21542;&#20851;&#27880;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#22270;&#20687;&#21306;&#22495;&#21602;&#65311;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25216;&#26415;&#22240;&#32032;&#19978;&#65292;&#20294;&#20154;&#31867;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#22240;&#32032;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29992;&#20110;&#24341;&#21457;&#20154;&#31867;&#27880;&#24847;&#21147;&#22270;&#30340;&#20219;&#21153;&#22914;&#20309;&#19982;&#22270;&#20687;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35843;&#33410;&#20154;&#31867;&#21644;CNN&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;&#20154;&#31867;&#20219;&#21153;&#30340;&#24847;&#22270;&#65292;&#20174;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#30340;&#33258;&#21457;&#27880;&#35270;&#21040;&#26377;&#24847;&#30340;&#27880;&#35270;&#25351;&#21521;&#65292;&#20877;&#21040;&#25163;&#21160;&#21306;&#22495;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25913;&#21464;&#20102;&#35201;&#36827;&#34892;&#20998;&#31867;&#30340;&#22270;&#20687;&#31867;&#22411;&#65292;&#21253;&#25324;&#21333;&#20010;&#26174;&#33879;&#23545;&#35937;&#12289;&#30001;&#23545;&#35937;&#32452;&#21512;&#32780;&#25104;&#30340;&#23460;&#20869;&#22330;&#26223;&#65292;&#20197;&#21450;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#31867;&#21035;&#30340;&#26223;&#35266;&#12290;&#20197;&#36825;&#31181;&#26041;&#24335;&#29983;&#25104;&#30340;&#20154;&#31867;&#27880;&#24847;&#21147;&#22270;&#19982;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;Grad-CAM&#65289;&#25581;&#31034;&#20986;&#30340;CNN&#27880;&#24847;&#21147;&#22270;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models like Convolutional Neural Networks (CNN) are powerful image classifiers, but what factors determine whether they attend to similar image areas as humans do? While previous studies have focused on technological factors, little is known about the role of factors that affect human attention. In the present study, we investigated how the tasks used to elicit human attention maps interact with image characteristics in modulating the similarity between humans and CNN. We varied the intentionality of human tasks, ranging from spontaneous gaze during categorization over intentional gaze-pointing up to manual area selection. Moreover, we varied the type of image to be categorized, using either singular, salient objects, indoor scenes consisting of object arrangements, or landscapes without distinct objects defining the category. The human attention maps generated in this way were compared to the CNN attention maps revealed by explainable artificial intelligence (Grad-CAM). 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#24605;&#32500;&#38142;&#21551;&#21457;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24182;&#27809;&#26377;&#22686;&#21152;&#19982;&#35821;&#20041;&#30456;&#20851;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25552;&#39640;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions. (arXiv:2307.13339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13339
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#24605;&#32500;&#38142;&#21551;&#21457;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24182;&#27809;&#26377;&#22686;&#21152;&#19982;&#35821;&#20041;&#30456;&#20851;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25552;&#39640;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#24605;&#32500;&#38142;&#21551;&#21457;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#23454;&#38469;&#30340;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#36825;&#31181;&#29616;&#35937;&#26159;&#26399;&#26395;&#30340;&#27169;&#22411;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#29702;&#35299;&#20026;&#20309;&#24605;&#32500;&#38142;&#21551;&#21457;&#26377;&#25928;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#34913;&#37327;&#36755;&#20837;&#26631;&#35760;&#23545;&#27169;&#22411;&#36755;&#20986;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#30740;&#31350;&#24605;&#32500;&#38142;&#21551;&#21457;&#26159;&#21542;&#20250;&#24433;&#21709;&#23427;&#20204;&#20998;&#37197;&#32473;&#29305;&#23450;&#36755;&#20837;&#26631;&#35760;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#21551;&#21457;&#30456;&#27604;&#65292;&#24605;&#32500;&#38142;&#21551;&#21457;&#24182;&#26410;&#22686;&#21152;&#20998;&#37197;&#32473;&#35821;&#20041;&#30456;&#20851;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#22823;&#23567;&#65292;&#20294;&#23427;&#25552;&#39640;&#20102;&#20998;&#37197;&#32473;&#38382;&#39064;&#30456;&#20851;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to quest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.13332</link><description>&lt;p&gt;
&#22312;&#38169;&#35823;&#25351;&#23450;&#30340;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30693;&#36947;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#38169;&#35823;&#25351;&#23450;&#20013;&#20250;&#20986;&#29616;&#20056;&#27861;&#25918;&#22823;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;\emph{&#36924;&#36817;&#22240;&#23376;}&#30340;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#26368;&#20339;&#24418;&#24335;&#65292;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#24191;&#27867;&#35774;&#32622;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20854;&#20013;&#20173;&#26377;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20363;&#22914;&#21152;&#26435;$L_2$&#33539;&#25968;&#65288;&#20854;&#20013;&#21152;&#26435;&#26159;&#31163;&#32447;&#29366;&#24577;&#20998;&#24067;&#65289;&#65292;$L_\infty$&#33539;&#25968;&#65292;&#29366;&#24577;&#21035;&#21517;&#30340;&#23384;&#22312;&#19982;&#21542;&#20197;&#21450;&#23545;&#29366;&#24577;&#31354;&#38388;&#30340;&#20840;&#38754;&#19982;&#37096;&#20998;&#35206;&#30422;&#12290;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65288;&#33267;&#22810;&#24120;&#25968;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#30830;&#23450;&#20102;$L_2(\mu)$&#33539;&#25968;&#30340;&#20004;&#20010;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22240;&#23376;&#21644;$L_\infty$&#33539;&#25968;&#30340;&#19968;&#20010;&#22240;&#23376;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28508;&#22312;&#20219;&#21153;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#36866;&#24212;&#26469;&#23398;&#20064;&#33258;&#20027;&#36229;&#22768;&#30340;&#26041;&#27861;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#23558;&#22810;&#27169;&#24335;&#36229;&#22768;&#25216;&#33021;&#21512;&#24182;&#20026;&#20302;&#32500;&#27010;&#29575;&#27169;&#22411;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#27010;&#29575;&#27169;&#22411;&#23558;&#36873;&#25321;&#21644;&#35780;&#20272;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.13323</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#20219;&#21153;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#36866;&#24212;&#23398;&#20064;&#33258;&#20027;&#36229;&#22768;
&lt;/p&gt;
&lt;p&gt;
Learning Autonomous Ultrasound via Latent Task Representation and Robotic Skills Adaptation. (arXiv:2307.13323v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28508;&#22312;&#20219;&#21153;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#36866;&#24212;&#26469;&#23398;&#20064;&#33258;&#20027;&#36229;&#22768;&#30340;&#26041;&#27861;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#23558;&#22810;&#27169;&#24335;&#36229;&#22768;&#25216;&#33021;&#21512;&#24182;&#20026;&#20302;&#32500;&#27010;&#29575;&#27169;&#22411;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#27010;&#29575;&#27169;&#22411;&#23558;&#36873;&#25321;&#21644;&#35780;&#20272;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#23398;&#36229;&#22768;&#25104;&#20026;&#22914;&#20170;&#27969;&#34892;&#30340;&#26816;&#26597;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36229;&#22768;&#31995;&#32479;&#21487;&#20197;&#20419;&#36827;&#25195;&#25551;&#36807;&#31243;&#65292;&#38450;&#27490;&#19987;&#19994;&#30340;&#36229;&#22768;&#21307;&#29983;&#37325;&#22797;&#20047;&#21619;&#30340;&#24037;&#20316;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#36229;&#22768;&#26816;&#26597;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#32570;&#20047;&#21512;&#36866;&#30340;&#20219;&#21153;&#34920;&#31034;&#26041;&#27861;&#20197;&#21450;&#22312;&#19981;&#21516;&#24739;&#32773;&#20043;&#38388;&#25512;&#24191;&#23398;&#20064;&#25216;&#33021;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#36229;&#22768;&#30340;&#28508;&#22312;&#20219;&#21153;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#36866;&#24212;&#26041;&#27861;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#36890;&#36807;&#20840;&#33258;&#25105;&#30417;&#30563;&#26694;&#26550;&#65292;&#23558;&#22810;&#27169;&#24335;&#36229;&#22768;&#25216;&#33021;&#21512;&#24182;&#24182;&#23553;&#35013;&#20026;&#20302;&#32500;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20020;&#24202;&#35777;&#26126;&#30340;&#36229;&#22768;&#22270;&#20687;&#12289;&#25506;&#22836;&#23450;&#21521;&#21644;&#25509;&#35302;&#21147;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#27010;&#29575;&#27169;&#22411;&#23558;&#36873;&#25321;&#21644;&#35780;&#20272;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As medical ultrasound is becoming a prevailing examination approach nowadays, robotic ultrasound systems can facilitate the scanning process and prevent professional sonographers from repetitive and tedious work. Despite the recent progress, it is still a challenge to enable robots to autonomously accomplish the ultrasound examination, which is largely due to the lack of a proper task representation method, and also an adaptation approach to generalize learned skills across different patients. To solve these problems, we propose the latent task representation and the robotic skills adaptation for autonomous ultrasound in this paper. During the offline stage, the multimodal ultrasound skills are merged and encapsulated into a low-dimensional probability model through a fully self-supervised framework, which takes clinically demonstrated ultrasound images, probe orientations, and contact forces into account. During the online stage, the probability model will select and evaluate the opti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LED&#29031;&#26126;&#35843;&#21046;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#19981;&#21487;&#23519;&#35273;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24555;&#36895;&#24378;&#24230;&#35843;&#21046;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#20142;&#24230;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#21367;&#24088;&#24555;&#38376;&#25928;&#24212;&#21521;&#25429;&#33719;&#30340;&#20154;&#33080;&#22270;&#20687;&#20013;&#27880;&#20837;&#20142;&#24230;&#20449;&#24687;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.13294</link><description>&lt;p&gt;
&#36890;&#36807;LED&#29031;&#26126;&#35843;&#21046;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#19981;&#21487;&#23519;&#35273;&#30340;&#29289;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation. (arXiv:2307.13294v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LED&#29031;&#26126;&#35843;&#21046;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#19981;&#21487;&#23519;&#35273;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24555;&#36895;&#24378;&#24230;&#35843;&#21046;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#20142;&#24230;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#21367;&#24088;&#24555;&#38376;&#25928;&#24212;&#21521;&#25429;&#33719;&#30340;&#20154;&#33080;&#22270;&#20687;&#20013;&#27880;&#20837;&#20142;&#24230;&#20449;&#24687;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#33080;&#35782;&#21035;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#24320;&#22987;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#20294;&#25105;&#20204;&#38656;&#35201;&#27880;&#24847;&#21040;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#33080;&#35782;&#21035;&#35270;&#35273;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20004;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#25968;&#23383;&#25915;&#20987;&#21644;&#29289;&#29702;&#25915;&#20987;&#65292;&#37117;&#26377;&#32570;&#28857;&#65292;&#21069;&#32773;&#19981;&#23454;&#29992;&#65292;&#21518;&#32773;&#26174;&#30524;&#12289;&#35745;&#31639;&#37327;&#22823;&#19988;&#19981;&#21487;&#25191;&#34892;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#12289;&#21487;&#25191;&#34892;&#12289;&#19981;&#26174;&#30524;&#19988;&#35745;&#31639;&#37327;&#36739;&#20302;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#22522;&#20110;LED&#29031;&#26126;&#35843;&#21046;&#12290;&#20026;&#20102;&#27450;&#39575;&#31995;&#32479;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#36890;&#36807;&#23545;&#22330;&#26223;LED&#29031;&#26126;&#36827;&#34892;&#24555;&#36895;&#24378;&#24230;&#35843;&#21046;&#65292;&#22312;&#20154;&#30524;&#30475;&#19981;&#21040;&#30340;&#33539;&#22260;&#20869;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#20142;&#24230;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;CMOS&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#21367;&#24088;&#24555;&#38376;&#25928;&#24212;&#65292;&#21521;&#25429;&#33719;&#30340;&#20154;&#33080;&#22270;&#20687;&#20013;&#27880;&#20837;&#20142;&#24230;&#20449;&#24687;&#25200;&#21160;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20154;&#33080;&#26816;&#27979;&#30340;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#20154;&#33080;&#39564;&#35777;&#30340;&#36530;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, the current two categories of adversarial attacks, namely digital attacks and physical attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and inexecutable. To address the issues, we propose a practical, executable, inconspicuous and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary,we present a denial-of-service (DoS) attack for face detection and a dodging attack for face verifica
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13275</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;DNN&#27169;&#22411;&#30340;&#26368;&#24120;&#29992;&#29305;&#24449;&#34920;&#31034;&#22522;&#20110;SMILES&#21644;&#20998;&#23376;&#22270;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#31616;&#27905;&#39640;&#25928;&#65292;&#20294;&#20063;&#38480;&#21046;&#20102;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837; Ricci &#26354;&#29575;&#31163;&#25955;&#21270;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23558;&#26354;&#29575;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#26399;&#38388;&#65292;&#25105;&#20204;&#23558;&#22270;&#30340;&#26354;&#29575;&#20449;&#24687;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#28155;&#21152;&#21040;&#33410;&#28857;&#29305;&#24449;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#32593;&#32476;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26354;&#29575;&#20449;&#24687;&#24341;&#20837;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#26367;&#25442;&#21333;&#20803;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#39640;&#25928;&#30340;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.13270</link><description>&lt;p&gt;
&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unbiased Weight Maximization. (arXiv:2307.13270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#26367;&#25442;&#21333;&#20803;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#39640;&#25928;&#30340;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#29983;&#29289;&#23398;&#21512;&#29702;&#30340;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;&#32593;&#32476;&#35270;&#20026;&#20195;&#29702;&#22242;&#38431;&#12290;&#22240;&#27492;&#65292;&#25152;&#26377;&#21333;&#20803;&#37117;&#21487;&#20197;&#36890;&#36807;REINFORCE&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#36827;&#34892;&#35843;&#33410;&#65292;&#26356;&#21152;&#31526;&#21512;&#29983;&#29289;&#35266;&#23519;&#21040;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36895;&#24230;&#36739;&#24930;&#65292;&#19988;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#22823;&#32780;&#25193;&#23637;&#24615;&#36739;&#24046;&#65292;&#22240;&#20026;&#21333;&#20010;&#22870;&#21169;&#20449;&#21495;&#34987;&#24191;&#25773;&#32473;&#25152;&#26377;&#21333;&#20803;&#32780;&#19981;&#32771;&#34385;&#20010;&#20307;&#36129;&#29486;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#37327;&#26368;&#22823;&#21270;&#65292;&#29992;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#26367;&#25442;&#21333;&#20803;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#20801;&#35768;&#27599;&#20010;&#38544;&#34255;&#21333;&#20803;&#26368;&#22823;&#21270;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#37325;&#37327;&#26368;&#22823;&#21270;&#30340;&#29702;&#35770;&#23646;&#24615;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#65292;&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13269</link><description>&lt;p&gt;
LoraHub: &#36890;&#36807;&#21160;&#24577;LoRA&#32452;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24120;&#24120;&#34987;&#29992;&#20110;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LoraHub&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#30446;&#30340;&#24615;&#32452;&#35013;&#22312;&#19981;&#21516;&#32473;&#23450;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#30340;&#25112;&#30053;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#20165;&#20973;&#20511;&#26469;&#33258;&#26032;&#20219;&#21153;&#30340;&#20960;&#20010;&#31034;&#20363;&#65292;LoraHub&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22359;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#32452;&#21512;&#26082;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20063;&#19981;&#38656;&#35201;&#26799;&#24230;&#12290;&#25105;&#20204;&#20174;Big-Bench Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#20986;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#36755;&#20837;&#26049;&#36793;&#19981;&#38656;&#35201;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22521;&#32946;&#19968;&#20010;LoRA&#31038;&#21306;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20854;&#20013;&#20998;&#20139;&#20182;&#20204;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
&lt;/p&gt;</description></item><item><title>&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#27491;&#26631;&#31614;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SFPL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#26469;&#25913;&#21892;&#22810;&#31867;&#21035;&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13266</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#27491;&#26631;&#31614;&#30340;&#36164;&#28304;&#21463;&#38480;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Split Learning with Only Positive Labels for resource-constrained IoT environment. (arXiv:2307.13266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13266
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#27491;&#26631;&#31614;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SFPL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#26469;&#25913;&#21892;&#22810;&#31867;&#21035;&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;DCML&#65289;&#26159;&#29289;&#32852;&#32593;&#39046;&#22495;&#20013;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#28857;&#26159;&#36890;&#36807;&#28040;&#38500;&#21407;&#22987;&#25968;&#25454;&#30340;&#38598;&#20013;&#32858;&#21512;&#26469;&#25913;&#21892;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20063;&#20026;&#20855;&#26377;&#20302;&#35745;&#31639;&#33021;&#21147;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#25552;&#20379;&#21160;&#21147;&#12290;&#22312;DCML&#26694;&#26550;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#20013;&#65292;&#31216;&#20026;splitfed&#23398;&#20064;&#65288;SFL&#65289;&#30340;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;&#26159;&#22312;&#35774;&#22791;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#26102;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21482;&#26377;&#27491;&#26631;&#35760;&#25968;&#25454;&#26102;&#65292;SFL&#20013;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#25910;&#25947;&#25110;&#25552;&#20379;&#27425;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#27491;&#26631;&#31614;&#30340;splitfed&#23398;&#20064;&#65288;SFPL&#65289;&#12290;SFPL&#22312;&#23558;&#23458;&#25143;&#31471;&#25509;&#25910;&#21040;&#30340;&#30772;&#30862;&#25968;&#25454;&#25552;&#20379;&#32473;&#26381;&#21153;&#22120;&#20043;&#21069;&#65292;&#23545;&#20854;&#24212;&#29992;&#38543;&#26426;&#27927;&#29260;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#19982;&#21327;&#35843;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#35843;&#33410;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#36890;&#36807;&#25552;&#39640;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#30340;&#25928;&#29575;&#26469;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.13256</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#19982;&#21327;&#35843;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Structural Credit Assignment with Coordinated Exploration. (arXiv:2307.13256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#19982;&#21327;&#35843;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#35843;&#33410;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#36890;&#36807;&#25552;&#39640;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#30340;&#25928;&#29575;&#26469;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21512;&#29702;&#30340;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#30340;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#19968;&#20010;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;(RL)&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;&#32593;&#32476;&#35270;&#20026;&#20195;&#29702;&#22242;&#38431;&#12290;&#22240;&#27492;&#65292;&#25152;&#26377;&#21333;&#20803;&#37117;&#21487;&#20197;&#36890;&#36807;REINFORCE&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#35843;&#33410;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#26356;&#25509;&#36817;&#29983;&#29289;&#35266;&#23519;&#21040;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#36739;&#24930;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#36866;&#24212;&#32593;&#32476;&#30340;&#35268;&#27169;&#12290;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#20027;&#35201;&#26159;&#30001;&#20004;&#20010;&#22240;&#32032;&#36896;&#25104;&#30340;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#65306;(i)&#25152;&#26377;&#21333;&#20803;&#29420;&#31435;&#25506;&#32034;&#32593;&#32476;&#65292;(ii)&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;&#26469;&#35780;&#20272;&#25152;&#26377;&#21333;&#20803;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#26088;&#22312;&#25913;&#21892;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#30340;&#26041;&#27861;&#36890;&#24120;&#21487;&#20998;&#20026;&#20004;&#31867;&#12290;&#31532;&#19968;&#31867;&#21253;&#25324;&#20801;&#35768;&#21333;&#20803;&#20043;&#38388;&#36827;&#34892;&#21327;&#35843;&#25506;&#32034;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;MAP&#20256;&#25773;&#12290;&#31532;&#20108;&#31867;&#28085;&#30422;&#20102;&#35745;&#31639;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GaPro&#65292;&#19968;&#31181;&#22522;&#20110;3D&#28857;&#20113;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#36724;&#23545;&#40784;&#30340;&#30418;&#23376;&#30417;&#30563;&#12290;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#35757;&#32451;&#32593;&#32476;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#20102;&#30418;&#23376;&#37325;&#21472;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13251</link><description>&lt;p&gt;
GaPro: &#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#20266;&#26631;&#31614;&#29983;&#25104;&#22120;&#30340;&#22522;&#20110;3D&#28857;&#20113;&#30340;&#30418;&#23376;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers. (arXiv:2307.13251v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GaPro&#65292;&#19968;&#31181;&#22522;&#20110;3D&#28857;&#20113;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#36724;&#23545;&#40784;&#30340;&#30418;&#23376;&#30417;&#30563;&#12290;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#35757;&#32451;&#32593;&#32476;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#20102;&#30418;&#23376;&#37325;&#21472;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#28857;&#20113;&#30340;&#23454;&#20363;&#20998;&#21106;&#65288;3DIS&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#20808;&#36827;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#20840;&#30417;&#30563;&#12290;&#30001;&#20110;&#26631;&#27880;&#30495;&#23454;&#23494;&#38598;&#23454;&#20363;&#25513;&#30721;&#26159;&#32321;&#29712;&#19988;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#24369;&#30417;&#30563;&#35299;&#20915;3DIS&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GaPro&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;3D&#28857;&#20113;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#36724;&#23545;&#40784;&#30340;3D&#21253;&#22260;&#30418;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#20004;&#27493;&#26041;&#27861;&#21253;&#25324;&#20174;&#30418;&#23376;&#27880;&#37322;&#20013;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#32467;&#26524;&#26631;&#31614;&#35757;&#32451;3DIS&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#35757;&#32451;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#36793;&#30028;&#26694;&#29983;&#25104;&#20266;&#23454;&#20363;&#25513;&#30721;&#65292;&#24182;&#22312;&#23427;&#20204;&#37325;&#21472;&#26102;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#20351;&#24471;&#20266;&#23454;&#20363;&#25513;&#30721;&#20855;&#26377;&#20854;&#19981;&#30830;&#23450;&#24615;&#20540;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GaPro&#20248;&#20110;&#20808;&#21069;&#30340;&#24369;&#30417;&#30563;3D&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge in computer vision, where state-of-the-art methods are mainly based on full supervision. As annotating ground truth dense instance masks is tedious and expensive, solving 3DIS with weak supervision has become more practical. In this paper, we propose GaPro, a new instance segmentation for 3D point clouds using axis-aligned 3D bounding box supervision. Our two-step approach involves generating pseudo labels from box annotations and training a 3DIS network with the resulting labels. Additionally, we employ the self-training strategy to improve the performance of our method further. We devise an effective Gaussian Process to generate pseudo instance masks from the bounding boxes and resolve ambiguities when they overlap, resulting in pseudo instance masks with their uncertainty values. Our experiments show that GaPro outperforms previous weakly supervised 3D instance segmentation methods and has competitive perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36136;&#37327;&#25554;&#20540;&#26041;&#27861;&#21019;&#36896;&#20102;&#36830;&#32493;&#24322;&#24120;&#24230;&#26631;&#31614;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#24322;&#24120;&#27745;&#26579;&#21644;&#31163;&#25955;&#30417;&#30563;&#20449;&#24687;&#21033;&#29992;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13239</link><description>&lt;p&gt;
RoSAS:&#20855;&#26377;&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision. (arXiv:2307.13239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36136;&#37327;&#25554;&#20540;&#26041;&#27861;&#21019;&#36896;&#20102;&#36830;&#32493;&#24322;&#24120;&#24230;&#26631;&#31614;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#24322;&#24120;&#27745;&#26579;&#21644;&#31163;&#25955;&#30417;&#30563;&#20449;&#24687;&#21033;&#29992;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21033;&#29992;&#19968;&#20123;&#24322;&#24120;&#26679;&#26412;&#65292;&#19982;&#26080;&#30417;&#30563;&#27169;&#22411;&#30456;&#27604;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1) &#26410;&#26631;&#35760;&#30340;&#24322;&#24120;&#65288;&#21363;&#24322;&#24120;&#27745;&#26579;&#65289;&#21487;&#33021;&#22312;&#23558;&#25152;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#29992;&#20316;&#20869;&#28857;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26102;&#35823;&#23548;&#23398;&#20064;&#36807;&#31243;; 2) &#21482;&#21033;&#29992;&#31163;&#25955;&#30340;&#30417;&#30563;&#20449;&#24687;&#65288;&#22914;&#20108;&#36827;&#21046;&#25110;&#39034;&#24207;&#25968;&#25454;&#26631;&#31614;&#65289;&#65292;&#36825;&#23548;&#33268;&#24322;&#24120;&#20998;&#25968;&#30340;&#23376;&#20248;&#23398;&#20064;&#65292;&#23454;&#36136;&#19978;&#37319;&#29992;&#36830;&#32493;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;"&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#20449;&#21495;"&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#25554;&#20540;&#26041;&#27861;&#26469;&#25193;&#25955;&#26631;&#35760;&#24322;&#24120;&#30340;&#24322;&#24120;&#31243;&#24230;&#65292;&#20174;&#32780;&#21019;&#24314;&#24102;&#26377;&#36830;&#32493;&#24322;&#24120;&#24230;&#26631;&#31614;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#32452;&#21512;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#24322;&#24120;&#31243;&#24230;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#35206;&#30422;&#21463;&#27745;&#26579;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#23558;&#36890;&#29992;&#30340;&#21644;&#29305;&#23450;&#30340;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#20010;&#20154;&#36755;&#20837;&#21644;&#20114;&#32852;&#32593;&#20449;&#24687;&#30456;&#20114;&#25913;&#36827;&#12290;&#36825;&#31181;&#27169;&#22411;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#21151;&#33021;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20840;&#23616;&#12289;&#39046;&#22495;&#21644;&#29992;&#25143;&#32423;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#26426;&#22120;&#19978;&#36816;&#34892;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2307.13221</link><description>&lt;p&gt;
&#20026;&#25152;&#26377;&#20154;&#35774;&#35745;&#30340;&#22810;&#23618;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multilevel Large Language Models for Everyone. (arXiv:2307.13221v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#23558;&#36890;&#29992;&#30340;&#21644;&#29305;&#23450;&#30340;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#20010;&#20154;&#36755;&#20837;&#21644;&#20114;&#32852;&#32593;&#20449;&#24687;&#30456;&#20114;&#25913;&#36827;&#12290;&#36825;&#31181;&#27169;&#22411;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#21151;&#33021;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20840;&#23616;&#12289;&#39046;&#22495;&#21644;&#29992;&#25143;&#32423;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#26426;&#22120;&#19978;&#36816;&#34892;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35201;&#20040;&#26159;&#36890;&#29992;&#30340;&#65292;&#35201;&#20040;&#26159;&#39046;&#22495;&#29305;&#23450;&#30340;&#65292;&#23558;&#31038;&#21306;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#21040;&#19968;&#20010;&#26356;&#22823;&#30340;&#22320;&#22270;&#20013;&#65292;&#23558;&#36890;&#29992;&#30340;&#21644;&#29305;&#23450;&#30340;&#27169;&#22411;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#20010;&#20154;&#36755;&#20837;&#21644;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#20449;&#24687;&#30456;&#20114;&#25913;&#36827;&#12290;&#23558;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38142;&#25509;&#22312;&#19968;&#36215;&#30340;&#24605;&#24819;&#21463;&#21040;&#20102;&#20154;&#31867;&#22823;&#33041;&#21151;&#33021;&#30340;&#21551;&#21457;&#12290;&#22823;&#33041;&#30382;&#23618;&#19978;&#30340;&#29305;&#23450;&#21306;&#22495;&#23545;&#20110;&#26576;&#20123;&#20302;&#23618;&#27425;&#21151;&#33021;&#26159;&#29305;&#23450;&#30340;&#12290;&#32780;&#36825;&#20123;&#21306;&#22495;&#21487;&#20197;&#20849;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#26356;&#22797;&#26434;&#30340;&#39640;&#23618;&#21151;&#33021;&#12290;&#20154;&#31867;&#22823;&#33041;&#30382;&#23618;&#19978;&#30340;&#36825;&#31181;&#34892;&#20026;&#20026;&#35774;&#35745;&#21253;&#21547;&#20840;&#23616;&#12289;&#39046;&#22495;&#21644;&#29992;&#25143;&#32423;&#27169;&#22411;&#30340;&#22810;&#23618;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;&#29992;&#25143;&#32423;&#27169;&#22411;&#22312;&#26412;&#22320;&#26426;&#22120;&#19978;&#36816;&#34892;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21709;&#24212;&#24182;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#36825;&#26679;&#30340;&#22810;&#23618;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in the past few years. However, they are either generic {\it or} field specific, splitting the community into different groups. In this paper, we unify these large language models into a larger map, where the generic {\it and} specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PISF&#30340;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#24555;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#22810;&#22330;&#26223;&#21487;&#25512;&#24191;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13220</link><description>&lt;p&gt;
&#19968;&#27425;&#22810;&#29992;&#65306;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20102;&#24555;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#21487;&#25512;&#24191;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction. (arXiv:2307.13220v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PISF&#30340;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#24555;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#22810;&#22330;&#26223;&#21487;&#25512;&#24191;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26159;&#19968;&#31181;&#25552;&#20379;&#26080;&#36752;&#23556;&#12289;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#26377;&#20851;&#25972;&#20010;&#20154;&#20307;&#21307;&#23398;&#35786;&#26029;&#20449;&#24687;&#30340;&#20027;&#35201;&#25918;&#23556;&#23398;&#26041;&#27861;&#65292;&#20294;&#20854;&#25195;&#25551;&#26102;&#38388;&#36739;&#38271;&#12290;&#36890;&#36807;k&#31354;&#38388;&#27424;&#37319;&#26679;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25195;&#25551;&#26102;&#38388;&#65292;&#20294;&#38656;&#35201;&#22312;&#22270;&#20687;&#37325;&#24314;&#20013;&#21435;&#38500;&#24341;&#20837;&#30340;&#20266;&#24433;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#32463;&#25104;&#20026;&#24555;&#36895;MRI&#22270;&#20687;&#37325;&#24314;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#20854;&#22312;&#22810;&#31181;&#25104;&#20687;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#19981;&#20165;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26114;&#36149;&#19988;&#21463;&#38480;&#20110;&#38544;&#31169;&#65292;&#29616;&#26377;&#30340;DL&#26041;&#27861;&#36824;&#38590;&#20197;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#23454;&#38469;&#19978;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#21305;&#37197;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#26694;&#26550;&#65288;PISF&#65289;&#29992;&#20110;&#24555;&#36895;MRI&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20165;&#20351;&#29992;&#19968;&#20010;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#21487;&#25512;&#24191;&#30340;&#22810;&#22330;&#26223;MRI&#37325;&#24314;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24335;&#28145;&#24230;&#23545;&#20914;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#20840;&#24066;&#22330;&#20013;&#36827;&#34892;&#34893;&#29983;&#21697;&#23545;&#20914;&#12290;&#36890;&#36807;&#23545;&#20914;&#32773;&#21644;&#29983;&#25104;&#22120;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#20986;&#26080;&#38656;&#36827;&#34892;&#20215;&#26684;&#36807;&#31243;&#24314;&#27169;&#30340;&#31283;&#20581;&#23545;&#20914;&#32773;&#12290;</title><link>http://arxiv.org/abs/2307.13217</link><description>&lt;p&gt;
&#23545;&#25239;&#24335;&#28145;&#24230;&#23545;&#20914;&#65306;&#26080;&#38656;&#20215;&#26684;&#36807;&#31243;&#24314;&#27169;&#30340;&#23545;&#20914;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Deep Hedging: Learning to Hedge without Price Process Modeling. (arXiv:2307.13217v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24335;&#28145;&#24230;&#23545;&#20914;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#20840;&#24066;&#22330;&#20013;&#36827;&#34892;&#34893;&#29983;&#21697;&#23545;&#20914;&#12290;&#36890;&#36807;&#23545;&#20914;&#32773;&#21644;&#29983;&#25104;&#22120;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#20986;&#26080;&#38656;&#36827;&#34892;&#20215;&#26684;&#36807;&#31243;&#24314;&#27169;&#30340;&#31283;&#20581;&#23545;&#20914;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23545;&#20914;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34893;&#29983;&#21697;&#23545;&#20914;&#26694;&#26550;&#65292;&#29992;&#20110;&#19981;&#23436;&#20840;&#24066;&#22330;&#20013;&#30340;&#23545;&#20914;&#12290;&#28145;&#24230;&#23545;&#20914;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#29616;&#23454;&#24066;&#22330;&#26465;&#20214;&#65292;&#22914;&#24066;&#22330;&#25705;&#25830;&#65292;&#32780;&#36825;&#22312;&#20256;&#32479;&#30340;&#25968;&#23398;&#37329;&#34701;&#26694;&#26550;&#20013;&#24456;&#38590;&#35299;&#20915;&#12290;&#30001;&#20110;&#28145;&#24230;&#23545;&#20914;&#20381;&#36182;&#20110;&#24066;&#22330;&#27169;&#25311;&#65292;&#22240;&#27492;&#24213;&#23618;&#36164;&#20135;&#20215;&#26684;&#36807;&#31243;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#28145;&#24230;&#23545;&#20914;&#30340;&#25991;&#29486;&#24448;&#24448;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25968;&#23398;&#37329;&#34701;&#27169;&#22411;&#65292;&#20363;&#22914;&#24067;&#26391;&#36816;&#21160;&#21644;&#38543;&#26426;&#27874;&#21160;&#27169;&#22411;&#65292;&#32780;&#23547;&#25214;&#26377;&#25928;&#30340;&#24213;&#23618;&#36164;&#20135;&#27169;&#22411;&#29992;&#20110;&#28145;&#24230;&#23545;&#20914;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#23545;&#25239;&#24335;&#28145;&#24230;&#23545;&#20914;&#65292;&#21463;&#21040;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#23545;&#20914;&#32773;&#21644;&#19968;&#20010;&#29983;&#25104;&#22120;&#20998;&#21035;&#23545;&#24213;&#23618;&#36164;&#20135;&#36807;&#31243;&#21644;&#24213;&#23618;&#36164;&#20135;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20197;&#23545;&#25239;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#31283;&#20581;&#30340;&#23545;&#20914;&#32773;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20215;&#26684;&#36807;&#31243;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hedging is a deep-learning-based framework for derivative hedging in incomplete markets. The advantage of deep hedging lies in its ability to handle various realistic market conditions, such as market frictions, which are challenging to address within the traditional mathematical finance framework. Since deep hedging relies on market simulation, the underlying asset price process model is crucial. However, existing literature on deep hedging often relies on traditional mathematical finance models, e.g., Brownian motion and stochastic volatility models, and discovering effective underlying asset models for deep hedging learning has been a challenge. In this study, we propose a new framework called adversarial deep hedging, inspired by adversarial learning. In this framework, a hedger and a generator, which respectively model the underlying asset process and the underlying asset process, are trained in an adversarial manner. The proposed method enables to learn a robust hedger witho
&lt;/p&gt;</description></item><item><title>FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2307.13214</link><description>&lt;p&gt;
FedMEKT: &#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13214
&lt;/p&gt;
&lt;p&gt;
FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#24191;&#20041;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#21482;&#26159;&#38024;&#23545;&#21333;&#27169;&#24577;&#25968;&#25454;&#25552;&#20986;&#20102;&#20856;&#22411;&#30340;FL&#31995;&#32479;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#23545;&#20110;&#21033;&#29992;&#23453;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26410;&#26469;&#20010;&#24615;&#21270;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#30001;&#20110;&#29992;&#25143;&#26080;&#27861;&#36827;&#34892;&#33258;&#27880;&#37322;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#26377;&#38480;&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;FL&#26694;&#26550;&#65292;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#23558;&#36825;&#20010;&#27010;&#24565;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;&#26426;&#21046;&#65292;&#31216;&#20026;FedMEKT&#65292;&#23427;&#20801;&#35768;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20132;&#25442;&#20174;&#23567;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#32852;&#21512;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#27493;&#24577;&#21608;&#26399;&#21551;&#21457;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#33181;&#20851;&#33410;&#35282;&#24230;&#20998;&#35299;&#20026;&#36816;&#21160;&#27169;&#24335;&#21644;&#25391;&#24133;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#23545;&#20154;&#20307;&#33181;&#20851;&#33410;&#36712;&#36857;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13209</link><description>&lt;p&gt;
Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG.
&lt;/p&gt;
&lt;p&gt;
Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG. (arXiv:2307.13209v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#27493;&#24577;&#21608;&#26399;&#21551;&#21457;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#33181;&#20851;&#33410;&#35282;&#24230;&#20998;&#35299;&#20026;&#36816;&#21160;&#27169;&#24335;&#21644;&#25391;&#24133;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#23545;&#20154;&#20307;&#33181;&#20851;&#33410;&#36712;&#36857;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19979;&#32930;&#36816;&#21160;&#24847;&#22270;&#23545;&#20110;&#25511;&#21046;&#22806;&#39592;&#39612;&#26426;&#22120;&#20154;&#21644;&#20551;&#32930;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#34920;&#38754;&#32908;&#30005;&#22270;(sEMG)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#23454;&#38469;&#36816;&#21160;&#20043;&#21069;&#25552;&#21069;&#39044;&#27979;&#36816;&#21160;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#20307;&#20869;&#37096;&#21644;&#20010;&#20307;&#38388;&#30340;&#24046;&#24322;&#65292;&#23545;&#20154;&#20307;&#20851;&#33410;&#36712;&#36857;&#30340;&#20272;&#35745;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#21069;&#32773;&#19982;&#20010;&#20307;&#30340;&#29983;&#29702;&#24046;&#24322;&#65288;&#22914;&#36523;&#39640;&#21644;&#20307;&#37325;&#65289;&#21644;&#20559;&#22909;&#30340;&#34892;&#36208;&#27169;&#24335;&#26377;&#20851;&#65292;&#32780;&#21518;&#32773;&#20027;&#35201;&#26159;&#30001;&#19981;&#35268;&#21017;&#21644;&#19982;&#27493;&#24577;&#26080;&#20851;&#30340;&#32908;&#32905;&#27963;&#21160;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#31181;&#21463;&#27493;&#24577;&#21608;&#26399;&#21551;&#21457;&#30340;&#23398;&#20064;&#31574;&#30053;&#26469;&#20943;&#36731;&#39044;&#27979;&#20154;&#20307;&#33181;&#20851;&#33410;&#36712;&#36857;&#30340;&#25361;&#25112;&#12290;&#31532;&#19968;&#31181;&#31574;&#30053;&#26159;&#23558;&#33181;&#20851;&#33410;&#35282;&#24230;&#20998;&#35299;&#20026;&#36816;&#21160;&#27169;&#24335;&#21644;&#25391;&#24133;&#65292;&#21069;&#32773;&#22312;&#20010;&#20307;&#38388;&#26174;&#31034;&#20986;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#65292;&#32780;&#21518;&#32773;&#22312;&#20010;&#20307;&#38388;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#36890;&#36807;&#21333;&#29420;&#30340;&#32593;&#32476;&#23454;&#20307;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#20307;&#33181;&#20851;&#33410;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting lower limb motion intent is vital for controlling exoskeleton robots and prosthetic limbs. Surface electromyography (sEMG) attracts increasing attention in recent years as it enables ahead-of-time prediction of motion intentions before actual movement. However, the estimation performance of human joint trajectory remains a challenging problem due to the inter- and intra-subject variations. The former is related to physiological differences (such as height and weight) and preferred walking patterns of individuals, while the latter is mainly caused by irregular and gait-irrelevant muscle activity. This paper proposes a model integrating two gait cycle-inspired learning strategies to mitigate the challenge for predicting human knee joint trajectory. The first strategy is to decouple knee joint angles into motion patterns and amplitudes former exhibit low variability while latter show high variability among individuals. By learning through separate network entities, the model ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;COUNTERPOL&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23545;&#31574;&#30053;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#26469;&#20998;&#26512;RL&#31574;&#30053;&#65292;&#24182;&#36798;&#21040;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;RL&#20013;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.13192</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation Policies in RL. (arXiv:2307.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;COUNTERPOL&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23545;&#31574;&#30053;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#26469;&#20998;&#26512;RL&#31574;&#30053;&#65292;&#24182;&#36798;&#21040;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;RL&#20013;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#20351;&#29992;&#22870;&#21169;&#20559;&#22909;&#30340;&#22810;&#26679;&#21270;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#30830;&#20445;&#36825;&#20123;&#26694;&#26550;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#33021;&#22815;&#35299;&#37322;&#21464;&#24471;&#24456;&#37325;&#35201;&#65292;&#21363;&#23558;&#35266;&#23519;&#26144;&#23556;&#21040;&#21487;&#33021;&#34892;&#21160;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#31574;&#30053;&#22914;&#20309;&#20197;&#23545;&#27604;&#30340;&#26041;&#24335;&#31995;&#32479;&#22320;&#29702;&#35299;&#65292;&#21363;&#65292;&#20351;&#20854;&#24615;&#33021;&#36798;&#21040;&#25152;&#38656;&#27700;&#24179;&#30340;&#31574;&#30053;&#26368;&#23567;&#25913;&#21464;&#26159;&#20160;&#20040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COUNTERPOL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#20998;&#26512;RL&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#23545;&#31574;&#30053;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#65292;&#36798;&#21040;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21453;&#20107;&#23454;&#34701;&#20837;RL&#20013;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26399;&#26395;&#25910;&#30410;&#35843;&#25511;&#30446;&#26631;&#32467;&#26524;&#65292;&#24314;&#31435;&#20102;Counterpol&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#22823;&#37327;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25968;&#23383;&#24773;&#32490;&#35843;&#33410;&#65292;&#24182;&#32508;&#21512;&#20102;&#20851;&#20110;&#31038;&#20132;&#23186;&#20307;&#24773;&#32490;&#35843;&#33410;&#24178;&#39044;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#22312;&#24773;&#32490;&#35843;&#33410;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#26377;&#19981;&#21516;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.13187</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25968;&#23383;&#24773;&#32490;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Digital Emotion Regulation on Social Media. (arXiv:2307.13187v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25968;&#23383;&#24773;&#32490;&#35843;&#33410;&#65292;&#24182;&#32508;&#21512;&#20102;&#20851;&#20110;&#31038;&#20132;&#23186;&#20307;&#24773;&#32490;&#35843;&#33410;&#24178;&#39044;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#22312;&#24773;&#32490;&#35843;&#33410;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#26377;&#19981;&#21516;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35843;&#33410;&#26159;&#26377;&#24847;&#35782;&#22320;&#25913;&#21464;&#33258;&#36523;&#24773;&#24863;&#29366;&#24577;&#30340;&#36807;&#31243;&#65292;&#21363;&#25913;&#21464;&#24184;&#31119;&#12289;&#33258;&#20449;&#12289;&#20869;&#30106;&#12289;&#24868;&#24594;&#31561;&#22522;&#26412;&#24773;&#32490;&#29366;&#24577;&#12290;&#26377;&#25928;&#22320;&#35843;&#33410;&#24773;&#32490;&#23545;&#20110;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#39640;&#25928;&#36816;&#20316;&#26159;&#24517;&#35201;&#30340;&#12290;&#22914;&#20170;&#65292;&#25968;&#23383;&#25216;&#26415;&#30340;&#26222;&#21450;&#27491;&#22312;&#34987;&#26377;&#30446;&#30340;&#22320;&#29992;&#26469;&#25913;&#21464;&#25105;&#20204;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#36825;&#19968;&#36807;&#31243;&#34987;&#31216;&#20026;&#25968;&#23383;&#24773;&#32490;&#35843;&#33410;&#12290;&#29702;&#35299;&#25968;&#23383;&#24773;&#32490;&#35843;&#33410;&#21487;&#20197;&#24110;&#21161;&#25903;&#25345;&#36947;&#24503;&#31185;&#25216;&#35774;&#35745;&#12289;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#23835;&#36215;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#20013;&#30340;&#25968;&#23383;&#24773;&#32490;&#35843;&#33410;&#65292;&#24182;&#32508;&#21512;&#20102;&#20851;&#20110;&#31038;&#20132;&#23186;&#20307;&#24773;&#32490;&#35843;&#33410;&#24178;&#39044;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#26368;&#26032;&#25991;&#29486;&#65292;&#20998;&#20139;&#20102;&#20851;&#20110;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#22312;&#24773;&#32490;&#35843;&#33410;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion regulation is the process of consciously altering one's affective state, that is the underlying emotional state such as happiness, confidence, guilt, anger etc. The ability to effectively regulate emotions is necessary for functioning efficiently in everyday life. Today, the pervasiveness of digital technology is being purposefully employed to modify our affective states, a process known as digital emotion regulation. Understanding digital emotion regulation can help support the rise of ethical technology design, development, and deployment. This article presents an overview of digital emotion regulation in social media applications, as well as a synthesis of recent research on emotion regulation interventions for social media. We share our findings from analysing state-of-the-art literature on how different social media applications are utilised at different stages in the process of emotion regulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35266;&#28857;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29305;&#23450;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21644;&#36716;&#31227;&#35266;&#28857;&#65292;&#24182;&#20445;&#25345;&#26497;&#24615;&#30340;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25366;&#25496;&#30495;&#23454;&#25991;&#26412;&#20013;&#30340;&#35266;&#28857;&#27934;&#23519;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13173</link><description>&lt;p&gt;
&#20351;&#29992;&#32463;&#36807;&#20154;&#32676;&#35843;&#25972;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35266;&#28857;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Opinion Mining Using Population-tuned Generative Language Models. (arXiv:2307.13173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35266;&#28857;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29305;&#23450;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21644;&#36716;&#31227;&#35266;&#28857;&#65292;&#24182;&#20445;&#25345;&#26497;&#24615;&#30340;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25366;&#25496;&#30495;&#23454;&#25991;&#26412;&#20013;&#30340;&#35266;&#28857;&#27934;&#23519;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35757;&#32451;&#20110;&#19981;&#21516;&#20154;&#32676;&#25968;&#25454;&#19978;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38598;&#21512;&#20013;&#25366;&#25496;&#35266;&#28857;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22522;&#26412;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#21644;&#35266;&#28857;&#27934;&#23519;&#25366;&#25496;&#30340;&#36890;&#29992;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#23450;&#21046;&#20869;&#23481;&#21644;&#23436;&#20840;&#26631;&#27880;&#30340;&#35266;&#28857;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21644;&#36716;&#31227;&#35266;&#28857;&#21040;&#35821;&#20041;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#26497;&#24615;&#30340;&#27604;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27934;&#23519;&#25366;&#25496;&#31995;&#32479;&#22312;&#23454;&#38469;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#35266;&#28857;&#27934;&#23519;&#30340;&#25193;&#23637;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for mining opinions from text collections using generative language models trained on data collected from different populations. We describe the basic definitions, methodology and a generic algorithm for opinion insight mining. We demonstrate the performance of our method in an experiment where a pre-trained generative model is fine-tuned using specifically tailored content with unnatural and fully annotated opinions. We show that our approach can learn and transfer the opinions to the semantic classes while maintaining the proportion of polarisation. Finally, we demonstrate the usage of an insight mining system to scale up the discovery of opinion insights from a real text corpus.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#21457;&#29616;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#21017;&#27809;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#33021;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.13165</link><description>&lt;p&gt;
&#30740;&#31350;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#23545;&#35757;&#32451;&#25968;&#25454;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study. (arXiv:2307.13165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#21457;&#29616;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#21017;&#27809;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#33021;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34892;&#20026;&#65292;&#28982;&#32780;&#20854;&#22312;&#38754;&#23545;&#35757;&#32451;&#25968;&#25454;&#25200;&#21160;&#26102;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#26102;&#38388;&#39034;&#24207;&#24207;&#21015;&#20013;&#19981;&#21516;&#20301;&#32622;&#19978;&#21024;&#38500;&#39033;&#30446;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#25240;&#29616;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#25351;&#26631;&#21644;&#25490;&#21517;&#25935;&#24863;&#24230;&#21015;&#34920;&#65288;Rank Sensitivity List&#65289;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;NDCG&#19979;&#38477;&#39640;&#36798;60&#65285;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21487;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EvilEye&#65292;&#21033;&#29992;&#36879;&#26126;&#26174;&#31034;&#22120;&#29983;&#25104;&#21160;&#24577;&#29289;&#29702;&#25932;&#23545;&#31034;&#20363;&#30340;&#24863;&#30693;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25668;&#20687;&#22836;&#30340;&#20809;&#23398;&#29305;&#24615;&#22312;&#22810;&#31181;&#29031;&#26126;&#26465;&#20214;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.13131</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20320;&#19981;&#28165;&#27905;&#30524;&#38236;&#65311;&#21033;&#29992;&#21160;&#24577;&#20809;&#23398;&#25200;&#21160;&#36827;&#34892;&#24863;&#30693;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (arXiv:2307.13131v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EvilEye&#65292;&#21033;&#29992;&#36879;&#26126;&#26174;&#31034;&#22120;&#29983;&#25104;&#21160;&#24577;&#29289;&#29702;&#25932;&#23545;&#31034;&#20363;&#30340;&#24863;&#30693;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25668;&#20687;&#22836;&#30340;&#20809;&#23398;&#29305;&#24615;&#22312;&#22810;&#31181;&#29031;&#26126;&#26465;&#20214;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25668;&#20687;&#22836;&#20026;&#22522;&#30784;&#30340;&#27169;&#25311;&#20154;&#31867;&#24863;&#30693;&#30340;&#33258;&#20027;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#38598;&#25104;&#21040;&#23433;&#20840;&#20851;&#38190;&#30340;&#24179;&#21488;&#20013;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#25991;&#29486;&#20307;&#31995;&#65292;&#25506;&#32034;&#23545;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25932;&#23545;&#25915;&#20987;&#12290;&#23558;&#25932;&#23545;&#25915;&#20987;&#36866;&#24212;&#20110;&#29616;&#23454;&#19990;&#30028;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#36825;&#28040;&#38500;&#20102;&#21361;&#23475;&#25968;&#23383;&#31995;&#32479;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#38754;&#20020;&#30528;&#19982;&#24863;&#30693;&#31649;&#36947;&#20013;&#30340;&#29615;&#22659;&#22122;&#22768;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#21160;&#24577;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#24863;&#22120;&#20026;&#20808;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EvilEye&#65292;&#19968;&#31181;&#21033;&#29992;&#36879;&#26126;&#26174;&#31034;&#22120;&#29983;&#25104;&#21160;&#24577;&#29289;&#29702;&#25932;&#23545;&#31034;&#20363;&#30340;&#20013;&#38388;&#20154;&#24863;&#30693;&#25915;&#20987;&#12290;EvilEye&#21033;&#29992;&#25668;&#20687;&#22836;&#30340;&#20809;&#23398;&#29305;&#24615;&#22312;&#21508;&#31181;&#29031;&#26126;&#26465;&#20214;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#29983;&#25104;&#21160;&#24577;&#25200;&#21160;&#65292;&#25105;&#20204;&#23558;&#25968;&#23383;&#25915;&#20987;&#30340;&#25237;&#24433;&#24418;&#24335;&#21270;&#20026;&#20809;&#23398;&#35270;&#35273;&#36870;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera-based autonomous systems that emulate human perception are increasingly being integrated into safety-critical platforms. Consequently, an established body of literature has emerged that explores adversarial attacks targeting the underlying machine learning models. Adapting adversarial attacks to the physical world is desirable for the attacker, as this removes the need to compromise digital systems. However, the real world poses challenges related to the "survivability" of adversarial manipulations given environmental noise in perception pipelines and the dynamicity of autonomous systems. In this paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle perception attack that leverages transparent displays to generate dynamic physical adversarial examples. EvilEye exploits the camera's optics to induce misclassifications under a variety of illumination conditions. To generate dynamic perturbations, we formalize the projection of a digital attack into the ph
&lt;/p&gt;</description></item><item><title>Pathway&#26159;&#19968;&#20010;&#24555;&#36895;&#28789;&#27963;&#30340;&#32479;&#19968;&#27969;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23427;&#33021;&#22815;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#30340;&#25968;&#25454;&#27969;&#19978;&#36816;&#34892;&#65292;&#36890;&#36807;Table API&#21644;&#20998;&#24067;&#24335;&#22686;&#37327;&#25968;&#25454;&#27969;&#39537;&#21160;&#65292;&#22312;&#25209;&#22788;&#29702;&#21644;&#27969;&#22788;&#29702;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13116</link><description>&lt;p&gt;
Pathway:&#19968;&#31181;&#24555;&#36895;&#28789;&#27963;&#30340;&#32479;&#19968;&#27969;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pathway: a fast and flexible unified stream data processing framework for analytical and Machine Learning applications. (arXiv:2307.13116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13116
&lt;/p&gt;
&lt;p&gt;
Pathway&#26159;&#19968;&#20010;&#24555;&#36895;&#28789;&#27963;&#30340;&#32479;&#19968;&#27969;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23427;&#33021;&#22815;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#30340;&#25968;&#25454;&#27969;&#19978;&#36816;&#34892;&#65292;&#36890;&#36807;Table API&#21644;&#20998;&#24067;&#24335;&#22686;&#37327;&#25968;&#25454;&#27969;&#39537;&#21160;&#65292;&#22312;&#25209;&#22788;&#29702;&#21644;&#27969;&#22788;&#29702;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pathway&#65292;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#25968;&#25454;&#27969;&#19978;&#36816;&#34892;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#24314;&#26368;&#21021;&#26159;&#20026;&#20102;&#35299;&#20915;&#22312;&#20998;&#26512;&#21644;&#22788;&#29702;&#29289;&#29702;&#32463;&#27982;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#29289;&#32852;&#32593;&#21644;&#20225;&#19994;&#31995;&#32479;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#12290;&#36825;&#20123;&#37117;&#38656;&#35201;&#24555;&#36895;&#21453;&#24212;&#65292;&#24182;&#38656;&#35201;&#24212;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#33539; paradigms&#65288;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#26512;&#65292;&#19978;&#19979;&#25991;&#20998;&#26512;&#21644;&#22797;&#26434;&#20107;&#20214;&#22788;&#29702;&#30340;&#20854;&#20182;&#20803;&#32032;&#65289;&#12290;Pathway&#37197;&#22791;&#20102;&#38024;&#23545;Python&#21644;Python/SQL&#24037;&#20316;&#27969;&#31243;&#37327;&#36523;&#23450;&#21046;&#30340;Table API&#65292;&#24182;&#30001;Rust&#20013;&#30340;&#20998;&#24067;&#24335;&#22686;&#37327;&#25968;&#25454;&#27969;&#39537;&#21160;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#21576;&#29616;&#20102;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#22312;&#25209;&#22788;&#29702;&#21644;&#27969;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#34892;&#19994;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#30001;Pathway&#22788;&#29702;&#30340;&#27969;&#22788;&#29702;&#29992;&#20363;&#65292;&#36825;&#20123;&#29992;&#20363;&#26080;&#27861;&#36731;&#26494;&#35299;&#20915;&#20197;&#29366;&#24577;&#20026;&#22522;&#30784;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Pathway, a new unified data processing framework that can run workloads on both bounded and unbounded data streams. The framework was created with the original motivation of resolving challenges faced when analyzing and processing data from the physical economy, including streams of data generated by IoT and enterprise systems. These required rapid reaction while calling for the application of advanced computation paradigms (machinelearning-powered analytics, contextual analysis, and other elements of complex event processing). Pathway is equipped with a Table API tailored for Python and Python/SQL workflows, and is powered by a distributed incremental dataflow in Rust. We describe the system and present benchmarking results which demonstrate its capabilities in both batch and streaming contexts, where it is able to surpass state-of-the-art industry frameworks in both scenarios. We also discuss streaming use cases handled by Pathway which cannot be easily resolved with state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;xGW-GAT&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#21151;&#33021;&#32593;&#32476;&#65292;&#20197;&#25512;&#21160;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.13108</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#21151;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment. (arXiv:2307.13108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;xGW-GAT&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#21151;&#33021;&#32593;&#32476;&#65292;&#20197;&#25512;&#21160;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#30340;&#19968;&#20010;&#26174;&#33879;&#30151;&#29366;&#26159;&#23039;&#21183;&#21453;&#23556;&#30340;&#36880;&#28176;&#20007;&#22833;&#65292;&#26368;&#32456;&#23548;&#33268;&#27493;&#24577;&#22256;&#38590;&#21644;&#24179;&#34913;&#38382;&#39064;&#12290;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#33041;&#21151;&#33021;&#32010;&#20081;&#23545;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24085;&#37329;&#26862;&#30149;&#30340;&#36816;&#21160;&#36827;&#23637;&#20197;&#21450;&#25512;&#21160;&#26356;&#26377;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#30340;&#21457;&#23637;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#12289;&#20960;&#20309;&#30340;&#12289;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;xGW-GAT&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#22256;&#38590;&#36827;&#23637;&#30340;&#21151;&#33021;&#32593;&#32476;&#12290;xGW-GAT&#22312;MDS&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#26631;&#20934;&#65288;MDS-UPDRS&#65289;&#19978;&#39044;&#27979;&#22810;&#31867;&#21035;&#30340;&#27493;&#24577;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#23558;&#21151;&#33021;&#36830;&#25509;&#32452;&#34920;&#31034;&#20026;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#65292;&#20197;&#26126;&#30830;&#32534;&#30721;&#25972;&#20010;&#36830;&#25509;&#32452;&#30340;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#65292;&#26681;&#25454;&#27492;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#20986;&#19968;&#20010;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#20197;&#20135;&#29983;&#20010;&#20307;&#21644;&#32676;&#20307;&#32423;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#65292;LLMs&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#20351;&#29992;LLMs&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#21040;&#24773;&#24863;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#35805;&#35821;&#20998;&#26512;&#31561;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#19988;&#36895;&#24230;&#24555;&#12290;&#36825;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#32534;&#31243;&#32463;&#39564;&#30340;&#23398;&#29983;&#21644;&#30740;&#31350;&#32773;&#26469;&#35828;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13106</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How to use LLMs for Text Analysis. (arXiv:2307.13106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#65292;LLMs&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#20351;&#29992;LLMs&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#21040;&#24773;&#24863;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#35805;&#35821;&#20998;&#26512;&#31561;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#19988;&#36895;&#24230;&#24555;&#12290;&#36825;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#32534;&#31243;&#32463;&#39564;&#30340;&#23398;&#29983;&#21644;&#30740;&#31350;&#32773;&#26469;&#35828;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25351;&#21335;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#31038;&#20250;&#31185;&#23398;&#20013;&#19968;&#31181;&#38750;&#24120;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#12290;&#30001;&#20110;LLMs&#26131;&#20110;&#20351;&#29992;&#12289;&#25104;&#26412;&#20302;&#12289;&#36895;&#24230;&#24555;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#65292;&#20174;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#21040;&#24773;&#24863;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#35805;&#35821;&#20998;&#26512;&#65292;&#35768;&#22810;&#23398;&#32773;&#35748;&#20026;LLMs&#23558;&#25913;&#21464;&#25105;&#20204;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#26041;&#24335;&#12290;&#26412;&#25351;&#21335;&#38754;&#21521;&#20855;&#26377;&#26377;&#38480;&#32534;&#31243;&#32463;&#39564;&#30340;&#23398;&#29983;&#21644;&#30740;&#31350;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;&#33258;&#24049;&#30340;&#30740;&#31350;&#39033;&#30446;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#31616;&#21333;&#20171;&#32461;&#65292;&#20197;&#21450;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;Python&#28436;&#31034;&#20351;&#29992;LLMs&#20998;&#26512;&#25991;&#26412;&#25968;&#25454;&#30340;&#27599;&#20010;&#27493;&#39588;&#65306;&#23433;&#35013;&#36719;&#20214;&#65292;&#35774;&#32622;API&#65292;&#21152;&#36733;&#25968;&#25454;&#65292;&#24320;&#21457;&#20998;&#26512;&#25552;&#31034;&#65292;&#20998;&#26512;&#25991;&#26412;&#21644;&#39564;&#35777;&#32467;&#26524;&#12290;&#20316;&#20026;&#19968;&#20010;&#35828;&#26126;&#24615;&#20363;&#23376;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#22312;&#25919;&#27835;&#25991;&#26412;&#20013;&#35782;&#21035;&#27665;&#31929;&#20027;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;LLMs&#22914;&#20309;&#36229;&#36234;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This guide introduces Large Language Models (LLM) as a highly versatile text analysis method within the social sciences. As LLMs are easy-to-use, cheap, fast, and applicable on a broad range of text analysis tasks, ranging from text annotation and classification to sentiment analysis and critical discourse analysis, many scholars believe that LLMs will transform how we do text analysis. This how-to guide is aimed at students and researchers with limited programming experience, and offers a simple introduction to how LLMs can be used for text analysis in your own research project, as well as advice on best practices. We will go through each of the steps of analyzing textual data with LLMs using Python: installing the software, setting up the API, loading the data, developing an analysis prompt, analyzing the text, and validating the results. As an illustrative example, we will use the challenging task of identifying populism in political texts, and show how LLMs move beyond the existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#27169;&#22411;&#30340;&#22810;&#27493;&#36716;&#31227;&#26469;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#27491;&#35268;&#21270;&#21644;&#26102;&#24046;&#26356;&#26032;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13101</link><description>&lt;p&gt;
&#23545;&#27604;&#31034;&#20363;&#39537;&#21160;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Contrastive Example-Based Control. (arXiv:2307.13101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#27169;&#22411;&#30340;&#22810;&#27493;&#36716;&#31227;&#26469;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#27491;&#35268;&#21270;&#21644;&#26102;&#24046;&#26356;&#26032;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#21463;&#30410;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#24456;&#23569;&#31526;&#21512;MDP&#27169;&#22411;&#65292;&#19982;&#29615;&#22659;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#20063;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20174;&#36716;&#31227;&#21160;&#24577;&#30340;&#26679;&#26412;&#21644;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20174;&#39640;&#22238;&#25253;&#29366;&#24577;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#29992;&#35813;&#22870;&#21169;&#20989;&#25968;&#26631;&#35760;&#36716;&#31227;&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#36825;&#20123;&#36716;&#31227;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#36890;&#24120;&#38656;&#35201;&#27491;&#35268;&#21270;&#21644;&#26102;&#24046;&#26356;&#26032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#12289;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#38544;&#24335;&#27169;&#22411;&#30340;&#22810;&#27493;&#36716;&#31227;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38544;&#24335;&#27169;&#22411;&#21487;&#20197;&#34920;&#31034;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;Q&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a ran
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRMetaText&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#27604;&#36739;&#20803;&#25968;&#25454;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#20998;&#26512;&#20803;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#25968;&#23398;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35782;&#21035;&#21487;&#26367;&#20195;&#26415;&#35821;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.13085</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#20803;&#25968;&#25454;&#26356;&#21152;FAIR
&lt;/p&gt;
&lt;p&gt;
Making Metadata More FAIR Using Large Language Models. (arXiv:2307.13085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRMetaText&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#27604;&#36739;&#20803;&#25968;&#25454;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#20998;&#26512;&#20803;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#25968;&#23398;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35782;&#21035;&#21487;&#26367;&#20195;&#26415;&#35821;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#23454;&#39564;&#25968;&#25454;&#36164;&#26009;&#30340;&#22686;&#21152;&#65292;&#32479;&#19968;&#21033;&#29992;&#36825;&#20123;&#36164;&#26009;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#31967;&#31957;&#30340;&#20803;&#25968;&#25454;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRMetaText&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#27604;&#36739;&#20803;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FAIRMetaText&#20998;&#26512;&#20803;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20004;&#20010;&#26415;&#35821;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#65292;&#36890;&#36807;&#24314;&#35758;&#31526;&#21512;&#24615;&#26415;&#35821;&#25110;&#20998;&#32452;&#30456;&#20284;&#26415;&#35821;&#26469;&#35782;&#21035;&#21487;&#26367;&#20195;&#26415;&#35821;&#12290;&#36890;&#36807;&#22312;&#20844;&#24320;&#30340;&#30740;&#31350;&#36164;&#26009;&#19978;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21508;&#31181;&#20803;&#25968;&#25454;&#30456;&#20851;&#20219;&#21153;&#19978;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#36825;&#20010;&#36719;&#20214;&#21487;&#20197;&#26497;&#22823;&#22320;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#23454;&#39564;&#25968;&#25454;&#36807;&#31243;&#20013;&#31579;&#36873;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the global increase in experimental data artifacts, harnessing them in a unified fashion leads to a major stumbling block - bad metadata. To bridge this gap, this work presents a Natural Language Processing (NLP) informed application, called FAIRMetaText, that compares metadata. Specifically, FAIRMetaText analyzes the natural language descriptions of metadata and provides a mathematical similarity measure between two terms. This measure can then be utilized for analyzing varied metadata, by suggesting terms for compliance or grouping similar terms for identification of replaceable terms. The efficacy of the algorithm is presented qualitatively and quantitatively on publicly available research artifacts and demonstrates large gains across metadata related tasks through an in-depth study of a wide variety of Large Language Models (LLMs). This software can drastically reduce the human effort in sifting through various natural language metadata while employing several experimental dat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.13081</link><description>&lt;p&gt;
&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#19979;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20551;&#35774;&#27169;&#22411;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#20154;&#21475;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#26399;&#38388;&#26410;&#20445;&#30041;&#35760;&#24405;&#25110;&#20986;&#20110;&#38544;&#31169;&#21407;&#22240;&#65292;&#23384;&#22312;&#20154;&#21475;&#20449;&#24687;&#37096;&#20998;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#19968;&#20010;&#23646;&#24615;&#20998;&#31867;&#22120;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#25935;&#24863;&#23646;&#24615;&#65288;&#20195;&#29702;&#65289;&#20173;&#28982;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#30495;&#23454;&#25935;&#24863;&#23646;&#24615;&#30456;&#27604;&#65292;&#20351;&#29992;&#20195;&#29702;&#25935;&#24863;&#23646;&#24615;&#20250;&#21152;&#21095;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26500;&#24314;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#20855;&#26377;&#25512;&#26029;&#20986;&#30340;&#26368;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#20154;&#21475;&#20449;&#24687;&#30340;&#26679;&#26412;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#26679;&#26412;&#19978;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#32422;&#26463;&#20250;&#25439;&#23475;&#31639;&#27861;&#30340;&#24635;&#20307;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26102;&#20351;&#29992;&#33258;&#36866;&#24212;&#35748;&#35777;&#21322;&#24452;&#30340;&#20851;&#38190;&#35266;&#28857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25512;&#36827;&#20102;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.13078</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#35748;&#35777;&#35757;&#32451;: &#36808;&#21521;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs. (arXiv:2307.13078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26102;&#20351;&#29992;&#33258;&#36866;&#24212;&#35748;&#35777;&#21322;&#24452;&#30340;&#20851;&#38190;&#35266;&#28857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25512;&#36827;&#20102;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#26029;&#36827;&#27493;&#21644;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#26085;&#30410;&#24191;&#27867;&#24212;&#29992;&#65292;&#40065;&#26834;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#22312;&#26576;&#20123;&#25200;&#21160;&#27700;&#24179;&#19978;&#33719;&#24471;&#39640;&#24230;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#22312;&#24178;&#20928;&#30340;&#26410;&#25200;&#21160;&#25968;&#25454;&#19978;&#20934;&#30830;&#24615;&#20005;&#37325;&#38477;&#20302;&#65292;&#20351;&#20854;&#19981;&#23454;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#29305;&#23450;&#30340;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#19978;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#26356;&#29616;&#23454;&#30340;&#35282;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#35266;&#28857;&#30340;&#26032;&#22411;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#33258;&#36866;&#24212;&#35748;&#35777;&#21322;&#24452;&#36827;&#34892;&#35757;&#32451;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25512;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;MNIST&#12289;CIFAR-10&#21644;TinyImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13014</link><description>&lt;p&gt;
&#29992;&#20110;&#31243;&#24207;&#20043;&#38388;&#21464;&#37327;&#26144;&#23556;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;&#25193;&#23637;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20998;&#26512;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24418;&#24335;&#26041;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#20110;&#31243;&#24207;&#31561;&#20215;&#38382;&#39064;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#65292;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#65292;&#38656;&#35201;&#23545;&#20004;&#20010;&#31243;&#24207;&#30340;&#21464;&#37327;&#38598;&#20043;&#38388;&#24314;&#31435;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#22312;&#35832;&#22914;&#31243;&#24207;&#31561;&#20215;&#24615;&#12289;&#31243;&#24207;&#20998;&#26512;&#12289;&#31243;&#24207;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#65292;&#26144;&#23556;&#20004;&#20010;&#31243;&#24207;&#20043;&#38388;&#30340;&#21464;&#37327;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#20004;&#20010;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#12290;&#20026;&#20102;&#23637;&#31034;&#21464;&#37327;&#26144;&#23556;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312;&#31243;&#24207;&#20462;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#36825;&#20123;&#26144;&#23556;&#30340;&#19977;&#20010;&#29992;&#20363;&#65292;&#20197;&#20462;&#22797;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#24120;&#35265;&#30340;&#21644;&#32463;&#24120;&#21457;&#29983;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19968;&#20010;&#21253;&#21547;4166&#23545;&#38169;&#35823;/&#20462;&#27491;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#22522;&#20934;&#65292;&#23545;&#19981;&#21516;&#30340;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#37325;&#21472;&#35762;&#35805;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22810;&#20010;&#38899;&#39057;&#35774;&#32622;&#21644;&#35821;&#38899;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.13012</link><description>&lt;p&gt;
&#32852;&#21512;&#35821;&#38899;&#21644;&#37325;&#21472;&#26816;&#27979;&#65306;&#22810;&#20010;&#38899;&#39057;&#35774;&#32622;&#21644;&#35821;&#38899;&#39046;&#22495;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains. (arXiv:2307.13012v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#22522;&#20934;&#65292;&#23545;&#19981;&#21516;&#30340;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#37325;&#21472;&#35762;&#35805;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22810;&#20010;&#38899;&#39057;&#35774;&#32622;&#21644;&#35821;&#38899;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#37325;&#21472;&#35762;&#35805;&#26816;&#27979;&#26159;&#35828;&#35805;&#32773;&#20998;&#21106;&#30340;&#20851;&#38190;&#39044;&#22788;&#29702;&#20219;&#21153;&#12290;&#26368;&#32456;&#30340;&#20998;&#27573;&#24615;&#33021;&#38750;&#24120;&#20381;&#36182;&#20110;&#36825;&#20123;&#23376;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#22810;&#31867;&#21035;&#20998;&#31867;&#27169;&#22411;&#26469;&#32852;&#21512;&#35757;&#32451;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#37325;&#21472;&#35762;&#35805;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#35821;&#38899;&#39046;&#22495;&#65292;&#32570;&#20047;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#26032;&#39062;&#30340;&#19981;&#21516;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#37325;&#21472;&#35762;&#35805;&#26816;&#27979;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#38899;&#39057;&#35774;&#32622;&#65288;&#21333;&#22768;&#36947;/&#22810;&#22768;&#36947;&#65289;&#21644;&#35821;&#38899;&#39046;&#22495;&#65288;&#20363;&#22914;&#23186;&#20307;&#12289;&#20250;&#35758;&#31561;&#65289;&#12290;&#25105;&#20204;&#30340;2/3&#31867;&#31995;&#32479;&#23558;&#26102;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#36866;&#24212;&#38899;&#39057;&#35774;&#32622;&#30340;&#35821;&#38899;&#34920;&#24449;&#30456;&#32467;&#21512;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#19982;&#20004;&#20010;&#19987;&#29992;&#30340;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21644;&#37325;&#21472;&#35762;&#35805;&#26816;&#27979;&#31995;&#32479;&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#36825;&#31181;&#29420;&#29305;&#30340;&#26550;&#26500;&#20063;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#27010;&#24565;&#30340;&#22270;&#24418;&#27744;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#32422;&#26463;&#22312;&#22270;&#24418;&#27744;&#21270;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13011</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#29992;&#20110;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
Maximal Independent Sets for Pooling in Graph Neural Networks. (arXiv:2307.13011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#27010;&#24565;&#30340;&#22270;&#24418;&#27744;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#32422;&#26463;&#22312;&#22270;&#24418;&#27744;&#21270;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#22270;&#20687;&#20998;&#31867;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#27744;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22270;&#24418;&#32780;&#35328;&#65292;&#24182;&#19981;&#23384;&#22312;&#28385;&#36275;&#36825;&#20123;&#24615;&#36136;&#30340;&#27744;&#21270;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#22270;&#24418;&#27744;&#21270;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#20043;&#19968;&#65306;&#22270;&#24418;&#26029;&#24320;&#25110;&#36830;&#25509;&#36807;&#24230;&#12289;&#38477;&#37319;&#26679;&#27604;&#36739;&#20302;&#12289;&#20197;&#21450;&#21024;&#38500;&#22823;&#37096;&#20998;&#22270;&#24418;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#27010;&#24565;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#22312;&#22270;&#24418;&#27744;&#21270;&#20013;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;Whisper&#27169;&#22411;&#36866;&#24212;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;wav2vec2&#27169;&#22411;&#30340;&#35843;&#20248;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20799;&#31461;&#35821;&#38899;&#19978;&#36827;&#34892;&#35843;&#20248;&#30340;Whisper&#27169;&#22411;&#33021;&#26174;&#33879;&#25552;&#39640;&#20799;&#31461;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#35843;&#20248;&#30340;wav2vec2&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.13008</link><description>&lt;p&gt;
&#23558;Whisper&#27169;&#22411;&#36866;&#24212;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adaptation of Whisper models to child speech recognition. (arXiv:2307.13008v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;Whisper&#27169;&#22411;&#36866;&#24212;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;wav2vec2&#27169;&#22411;&#30340;&#35843;&#20248;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20799;&#31461;&#35821;&#38899;&#19978;&#36827;&#34892;&#35843;&#20248;&#30340;Whisper&#27169;&#22411;&#33021;&#26174;&#33879;&#25552;&#39640;&#20799;&#31461;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#35843;&#20248;&#30340;wav2vec2&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#36716;&#24405;&#20799;&#31461;&#35821;&#38899;&#26102;&#24120;&#24120;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20934;&#30830;&#35757;&#32451;&#20799;&#31461;&#21451;&#22909;&#22411;ASR&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#30528;&#22823;&#37327;&#29992;&#20110;&#21019;&#24314;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65288;&#22914;Whisper&#65289;&#30340;&#25104;&#24180;&#20154;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#36866;&#24212;&#20799;&#31461;&#35821;&#38899;&#65292;&#20197;&#25913;&#36827;&#20799;&#31461;ASR&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;Whisper&#30340;&#20799;&#31461;&#36866;&#24212;&#24615;&#19982;&#33258;&#30417;&#30563;&#27169;&#22411;&#65288;&#22914;wav2vec2&#65289;&#30340;&#35843;&#20248;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;Whisper&#22312;&#20799;&#31461;&#35821;&#38899;&#19978;&#36827;&#34892;&#35843;&#20248;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20799;&#31461;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#19981;&#35843;&#20248;&#30340;Whisper&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22312;&#20799;&#31461;&#35821;&#38899;&#19978;&#36827;&#34892;&#35843;&#20248;&#30340;&#33258;&#30417;&#30563;wav2vec2&#27169;&#22411;&#36229;&#36234;&#20102;Whisper&#30340;&#35843;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;IteraTTA&#30340;&#30028;&#38754;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#25991;&#26412;&#25552;&#31034;&#21644;&#38899;&#39057;&#20808;&#39564;&#23545;&#29983;&#25104;&#38899;&#20048;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#29992;&#25143;&#33021;&#22815;&#36880;&#27493;&#23454;&#29616;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2307.13005</link><description>&lt;p&gt;
IteraTTA:&#19968;&#31181;&#29992;&#20110;&#22312;&#29983;&#25104;&#38899;&#20048;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#20013;&#25506;&#32034;&#25991;&#26412;&#25552;&#31034;&#21644;&#38899;&#39057;&#20808;&#39564;&#30340;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models. (arXiv:2307.13005v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;IteraTTA&#30340;&#30028;&#38754;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#25991;&#26412;&#25552;&#31034;&#21644;&#38899;&#39057;&#20808;&#39564;&#23545;&#29983;&#25104;&#38899;&#20048;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#29992;&#25143;&#33021;&#22815;&#36880;&#27493;&#23454;&#29616;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#25216;&#26415;&#20351;&#24471;&#26032;&#25163;&#29992;&#25143;&#21487;&#20197;&#33258;&#30001;&#22320;&#29983;&#25104;&#38899;&#20048;&#38899;&#39057;&#12290;&#21363;&#20351;&#20182;&#20204;&#27809;&#26377;&#20851;&#20110;&#21644;&#24358;&#36827;&#34892;&#21644;&#20048;&#22120;&#30340;&#38899;&#20048;&#30693;&#35782;&#65292;&#29992;&#25143;&#20063;&#21487;&#20197;&#23581;&#35797;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#19982;&#22270;&#20687;&#39046;&#22495;&#30456;&#27604;&#65292;&#23545;&#21487;&#33021;&#30340;&#38899;&#20048;&#38899;&#39057;&#31354;&#38388;&#30340;&#28165;&#26224;&#29702;&#35299;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#29992;&#25143;&#19981;&#33021;&#21516;&#26102;&#21548;&#21040;&#29983;&#25104;&#38899;&#39057;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#25991;&#26412;&#25552;&#31034;&#21644;&#38899;&#39057;&#20808;&#39564;&#30340;&#36845;&#20195;&#27604;&#36739;&#65292;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#19981;&#20165;&#25991;&#26412;&#25552;&#31034;&#36824;&#26377;&#38899;&#39057;&#20808;&#39564;&#23545;&#29983;&#25104;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#30028;&#38754; IteraTTA&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#32454;&#21270;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20174;&#29983;&#25104;&#30340;&#38899;&#39057;&#20013;&#36873;&#25321;&#26377;&#21033;&#30340;&#38899;&#39057;&#20808;&#39564;&#12290;&#26377;&#20102;&#36825;&#20010;&#30028;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#36880;&#27493;&#23454;&#29616;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#65292;&#36890;&#36807;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#21644;&#24341;&#20837;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#21644;+1.54%&#30340;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.12996</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. (arXiv:2307.12996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#65292;&#36890;&#36807;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#21644;&#24341;&#20837;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#21644;+1.54%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#29983;&#29289;&#21270;&#23398;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20256;&#32479;&#19978;&#19987;&#27880;&#20110;&#20998;&#23376;&#22270;&#31070;&#32463;&#34920;&#24449;&#65307;&#28982;&#32780;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#31361;&#26174;&#20102;&#25991;&#26412;&#20013;&#25152;&#32534;&#30721;&#30340;&#31185;&#23398;&#30693;&#35782;&#37327;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20004;&#31181;&#27169;&#24577;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#20174;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#22270;&#34920;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#31070;&#32463;&#22270;&#34920;&#24449;&#19982;&#20854;&#29305;&#24449;&#30340;&#25991;&#26412;&#25551;&#36848;&#34920;&#24449;&#23545;&#40784;&#21518;&#65292;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#31070;&#32463;&#30456;&#20851;&#24615;&#35780;&#20998;&#31574;&#30053;&#20197;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#26377;&#26426;&#21453;&#24212;&#21551;&#21457;&#30340;&#26032;&#39062;&#21512;&#27861;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#65292;&#24182;&#22312;&#19979;&#28216;&#30340;MoleculeNet&#23646;&#24615;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#65292;&#24182;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23376;&#22270;/&#25991;&#26412;&#23545;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;+1.54%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#24863;&#30693;&#26816;&#27979;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26816;&#27979;&#22270;&#38598;&#20869;&#24322;&#24120;&#22270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.12994</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-representations Space Separation based Graph-level Anomaly-aware Detection. (arXiv:2307.12994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#24863;&#30693;&#26816;&#27979;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26816;&#27979;&#22270;&#38598;&#20869;&#24322;&#24120;&#22270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#32467;&#26500;&#27169;&#24335;&#34987;&#24191;&#27867;&#29992;&#20110;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#24314;&#27169;&#12290;&#22914;&#20309;&#26816;&#27979;&#36825;&#20123;&#22270;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#22270;&#20449;&#24687;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#38598;&#20013;&#22312;&#22914;&#20309;&#26816;&#27979;&#22270;&#38598;&#20869;&#30340;&#24322;&#24120;&#22270;&#36825;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24322;&#24120;&#22270;&#20027;&#35201;&#34920;&#29616;&#20026;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#24322;&#24120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#35780;&#20272;&#24322;&#24120;&#22270;&#26102;&#21516;&#31561;&#23545;&#24453;&#19978;&#36848;&#20004;&#31181;&#24322;&#24120;&#24418;&#24335;&#65292;&#32780;&#20107;&#23454;&#19978;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#22270;&#25968;&#25454;&#22312;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#24322;&#24120;&#26041;&#38754;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#27491;&#24120;&#22270;&#20855;&#26377;&#24494;&#22937;&#24046;&#24322;&#30340;&#24322;&#24120;&#22270;&#24456;&#23481;&#26131;&#36867;&#36991;&#29616;&#26377;&#26041;&#27861;&#30340;&#26816;&#27979;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#24863;&#30693;&#26816;&#27979;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anoma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#30340;&#22522;&#30784;&#27169;&#22411;CONCH&#65292;&#36890;&#36807;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;-&#26631;&#39064;&#23545;&#30340;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#28041;&#21450;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12914</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#26500;&#24314;&#20197;&#35270;&#35273;&#35821;&#35328;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a Visual-Language Foundation Model for Computational Pathology. (arXiv:2307.12914v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12914
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#30340;&#22522;&#30784;&#27169;&#22411;CONCH&#65292;&#36890;&#36807;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;-&#26631;&#39064;&#23545;&#30340;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#28041;&#21450;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#20351;&#24471;&#22312;&#21508;&#31181;&#30142;&#30149;&#21644;&#24739;&#32773;&#32676;&#20307;&#20013;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#26631;&#31614;&#31232;&#32570;&#24615;&#65292;&#27169;&#22411;&#30340;&#35757;&#32451;&#24448;&#24448;&#22256;&#38590;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#30340;&#20351;&#29992;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#21644;&#30142;&#30149;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#32452;&#32455;&#30149;&#29702;&#23398;&#27169;&#22411;&#20165;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#65292;&#19982;&#20154;&#31867;&#30456;&#20114;&#25945;&#23548;&#21644;&#25512;&#29702;&#32452;&#32455;&#30149;&#29702;&#23398;&#23454;&#20307;&#30340;&#26041;&#24335;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#30340;&#22522;&#30784;&#27169;&#22411;CONCH&#65292;&#23427;&#20351;&#29992;&#22810;&#31181;&#26469;&#28304;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#26080;&#20219;&#21153;&#39044;&#35757;&#32451;&#33719;&#24471;&#20102;117&#19975;&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#12290;&#32463;&#36807;13&#31181;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#65292;CONCH &#21487;&#20197;&#36801;&#31227;&#21040;&#28041;&#21450;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GridMM&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#26684;&#35760;&#24518;&#22270;&#65292;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#32467;&#26500;&#21270;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12907</link><description>&lt;p&gt;
GridMM:&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#32593;&#26684;&#35760;&#24518;&#22270;
&lt;/p&gt;
&lt;p&gt;
GridMM: Grid Memory Map for Vision-and-Language Navigation. (arXiv:2307.12907v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GridMM&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#26684;&#35760;&#24518;&#22270;&#65292;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#32467;&#26500;&#21270;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;3D&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#36828;&#31243;&#20301;&#32622;&#12290;&#20026;&#20102;&#34920;&#31034;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#65292;VLN&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#32463;&#24120;&#24615;&#29366;&#24577;&#12289;&#25299;&#25169;&#22320;&#22270;&#25110;&#33258;&#39030;&#21521;&#19979;&#30340;&#35821;&#20041;&#22320;&#22270;&#26469;&#23454;&#29616;&#35760;&#24518;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#24182;&#21160;&#24577;&#22686;&#38271;&#30340;&#32593;&#26684;&#35760;&#24518;&#22270;&#65288;&#21363;GridMM&#65289;&#26469;&#32467;&#26500;&#21270;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#20174;&#20840;&#23616;&#35270;&#35282;&#26469;&#30475;&#65292;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#22312;&#33258;&#19978;&#32780;&#19979;&#30340;&#35270;&#22270;&#20013;&#34987;&#25237;&#24433;&#21040;&#32479;&#19968;&#30340;&#32593;&#26684;&#22320;&#22270;&#20013;&#65292;&#36825;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29615;&#22659;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#20174;&#23616;&#37096;&#35270;&#35282;&#26469;&#30475;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#30456;&#20851;&#24615;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#32593;&#26684;&#21306;&#22495;&#20013;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#22312;&#31163;&#25955;&#29615;&#22659;&#20013;&#23545;REVERIE&#12289;R2R&#12289;SOON&#25968;&#25454;&#38598;&#20197;&#21450;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340;R2R-CE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item><item><title>TF-ICON&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TF-ICON&#21487;&#20197;&#22312;&#19981;&#38656;&#39069;&#22806;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;&#26469;&#20934;&#30830;&#22320;&#21453;&#36716;&#30495;&#23454;&#22270;&#20687;&#20026;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.12493</link><description>&lt;p&gt;
TF-ICON: &#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36328;&#39046;&#22495;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition. (arXiv:2307.12493v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12493
&lt;/p&gt;
&lt;p&gt;
TF-ICON&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TF-ICON&#21487;&#20197;&#22312;&#19981;&#38656;&#39069;&#22806;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;&#26469;&#20934;&#30830;&#22320;&#21453;&#36716;&#30495;&#23454;&#22270;&#20687;&#20026;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TF-ICON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#26114;&#36149;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20248;&#21270;&#25110;&#22312;&#23450;&#21046;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#20854;&#20016;&#23500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;TF-ICON&#21487;&#20197;&#21033;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;(&#21547;&#26080;&#20449;&#24687;)&#26469;&#24110;&#21161;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#20934;&#30830;&#22320;&#23558;&#30495;&#23454;&#22270;&#20687;&#21453;&#36716;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#21512;&#25104;&#25552;&#20379;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-ICON&#22312;&#19981;&#21516;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20043;&#38388;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experim
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11768</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39564;&#35777;&#20854;&#34892;&#20026;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#35201;&#27714;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20197;&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#24335;&#22806;&#21270;&#20854;&#25512;&#29702;&#36807;&#31243;&#65288;&#24605;&#32500;&#38142;&#65307;CoT&#65289;&#12290;&#25512;&#29702;&#36807;&#31243;&#21487;&#20197;&#35753;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#25152;&#38472;&#36848;&#30340;&#25512;&#29702;&#33021;&#22815;&#24544;&#23454;&#22320;&#21453;&#26144;&#27169;&#22411;&#30340;&#23454;&#38469;&#25512;&#29702;&#65292;&#32780;&#36825;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#25552;&#39640;CoT&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#26469;&#29983;&#25104;&#25512;&#29702;&#12290;&#22522;&#20110;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#25509;&#36817;CoT&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#25152;&#38472;&#36848;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#22312;&#21333;&#29420;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#31572;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#22823;&#22823;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#22312;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20559;&#35265;&#21644;&#20260;&#23475;&#26102;&#25972;&#21512;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#37239;&#20799;&#31038;&#21306;&#20026;&#35270;&#35282;&#37325;&#26032;&#35774;&#35745;&#20559;&#35265;&#22870;&#37329;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10223</link><description>&lt;p&gt;
&#21463;&#22870;&#36175;&#32422;&#26463;&#65306;&#20849;&#21516;&#26500;&#24314;&#35780;&#20272;&#37239;&#20799;&#20154;&#24037;&#26234;&#33021;&#20260;&#23475;&#30340;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. (arXiv:2307.10223v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#22312;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20559;&#35265;&#21644;&#20260;&#23475;&#26102;&#25972;&#21512;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#37239;&#20799;&#31038;&#21306;&#20026;&#35270;&#35282;&#37325;&#26032;&#35774;&#35745;&#20559;&#35265;&#22870;&#37329;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#35780;&#20272;&#22522;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#25991;&#26723;&#24050;&#25104;&#20026;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20559;&#35265;&#21644;&#20260;&#23475;&#30340;&#26680;&#24515;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23457;&#35745;&#36807;&#31243;&#22240;&#26410;&#25972;&#21512;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#30693;&#35782;&#24182;&#32771;&#34385;&#23457;&#35745;&#21592;&#19982;&#31038;&#21306;&#20043;&#38388;&#30340;&#26435;&#21147;&#21160;&#24577;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#19982;&#21463;&#24433;&#21709;&#31038;&#21306;&#35782;&#21035;&#21644;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20260;&#23475;&#30340;&#20559;&#35265;&#35780;&#20272;&#26041;&#24335;&#65288;&#20363;&#22914;&#20559;&#35265;&#22870;&#37329;&#65289;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20851;&#20110;&#36793;&#32536;&#21270;&#31038;&#21306;&#23545;&#27492;&#31867;&#23457;&#35745;&#36807;&#31243;&#30340;&#26399;&#26395;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21521;&#37239;&#20799;&#31038;&#21306;&#24449;&#27714;&#20182;&#20204;&#23545;&#23457;&#35745;&#36807;&#31243;&#30340;&#31435;&#22330;&#21644;&#26399;&#26395;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#20010;&#21442;&#19982;&#24335;&#30740;&#35752;&#20250;&#65292;&#20174;&#37239;&#20799;&#30340;&#35282;&#24230;&#23545;&#20559;&#35265;&#22870;&#37329;&#36827;&#34892;&#25209;&#21028;&#24615;&#30340;&#37325;&#26032;&#35774;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#26377;&#31354;&#38388;&#26102;&#65292;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#33539;&#22260;&#36828;&#36828;&#36229;&#20986;&#20102;&#20559;&#35265;&#22870;&#37329;&#25152;&#33021;&#25552;&#20379;&#30340;&#33539;&#22260;&#65292;&#21442;&#19982;&#32773; que
&lt;/p&gt;
&lt;p&gt;
Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants que
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30828;&#26408;&#31181;&#31867;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20154;&#31867;&#19987;&#23478;&#31867;&#20284;&#65292;&#26410;&#26469;&#23558;&#26377;&#21161;&#20110;&#20445;&#25252;&#26862;&#26519;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.09588</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#21270;&#36827;&#34892;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#26408;&#26448;&#31181;&#31867;&#26816;&#27979;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning. (arXiv:2307.09588v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30828;&#26408;&#31181;&#31867;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20154;&#31867;&#19987;&#23478;&#31867;&#20284;&#65292;&#26410;&#26469;&#23558;&#26377;&#21161;&#20110;&#20445;&#25252;&#26862;&#26519;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#29983;&#25104;&#22823;&#37327;&#30340;&#30772;&#35299;&#26408;&#26448;&#21442;&#32771;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#29983;&#25104;&#20102;&#20061;&#20010;&#30828;&#26408;&#31181;&#23646;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65292;&#39318;&#27425;&#33258;&#21160;&#21270;&#35782;&#21035;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30828;&#26408;&#31181;&#31867;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#28789;&#27963;&#30340;&#31649;&#36947;&#65292;&#20415;&#20110;&#23545;&#23548;&#31649;&#20803;&#32032;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#12290;&#23558;&#26469;&#65292;&#36825;&#23558;&#25913;&#21892;&#23545;&#20840;&#29699;&#26408;&#36136;&#32420;&#32500;&#20135;&#21697;&#27969;&#30340;&#25511;&#21046;&#65292;&#20197;&#20445;&#25252;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have developed a methodology for the systematic generation of a large image dataset of macerated wood references, which we used to generate image data for nine hardwood genera. This is the basis for a substantial approach to automate, for the first time, the identification of hardwood species in microscopic images of fibrous materials by deep learning. Our methodology includes a flexible pipeline for easy annotation of vessel elements. We compare the performance of different neural network architectures and hyperparameters. Our proposed method performs similarly well to human experts. In the future, this will improve controls on global wood fiber product flows to protect forests.
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05747</link><description>&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#30456;&#32467;&#21512;&#65306;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05747
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#33719;&#21462;&#26032;&#25216;&#33021;&#25110;&#30693;&#35782;&#26102;&#65292;&#36890;&#36807;&#35838;&#31243;&#36827;&#34892;&#23398;&#20064;&#21644;&#22797;&#20064;&#12290;&#36825;&#31181;&#20154;&#31867;&#23398;&#20064;&#34892;&#20026;&#21551;&#21457;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#30693;&#35782;&#20445;&#30041;&#21644;&#20419;&#36827;&#23398;&#20064;&#36716;&#31227;&#12290;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#22238;&#25918;&#26041;&#27861;&#28041;&#21450;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#38543;&#26426;&#36873;&#25321;&#21644;&#25490;&#24207;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#19977;&#20010;&#20855;&#20307;&#26041;&#38754;&#65306;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#65292;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#65292;&#20197;&#21450;&#36873;&#25321;&#23454;&#20363;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#35838;&#31243;&#35774;&#35745;&#30340;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
&lt;/p&gt;</description></item><item><title>&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10891</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10891
&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#20934;&#30830;&#30340;&#36127;&#36733;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#22312;&#26412;&#22320;&#24179;&#34913;&#20379;&#38656;&#65292;&#24182;&#38450;&#27490;&#30005;&#32593;&#25925;&#38556;&#12290;&#23613;&#31649;&#34987;&#30417;&#27979;&#30340;&#23458;&#25143;&#25968;&#37327;&#23558;&#38543;&#30528;&#19981;&#26029;&#25512;&#36827;&#30340;&#26234;&#33021;&#30005;&#34920;&#23433;&#35013;&#32780;&#22686;&#21152;&#65292;&#20294;&#27599;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#37327;&#22987;&#32456;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#25442;&#22120;&#36127;&#36733;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#21463;&#30410;&#20110;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#21363;&#22312;&#22810;&#20010;&#23458;&#25143;&#30340;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#20840;&#23616;&#30340;&#21333;&#21464;&#37327;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#20004;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#20248;&#20110;&#30456;&#20851;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;&#20854;&#20182;&#20004;&#31181;&#31574;&#30053;&#30456;&#27604;&#65292;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#22312;&#20174;&#26410;&#26469;&#19968;&#22825;&#21040;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;LSTM&#27169;&#22411;&#30340;&#27604;&#36739;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#20998;&#21106;&#21644;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#65292;&#20174;&#33050;&#25163;&#26550;&#36974;&#25377;&#20013;&#24674;&#22797;&#24314;&#31569;&#22330;&#26223;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33050;&#25163;&#26550;&#20998;&#21106;&#21644;&#22330;&#26223;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.18810</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20174;&#33050;&#25163;&#26550;&#36974;&#25377;&#20013;&#36827;&#34892;&#22330;&#26223;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Scene restoration from scaffold occlusion using deep learning-based methods. (arXiv:2305.18810v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#20998;&#21106;&#21644;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#65292;&#20174;&#33050;&#25163;&#26550;&#36974;&#25377;&#20013;&#24674;&#22797;&#24314;&#31569;&#22330;&#26223;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33050;&#25163;&#26550;&#20998;&#21106;&#21644;&#22330;&#26223;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20013;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#24212;&#29992;&#30340;&#36974;&#25377;&#38382;&#39064;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#30001;&#24191;&#35206;&#30422;&#12289;&#20132;&#21449;&#21644;&#19981;&#21487;&#31227;&#21160;&#30340;&#33050;&#25163;&#26550;&#24341;&#36215;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;&#30452;&#35266;&#22320;&#65292;&#21435;&#38500;&#33050;&#25163;&#26550;&#24182;&#24674;&#22797;&#34987;&#36974;&#25377;&#30340;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#20026;CV&#31995;&#32479;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#22330;&#26223;&#35270;&#22270;&#65292;&#20174;&#32780;&#24110;&#21161;&#23427;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24314;&#31569;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#32467;&#21512;&#20687;&#32032;&#32423;&#20998;&#21106;&#21644;&#22270;&#20687;&#20462;&#22797;&#65292;&#29992;&#20110;&#20174;&#33050;&#25163;&#26550;&#36974;&#25377;&#20013;&#24674;&#22797;&#24314;&#31569;&#22330;&#26223;&#12290;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20302;&#25104;&#26412;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#30701;&#32570;&#30340;&#22256;&#22659;&#12290;&#23545;&#21512;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33050;&#25163;&#26550;&#20998;&#21106;&#26041;&#38754;&#30340;&#24179;&#22343;&#20132;&#21449;&#32852;&#21512;&#29575;&#65288;MIoU&#65289;&#36798;&#21040;92&#65285;&#65292;&#22312;&#20174;&#33050;&#25163;&#26550;&#36974;&#25377;&#20013;&#24674;&#22797;&#22330;&#26223;&#26041;&#38754;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65288;SSIM&#65289;&#36798;&#21040;82&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The occlusion issues of computer vision (CV) applications in construction have attracted significant attention, especially those caused by the wide-coverage, crisscrossed, and immovable scaffold. Intuitively, removing the scaffold and restoring the occluded visual information can provide CV agents with clearer site views and thus help them better understand the construction scenes. Therefore, this study proposes a novel two-step method combining pixel-level segmentation and image inpainting for restoring construction scenes from scaffold occlusion. A low-cost data synthesis method based only on unlabeled data is developed to address the shortage dilemma of labeled data. Experiments on the synthesized test data show that the proposed method achieves performances of 92% mean intersection over union (MIoU) for scaffold segmentation and over 82% structural similarity (SSIM) for scene restoration from scaffold occlusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#26032;&#30340;&#26410;&#27880;&#24847;&#21040;&#30340;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2305.13127</link><description>&lt;p&gt;
&#20160;&#20040;&#30151;&#29366;&#20197;&#21450;&#25345;&#32493;&#22810;&#20037;&#65311;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media. (arXiv:2305.13127v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#26032;&#30340;&#26410;&#27880;&#24847;&#21040;&#30340;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#26368;&#24120;&#35265;&#21644;&#20005;&#37325;&#30340;&#31934;&#31070;&#30142;&#30149;&#65292;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#23545;&#20110;&#26089;&#26399;&#24178;&#39044;&#20197;&#20943;&#36731;&#36825;&#20123;&#21518;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#26412;&#36136;&#19978;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#25233;&#37057;&#30151;&#26816;&#27979;&#30740;&#31350;&#35797;&#22270;&#22522;&#20110;&#37325;&#35201;&#24615;&#20998;&#25968;&#25110;&#20851;&#27880;&#26435;&#37325;&#35299;&#37322;&#20915;&#31574;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#19982;&#20020;&#24202;&#25233;&#37057;&#30151;&#35786;&#26029;&#26631;&#20934;&#19981;&#19968;&#33268;&#65292;&#21518;&#32773;&#22522;&#20110;&#25233;&#37057;&#30151;&#29366;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36981;&#24490;&#35745;&#31639;&#35774;&#35745;&#31185;&#23398;&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21407;&#22411;&#32593;&#32476;(MSTPNet)&#12290;MSTPNet&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#12290;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;MSTPNet&#22312;F1&#20998;&#25968;0.851&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#12290;&#36825;&#20010;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#26410;&#22312;&#35843;&#26597;&#26041;&#27861;&#20013;&#27880;&#24847;&#21040;&#30340;&#26032;&#30151;&#29366;&#65292;&#20363;&#22914;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is the most prevalent and serious mental illness, which induces grave financial and societal ramifications. Depression detection is key for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few depression detection studies attempt to explain the decision based on the importance score or attention weights, these explanations misalign with the clinical depression diagnosis criterion that is based on depressive symptoms. To fill this gap, we follow the computational design science paradigm to develop a novel Multi-Scale Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and interprets depressive symptoms as well as how long they last. Extensive empirical analyses using a large-scale dataset show that MSTPNet outperforms state-of-the-art depression detection methods with an F1-score of 0.851. This result also reveals new symptoms that are unnoted in the survey approach, such as shari
&lt;/p&gt;</description></item><item><title>LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09599</link><description>&lt;p&gt;
LEA: &#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36229;&#36234;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
LEA: Beyond Evolutionary Algorithms via Learned Optimization Strategy. (arXiv:2304.09599v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09599
&lt;/p&gt;
&lt;p&gt;
LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#25104;&#20026;&#26114;&#36149;&#40657;&#30418;&#20248;&#21270;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#40657;&#30418;&#20248;&#21270;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#20851;&#38190;&#30340;&#38556;&#30861;&#26159;&#25214;&#20986;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#34920;&#24449;&#19981;&#36275;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20302;&#25928;&#20132;&#20114;&#32780;&#26174;&#24471;&#34180;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65288;LEA&#65289;&#65292;&#20197;&#23454;&#29616;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#21040;&#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36716;&#25442;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#21442;&#25968;&#21644;&#26356;&#26032;&#35268;&#21017;&#12290;&#19982;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#19981;&#21516;&#65292;LEA&#23545;&#30446;&#26631;&#20219;&#21153;&#20855;&#26377;&#39640;&#36866;&#24212;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LEA&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EAs) have emerged as a powerful framework for expensive black-box optimization. Obtaining better solutions with less computational cost is essential and challenging for black-box optimization. The most critical obstacle is figuring out how to effectively use the target task information to form an efficient optimization strategy. However, current methods are weak due to the poor representation of the optimization strategy and the inefficient interaction between the optimization strategy and the target task. To overcome the above limitations, we design a learned EA (LEA) to realize the move from hand-designed optimization strategies to learned optimization strategies, including not only hyperparameters but also update rules. Unlike traditional EAs, LEA has high adaptability to the target task and can obtain better solutions with less computational cost. LEA is also able to effectively utilize the low-fidelity information of the target task to form an efficient op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#19988;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#26368;&#20027;&#23548;&#30340;&#39057;&#29575;&#36827;&#34892;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02034</link><description>&lt;p&gt;
&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#26368;&#20027;&#23548;&#30340;&#39057;&#29575;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies. (arXiv:2303.02034v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#19988;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#26368;&#20027;&#23548;&#30340;&#39057;&#29575;&#36827;&#34892;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29702;&#35770;&#30340;&#19968;&#20010;&#31361;&#30772;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#28436;&#21464;&#26159;&#30001;&#25968;&#25454;&#38598;&#32467;&#26500;&#21644;&#21367;&#31215;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25152;&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#38750;&#32447;&#24615;&#12289;&#26377;&#24207;&#12289;&#38454;&#27573;&#24615;&#30340;&#36716;&#21464;&#26469;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#19988;&#21457;&#29616;&#21457;&#29616;&#30340;&#36895;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#21644;&#21367;&#31215;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#26159;&#25105;&#20204;&#25152;&#31216;&#30340;&#8220;&#20027;&#23548;&#39057;&#29575;&#20559;&#24046;&#8221;&#30340;&#26680;&#24515;&#65292;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#32467;&#26500;&#37096;&#20998;&#30340;&#20027;&#23548;&#39057;&#29575;&#26469;&#36827;&#34892;&#36825;&#20123;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We here present a stepping stone towards a deeper understanding of convolutional neural networks (CNNs) in the form of a theory of learning in linear CNNs. Through analyzing the gradient descent equations, we discover that the evolution of the network during training is determined by the interplay between the dataset structure and the convolutional network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, ordered, stage-like transitions, and that the speed of discovery changes depending on the relationship between the dataset and the convolutional network structure. Moreover, we find that this interplay lies at the heart of what we call the ``dominant frequency bias'', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset. We furthermore provide experiments that show how our theory relates to deep, non-linear CNNs used in practice. Our findings shed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;Transformer&#24341;&#23548;&#30340;&#25193;&#25955;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#36924;&#30495;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2302.13434</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;Transformer&#24341;&#23548;&#30340;&#25193;&#25955;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition. (arXiv:2302.13434v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;Transformer&#24341;&#23548;&#30340;&#25193;&#25955;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#36924;&#30495;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#25104;&#20026;&#20102;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#22240;&#20026;&#20154;&#20307;&#39592;&#39612;&#30340;&#32039;&#20945;&#34920;&#36798;&#32473;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#27880;&#20837;&#20102;&#26032;&#27963;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#22987;&#27880;&#24847;&#21040;&#20351;&#29992;RGB&#25110;&#20854;&#20182;&#20256;&#24863;&#22120;&#26469;&#36890;&#36807;&#25552;&#21462;&#39592;&#39612;&#20449;&#24687;&#20998;&#26512;&#20154;&#20307;&#21160;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24635;&#26159;&#38656;&#35201;&#39640;&#36136;&#37327;&#21644;&#20805;&#36275;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#38656;&#35201;&#39640;&#26114;&#30340;&#36153;&#29992;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#21160;&#20316;&#12290;&#20026;&#20102;&#33719;&#24471;&#33258;&#28982;&#21644;&#36924;&#30495;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#19968;&#31995;&#21015;&#21512;&#25104;&#21160;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action seque
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#24694;&#24847;&#21442;&#25968;&#24178;&#25200;&#20840;&#23616;&#27169;&#22411;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#36896;&#25104;&#26174;&#33879;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#38450;&#27490;&#25915;&#20987;&#25152;&#38656;&#30340;&#20445;&#25252;&#38544;&#31169;&#22122;&#22768;&#20250;&#20005;&#37325;&#25439;&#23475;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.12685</link><description>&lt;p&gt;
&#20197;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Active Membership Inference Attack under Local Differential Privacy in Federated Learning. (arXiv:2302.12685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12685
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#24694;&#24847;&#21442;&#25968;&#24178;&#25200;&#20840;&#23616;&#27169;&#22411;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#36896;&#25104;&#26174;&#33879;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#38450;&#27490;&#25915;&#20987;&#25152;&#38656;&#30340;&#20445;&#25252;&#38544;&#31169;&#22122;&#22768;&#20250;&#20005;&#37325;&#25439;&#23475;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26368;&#21021;&#34987;&#35270;&#20026;&#22312;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#35843;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#19981;&#35802;&#23454;&#26381;&#21153;&#22120;&#22312;FL&#20013;&#36827;&#34892;&#30340;&#26032;&#22411;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#65288;AMI&#65289;&#25915;&#20987;&#12290;&#22312;AMI&#25915;&#20987;&#20013;&#65292;&#26381;&#21153;&#22120;&#21046;&#36896;&#24182;&#23884;&#20837;&#24694;&#24847;&#21442;&#25968;&#21040;&#20840;&#23616;&#27169;&#22411;&#20013;&#65292;&#20197;&#26377;&#25928;&#25512;&#26029;&#30446;&#26631;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36890;&#36807;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#65292;AMI&#25915;&#20987;&#22312;&#20005;&#26684;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#20445;&#25252;&#19979;&#21487;&#20197;&#23454;&#29616;&#26497;&#39640;&#30340;&#25104;&#21151;&#29575;&#65292;&#20174;&#32780;&#20351;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#38754;&#20020;&#26174;&#33879;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#38450;&#27490;&#25105;&#20204;&#30340;&#25915;&#20987;&#32780;&#28155;&#21152;&#36275;&#22815;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#22122;&#22768;&#20250;&#26174;&#33879;&#25439;&#23475;FL&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client's private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients' training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL's model utility.
&lt;/p&gt;</description></item><item><title>&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.11007</link><description>&lt;p&gt;
&#27969;&#34892;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11007
&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#37319;&#29992;&#20102;&#20998;&#25968;&#24494;&#31215;&#20998;&#30340;Mittag-Leffler&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#32039;&#20945;&#30340;&#21151;&#33021;&#24418;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#20943;&#36731;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25152;&#25552;&#20986;&#30340;&#38376;&#25511;&#34920;&#31034;&#25193;&#23637;&#20102;&#22266;&#23450;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#33258;&#36866;&#24212;&#23545;&#24212;&#29289;&#65292;&#20854;&#24418;&#29366;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23548;&#25968;&#20063;&#21487;&#20197;&#29992;Mittag-Leffler&#20989;&#25968;&#34920;&#31034;&#65292;&#22240;&#27492;&#23427;&#26159;&#26799;&#24230;&#19979;&#38477;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#21512;&#36866;&#20505;&#36873;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#32479;&#19968;&#30340;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#34920;&#31034;&#20026;&#21508;&#31181;&#20869;&#32622;&#23454;&#29616;&#30340;&#32463;&#27982;&#30340;&#21644;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02119</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#25112;&#23454;&#29616;&#22810;&#26679;&#21270;&#35825;&#23548;&#30340;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29615;&#22659;&#20998;&#24067;&#35774;&#35745;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20986;&#35757;&#32451;&#26377;&#25928;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#30340;&#21069;&#26223;&#12290;&#23427;&#30340;&#25104;&#21151;&#37096;&#20998;&#22312;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#35838;&#31243;&#23398;&#20064;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#32463;&#24120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21457;&#29616;&#26377;&#25928;&#32423;&#21035;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#39640;&#25104;&#26412;&#20132;&#20114;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26694;&#26550;&#20013;&#24341;&#20837;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#32473;&#23450;&#32423;&#21035;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#20004;&#20010;&#32423;&#21035;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#65292;&#20351;&#24471;&#29615;&#22659;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
&lt;/p&gt;</description></item><item><title>FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07160</link><description>&lt;p&gt;
FedTracker&#65306;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07160
&lt;/p&gt;
&lt;p&gt;
FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#38656;&#35201;&#23558;&#27169;&#22411;&#26292;&#38706;&#32473;&#21508;&#31181;&#21442;&#19982;&#32773;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24694;&#24847;&#23458;&#25143;&#31471;&#26410;&#32463;&#25480;&#26435;&#22320;&#20998;&#21457;&#25110;&#36716;&#21806;&#27169;&#22411;&#65292;&#20174;&#32780;&#25439;&#23475;FL&#22242;&#38431;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#20102;&#38459;&#27490;&#36825;&#31181;&#19981;&#24403;&#34892;&#20026;&#65292;&#24314;&#31435;&#19968;&#31181;&#39564;&#35777;&#27169;&#22411;&#25152;&#26377;&#26435;&#24182;&#36861;&#28335;&#27844;&#38706;&#32773;&#30340;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedTracker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;FL&#27169;&#22411;&#20445;&#25252;&#26694;&#26550;&#12290;FedTracker&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#21253;&#25324;&#20840;&#23616;&#27700;&#21360;&#26426;&#21046;&#21644;&#26412;&#22320;&#25351;&#32441;&#26426;&#21046;&#12290;&#21069;&#32773;&#29992;&#20110;&#39564;&#35777;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#65292;&#32780;&#21518;&#32773;&#29992;&#20110;&#35782;&#21035;&#35813;&#27169;&#22411;&#26469;&#33258;&#21738;&#20010;&#23458;&#25143;&#31471;&#12290;FedTracker&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21407;&#21017;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20462;&#35746;Transformer&#65288;RiT&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#25463;&#24452;&#23398;&#20064;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#20197;&#20415;&#26356;&#26041;&#20415;&#22320;&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;RiT&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#28165;&#26224;&#32467;&#26500;&#30340;&#20462;&#35746;&#24341;&#25806;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#21162;&#21147;&#21644;&#29992;&#25143;&#20114;&#21160;&#65292;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#22312;&#36947;&#24503;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;RiT&#22312;&#27169;&#22411;&#20462;&#35746;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10332</link><description>&lt;p&gt;
&#20462;&#35746;Transformer&#65306;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#25913;&#21464;&#20854;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Revision Transformers: Instructing Language Models to Change their Values. (arXiv:2210.10332v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20462;&#35746;Transformer&#65288;RiT&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#25463;&#24452;&#23398;&#20064;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#20197;&#20415;&#26356;&#26041;&#20415;&#22320;&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;RiT&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#28165;&#26224;&#32467;&#26500;&#30340;&#20462;&#35746;&#24341;&#25806;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#21162;&#21147;&#21644;&#29992;&#25143;&#20114;&#21160;&#65292;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#22312;&#36947;&#24503;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;RiT&#22312;&#27169;&#22411;&#20462;&#35746;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#23481;&#26131;&#20986;&#29616;&#25463;&#24452;&#23398;&#20064;&#21644;&#20559;&#35265;&#12290;&#36890;&#36807;&#21442;&#25968;&#35843;&#25972;&#26469;&#35299;&#20915;&#36825;&#31867;&#19981;&#27491;&#30830;&#30340;&#27169;&#22411;&#34892;&#20026;&#38750;&#24120;&#26114;&#36149;&#12290;&#23545;&#20110;&#26356;&#26032;&#25991;&#21270;&#25110;&#20010;&#20154;&#20043;&#38388;&#21464;&#21270;&#30340;&#36947;&#24503;&#20215;&#20540;&#31561;&#21160;&#24577;&#27010;&#24565;&#23588;&#20854;&#26840;&#25163;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#23558;&#25152;&#26377;&#20449;&#24687;&#23384;&#20648;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#24403;&#21069;&#24120;&#35265;&#20570;&#27861;&#25552;&#20986;&#36136;&#30097;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#35746;Transformer&#65288;RiT&#65289;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#36731;&#26494;&#26356;&#26032;&#12290;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#28165;&#26224;&#32467;&#26500;&#30340;&#20462;&#35746;&#24341;&#25806;&#30340;&#29305;&#23450;&#32452;&#21512;&#20351;&#24471;&#22312;&#23569;&#37327;&#30340;&#21162;&#21147;&#21644;&#29992;&#25143;&#20114;&#21160;&#30340;&#24110;&#21161;&#19979;&#26356;&#26032;&#27169;&#22411;&#30340;&#30693;&#35782;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36947;&#24503;&#25968;&#25454;&#38598;&#19978;&#31034;&#33539;&#20102;RiT&#65292;&#24182;&#27169;&#25311;&#20102;&#29992;&#25143;&#21453;&#39304;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#20462;&#35746;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current transformer language models (LM) are large-scale models with billions of parameters. They have been shown to provide high performances on a variety of tasks but are also prone to shortcut learning and bias. Addressing such incorrect model behavior via parameter adjustments is very costly. This is particularly problematic for updating dynamic concepts, such as moral values, which vary culturally or interpersonally. In this work, we question the current common practice of storing all information in the model parameters and propose the Revision Transformer (RiT) to facilitate easy model updating. The specific combination of a large-scale pre-trained LM that inherently but also diffusely encodes world knowledge with a clear-structured revision engine makes it possible to update the model's knowledge with little effort and the help of user interaction. We exemplify RiT on a moral dataset and simulate user feedback demonstrating strong performance in model revision even with small da
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06433</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#20135;&#29983;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35266;&#23519;&#23545;&#35937;&#21644;&#22330;&#26223;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#26102;&#38388;&#29702;&#35299;&#30340;&#29305;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#38745;&#24577;&#22270;&#20687;&#39044;&#35757;&#32451;&#20173;&#28982;&#26159;&#23398;&#20064;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#19981;&#21305;&#37197;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#19988;&#38382;&#26159;&#21542;&#35270;&#39057;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65306;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#12289;&#23545;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31579;&#36873;&#35270;&#39057;&#30340;&#26032;&#39062;&#31243;&#24207;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#27604;&#24615;&#26694;&#26550;&#65292;&#20174;&#20854;&#20013;&#30340;&#22797;&#26434;&#36716;&#25442;&#20013;&#23398;&#20064;&#12290;&#36825;&#31181;&#20174;&#35270;&#39057;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#31616;&#21333;&#33539;&#24335;&#34987;&#31216;&#20026;VITO&#65292;&#23427;&#20135;&#29983;&#30340;&#19968;&#33324;&#34920;&#31034;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#19978;&#36828;&#36828;&#20248;&#20110;&#20808;&#21069;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;VITO&#34920;&#31034;&#23545;&#33258;&#28982;&#21644;&#21512;&#25104;&#24418;&#21464;&#30340;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26368;&#23567;&#35748;&#30693;&#26550;&#26500;&#30340;&#20154;&#24037;&#20195;&#29702;&#65292;&#23637;&#31034;&#20986;&#32047;&#31215;&#25991;&#21270;&#21487;&#20197;&#22312;&#31038;&#20132;&#21644;&#35760;&#24518;&#24341;&#23548;&#19979;&#33258;&#21457;&#20986;&#29616;&#65292;&#19981;&#20381;&#36182;&#20110;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#23548;&#33322;&#32773;&#21487;&#20197;&#36890;&#36807;&#24050;&#24314;&#31435;&#30340;&#36335;&#24452;&#20256;&#25480;&#32473;&#24188;&#31258;&#20010;&#20307;&#65292;&#32780;&#36825;&#26679;&#20570;&#23545;&#20110;&#23548;&#33322;&#32773;&#26469;&#35828;&#20063;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.06281</link><description>&lt;p&gt;
&#20154;&#24037;&#23548;&#33322;&#32773;&#33258;&#21457;&#22320;&#24418;&#25104;&#20102;&#20855;&#26377;&#31038;&#20132;&#21644;&#35760;&#24518;&#24341;&#23548;&#30340;&#32047;&#31215;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cumulative culture spontaneously emerges in artificial navigators who are social and memory-guided. (arXiv:2206.06281v3 [q-bio.PE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26368;&#23567;&#35748;&#30693;&#26550;&#26500;&#30340;&#20154;&#24037;&#20195;&#29702;&#65292;&#23637;&#31034;&#20986;&#32047;&#31215;&#25991;&#21270;&#21487;&#20197;&#22312;&#31038;&#20132;&#21644;&#35760;&#24518;&#24341;&#23548;&#19979;&#33258;&#21457;&#20986;&#29616;&#65292;&#19981;&#20381;&#36182;&#20110;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#23548;&#33322;&#32773;&#21487;&#20197;&#36890;&#36807;&#24050;&#24314;&#31435;&#30340;&#36335;&#24452;&#20256;&#25480;&#32473;&#24188;&#31258;&#20010;&#20307;&#65292;&#32780;&#36825;&#26679;&#20570;&#23545;&#20110;&#23548;&#33322;&#32773;&#26469;&#35828;&#20063;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32047;&#31215;&#25991;&#21270;&#36827;&#21270;&#21457;&#29983;&#22312;&#36890;&#36807;&#31038;&#20132;&#23398;&#20064;&#23558;&#36866;&#24212;&#24615;&#21019;&#26032;&#20256;&#36882;&#32473;&#36830;&#32493;&#30340;&#21518;&#20195;&#26102;&#12290;&#36825;&#20010;&#36807;&#31243;&#22609;&#36896;&#20102;&#20154;&#31867;&#30340;&#25216;&#26415;&#21019;&#26032;&#65292;&#20294;&#20063;&#21457;&#29983;&#22312;&#38750;&#20154;&#31867;&#29289;&#31181;&#20013;&#12290;&#23613;&#31649;&#20256;&#32479;&#19978;&#35748;&#20026;&#32047;&#31215;&#25991;&#21270;&#20381;&#36182;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#31038;&#20132;&#20256;&#36755;&#21644;&#20808;&#36827;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20294;&#25105;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#31995;&#32479;&#36275;&#22815;&#12290;&#32047;&#31215;&#25991;&#21270;&#22312;&#20855;&#26377;&#30446;&#26631;&#23548;&#21521;&#12289;&#31038;&#20132;&#25509;&#36817;&#24615;&#21644;&#36335;&#24452;&#35760;&#24518;&#30340;&#26368;&#23567;&#35748;&#30693;&#26550;&#26500;&#30340;&#20154;&#24037;&#20195;&#29702;&#20013;&#33258;&#21457;&#20986;&#29616;&#12290;&#22312;&#27599;&#19968;&#20195;&#20013;&#65292;&#32463;&#39564;&#26377;&#38480;&#30340;&#20010;&#20307;&#36890;&#36807;&#19982;&#32463;&#39564;&#23548;&#33322;&#32773;&#37197;&#23545;&#20174;&#32780;&#21463;&#30410;&#65292;&#22240;&#20026;&#20182;&#20204;&#21487;&#20197;&#36981;&#24490;&#20808;&#21069;&#24314;&#31435;&#30340;&#36335;&#24452;&#12290;&#20851;&#38190;&#26159;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#23548;&#33322;&#32773;&#20063;&#36890;&#36807;&#22238;&#24402;&#21040;&#30446;&#26631;&#21463;&#30410;&#20110;&#26377;&#32463;&#39564;&#30340;&#23548;&#33322;&#32773;&#30340;&#23384;&#22312;&#12290;&#24403;&#32463;&#39564;&#20016;&#23500;&#30340;&#20195;&#29702;&#20154;&#36981;&#24490;&#20182;&#20204;&#35760;&#24518;&#20013;&#30340;&#36335;&#24452;&#26102;&#65292;&#20182;&#20204;&#27809;&#26377;&#36335;&#24452;&#35760;&#24518;&#30340;&#24188;&#31258;&#21516;&#20276;&#26356;&#26377;&#21487;&#33021;&#26397;&#21521;&#30446;&#26631;&#32780;&#19981;&#26159;&#36828;&#31163;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cumulative cultural evolution occurs when adaptive innovations are passed down to consecutive generations through social learning. This process has shaped human technological innovation, but also occurs in non-human species. While it is traditionally argued that cumulative culture relies on high-fidelity social transmission and advanced cognitive skills, here I show that a much simpler system suffices. Cumulative culture spontaneously emerged in artificial agents who navigate with a minimal cognitive architecture of goal-direction, social proximity, and route memory. Within each generation, naive individuals benefitted from being paired with experienced navigators because they could follow previously established routes. Crucially, experienced navigators also benefitted from the presence of naive individuals through regression to the goal. As experienced agents followed their memorised path, their naive counterparts (unhindered by route memory) were more likely to err towards than away 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09736</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24352;&#37327;&#21644;&#30697;&#38453;&#20302;&#31209;&#20540;&#20989;&#25968;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning. (arXiv:2201.09736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#65288;VF&#65289;&#30340;&#36817;&#20284;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;VF&#20272;&#35745;&#22312;&#32500;&#24230;&#28798;&#38590;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20154;&#20204;&#37319;&#29992;&#20102;&#31616;&#27905;&#30340;&#21442;&#25968;&#27169;&#22411;&#26469;&#36817;&#20284;VF&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#19978;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#31616;&#27905;&#30340;&#38750;&#21442;&#25968;&#8221;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#20197;&#22312;&#32447;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#20272;&#35745;VF&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;VF&#24448;&#24448;&#26159;&#22810;&#32500;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#24352;&#37327;&#65288;&#22810;&#32500;&#25968;&#32452;&#65289;&#34920;&#31034;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;VF&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;PARAFAC&#20998;&#35299;&#26469;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-function (VF) approximation is a central problem in Reinforcement Learning (RL). Classical non-parametric VF estimation suffers from the curse of dimensionality. As a result, parsimonious parametric models have been adopted to approximate VFs in high-dimensional spaces, with most efforts being focused on linear and neural-network-based approaches. Differently, this paper puts forth a a \emph{parsimonious non-parametric} approach, where we use \emph{stochastic low-rank algorithms} to estimate the VF matrix in an online and model-free fashion. Furthermore, as VFs tend to be multi-dimensional, we propose replacing the classical VF matrix representation with a tensor (multi-way array) representation and, then, use the PARAFAC decomposition to design an online model-free tensor low-rank algorithm. Different versions of the algorithms are proposed, their complexity is analyzed, and their performance is assessed numerically using standardized RL environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#39537;&#21160;&#30340;&#25552;&#26696;&#29983;&#25104;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#25991;&#23383;&#20154;&#29289;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#34892;&#20154;&#26816;&#27979;&#12289;&#36523;&#20221;&#35782;&#21035;&#21644;&#35270;&#35273;-&#35821;&#20041;&#29305;&#24449;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#35821;&#20041;&#29305;&#24449;&#25351;&#23548;&#21306;&#22495;&#24314;&#35758;&#32593;&#32476;&#20851;&#27880;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#26696;&#12290;&#20351;&#29992;&#36328;&#23610;&#24230;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;</title><link>http://arxiv.org/abs/2109.12965</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#39537;&#21160;&#30340;&#25552;&#26696;&#29983;&#25104;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#25991;&#23383;&#20154;&#29289;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text-based Person Search in Full Images via Semantic-Driven Proposal Generation. (arXiv:2109.12965v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#39537;&#21160;&#30340;&#25552;&#26696;&#29983;&#25104;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#25991;&#23383;&#20154;&#29289;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#34892;&#20154;&#26816;&#27979;&#12289;&#36523;&#20221;&#35782;&#21035;&#21644;&#35270;&#35273;-&#35821;&#20041;&#29305;&#24449;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#35821;&#20041;&#29305;&#24449;&#25351;&#23548;&#21306;&#22495;&#24314;&#35758;&#32593;&#32476;&#20851;&#27880;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#26696;&#12290;&#20351;&#29992;&#36328;&#23610;&#24230;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#22312;&#20840;&#22330;&#26223;&#22270;&#20687;&#20013;&#25214;&#21040;&#30446;&#26631;&#20154;&#29289;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#19981;&#21516;&#65292;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#29289;&#26816;&#32034;&#26041;&#27861;&#20013;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#25991;&#26412;&#25551;&#36848;&#21644;&#35009;&#21098;&#30340;&#34892;&#20154;&#22270;&#20687;&#24211;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#21305;&#37197;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#20154;&#29289;&#25628;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#20248;&#21270;&#34892;&#20154;&#26816;&#27979;&#12289;&#36523;&#20221;&#35782;&#21035;&#21644;&#35270;&#35273;-&#35821;&#20041;&#29305;&#24449;&#23884;&#20837;&#20219;&#21153;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#26597;&#35810;&#25991;&#26412;&#65292;&#35821;&#20041;&#29305;&#24449;&#34987;&#21033;&#29992;&#26469;&#25351;&#23548;&#21306;&#22495;&#24314;&#35758;&#32593;&#32476;&#26356;&#20851;&#27880;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#21033;&#29992;&#20102;&#36328;&#23610;&#24230;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#26631;&#27880;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding target persons in full scene images with a query of text description has important practical applications in intelligent video surveillance.However, different from the real-world scenarios where the bounding boxes are not available, existing text-based person retrieval methods mainly focus on the cross modal matching between the query text descriptions and the gallery of cropped pedestrian images. To close the gap, we study the problem of text-based person search in full images by proposing a new end-to-end learning framework which jointly optimize the pedestrian detection, identification and visual-semantic feature embedding tasks. To take full advantage of the query text, the semantic features are leveraged to instruct the Region Proposal Network to pay more attention to the text-described proposals. Besides, a cross-scale visual-semantic embedding mechanism is utilized to improve the performance. To validate the proposed method, we collect and annotate two large-scale benchm
&lt;/p&gt;</description></item></channel></rss>