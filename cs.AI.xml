<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#32508;&#36848;&#23545;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#20027;&#35201;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01166</link><description>&lt;p&gt;
&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on 3D Content Generation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#20027;&#35201;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20855;&#26377;&#22810;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#19977;&#32500;&#12290;&#19977;&#32500;&#26159;&#26368;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#19977;&#32500;&#29615;&#22659;&#30340;&#21487;&#35270;&#27169;&#24577;&#65292;&#24182;&#20855;&#26377;&#24040;&#22823;&#30340;&#30693;&#35782;&#37327;&#12290;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#19981;&#20165;&#20855;&#26377;&#23398;&#26415;&#21644;&#23454;&#36341;&#20215;&#20540;&#65292;&#32780;&#19988;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25972;&#21512;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#19977;&#32500;&#26412;&#26426;&#29983;&#25104;&#26041;&#27861;&#12289;&#22522;&#20110;&#20108;&#32500;&#20808;&#39564;&#30340;&#19977;&#32500;&#29983;&#25104;&#26041;&#27861;&#21644;&#28151;&#21512;&#19977;&#32500;&#29983;&#25104;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#32422;60&#31687;&#28085;&#30422;&#20027;&#35201;&#25216;&#26415;&#30340;&#35770;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#37197;&#21512;&#26412;&#32508;&#36848;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#39033;&#30446;&#32593;&#31449;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30456;&#20851;&#36164;&#28304;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed remarkable advances in artificial intelligence generated content(AIGC), with diverse input modalities, e.g., text, image, video, audio and 3D. The 3D is the most close visual modality to real-world 3D environment and carries enormous knowledge. The 3D content generation shows both academic and practical values while also presenting formidable technical challenges. This review aims to consolidate developments within the burgeoning domain of 3D content generation. Specifically, a new taxonomy is proposed that categorizes existing approaches into three types: 3D native generative methods, 2D prior-based 3D generative methods, and hybrid 3D generative methods. The survey covers approximately 60 papers spanning the major techniques. Besides, we discuss limitations of current 3D content generation techniques, and point out open challenges as well as promising directions for future work. Accompanied with this survey, we have established a project website where the 
&lt;/p&gt;</description></item><item><title>TexTile&#26159;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32441;&#29702;&#22270;&#20687;&#30340;&#24179;&#38138;&#24615;&#33021;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#26126;&#26234;&#22320;&#21512;&#25104;&#21644;&#20998;&#26512;&#24179;&#38138;&#32441;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.12961</link><description>&lt;p&gt;
TexTile&#65306;&#19968;&#31181;&#21487;&#24494;&#30340;&#32441;&#29702;&#24179;&#38138;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
TexTile: A Differentiable Metric for Texture Tileability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12961
&lt;/p&gt;
&lt;p&gt;
TexTile&#26159;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32441;&#29702;&#22270;&#20687;&#30340;&#24179;&#38138;&#24615;&#33021;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#26126;&#26234;&#22320;&#21512;&#25104;&#21644;&#20998;&#26512;&#24179;&#38138;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;TexTile&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#24230;&#37327;&#26041;&#24335;&#65292;&#29992;&#20110;&#37327;&#21270;&#32441;&#29702;&#22270;&#20687;&#21487;&#20197;&#22914;&#20309;&#19982;&#33258;&#36523;&#36830;&#25509;&#32780;&#19981;&#24341;&#20837;&#37325;&#22797;&#20266;&#24433;&#65288;&#21363;&#24179;&#38138;&#24615;&#65289;&#12290;&#29616;&#26377;&#30340;&#21487;&#24179;&#38138;&#32441;&#29702;&#21512;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#19968;&#33324;&#32441;&#29702;&#36136;&#37327;&#65292;&#20294;&#32570;&#20047;&#23545;&#32441;&#29702;&#22266;&#26377;&#21487;&#37325;&#22797;&#24615;&#23646;&#24615;&#30340;&#26174;&#24335;&#20998;&#26512;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;TexTile&#24230;&#37327;&#26377;&#25928;&#35780;&#20272;&#32441;&#29702;&#30340;&#21487;&#24179;&#38138;&#24615;&#65292;&#20026;&#26356;&#26126;&#26234;&#30340;&#24179;&#38138;&#32441;&#29702;&#21512;&#25104;&#21644;&#20998;&#26512;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#22312;&#32972;&#21518;&#65292;TexTile&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#20180;&#32454;&#26500;&#24314;&#33258;&#19981;&#21516;&#39118;&#26684;&#12289;&#35821;&#20041;&#12289;&#35268;&#24459;&#21644;&#20154;&#31867;&#27880;&#37322;&#30340;&#22823;&#22411;&#32441;&#29702;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31995;&#21015;&#26550;&#26500;&#20462;&#25913;&#65292;&#20351;&#22522;&#32447;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#33021;&#22815;&#20811;&#26381;&#20854;&#22312;&#34913;&#37327;&#24179;&#38138;&#24615;&#26041;&#38754;&#30340;&#32570;&#38519;&#65292;&#20197;&#21450;&#38024;&#23545;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#22686;&#24378;&#21644;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12961v1 Announce Type: cross  Abstract: We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing rob
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WHAC&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#19990;&#30028;&#22352;&#26631;&#31995;&#30340;&#20016;&#23500;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#65292;&#21516;&#26102;&#36827;&#34892;&#25668;&#20687;&#26426;&#23039;&#21183;&#20272;&#35745;&#65292;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.12959</link><description>&lt;p&gt;
WHAC: &#19990;&#30028;&#22522;&#20934;&#20154;&#31867;&#19982;&#25668;&#20687;&#26426;
&lt;/p&gt;
&lt;p&gt;
WHAC: World-grounded Humans and Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WHAC&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#19990;&#30028;&#22352;&#26631;&#31995;&#30340;&#20016;&#23500;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#65292;&#21516;&#26102;&#36827;&#34892;&#25668;&#20687;&#26426;&#23039;&#21183;&#20272;&#35745;&#65292;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#20934;&#30830;&#20272;&#35745;&#19990;&#30028;&#22352;&#26631;&#31995;&#20013;&#20855;&#26377;&#27604;&#20363;&#30340;&#20154;&#31867;&#21644;&#25668;&#20687;&#26426;&#36712;&#36857;&#26159;&#19968;&#39033;&#38750;&#24120;&#29702;&#24819;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#27169;&#31946;&#23450;&#20041;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#12289;&#20154;&#31867;&#21644;&#25668;&#20687;&#26426;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#32852;&#21512;&#24674;&#22797;&#34920;&#29616;&#21147;&#24378;&#30340;&#21442;&#25968;&#21270;&#20154;&#20307;&#27169;&#22411;&#65288;&#21363;SMPL-X&#65289;&#21644;&#30456;&#24212;&#30340;&#25668;&#20687;&#26426;&#23039;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#12290;&#39318;&#20808;&#65292;&#30456;&#26426;&#26694;&#26550;&#19979;&#30340;SMPL-X&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24674;&#22797;&#32477;&#23545;&#20154;&#31867;&#28145;&#24230;&#12290;&#20854;&#27425;&#65292;&#20154;&#20307;&#21160;&#20316;&#26412;&#36136;&#19978;&#25552;&#20379;&#32477;&#23545;&#31354;&#38388;&#32447;&#32034;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;WHAC&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#22522;&#20110;&#19990;&#30028;&#30340;&#34920;&#29616;&#21147;&#24378;&#30340;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#65288;EHPS&#65289;&#65292;&#21516;&#26102;&#36827;&#34892;&#25668;&#20687;&#26426;&#23039;&#21183;&#20272;&#35745;&#65292;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;WHAC-A-Mole&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#31934;&#30830;&#27880;&#37322;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12959v1 Announce Type: cross  Abstract: Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated h
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;</title><link>https://arxiv.org/abs/2403.12952</link><description>&lt;p&gt;
&#21482;&#38656;&#36716;&#31227;&#23427;&#65306;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12952
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#22240;&#20026;&#39046;&#22495;&#36716;&#31227;&#32780;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20351;&#29992;&#26631;&#35760;&#27979;&#35797;&#36755;&#20837;&#26469;&#20351;VLM&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#35843;&#33410;&#27599;&#20010;&#31867;&#21035;&#30340;&#21407;&#22411;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#24182;&#32531;&#23384;&#21407;&#22411;&#65292;TPS&#19981;&#20165;&#20419;&#36827;&#20102;&#26080;&#38656;&#20248;&#21270;&#30340;&#21407;&#22411;&#37325;&#29992;&#36827;&#34892;&#21518;&#32493;&#39044;&#27979;&#65292;&#36824;&#35753;&#20854;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#24403;&#21069;&#36827;&#23637;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;TPS&#20165;&#22522;&#20110;&#32473;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12952v1 Announce Type: cross  Abstract: Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing class
&lt;/p&gt;</description></item><item><title>Vid2Robot&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35270;&#39057;&#26465;&#20214;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#34701;&#21512;&#35270;&#39057;&#29305;&#24449;&#21644;&#26426;&#22120;&#20154;&#29366;&#24577;&#65292;&#30452;&#25509;&#29983;&#25104;&#27169;&#20223;&#25152;&#35266;&#23519;&#20219;&#21153;&#30340;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.12943</link><description>&lt;p&gt;
Vid2Robot&#65306;&#22522;&#20110;&#35270;&#39057;&#26465;&#20214;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#20132;&#21449;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12943
&lt;/p&gt;
&lt;p&gt;
Vid2Robot&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35270;&#39057;&#26465;&#20214;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#34701;&#21512;&#35270;&#39057;&#29305;&#24449;&#21644;&#26426;&#22120;&#20154;&#29366;&#24577;&#65292;&#30452;&#25509;&#29983;&#25104;&#27169;&#20223;&#25152;&#35266;&#23519;&#20219;&#21153;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#20219;&#21153;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#26426;&#22120;&#20154;&#33021;&#21542;&#30452;&#25509;&#20174;&#35266;&#23519;&#20154;&#31867;&#25512;&#26029;&#20219;&#21153;&#65311;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#26426;&#22120;&#20154;&#33021;&#22815;&#35299;&#30721;&#20154;&#31867;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#22312;&#20854;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#20869;&#25191;&#34892;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Vid2Robot&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26426;&#22120;&#20154;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#20010;&#25805;&#20316;&#20219;&#21153;&#30340;&#35270;&#39057;&#28436;&#31034;&#21644;&#24403;&#21069;&#30340;&#35270;&#35273;&#35266;&#23519;&#65292;Vid2Robot&#30452;&#25509;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#21644;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32479;&#19968;&#34920;&#31034;&#27169;&#22411;&#23454;&#29616;&#30340;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34701;&#21512;&#25552;&#31034;&#35270;&#39057;&#29305;&#24449;&#19982;&#26426;&#22120;&#20154;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#29983;&#25104;&#27169;&#20223;&#25152;&#35266;&#23519;&#20219;&#21153;&#30340;&#36866;&#24403;&#21160;&#20316;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36741;&#21161;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#22686;&#24378;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12943v1 Announce Type: cross  Abstract: While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#33258;&#21160;&#20174;&#33521;&#22269;&#38599;&#20323;&#27861;&#24237;&#26696;&#20363;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#39564;&#35777;&#30830;&#20445;&#20102;&#25552;&#21462;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;</title><link>https://arxiv.org/abs/2403.12936</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#38599;&#20323;&#27861;&#24237;&#35009;&#20915;&#20013;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#33258;&#21160;&#20174;&#33521;&#22269;&#38599;&#20323;&#27861;&#24237;&#26696;&#20363;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#39564;&#35777;&#30830;&#20445;&#20102;&#25552;&#21462;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24237;&#35760;&#24405;&#21644;&#21028;&#20915;&#26159;&#27861;&#24459;&#30693;&#35782;&#30340;&#20016;&#23500;&#36164;&#28304;&#65292;&#35814;&#32454;&#25551;&#36848;&#26696;&#20214;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#21496;&#27861;&#20915;&#23450;&#32972;&#21518;&#30340;&#29702;&#30001;&#12290;&#20174;&#36825;&#20123;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#25552;&#20379;&#20102;&#26696;&#20214;&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#23545;&#20110;&#27861;&#24459;&#19987;&#23478;&#21644;&#20844;&#20247;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#33258;&#21160;&#20449;&#24687;&#25552;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;&#26412;&#25991;&#23545;GPT-4&#65288;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#20174;&#33521;&#22269;&#38599;&#20323;&#27861;&#24237;&#65288;UKET&#65289;&#26696;&#20363;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#39564;&#35777;&#36807;&#31243;&#23545;GPT-4&#22312;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26041;&#38754;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#25552;&#21462;&#30340;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#20027;&#35201;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#23637;&#24320;&#65306;&#31532;&#19968;&#20010;&#20219;&#21153;&#28041;&#21450;&#23545;&#23545;&#27861;&#24459;&#19987;&#23478;&#21644;&#20844;&#20247;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20843;&#20010;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#36890;&#29992;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12936v1 Announce Type: cross  Abstract: Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases. We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12918</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#27867;&#21270;&#21644;&#31283;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;PLMs&#20250;&#38754;&#20020;&#35832;&#22914;&#19981;&#31283;&#23450;&#24615;&#21644;&#36807;&#25311;&#21512;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#31574;&#30053;&#36873;&#25321;&#30340;&#23376;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#26435;&#37325;&#22266;&#23450;&#20026;&#39044;&#35757;&#32451;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#23376;&#32593;&#32476;&#36873;&#25321;&#26631;&#20934;&#65292;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#24494;&#35843;PLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#32593;&#32476;&#26435;&#37325;&#34920;&#31034;&#20026;&#20219;&#21153;&#29305;&#23450;&#26435;&#37325;&#21644;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#28151;&#21512;&#65292;&#30001;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#21442;&#25968;&#25511;&#21046;&#65292;&#25552;&#20379;&#23545;&#23376;&#32593;&#32476;&#36873;&#25321;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#21333;&#29420;&#25286;&#20998;&#19978;&#20351;&#29992;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#30340;&#26694;&#26550;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12918v1 Announce Type: cross  Abstract: Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving ge
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35821;&#35328;&#32416;&#27491;&#65292;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#25345;&#32493;&#25913;&#36827;&#38271;&#20037;&#20219;&#21153;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.12910</link><description>&lt;p&gt;
&#23545;&#24744;&#30340;&#26426;&#22120;&#20154;&#22823;&#21898;&#65306;&#20174;&#35821;&#35328;&#32416;&#27491;&#20013;&#23454;&#26102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Yell At Your Robot: Improving On-the-Fly from Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35821;&#35328;&#32416;&#27491;&#65292;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#25345;&#32493;&#25913;&#36827;&#38271;&#20037;&#20219;&#21153;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#35821;&#35328;&#21644;&#20302;&#32423;&#25511;&#21046;&#30340;&#20998;&#23618;&#31574;&#30053;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38271;&#35270;&#37326;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LLMs/VLMs&#65289;&#25110;&#22312;&#27880;&#37322;&#30340;&#26426;&#22120;&#20154;&#28436;&#31034;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#22797;&#26434;&#21644;&#28789;&#24039;&#30340;&#25216;&#33021;&#65292;&#23454;&#29616;&#22312;&#38271;&#35270;&#37326;&#20219;&#21153;&#19978;&#39640;&#25104;&#21151;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#8212;&#8212;&#20219;&#21153;&#36234;&#38271;&#65292;&#26576;&#20010;&#38454;&#27573;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#23601;&#36234;&#22823;&#12290;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#30452;&#35266;&#33258;&#28982;&#30340;&#21453;&#39304;&#24110;&#21161;&#26426;&#22120;&#20154;&#25345;&#32493;&#25913;&#36827;&#20854;&#38271;&#35270;&#37326;&#20219;&#21153;&#24615;&#33021;&#21527;&#65311;&#26412;&#25991;&#20013;&#25105;&#20204;&#21457;&#29616;&#65306;&#21487;&#20197;&#36890;&#36807;&#23500;&#21547;&#34920;&#36798;&#21147;&#30340;&#20302;&#32423;&#35821;&#35328;&#26465;&#20214;&#25216;&#33021;&#32034;&#24341;&#21040;&#39640;&#27700;&#24179;&#31574;&#30053;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#35821;&#35328;&#32416;&#27491;&#30340;&#24418;&#24335;&#36827;&#34892;&#20154;&#31867;&#30417;&#30563;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#29978;&#33267;&#31934;&#32454;&#30340;&#32416;&#27491;&#65292;&#22914;&#23567;&#21160;&#20316;&#65288;&#8220;&#31227;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12910v1 Announce Type: cross  Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements ("move a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25351;&#20196;&#30340;&#27010;&#24565;&#65292;&#24179;&#34913;&#20102;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26381;&#21153;&#30899;&#25490;&#25918;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.12900</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#25345;&#32493;&#30340;GenAI&#65306;&#20351;&#29992;&#29983;&#25104;&#25351;&#20196;&#23454;&#29616;&#30899;&#21451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25351;&#20196;&#30340;&#27010;&#24565;&#65292;&#24179;&#34913;&#20102;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26381;&#21153;&#30899;&#25490;&#25918;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#24341;&#36215;&#20102;&#29615;&#22659;&#26041;&#38754;&#30340;&#37325;&#35201;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20113;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#30899;&#25490;&#25918;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#26381;&#21153;&#30340;&#30899;&#36275;&#36857;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;Sprout&#21033;&#29992;&#21019;&#26032;&#27010;&#24565;&#8220;&#29983;&#25104;&#25351;&#20196;&#8221;&#26469;&#24341;&#23548;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#24378;&#30899;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31934;&#24515;&#24179;&#34913;&#20102;&#23545;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#25104;&#26524;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#25351;&#20196;&#20248;&#21270;&#22120;&#26469;&#23545;&#29992;&#25143;&#25552;&#31034;&#36827;&#34892;&#29983;&#25104;&#25351;&#20196;&#30340;&#25112;&#30053;&#20998;&#37197;&#21644;&#19968;&#20010;&#21407;&#21019;&#30340;&#31163;&#32447;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;Sprout&#22312;&#23454;&#38469;&#35780;&#20272;&#20013;&#26174;&#33879;&#20943;&#23569;&#20102;40%&#20197;&#19978;&#30340;&#30899;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12900v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of "generation directives" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AVIL&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#24863;&#30693;&#19982;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#19981;&#21516;&#30871;&#37197;&#32622;&#21644;&#39135;&#29289;&#31867;&#22411;&#19979;&#30340;&#28789;&#27963;&#21644;&#31283;&#20581;&#30340;&#36741;&#21161;&#21890;&#39135;&#12290;</title><link>https://arxiv.org/abs/2403.12891</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#22312;&#19981;&#21516;&#30871;&#37197;&#32622;&#21644;&#39135;&#29289;&#31867;&#22411;&#19979;&#30340;&#26426;&#22120;&#20154;&#36741;&#21161;&#21890;&#39135;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AVIL&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#24863;&#30693;&#19982;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#19981;&#21516;&#30871;&#37197;&#32622;&#21644;&#39135;&#29289;&#31867;&#22411;&#19979;&#30340;&#28789;&#27963;&#21644;&#31283;&#20581;&#30340;&#36741;&#21161;&#21890;&#39135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24102;&#26377;&#31354;&#38388;&#27880;&#24847;&#27169;&#22359;&#30340;&#26032;&#22411;&#35270;&#35273;&#27169;&#20223;&#32593;&#32476;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#21890;&#39135;&#65288;RAF&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#30871;&#20013;&#33719;&#21462;&#65288;&#21363;&#38130;&#21462;&#65289;&#39135;&#29289;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#24378;&#20581;&#21644;&#28789;&#27963;&#30340;&#39135;&#29289;&#25805;&#32437;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#24863;&#30693;&#19982;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#38130;&#21462;&#36807;&#31243;&#20013;&#22788;&#29702;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AVIL&#65288;&#33258;&#36866;&#24212;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65289;&#65292;&#22312;&#26448;&#26009;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#31561;&#26041;&#38754;&#30340;&#19981;&#21516;&#30871;&#37197;&#32622;&#20197;&#21450;&#21253;&#25324;&#39063;&#31890;&#29366;&#12289;&#21322;&#22266;&#20307;&#21644;&#28082;&#20307;&#22312;&#20869;&#30340;&#21508;&#31181;&#39135;&#29289;&#31867;&#22411;&#20013;&#23637;&#29616;&#20102;&#36866;&#24212;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#29978;&#33267;&#22312;&#24178;&#25200;&#29289;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#22522;&#20934;&#32447;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#20934;&#32447;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12891v1 Announce Type: cross  Abstract: In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Vampire&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#20351;&#29992;&#34584;&#34523;&#24335;&#31995;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26500;&#24314;&#24378;&#22823;&#26085;&#31243;&#34920;&#30340;&#38590;&#26131;&#31243;&#24230;&#20197;&#21450;&#26085;&#31243;&#27867;&#21270;&#21040;&#26410;&#30693;&#38382;&#39064;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.12869</link><description>&lt;p&gt;
&#34584;&#34523;&#24335;&#31574;&#30053;&#21457;&#29616;&#21644;&#26085;&#31243;&#26500;&#24314;&#20013;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regularization in Spider-Style Strategy Discovery and Schedule Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Vampire&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#20351;&#29992;&#34584;&#34523;&#24335;&#31995;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26500;&#24314;&#24378;&#22823;&#26085;&#31243;&#34920;&#30340;&#38590;&#26131;&#31243;&#24230;&#20197;&#21450;&#26085;&#31243;&#27867;&#21270;&#21040;&#26410;&#30693;&#38382;&#39064;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#22810;&#26679;&#30340;&#35777;&#26126;&#31574;&#30053;&#26085;&#31243;&#34920;&#65292;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#23581;&#35797;&#65288;&#39034;&#24207;&#25110;&#24182;&#34892;&#65289;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#39033;&#38024;&#23545;Vampire&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#31574;&#30053;&#21457;&#29616;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#38024;&#23545;TPTP&#24211;&#30340;FOF&#29255;&#27573;&#24182;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;Andrei Voronkov&#30340;Spider&#31995;&#32479;&#24605;&#24819;&#30340;&#26085;&#31243;&#34920;&#12290;&#25105;&#20204;&#20174;&#21508;&#20010;&#35282;&#24230;&#23457;&#35270;&#35813;&#36807;&#31243;&#65292;&#35752;&#35770;&#20102;&#20026;CASC&#31454;&#36187;&#33719;&#24471;&#24378;&#22823;&#30340;Vampire&#26085;&#31243;&#34920;&#30340;&#38590;&#26131;&#31243;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26085;&#31243;&#34920;&#33021;&#22815;&#23545;&#26410;&#30693;&#38382;&#39064;&#27867;&#21270;&#21040;&#20309;&#31181;&#31243;&#24230;&#20197;&#21450;&#24433;&#21709;&#27492;&#23646;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12869v1 Announce Type: new  Abstract: To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem. In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider. We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#21487;&#22312;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.12853</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#29615;&#22659;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#37325;&#26500;&#20316;&#21160;&#21644;&#20256;&#24863;&#24179;&#21488;RASP
&lt;/p&gt;
&lt;p&gt;
RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#21487;&#22312;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#28040;&#36153;&#32423;&#26080;&#20154;&#26426;&#19982;&#25105;&#20204;&#23478;&#20013;&#30340;&#21560;&#23576;&#26426;&#22120;&#20154;&#25110;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20010;&#20154;&#26234;&#33021;&#25163;&#26426;&#19968;&#26679;&#26377;&#29992;&#65292;&#38656;&#35201;&#26080;&#20154;&#26426;&#33021;&#24863;&#30693;&#12289;&#39537;&#21160;&#21644;&#21709;&#24212;&#21487;&#33021;&#20986;&#29616;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#30340;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20801;&#35768;&#26080;&#20154;&#26426;&#22312;&#20165;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#26426;&#36733;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#20351;&#21333;&#20010;&#26080;&#20154;&#26426;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#12290;RASP&#21253;&#25324;&#19968;&#20010;&#26426;&#26800;&#23618;&#65292;&#29992;&#20110;&#29289;&#29702;&#26356;&#25442;&#20256;&#24863;&#22120;&#27169;&#22359;&#65292;&#19968;&#20010;&#30005;&#27668;&#23618;&#65292;&#29992;&#20110;&#32500;&#25252;&#20256;&#24863;&#22120;/&#25191;&#34892;&#22120;&#30340;&#30005;&#28304;&#21644;&#36890;&#20449;&#32447;&#36335;&#65292;&#20197;&#21450;&#19968;&#20010;&#36719;&#20214;&#23618;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#21644;&#25105;&#20204;&#24179;&#21488;&#19978;&#30340;&#20219;&#20309;&#20256;&#24863;&#22120;&#27169;&#22359;&#20043;&#38388;&#32500;&#25252;&#19968;&#20010;&#20844;&#20849;&#25509;&#21475;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;RASP&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#30340;&#26550;&#26500;&#12289;&#23454;&#29616;&#21644;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12853v1 Announce Type: cross  Abstract: Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28789;&#27963;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#22411;&#21644;&#20915;&#31574;&#27169;&#22411;&#21644;&#31526;&#21495;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#21672;&#35810;&#24072;&#20195;&#34920;&#22797;&#26434;&#35268;&#21017;&#35774;&#35745;&#28789;&#27963;&#34218;&#37228;&#31649;&#29702;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.12823</link><description>&lt;p&gt;
&#28789;&#27963;&#34218;&#37228;&#31649;&#29702;&#30340;&#31572;&#26696;&#38598;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Answer Set Programming for Flexible Payroll Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28789;&#27963;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#22411;&#21644;&#20915;&#31574;&#27169;&#22411;&#21644;&#31526;&#21495;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#21672;&#35810;&#24072;&#20195;&#34920;&#22797;&#26434;&#35268;&#21017;&#35774;&#35745;&#28789;&#27963;&#34218;&#37228;&#31649;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34218;&#37228;&#31649;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#19994;&#21153;&#20219;&#21153;&#65292;&#21463;&#21040;&#22823;&#37327;&#35268;&#21017;&#30340;&#32422;&#26463;&#65292;&#36825;&#20123;&#35268;&#21017;&#22312;&#20844;&#21496;&#12289;&#34892;&#19994;&#21644;&#22269;&#23478;&#20043;&#38388;&#21464;&#21270;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35268;&#21017;&#36890;&#24120;&#24456;&#22797;&#26434;&#24182;&#19988;&#32463;&#24120;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#34218;&#37228;&#31649;&#29702;&#31995;&#32479;&#24517;&#39035;&#22312;&#35774;&#35745;&#19978;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28789;&#27963;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#27169;&#22411;&#21644;&#22522;&#20110;&#20915;&#31574;&#27169;&#22411;&#21644;&#31526;&#21495;&#26631;&#35760;&#65288;DMN&#65289;&#26631;&#20934;&#30340;&#26131;&#20110;&#38405;&#35835;&#30340;&#34920;&#26684;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#20154;&#21147;&#36164;&#28304;&#21672;&#35810;&#24072;&#20195;&#34920;&#22797;&#26434;&#35268;&#21017;&#65292;&#32780;&#26080;&#38656;&#36719;&#20214;&#24037;&#31243;&#24072;&#30340;&#21442;&#19982;&#65292;&#24182;&#26368;&#32456;&#20026;&#21508;&#31181;&#19981;&#21516;&#24773;&#26223;&#35774;&#35745;&#34218;&#37228;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;clingo ASP&#31995;&#32479;&#30340;&#22810;&#27425;&#27714;&#35299;&#33021;&#21147;&#22914;&#20309;&#21487;&#20197;&#29992;&#20110;&#36798;&#21040;&#22788;&#29702;&#23454;&#38469;&#24773;&#20917;&#25152;&#38656;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12823v1 Announce Type: cross  Abstract: Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries. Moreover, the rules are often complex and change regularly. Therefore, payroll management systems must be flexible in design. In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard. It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios. We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances.
&lt;/p&gt;</description></item><item><title>FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.12821</link><description>&lt;p&gt;
FlowerFormer: &#20351;&#29992;&#22522;&#20110;&#27969;&#24863;&#30693;&#30340;&#22270;&#21464;&#25442;&#22120;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12821
&lt;/p&gt;
&lt;p&gt;
FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25104;&#21151;&#19982;&#20854;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#65307;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#31070;&#32463;&#32467;&#26500;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#35757;&#32451;&#25110;&#35780;&#20272;&#12290;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#22312;&#20272;&#35745;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23558;&#26550;&#26500;&#35270;&#20026;&#22270;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FlowerFormer&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#23427;&#34701;&#20837;&#20102;&#31070;&#32463;&#32467;&#26500;&#20869;&#30340;&#20449;&#24687;&#27969;&#12290; FlowerFormer&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;a&#65289;&#21463;&#27969;&#31243;&#21551;&#21457;&#30340;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#65307;&#65288;b&#65289;&#24314;&#31435;&#22312;&#22522;&#20110;&#27969;&#31243;&#30340;&#25513;&#30721;&#19978;&#30340;&#20840;&#23616;&#20851;&#27880;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FlowerFormer&#20248;&#20110;&#29616;&#26377;&#31070;&#32463;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural 
&lt;/p&gt;</description></item><item><title>&#30456;&#23545;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#20197;&#39640;&#20934;&#30830;&#29575;&#37325;&#26032;&#35782;&#21035;&#24739;&#32773;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39118;&#38505;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.12816</link><description>&lt;p&gt;
&#20174;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#37325;&#26032;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Re-identification from histopathology images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12816
&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#20197;&#39640;&#20934;&#30830;&#29575;&#37325;&#26032;&#35782;&#21035;&#24739;&#32773;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39118;&#38505;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#25581;&#31034;&#32959;&#30244;&#20122;&#22411;&#25110;&#36716;&#31227;&#30340;&#21407;&#21457;&#37096;&#20301;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24517;&#39035;&#36827;&#34892;&#21311;&#21517;&#21270;&#22788;&#29702;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#24739;&#32773;&#36523;&#20221;&#27844;&#38706;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#30456;&#23545;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20063;&#21487;&#20197;&#22312;&#24222;&#22823;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#20013;&#20197;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#29575;&#37325;&#26032;&#35782;&#21035;&#24739;&#32773;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32954;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;LSCC&#65289;&#21644;&#32954;&#33146;&#30284;&#65288;LUAD&#65289;&#22312;&#20869;&#30340;&#20004;&#20010;TCIA&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#33258;&#21046;&#30340;&#33041;&#33180;&#30244;&#32452;&#32455;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#39118;&#38505;&#35780;&#20272;&#26041;&#26696;&#26469;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12816v1 Announce Type: cross  Abstract: In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases. These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks. This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy. We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD). We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue. We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset. Based on our findings, we formulated a risk assessment scheme to estimate 
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#21644;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#23384;&#22312;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#27169;&#22411;&#36234;&#22823;&#65292;FA&#30456;&#23545;&#20110;&#21333;&#35821;&#27169;&#22411;&#26469;&#35828;&#36234;&#19981;&#24544;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.12809</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#35821;&#21644;&#21333;&#35821;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#35299;&#37322;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12809
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#21644;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#23384;&#22312;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#27169;&#22411;&#36234;&#22823;&#65292;FA&#30456;&#23545;&#20110;&#21333;&#35821;&#27169;&#22411;&#26469;&#35828;&#36234;&#19981;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#20174;&#19994;&#32773;&#19981;&#20165;&#26088;&#22312;&#26368;&#22823;&#21270;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#23547;&#27714;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24544;&#23454;&#35299;&#37322;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32473;&#20986;&#30340;&#29702;&#30001;&#21644;&#37325;&#35201;&#24615;&#20998;&#24067;&#25581;&#31034;&#20102;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#24544;&#23454;&#24230;&#65292;&#20027;&#35201;&#26159;&#22312;&#21333;&#35821;&#33521;&#35821;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#20043;&#38388;&#30340;FA&#24544;&#23454;&#24230;&#24046;&#24322;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;FA&#30340;&#24544;&#23454;&#24230;&#22312;&#22810;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#20043;&#38388;&#26377;&#25152;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#35821;&#27169;&#22411;&#36234;&#22823;&#65292;FA&#30456;&#23545;&#20110;&#20854;&#23545;&#24212;&#30340;&#21333;&#35821;&#27169;&#22411;&#26469;&#35828;&#36234;&#19981;&#24544;&#23454;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#26174;&#31034;&#65292;&#24544;&#23454;&#24230;&#30340;&#24046;&#24322;&#26159;&#28508;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12809v1 Announce Type: cross  Abstract: In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potenti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#22659;&#32858;&#21512;&#30340;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#31995;&#32479;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.12805</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#32858;&#21512;&#23454;&#29616;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Contextual Moral Value Alignment Through Context-Based Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#22659;&#32858;&#21512;&#30340;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#31995;&#32479;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20215;&#20540;&#23545;&#40784;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19968;&#20010;&#22797;&#26434;&#32780;&#25345;&#32493;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#65292;&#23558;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#23545;&#35805;&#20195;&#29702;&#25972;&#21512;&#20026;&#19968;&#20010;&#32479;&#19968;&#31995;&#32479;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#24182;&#19982;&#22810;&#20010;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#65292;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#22659;&#32858;&#21512;&#30340;&#24773;&#22659;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#31995;&#32479;&#12290;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#32858;&#21512;&#34987;&#23450;&#20041;&#20026;&#25972;&#21512;&#36866;&#21512;&#22238;&#22797;&#29992;&#25143;&#36755;&#20837;&#30340;LLM&#21709;&#24212;&#23376;&#38598;&#30340;&#36807;&#31243;&#65292;&#32771;&#34385;&#20102;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#25552;&#21462;&#20986;&#30340;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12805v1 Announce Type: new  Abstract: Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#25991;&#26723;&#25130;&#26029;&#21644;&#25688;&#35201;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#25688;&#35201;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#32988;&#36807;&#25130;&#26029;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#26368;&#20339;&#31574;&#30053;&#20026;&#21462;&#25991;&#26723;&#30340;&#24320;&#22836;&#12290;</title><link>https://arxiv.org/abs/2403.12799</link><description>&lt;p&gt;
&#25506;&#31350;BERT&#20013;&#30340;&#25991;&#26412;&#32553;&#30701;&#31574;&#30053;&#65306;&#25130;&#26029; vs &#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Investigating Text Shortening Strategy in BERT: Truncation vs Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#25991;&#26723;&#25130;&#26029;&#21644;&#25688;&#35201;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#25688;&#35201;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#32988;&#36807;&#25130;&#26029;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#26368;&#20339;&#31574;&#30053;&#20026;&#21462;&#25991;&#26723;&#30340;&#24320;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#24182;&#34892;&#24615;&#20197;&#20854;&#36755;&#20837;&#26368;&#22823;&#38271;&#24230;&#20026;&#20195;&#20215;&#12290;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20294;&#20854;&#20013;&#26410;&#26377;&#25253;&#21578;&#25688;&#35201;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25991;&#26723;&#25130;&#26029;&#21644;&#25688;&#35201;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#27599;&#31181;&#26041;&#27861;&#37117;&#24471;&#21040;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#21464;&#20307;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#19982;&#20840;&#25991;&#34920;&#29616;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#21360;&#23612;&#26032;&#38395;&#25991;&#31456;(IndoSum)&#30340;&#25688;&#35201;&#20219;&#21153;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#20998;&#31867;&#27979;&#35797;&#12290;&#26412;&#30740;&#31350;&#26174;&#31034;&#20986;&#25688;&#35201;&#32988;&#36807;&#20102;&#22823;&#37096;&#20998;&#25130;&#26029;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21482;&#36755;&#32473;&#20102;&#19968;&#20010;&#12290;&#26412;&#30740;&#31350;&#24471;&#21040;&#30340;&#26368;&#20339;&#31574;&#30053;&#26159;&#21462;&#25991;&#26723;&#30340;&#24320;&#22836;&#12290;&#20854;&#27425;&#26159;&#25277;&#21462;&#24335;&#25688;&#35201;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#24341;&#39046;&#30528;&#23545;&#20110;&#21033;&#29992;&#28508;&#21147;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12799v1 Announce Type: cross  Abstract: The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative. In this study, we investigate the performance of document truncation and summarization in text classification tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive summarization. This study explains what happened to the result, leading to further research in order to exploit the potenti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20998;&#35299;&#12289;&#35299;&#37322;&#21644;&#20943;&#36731;&#65288;DIM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;, &#29992;&#20110;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#21644;&#20943;&#36731;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12777</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#21644;&#20943;&#36731;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;
&lt;/p&gt;
&lt;p&gt;
Discover and Mitigate Multiple Biased Subgroups in Image Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20998;&#35299;&#12289;&#35299;&#37322;&#21644;&#20943;&#36731;&#65288;DIM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;, &#29992;&#20110;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#21644;&#20943;&#36731;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24120;&#24120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#26377;&#20559;&#23376;&#32676;&#20307;&#19978;&#22833;&#36133;&#65292;&#24433;&#21709;&#27169;&#22411;&#23545;&#21487;&#38752;&#24212;&#29992;&#30340;&#25239;&#24178;&#25200;&#24615;&#12290;&#36825;&#20123;&#23376;&#32676;&#20307;&#36890;&#24120;&#30001;&#20110;&#32570;&#20047;&#23376;&#32676;&#20307;&#26631;&#31614;&#32780;&#26410;&#30693;&#12290;&#21457;&#29616;&#26377;&#20559;&#23376;&#32676;&#20307;&#26159;&#29702;&#35299;&#27169;&#22411;&#22833;&#36133;&#27169;&#24335;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20998;&#35299;&#12289;&#35299;&#37322;&#21644;&#20943;&#36731;&#65288;DIM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#20063;&#26356;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22270;&#20687;&#29305;&#24449;&#20998;&#35299;&#20026;&#20195;&#34920;&#22810;&#20010;&#23376;&#32676;&#20307;&#30340;&#22810;&#20010;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12777v1 Announce Type: cross  Abstract: Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#27493;&#39588;FLIM&#26041;&#27861;&#65292;&#21482;&#22312;&#31532;&#19968;&#20010;&#21367;&#31215;&#23618;&#20013;&#20351;&#29992;&#29992;&#25143;&#36741;&#21161;&#30340;&#26041;&#24335;&#26469;&#20272;&#35745;&#21644;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#33041;&#32959;&#30244;&#20998;&#21106;&#32593;&#32476;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.12748</link><description>&lt;p&gt;
&#21033;&#29992;&#29992;&#25143;&#36741;&#21161;&#30340;&#28388;&#27874;&#22120;&#20272;&#35745;&#21644;&#36873;&#25321;&#26500;&#24314;&#33041;&#32959;&#30244;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12748
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#27493;&#39588;FLIM&#26041;&#27861;&#65292;&#21482;&#22312;&#31532;&#19968;&#20010;&#21367;&#31215;&#23618;&#20013;&#20351;&#29992;&#29992;&#25143;&#36741;&#21161;&#30340;&#26041;&#24335;&#26469;&#20272;&#35745;&#21644;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#33041;&#32959;&#30244;&#20998;&#21106;&#32593;&#32476;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#32959;&#30244;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20174;&#35768;&#22810;&#39044;&#27880;&#37322;&#22270;&#20687;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#24335;&#30041;&#19979;&#20102;&#19968;&#20123;&#26410;&#35299;&#20043;&#35868;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20174;&#22270;&#20687;&#26631;&#35760;&#20013;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#65288;Feature Learning from Image Markers&#65292;FLIM&#65289;&#65292;&#24050;&#32463;&#23558;&#19987;&#23478;&#24341;&#20837;&#23398;&#20064;&#29615;&#33410;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#27880;&#37322;&#30340;&#20154;&#21147;&#24037;&#20316;&#65292;&#24182;&#26500;&#24314;&#36275;&#22815;&#28145;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#65288;MS&#65289;FLIM - &#19968;&#31181;&#29992;&#25143;&#36741;&#21161;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;FLIM&#25191;&#34892;&#20013;&#20272;&#35745;&#21644;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#28388;&#27874;&#22120;&#12290;MS-FLIM&#20165;&#29992;&#20110;&#31532;&#19968;&#20010;&#21367;&#31215;&#23618;&#65292;&#32467;&#26524;&#24050;&#32463;&#34920;&#26126;&#30456;&#27604;FLIM&#26377;&#25152;&#25913;&#36827;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;U&#22411;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21629;&#21517;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12748v1 Announce Type: cross  Abstract: Brain tumor image segmentation is a challenging research topic in which deep-learning models have presented the best results. However, the traditional way of training those models from many pre-annotated images leaves several unanswered questions. Hence methodologies, such as Feature Learning from Image Markers (FLIM), have involved an expert in the learning loop to reduce human effort in data annotation and build models sufficiently deep for a given problem. FLIM has been successfully used to create encoders, estimating the filters of all convolutional layers from patches centered at marker voxels. In this work, we present Multi-Step (MS) FLIM - a user-assisted approach to estimating and selecting the most relevant filters from multiple FLIM executions. MS-FLIM is used only for the first convolutional layer, and the results already indicate improvement over FLIM. For evaluation, we build a simple U-shaped encoder-decoder network, name
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#19981;&#36807;&#24230;&#20381;&#36182;&#20110;&#20219;&#20309;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#26041;&#38754;&#65292;&#24182;&#25215;&#35748;&#20854;&#22266;&#26377;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#31995;&#32479;&#22320;&#25512;&#29702;&#26377;&#20851;&#36825;&#20123;&#24615;&#36136;&#30340;&#20869;&#23481;</title><link>https://arxiv.org/abs/2403.12730</link><description>&lt;p&gt;
&#35770;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#23454;&#38469;&#21578;&#35785;&#25105;&#20204;&#20160;&#20040;&#65311;&#25903;&#25345;&#23545;XAI&#26500;&#24314;&#27169;&#22359;&#36827;&#34892;&#32452;&#21512;&#21644;&#24773;&#22659;&#39564;&#35777;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#19981;&#36807;&#24230;&#20381;&#36182;&#20110;&#20219;&#20309;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#26041;&#38754;&#65292;&#24182;&#25215;&#35748;&#20854;&#22266;&#26377;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#31995;&#32479;&#22320;&#25512;&#29702;&#26377;&#20851;&#36825;&#20123;&#24615;&#36136;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#35780;&#20272;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#19981;&#36807;&#24230;&#20381;&#36182;&#20110;&#36825;&#20123;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#20219;&#20309;&#19968;&#20010;&#26041;&#38754;&#65292;&#24182;&#35748;&#35782;&#21040;&#23427;&#20204;&#22266;&#26377;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65306;&#25216;&#26415;&#26500;&#24314;&#27169;&#22359;&#12289;&#38754;&#21521;&#29992;&#25143;&#30340;&#35299;&#37322;&#24037;&#20214;&#21644;&#31038;&#20132;&#36890;&#20449;&#21327;&#35758;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#24847;&#29992;&#25143;&#30740;&#31350;&#22312;&#35780;&#20272;&#35299;&#37322;&#21576;&#29616;&#21644;&#20132;&#20184;&#31574;&#30053;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#20855;&#26377;&#19981;&#21487;&#20272;&#37327;&#30340;&#20215;&#20540;&#65292;&#29305;&#21035;&#26159;&#20174;&#29305;&#23450;&#37096;&#32626;&#24773;&#22659;&#20013;&#35299;&#37322;&#32773;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20294;&#28508;&#22312;&#30340;&#35299;&#37322;&#29983;&#25104;&#26426;&#21046;&#38656;&#35201;&#19968;&#20010;&#21333;&#29420;&#30340;&#12289;&#20027;&#35201;&#26159;&#31639;&#27861;&#30340;&#39564;&#35777;&#31574;&#30053;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#65288;&#25968;&#20540;&#65289;&#36755;&#20986;&#30340;&#25216;&#26415;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26399;&#26395;&#12290;&#36825;&#26679;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20250;&#25216;&#26415;&#23454;&#29992;&#24230;&#35780;&#20272;&#26694;&#26550;&#21487;&#20197;&#35753;&#25105;&#20204;&#31995;&#32479;&#22320;&#25512;&#29702;&#26377;&#20851;&#36825;&#20123;&#24615;&#36136;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12730v1 Announce Type: cross  Abstract: Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Python&#39033;&#30446;&#30340;&#21160;&#24577;&#20998;&#26512;&#31649;&#36947;&#65292;&#32467;&#21512;&#27169;&#31946;&#27979;&#35797;&#12289;&#35821;&#26009;&#24211;&#26368;&#23567;&#21270;&#12289;&#23849;&#28291;&#20998;&#31867;&#21644;&#35206;&#30422;&#29575;&#25910;&#38598;&#65292;&#20197;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12723</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;Python&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Python Fuzzing for Trustworthy Machine Learning Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12723
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Python&#39033;&#30446;&#30340;&#21160;&#24577;&#20998;&#26512;&#31649;&#36947;&#65292;&#32467;&#21512;&#27169;&#31946;&#27979;&#35797;&#12289;&#35821;&#26009;&#24211;&#26368;&#23567;&#21270;&#12289;&#23849;&#28291;&#20998;&#31867;&#21644;&#35206;&#30422;&#29575;&#25910;&#38598;&#65292;&#20197;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#31946;&#27979;&#35797;&#26159;&#23433;&#20840;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65288;SSDLC&#65289;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#23433;&#20840;&#21644;&#20581;&#22766;&#30340;&#36719;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Sydr-Fuzz&#24037;&#20855;&#38598;&#38024;&#23545;Python&#39033;&#30446;&#30340;&#21160;&#24577;&#20998;&#26512;&#31649;&#36947;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#21253;&#25324;&#27169;&#31946;&#27979;&#35797;&#12289;&#35821;&#26009;&#24211;&#26368;&#23567;&#21270;&#12289;&#23849;&#28291;&#20998;&#31867;&#21644;&#35206;&#30422;&#29575;&#25910;&#38598;&#12290;&#23849;&#28291;&#20998;&#31867;&#21644;&#20005;&#37325;&#24615;&#35780;&#20272;&#26159;&#30830;&#20445;&#21450;&#26102;&#35299;&#20915;&#26368;&#20851;&#38190;&#28431;&#27934;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#31649;&#36947;&#38598;&#25104;&#22312;GitLab CI&#20013;&#12290;&#20026;&#20102;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#20998;&#26512;&#23427;&#20204;&#28508;&#22312;&#30340;&#25915;&#20987;&#38754;&#65292;&#24182;&#20026;PyTorch&#12289;TensorFlow&#24320;&#21457;&#27169;&#31946;&#27979;&#35797;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12723v1 Announce Type: cross  Abstract: Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems. Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software. Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python. We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is integrated in GitLab CI. To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, 
&lt;/p&gt;</description></item><item><title>AnimateDiff-Lightning&#27169;&#22411;&#21033;&#29992;&#28176;&#36827;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;&#65292;&#22312;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#33976;&#39311;&#22810;&#20010;&#22522;&#30784;&#25193;&#25955;&#27169;&#22411;&#27010;&#29575;&#27969;&#30340;&#26041;&#27861;&#65292;&#24418;&#25104;&#20102;&#20855;&#26377;&#26356;&#24191;&#27867;&#26679;&#24335;&#20860;&#23481;&#24615;&#30340;&#21333;&#19968;&#33976;&#39311;&#36816;&#21160;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2403.12706</link><description>&lt;p&gt;
AnimateDiff-Lightning&#65306;&#36328;&#27169;&#22411;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
AnimateDiff-Lightning: Cross-Model Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12706
&lt;/p&gt;
&lt;p&gt;
AnimateDiff-Lightning&#27169;&#22411;&#21033;&#29992;&#28176;&#36827;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;&#65292;&#22312;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#33976;&#39311;&#22810;&#20010;&#22522;&#30784;&#25193;&#25955;&#27169;&#22411;&#27010;&#29575;&#27969;&#30340;&#26041;&#27861;&#65292;&#24418;&#25104;&#20102;&#20855;&#26377;&#26356;&#24191;&#27867;&#26679;&#24335;&#20860;&#23481;&#24615;&#30340;&#21333;&#19968;&#33976;&#39311;&#36816;&#21160;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;AnimateDiff-Lightning&#65292;&#29992;&#20110;&#24555;&#36895;&#29983;&#25104;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#28176;&#36827;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;&#65292;&#22312;&#23569;&#25968;&#27493;&#39588;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#20197;&#36866;&#24212;&#35270;&#39057;&#27169;&#24577;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#21516;&#26102;&#33976;&#39311;&#22810;&#20010;&#22522;&#30784;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#27969;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#26356;&#24191;&#27867;&#26679;&#24335;&#20860;&#23481;&#24615;&#30340;&#21333;&#19968;&#33976;&#39311;&#36816;&#21160;&#27169;&#22359;&#12290;&#25105;&#20204;&#24456;&#39640;&#20852;&#20026;&#31038;&#21306;&#21457;&#24067;&#25105;&#20204;&#33976;&#39311;&#30340;AnimateDiff-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12706v1 Announce Type: cross  Abstract: We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility. We are pleased to release our distilled AnimateDiff-Lightning model for the community's use.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#27454;&#20851;&#20110;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#23458;&#26435;&#21033;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24110;&#21161;&#26053;&#23458;&#29702;&#35299;&#21644;&#21033;&#29992;&#30456;&#20851;&#31354;&#20013;&#26053;&#34892;&#27861;&#35268;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#29992;&#25143;&#36755;&#20837;&#22797;&#26434;&#21644;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.12678</link><description>&lt;p&gt;
&#20026;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#34892;&#32773;&#36171;&#26435;&#65306;&#19968;&#27454;&#20851;&#20110;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#23458;&#26435;&#21033;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12678
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#27454;&#20851;&#20110;&#21152;&#25343;&#22823;&#31354;&#20013;&#26053;&#23458;&#26435;&#21033;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24110;&#21161;&#26053;&#23458;&#29702;&#35299;&#21644;&#21033;&#29992;&#30456;&#20851;&#31354;&#20013;&#26053;&#34892;&#27861;&#35268;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#29992;&#25143;&#36755;&#20837;&#22797;&#26434;&#21644;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25343;&#22823;&#33322;&#31354;&#26053;&#34892;&#39046;&#22495;&#30340;&#33322;&#29677;&#24310;&#35823;&#12289;&#21462;&#28040;&#21644;&#20854;&#20182;&#20851;&#20110;&#26053;&#23458;&#26435;&#21033;&#30340;&#38382;&#39064;&#26377;&#20102;&#26174;&#33879;&#22686;&#21152;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#21327;&#21161;&#26053;&#23458;&#24182;&#25945;&#32946;&#20182;&#20204;&#20102;&#35299;&#33258;&#24049;&#30340;&#26435;&#21033;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#22797;&#26434;&#30340;&#29992;&#25143;&#36755;&#20837;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#26597;&#35810;&#65292;&#29992;&#20110;&#26816;&#32034;&#35814;&#32454;&#31354;&#20013;&#26053;&#34892;&#27861;&#35268;&#30340;&#25991;&#26723;&#38598;&#20013;&#30340;&#20449;&#24687;&#12290;&#20174;&#36825;&#20123;&#25991;&#26723;&#20013;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#65292;&#24182;&#25552;&#20379;&#21407;&#22987;&#25991;&#26723;&#21644;&#29983;&#25104;&#30340;&#26597;&#35810;&#30340;&#38142;&#25509;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23558;&#20449;&#24687;&#32454;&#20998;&#24182;&#21033;&#29992;&#20110;&#20854;&#29420;&#29305;&#24773;&#20917;&#12290;&#35813;&#31995;&#32479;&#25104;&#21151;&#20811;&#26381;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#29702;&#35299;&#22797;&#26434;&#30340;&#29992;&#25143;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#31572;&#26696;&#65292;&#27809;&#26377;&#24187;&#35273;&#65292;&#36825;&#20123;&#31572;&#26696;&#21487;&#20197;&#20379;&#26053;&#23458;&#20381;&#36182;&#20197;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#19968;&#39033;&#27604;&#36739;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35895;&#27468;&#25628;&#32034;&#30340;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23454;&#29992;&#24615;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12678v1 Announce Type: cross  Abstract: The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#25913;&#36827;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#24230;&#37327;&#35774;&#32622;&#38408;&#20540;&#30340;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12672</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#20271;&#21162;&#21033;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#25913;&#36827;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#24230;&#37327;&#35774;&#32622;&#38408;&#20540;&#30340;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#20271;&#21162;&#21033;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;GBRBM&#65289;&#24120;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#22522;&#20110;GBRBM&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#26681;&#25454;&#36793;&#32536;GBRBM&#30340;&#33021;&#37327;&#20989;&#25968;&#30456;&#21516;&#30340;&#35780;&#20998;&#26469;&#23545;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#27861;&#35299;&#37322;&#35813;&#35780;&#20998;&#65292;&#24456;&#38590;&#35774;&#32622;&#36866;&#24403;&#30340;&#20998;&#31867;&#38408;&#20540;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#25913;&#36827;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#24230;&#37327;&#35774;&#32622;&#38408;&#20540;&#30340;&#20934;&#21017;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#28857;&#35774;&#32622;&#38408;&#20540;&#26102;&#65292;&#35813;&#20934;&#21017;&#26159;&#21512;&#29702;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35782;&#21035;&#24230;&#37327;&#28041;&#21450;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#26368;&#23567;&#35780;&#20998;&#20540;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#35780;&#20998;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12672v1 Announce Type: cross  Abstract: Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points. In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22686;&#24378; GitHub Copilot &#30340; AI &#20195;&#30721;&#21512;&#25104;&#23433;&#20840;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#25552;&#31034;&#25913;&#21464;&#26041;&#27861;&#65306;&#29305;&#23450;&#22330;&#26223;&#30340;&#12289;&#36845;&#20195;&#30340;&#21644;&#36890;&#29992;&#30340;&#20174;&#21477;&#65292;&#21516;&#26102;&#35752;&#35770;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.12671</link><description>&lt;p&gt;
&#36890;&#36807;&#24265;&#20215;&#39640;&#25928;&#30340;&#25552;&#31034;&#24037;&#31243;&#22686;&#24378; GitHub Copilot &#30340; AI &#20195;&#30721;&#21512;&#25104;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22686;&#24378; GitHub Copilot &#30340; AI &#20195;&#30721;&#21512;&#25104;&#23433;&#20840;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#25552;&#31034;&#25913;&#21464;&#26041;&#27861;&#65306;&#29305;&#23450;&#22330;&#26223;&#30340;&#12289;&#36845;&#20195;&#30340;&#21644;&#36890;&#29992;&#30340;&#20174;&#21477;&#65292;&#21516;&#26102;&#35752;&#35770;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#30340; AI &#21161;&#25163;&#27491;&#22312;&#20852;&#36215;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#20844;&#21496;&#36991;&#20813;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#23384;&#30097;&#12290;&#26412;&#25991;&#39318;&#20808;&#22238;&#39038;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#25913;&#21464;&#26041;&#27861;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20195;&#30721;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#26159;&#38024;&#23545;&#19987;&#26377;&#40657;&#30418;&#30340; AI &#20195;&#30721;&#29983;&#25104;&#22120;&#65292;&#22914; GitHub Copilot&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#29992;&#25143;&#35282;&#24230;&#12289;&#35745;&#31639;&#36164;&#28304;&#21644;&#36816;&#33829;&#25104;&#26412;&#30340;&#22797;&#26434;&#24615;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#25552;&#31034;&#25913;&#21464;&#26041;&#27861;&#65306;&#65288;1&#65289;&#29305;&#23450;&#22330;&#26223;&#30340;&#65292;&#65288;2&#65289;&#36845;&#20195;&#30340;&#65292;&#21644;&#65288;3&#65289;&#36890;&#29992;&#30340;&#20174;&#21477;&#65292;&#21516;&#26102;&#35752;&#35770;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#19982;&#23545;&#20195;&#30721;&#23433;&#20840;&#24615;&#30340;&#23457;&#35745;&#30456;&#21453;&#65292;&#26368;&#21518;&#20004;&#31181;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12671v1 Announce Type: cross  Abstract: AI assistants for coding are on the rise. However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code. This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue. Then, we propose a systematic approach based on prompt-altering methods to achieve better code security of (even proprietary black-box) AI-based code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs. In sum, we propose and evaluate three prompt altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination. Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user. We assess the effectiveness of the proposed methods on the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;cattleia&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#23494;&#29992;&#20110;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;AutoML&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35780;&#20272;&#25351;&#26631;&#21644;&#26032;&#24230;&#37327;&#25351;&#26631;&#65292;&#20998;&#26512;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2403.12664</link><description>&lt;p&gt;
&#35299;&#23494;AutoML&#38598;&#25104;&#65306;cattleia&#22312;&#20915;&#31574;&#20013;&#30340;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12664
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;cattleia&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#23494;&#29992;&#20110;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;AutoML&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35780;&#20272;&#25351;&#26631;&#21644;&#26032;&#24230;&#37327;&#25351;&#26631;&#65292;&#20998;&#26512;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#38598;&#25104;&#34987;&#35777;&#26126;&#27604;&#21333;&#20010;&#39044;&#27979;&#27169;&#22411;&#26356;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#20013;&#65292;&#23427;&#26159;&#26368;&#24120;&#35265;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;cattleia - &#19968;&#31181;&#33021;&#35299;&#23494;&#29992;&#20110;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#38598;&#25104;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#24037;&#20855;&#19982;&#19977;&#20010;AutoML&#21253;&#26500;&#24314;&#30340;&#27169;&#22411;&#19968;&#36215;&#24037;&#20316;&#65306;auto-sklearn&#12289;AutoGluon&#21644;FLAML&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#32473;&#23450;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#38598;&#25104;&#21450;&#20854;&#32452;&#20214;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#39044;&#27979;&#24615;&#33021;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#21644;&#20114;&#34917;&#24615;&#25193;&#23637;&#39564;&#35777;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#26469;&#26816;&#26597;v&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12664v1 Announce Type: cross  Abstract: In many applications, model ensembling proves to be better than a single predictive model. Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML). The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models. In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks. This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The given ensemble is analyzed from different perspectives. We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models. We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions. Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of v
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12660</link><description>&lt;p&gt;
ERASE&#65306;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12660
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;(DRS)&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#22823;&#37327;&#29305;&#24449;&#23383;&#27573;&#26469;&#25552;&#20379;&#26356;&#31934;&#20934;&#30340;&#25512;&#33616;&#12290;&#26377;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#20248;&#21270;&#23384;&#20648;&#25928;&#29575;&#65292;&#20197;&#28385;&#36275;&#37096;&#32626;&#38656;&#27714;&#12290;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;DRS&#30340;&#32972;&#26223;&#19979;&#65292;&#23578;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#38754;&#20020;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#23454;&#39564;&#35774;&#32622;&#30340;&#24046;&#24322;&#24448;&#24448;&#23548;&#33268;&#19981;&#20844;&#24179;&#27604;&#36739;&#65292;&#36974;&#34109;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#23646;&#24615;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#36873;&#25321;&#25216;&#26415;&#21644;DRS&#39592;&#24178;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#38480;&#21046;&#24615;&#25991;&#31456;&#30340;&#36890;&#29992;&#24615;&#30740;&#31350;&#21644;&#37096;&#32626;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#24448;&#24448;&#19987;&#27880;&#20110;&#27604;&#36739;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21487;&#36798;&#21040;&#30340;&#23792;&#20540;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#22312;&#35745;&#31639;&#26041;&#38754;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20852;&#36259;&#26694;Embedding&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12649</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20852;&#36259;&#26694;Embedding&#30340;&#25512;&#33616;&#31995;&#32479;InBox
&lt;/p&gt;
&lt;p&gt;
InBox: Recommendation with Knowledge Graph using Interest Box Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20852;&#36259;&#26694;Embedding&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;(KGs)&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#20852;&#36259;&#23545;&#24212;&#20110;&#28508;&#22312;&#25968;&#37327;&#24222;&#22823;&#30340;&#30456;&#20851;&#39033;&#30446;&#38598;&#65292;2&#65289;&#23545;KG&#20449;&#24687;&#21644;&#20852;&#36259;&#36830;&#25509;&#24615;&#32570;&#20047;&#26126;&#30830;&#12289;&#32454;&#31890;&#24230;&#30340;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;embedding&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12649v1 Announce Type: cross  Abstract: Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability. Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of knowledge graph data and interest connectivity. To address these limitations, we introduce a novel embedding-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33137;&#39537;&#21160;&#36719;&#26426;&#22120;&#20154;&#25163;&#22871;&#30340;&#22522;&#20110;&#28857;&#20113;&#30340;&#25235;&#21462;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#35937;&#20960;&#20309;&#24418;&#29366;&#26469;&#25903;&#25345;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#20013;&#30340;&#23450;&#21046;&#25235;&#21462;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.12631</link><description>&lt;p&gt;
PointGrasp&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#29992;&#20110;&#33137;&#39537;&#21160;&#36719;&#26426;&#22120;&#20154;&#25163;&#22871;&#24212;&#29992;&#30340;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic Glove Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33137;&#39537;&#21160;&#36719;&#26426;&#22120;&#20154;&#25163;&#22871;&#30340;&#22522;&#20110;&#28857;&#20113;&#30340;&#25235;&#21462;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#35937;&#20960;&#20309;&#24418;&#29366;&#26469;&#25903;&#25345;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#20013;&#30340;&#23450;&#21046;&#25235;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#25163;&#37096;&#22806;&#39592;&#39612;&#26469;&#36741;&#21161;&#36827;&#34892;&#25235;&#21462;&#20219;&#21153;&#23545;&#20110;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#30340;&#22256;&#38590;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#65292;&#36890;&#36807;&#20174;3D&#28857;&#20113;&#20013;&#20998;&#26512;&#23545;&#35937;&#30340;&#20960;&#20309;&#24418;&#29366;&#65288;&#31616;&#21333;&#21644;&#22797;&#26434;&#65289;&#65292;&#21487;&#20197;&#25512;&#26029;&#22823;&#22810;&#25968;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#20013;&#30340;&#25235;&#21462;&#20219;&#21153;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;PointGrasp&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23478;&#24237;&#22330;&#26223;&#30340;&#35821;&#20041;&#26469;&#25903;&#25345;&#21644;&#22686;&#24378;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#20013;&#23545;&#23450;&#21046;&#31471;&#21040;&#31471;&#25235;&#21462;&#20219;&#21153;&#30340;&#36741;&#21161;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;RGB-D&#30456;&#26426;&#65292;&#19968;&#20010;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#21644;&#19968;&#20010;&#38598;&#25104;&#22312;&#33137;&#39537;&#21160;&#36719;&#26426;&#22120;&#20154;&#25163;&#22871;&#20013;&#30340;&#24494;&#22788;&#29702;&#22120;&#12290;RGB-D&#30456;&#26426;&#20197;&#36229;&#36807;30&#24103;/&#31186;&#30340;&#36895;&#29575;&#22788;&#29702;3D&#22330;&#26223;&#12290;&#25552;&#20986;&#30340;&#27969;&#31243;&#23637;&#31034;&#20102;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#30340;&#24179;&#22343;RMSE&#20026;0.8 &#177; 0.39&#21400;&#31859;&#65292;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#30340;&#20026;0.11 &#177; 0.06&#21400;&#31859;&#12290;&#22312;&#27599;&#31181;&#27169;&#24335;&#19979;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#21644;&#23450;&#20301;&#21487;&#25235;&#21462;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12631v1 Announce Type: cross  Abstract: Controlling hand exoskeletons to assist individuals with grasping tasks poses a challenge due to the difficulty in understanding user intentions. We propose that most daily grasping tasks during activities of daily living (ADL) can be deduced by analyzing object geometries (simple and complex) from 3D point clouds. The study introduces PointGrasp, a real-time system designed for identifying household scenes semantically, aiming to support and enhance assistance during ADL for tailored end-to-end grasping tasks. The system comprises an RGB-D camera with an inertial measurement unit and a microprocessor integrated into a tendon-driven soft robotic glove. The RGB-D camera processes 3D scenes at a rate exceeding 30 frames per second. The proposed pipeline demonstrates an average RMSE of 0.8 $\pm$ 0.39 cm for simple and 0.11 $\pm$ 0.06 cm for complex geometries. Within each mode, it identifies and pinpoints reachable objects. This system sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#21644;&#29983;&#25104;Coq&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25512;&#21160;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12627</link><description>&lt;p&gt;
&#21152;&#24378;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65306;&#29992;&#20110;&#22312;Coq&#20195;&#30721;&#19978;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#21644;&#29983;&#25104;Coq&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25512;&#21160;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#65292;Coq&#35777;&#26126;&#36741;&#21161;&#24037;&#20855;&#20197;&#20854;&#23545;&#39564;&#35777;&#25968;&#23398;&#26029;&#35328;&#21644;&#36719;&#20214;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#26041;&#27861;&#33073;&#39040;&#32780;&#20986;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;Coq&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#29305;&#27530;&#24615;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#35299;&#37322;&#21644;&#29983;&#25104;Coq&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#35813;&#25968;&#25454;&#38598;&#28304;&#33258;&#19968;&#32452;&#36229;&#36807;10,000&#20010;Coq&#28304;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#21629;&#39064;&#12289;&#35777;&#26126;&#21644;&#23450;&#20041;&#65292;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21253;&#25324;&#28304;&#24341;&#29992;&#21644;&#35768;&#21487;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#19988;&#35821;&#20041;&#20016;&#23500;&#30340;Coq&#26500;&#36896;&#30340;LLMs&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#25512;&#36827;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#21069;&#27839;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12627v1 Announce Type: new  Abstract: In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness. Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs). Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code. This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information. Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving. Initial experiments with this datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#36275;&#37096;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#32447;&#25512;&#29702;&#30340;&#35745;&#31639;&#38656;&#27714;&#26497;&#20302;&#65292;&#26080;&#21551;&#21457;&#24335;&#65292;&#24182;&#20381;&#36182;&#36830;&#32493;&#30340;&#21160;&#20316;&#29983;&#25104;&#21487;&#34892;&#30340;&#36275;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.12589</link><description>&lt;p&gt;
FootstepNet: &#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#22312;&#32447;&#21452;&#36275;&#36275;&#37096;&#27493;&#24577;&#35268;&#21010;&#21644;&#39044;&#27979;&#30340;&#39640;&#25928;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#36275;&#37096;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#32447;&#25512;&#29702;&#30340;&#35745;&#31639;&#38656;&#27714;&#26497;&#20302;&#65292;&#26080;&#21551;&#21457;&#24335;&#65292;&#24182;&#20381;&#36182;&#36830;&#32493;&#30340;&#21160;&#20316;&#29983;&#25104;&#21487;&#34892;&#30340;&#36275;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#20154;&#24418;&#36816;&#21160;&#25511;&#21046;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36890;&#24120;&#20998;&#20026;&#20960;&#20010;&#23376;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#36275;&#37096;&#27493;&#24577;&#35268;&#21010;&#65292;&#20854;&#20013;&#23450;&#20041;&#20102;&#36275;&#27493;&#30340;&#39034;&#24207;&#12290;&#21363;&#20351;&#22312;&#26356;&#31616;&#21333;&#30340;&#29615;&#22659;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#26368;&#23567;&#24207;&#21015;&#65292;&#29978;&#33267;&#19968;&#20010;&#21487;&#34892;&#30340;&#24207;&#21015;&#65292;&#20063;&#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;&#20363;&#22914;A*&#30340;&#21464;&#31181;&#65289;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#65292;&#35201;&#20040;&#20381;&#36182;&#25163;&#24037;&#35843;&#25972;&#22810;&#20010;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27493;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#23616;&#37096;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25216;&#26415;&#65292;&#23545;&#22312;&#32447;&#25512;&#29702;&#35201;&#27714;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#21551;&#21457;&#30340;&#65292;&#20381;&#36182;&#20110;&#19968;&#32452;&#36830;&#32493;&#30340;&#21160;&#20316;&#26469;&#29983;&#25104;&#21487;&#34892;&#30340;&#36275;&#27493;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20854;&#20182;&#26041;&#27861;&#38656;&#35201;&#36873;&#25321;&#19968;&#32452;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12589v1 Announce Type: cross  Abstract: Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a rel
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.12588</link><description>&lt;p&gt;
&#20027;&#35201;&#20998;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning of the Prime Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12588
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#21253;&#25324;&#21704;&#20195;-&#25289;&#39532;&#21162;&#37329;&#23450;&#29702;&#30340;&#19968;&#20010;&#29256;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#35770;&#35777;&#65292;&#35299;&#37322;&#20102;Y.-H. He&#20851;&#20110;&#32032;&#25968;&#21487;&#23398;&#24615;&#30340;&#23454;&#39564;&#35266;&#23519;&#65292;&#24182;&#20551;&#35774;Erd\H{o}s-Kac&#23450;&#24459;&#26497;&#19981;&#21487;&#33021;&#34987;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12588v1 Announce Type: cross  Abstract: In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.12574</link><description>&lt;p&gt;
EAS-SNN&#65306;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#34920;&#31034;&#65292;&#29992;&#20110;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25668;&#20687;&#22836;&#20197;&#20854;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#21160;&#24577;&#27169;&#31946;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#29031;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#27880;&#37325;&#20248;&#21270;&#20855;&#26377;&#20808;&#36827;&#26816;&#27979;&#39592;&#24178;&#21644;&#26089;&#26399;&#32858;&#21512;&#21151;&#33021;&#30340;&#26102;&#31354;&#34920;&#31034;&#65292;&#32780;&#33258;&#36866;&#24212;&#20107;&#20214;&#37319;&#26679;&#30340;&#20851;&#38190;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#33033;&#20914;&#36890;&#20449;&#36816;&#34892;&#30340;&#20107;&#20214;&#39537;&#21160;&#33539;&#24335;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#22825;&#28982;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#23494;&#20999;&#30456;&#31526;&#12290;&#22312;&#36825;&#19968;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#35760;&#24518;&#30340;&#24490;&#29615;&#21367;&#31215;SNN&#22686;&#24378;&#65292;&#20026;&#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#23436;&#20840;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12574v1 Announce Type: cross  Abstract: Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#21512;&#34920;&#24773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#32593;&#32476;&#12289;&#35270;&#35273;Transformer&#21644;&#22810;&#23610;&#24230;&#23616;&#37096;&#27880;&#24847;&#32593;&#32476;&#65292;&#36890;&#36807;&#24310;&#36831;&#34701;&#21512;&#30340;&#26041;&#24335;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12572</link><description>&lt;p&gt;
&#22810;&#27169;&#22411;&#38598;&#25104;&#23454;&#29616;&#22797;&#21512;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Compound Expression Recognition via Multi Model Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12572
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#21512;&#34920;&#24773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#32593;&#32476;&#12289;&#35270;&#35273;Transformer&#21644;&#22810;&#23610;&#24230;&#23616;&#37096;&#27880;&#24847;&#32593;&#32476;&#65292;&#36890;&#36807;&#24310;&#36831;&#34701;&#21512;&#30340;&#26041;&#24335;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#34920;&#24773;&#35782;&#21035;&#22312;&#20154;&#38469;&#20114;&#21160;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#22797;&#21512;&#34920;&#24773;&#30340;&#23384;&#22312;&#65292;&#20154;&#31867;&#24773;&#32490;&#34920;&#36798;&#21464;&#24471;&#22797;&#26434;&#65292;&#38656;&#35201;&#32771;&#34385;&#23616;&#37096;&#21644;&#20840;&#23616;&#38754;&#37096;&#34920;&#24773;&#26469;&#20570;&#20986;&#21028;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#21512;&#34920;&#24773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#20998;&#31867;&#65292;&#25105;&#20204;&#22522;&#20110;&#21367;&#31215;&#32593;&#32476;&#12289;&#35270;&#35273;Transformer&#21644;&#22810;&#23610;&#24230;&#23616;&#37096;&#27880;&#24847;&#32593;&#32476;&#35757;&#32451;&#19977;&#20010;&#34920;&#24773;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#24310;&#36831;&#34701;&#21512;&#30340;&#27169;&#22411;&#38598;&#25104;&#65292;&#25105;&#20204;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#39044;&#27979;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;RAF-DB&#19978;&#23454;&#29616;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;C-EXPR-DB&#30340;&#26576;&#20123;&#37096;&#20998;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35782;&#21035;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12572v1 Announce Type: cross  Abstract: Compound Expression Recognition (CER) plays a crucial role in interpersonal interactions. Due to the existence of Compound Expressions , human emotional expressions are complex, requiring consideration of both local and global facial expressions to make judgments. In this paper, to address this issue, we propose a solution based on ensemble learning methods for Compound Expression Recognition. Specifically, our task is classification, where we train three expression classification models based on convolutional networks, Vision Transformers, and multi-scale local attention networks. Then, through model ensemble using late fusion, we merge the outputs of multiple models to predict the final result. Our method achieves high accuracy on RAF-DB and is able to recognize expressions through zero-shot on certain portions of C-EXPR-DB.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;TrustZone&#20013;&#36827;&#34892;&#39640;&#32423;&#27169;&#22411;&#37096;&#32626;&#30340;&#26032;&#26041;&#27861;&#65292;&#30830;&#20445;&#27169;&#22411;&#25512;&#26029;&#26399;&#38388;&#20840;&#38754;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2403.12568</link><description>&lt;p&gt;
&#22522;&#20110;TrustZone&#21551;&#29992;&#30340;&#28040;&#36153;&#32773;IoT&#35774;&#22791;&#19978;&#30340;&#20869;&#23384;&#39640;&#25928;&#21644;&#23433;&#20840;&#30340;DNN&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;TrustZone&#20013;&#36827;&#34892;&#39640;&#32423;&#27169;&#22411;&#37096;&#32626;&#30340;&#26032;&#26041;&#27861;&#65292;&#30830;&#20445;&#27169;&#22411;&#25512;&#26029;&#26399;&#38388;&#20840;&#38754;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#20351;&#36164;&#28304;&#38656;&#27714;&#39640;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#20013;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#12290;&#38024;&#23545;&#38544;&#31169;&#25935;&#24863;&#22411;&#24212;&#29992;&#65292;&#23558;&#27169;&#22411;&#37096;&#32626;&#22312;&#30828;&#20214;&#38548;&#31163;&#30340;&#21463;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEEs&#65289;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;TEE&#20013;&#26377;&#38480;&#30340;&#23433;&#20840;&#20869;&#23384;&#23545;&#20110;&#37096;&#32626;DNN&#25512;&#26029;&#26500;&#25104;&#25361;&#25112;&#65292;&#32780;&#35832;&#22914;&#27169;&#22411;&#20998;&#21306;&#21644;&#21368;&#36733;&#31561;&#26367;&#20195;&#25216;&#26415;&#21017;&#24341;&#20837;&#20102;&#24615;&#33021;&#38477;&#32423;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;TrustZone&#20013;&#23454;&#29616;&#20808;&#36827;&#30340;&#27169;&#22411;&#37096;&#32626;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#25512;&#26029;&#26399;&#38388;&#20840;&#38754;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#31649;&#29702;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;TEE&#20013;&#30340;&#20869;&#23384;&#38656;&#27714;&#25512;&#26029;&#12290;&#36890;&#36807;&#35843;&#25972;&#20869;&#23384;&#20248;&#20808;&#32423;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20869;&#23384;&#27844;&#28431;&#39118;&#38505;&#21644;&#20869;&#23384;&#37325;&#21472;&#20914;&#31361;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;32&#34892;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12568v1 Announce Type: cross  Abstract: Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices. For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential. However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues. In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference. We design a memory-efficient management method to support memory-demanding inference in TEEs. By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#26377;&#38480;&#36164;&#28304;&#19978;&#36880;&#27493;&#25191;&#34892;&#39640;&#25928;&#21160;&#24577;&#30340;HPO&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#37325;&#23558;Transformers&#27169;&#22411;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#31616;&#21333;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2403.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#21644;&#20869;&#23384;&#21463;&#38480;&#30340;GPU&#26381;&#21153;&#19978;&#30340;&#22823;&#25991;&#26412;&#20998;&#31867;&#30340;Transformer&#31616;&#21333;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#26377;&#38480;&#36164;&#28304;&#19978;&#36880;&#27493;&#25191;&#34892;&#39640;&#25928;&#21160;&#24577;&#30340;HPO&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#37325;&#23558;Transformers&#27169;&#22411;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#31616;&#21333;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;NLP&#30740;&#31350;&#20154;&#21592;&#20381;&#36182;&#20813;&#36153;&#30340;&#35745;&#31639;&#26381;&#21153;&#65292;&#22914;Google Colab&#65292;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;Transformer&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#35813;&#26041;&#27861;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24615;&#24182;&#38656;&#35201;&#26356;&#22823;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#38271;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#21360;&#23612;&#65292;&#20165;&#21457;&#29616;&#20102;&#23569;&#37327;&#20851;&#20110;&#20351;&#29992;Transformer&#36827;&#34892;&#38271;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#27809;&#26377;&#25253;&#21578;&#20219;&#20309;HPO&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;18k&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#35789;&#22120;&#36755;&#20986;&#38271;&#24230;&#24314;&#35758;&#20351;&#29992;&#21738;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#20123;&#32553;&#30701;&#21644;&#20016;&#23500;&#24207;&#21015;&#30340;&#25216;&#24039;&#65292;&#21253;&#25324;&#20572;&#29992;&#35789;&#12289;&#26631;&#28857;&#31526;&#21495;&#12289;&#20302;&#39057;&#35789;&#21644;&#37325;&#22797;&#35789;&#30340;&#21435;&#38500;&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#36816;&#34892;&#20102;&#19968;&#31181;&#39640;&#25928;&#21160;&#24577;&#30340;HPO&#36807;&#31243;&#65292;&#21487;&#20197;&#36880;&#27493;&#22312;&#26377;&#38480;&#36164;&#28304;&#19978;&#36827;&#34892;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#38271;&#26102;&#38388;&#36816;&#34892;&#30340;&#20248;&#21270;&#24211;&#12290;&#21033;&#29992;&#26368;&#20339;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12563v1 Announce Type: cross  Abstract: Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best ha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12562</link><description>&lt;p&gt;
&#36890;&#36807;&#33719;&#21462;&#36171;&#26435;&#65306;&#25903;&#25345;&#23567;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Equity through Access: A Case for Small-scale Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24471;&#30410;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#35745;&#31639;&#21147;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#22823;&#35268;&#27169;&#36164;&#28304;&#34987;&#29992;&#20110;&#35757;&#32451;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35745;&#31639;&#12289;&#25968;&#25454;&#12289;&#33021;&#28304;&#21644;&#30899;&#25490;&#25918;&#26041;&#38754;&#28040;&#32791;&#24040;&#22823;&#12290;&#36825;&#20123;&#25104;&#26412;&#27491;&#22312;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#26032;&#22411;&#20934;&#20837;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#22312;&#20840;&#29699;&#21335;&#26041;&#22320;&#21306;&#36164;&#28304;&#26377;&#38480;&#30340;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#35270;&#20102;&#29616;&#26377;&#35270;&#35273;&#20219;&#21153;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32771;&#34385;DL&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#24615;&#33021;&#19982;&#36164;&#28304;&#21333;&#20803;&#30340;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;PePR&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#65288;&#36328;&#24230;&#20174;1M&#21040;130M&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#65289;&#21644;&#19977;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#26377;&#20851;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20851;&#31995;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#21387;&#22120;(M2DA)&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#65292;&#36890;&#36807;&#24341;&#20837;LVAFusion&#27169;&#22359;&#21644;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25968;&#25454;&#34701;&#21512;&#25928;&#29575;&#21644;&#20154;&#31867;&#21270;&#30340;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12552</link><description>&lt;p&gt;
M2DA: &#34701;&#21512;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#21387;&#22120;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#21387;&#22120;(M2DA)&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#65292;&#36890;&#36807;&#24341;&#20837;LVAFusion&#27169;&#22359;&#21644;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25968;&#25454;&#34701;&#21512;&#25928;&#29575;&#21644;&#20154;&#31867;&#21270;&#30340;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#31471;&#21040;&#31471;&#27169;&#24335;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;1) &#22810;&#27169;&#24577;&#29615;&#22659;&#24863;&#30693;&#20302;&#25928;&#65306;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#38598;&#25104;&#26469;&#33258;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65307;2) &#38750;&#20154;&#31867;&#33324;&#30340;&#22330;&#26223;&#29702;&#35299;&#65306;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#23450;&#20301;&#21644;&#39044;&#27979;&#20851;&#38190;&#30340;&#39118;&#38505;&#20195;&#29702;&#20154;&#65292;&#20687;&#26377;&#32463;&#39564;&#30340;&#39550;&#39542;&#21592;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#21464;&#21387;&#22120; (M2DA) &#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#24182;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#26356;&#39640;&#30340;&#23545;&#40784;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;-&#35270;&#35273;-&#27880;&#24847;&#21147;&#34701;&#21512; (LVAFusion) &#27169;&#22359;&#12290;&#36890;&#36807;&#24341;&#20837;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#36171;&#20104;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#31867;&#20284;&#20154;&#31867;&#30340;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#65292;&#33021;&#22815;&#31934;&#30830;&#35782;&#21035;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20851;&#38190;&#21306;&#22495;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12552v1 Announce Type: cross  Abstract: End-to-end autonomous driving has witnessed remarkable progress. However, the extensive deployment of autonomous vehicles has yet to be realized, primarily due to 1) inefficient multi-modal environment perception: how to integrate data from multi-modal sensors more efficiently; 2) non-human-like scene understanding: how to effectively locate and predict critical risky agents in traffic scenarios like an experienced driver. To overcome these challenges, in this paper, we propose a Multi-Modal fusion transformer incorporating Driver Attention (M2DA) for autonomous driving. To better fuse multi-modal data and achieve higher alignment between different modalities, a novel Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By incorporating driver attention, we empower the human-like scene understanding ability to autonomous vehicles to identify crucial areas within complex scenarios precisely and ensure safety. We conduct e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#20132;&#20114;&#27010;&#24565;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12533</link><description>&lt;p&gt;
&#26159;&#21542;&#24110;&#21161;&#65306;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#19982;&#20154;&#26426;&#32676;&#20307;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#20132;&#20114;&#27010;&#24565;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#20154;&#31867;&#32676;&#20307;&#20013;&#25552;&#20379;&#19981;&#24341;&#20154;&#27880;&#30446;&#30340;&#29289;&#29702;&#25903;&#25345;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;Attentive Support&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#32676;&#20307;&#36827;&#34892;&#25903;&#25345;&#30340;&#20132;&#20114;&#27010;&#24565;&#12290;&#23427;&#23558;&#22330;&#26223;&#24863;&#30693;&#12289;&#23545;&#35805;&#33719;&#21462;&#12289;&#24773;&#20917;&#29702;&#35299;&#21644;&#34892;&#20026;&#29983;&#25104;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;Attentive Support&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25903;&#25345;&#20154;&#31867;&#65292;&#24182;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#26102;&#20445;&#25345;&#27785;&#40664;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#21644;&#35780;&#20272;&#20102;&#26426;&#22120;&#20154;&#30340;&#19987;&#27880;&#34892;&#20026;&#65292;&#24403;&#38656;&#35201;&#26102;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#65292;&#32780;&#22914;&#26524;&#19981;&#38656;&#35201;&#24110;&#21161;&#65292;&#21017;&#19981;&#20250;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12533v1 Announce Type: cross  Abstract: How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphERE&#30340;&#22810;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#65292;&#25193;&#23637;&#20102;&#20107;&#20214;&#23884;&#20837;&#30340;&#20107;&#20214;&#21442;&#25968;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20107;&#20214;&#35302;&#21457;&#22120;&#23884;&#20837;&#30340;&#23616;&#38480;&#20197;&#21450;&#20851;&#31995;&#20043;&#38388;&#20114;&#36830;&#34987;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12523</link><description>&lt;p&gt;
GraphERE: &#22522;&#20110;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#30340;&#22810;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12523
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphERE&#30340;&#22810;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#65292;&#25193;&#23637;&#20102;&#20107;&#20214;&#23884;&#20837;&#30340;&#20107;&#20214;&#21442;&#25968;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20107;&#20214;&#35302;&#21457;&#22120;&#23884;&#20837;&#30340;&#23616;&#38480;&#20197;&#21450;&#20851;&#31995;&#20043;&#38388;&#20114;&#36830;&#34987;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25551;&#36848;&#23454;&#20307;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;&#22312;&#25991;&#26723;&#20013;&#65292;&#22810;&#20010;&#20107;&#20214;&#36890;&#36807;&#21508;&#31181;&#20851;&#31995;&#30456;&#20114;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;&#20849;&#25351;&#12289;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#23376;&#20107;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#21462;&#65288;ERE&#65289;&#33719;&#21462;&#20107;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#23545;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;ERE&#24037;&#20316;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;a. &#20165;&#20351;&#29992;&#20107;&#20214;&#35302;&#21457;&#22120;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#20107;&#20214;&#29305;&#24449;&#65292;&#24573;&#30053;&#20107;&#20214;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;&#12289;&#22320;&#28857;&#12289;&#20154;&#29289;&#31561;&#65289;&#21450;&#20854;&#22312;&#20107;&#20214;&#20869;&#30340;&#32467;&#26500;&#12290;b. &#20851;&#31995;&#20043;&#38388;&#30340;&#20114;&#36830;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;&#21644;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#20250;&#30456;&#20114;&#24433;&#21709;&#65289;&#34987;&#24573;&#30053;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphERE&#22522;&#20110;&#22270;&#22686;&#24378;&#20107;&#20214;&#23884;&#20837;&#30340;&#22810;&#37325;ERE&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#38745;&#24577;AMR&#22270;&#21644;IE&#22270;&#20016;&#23500;&#20107;&#20214;&#23884;&#20837;&#30340;&#20107;&#20214;&#21442;&#25968;&#21644;&#32467;&#26500;&#29305;&#24449;&#65307;&#28982;&#21518;&#65292;&#20026;&#20102;&#32852;&#21512;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12523v1 Announce Type: cross  Abstract: Events describe the state changes of entities. In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent). Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language. There are two main problems in the current ERE works: a. Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event. b. The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored. To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.12503</link><description>&lt;p&gt;
&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23041;&#32961;&#12289;&#28431;&#27934;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26684;&#23616;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#22788;&#29702;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23427;&#20204;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#29992;&#24615;&#22806;&#65292;LLMs&#36824;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#32771;&#34385;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65292;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;LLMs&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20174;&#20116;&#20010;&#20027;&#39064;&#35282;&#24230;&#36827;&#34892;&#65306;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12289;LLMs&#35823;&#29992;&#21487;&#33021;&#36896;&#25104;&#30340;&#28508;&#22312;&#21361;&#23475;&#12289;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#35782;&#21035;&#24403;&#21069;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12503v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
&lt;/p&gt;</description></item><item><title>DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.12488</link><description>&lt;p&gt;
DetToolChain&#65306;&#19968;&#31181;&#37322;&#25918;MLLM&#26816;&#27979;&#33021;&#21147;&#30340;&#26032;&#25552;&#31034;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12488
&lt;/p&gt;
&lt;p&gt;
DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DetToolChain&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#29992;&#20110;&#37322;&#25918;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#22914;GPT-4V&#21644;Gemini&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21463;&#39640;&#31934;&#24230;&#26816;&#27979;&#20808;&#39564;&#21551;&#21457;&#30340;&#26816;&#27979;&#25552;&#31034;&#24037;&#20855;&#21253;&#21644;&#19968;&#20010;&#23454;&#29616;&#36825;&#20123;&#25552;&#31034;&#30340;&#26032;Chain-of-Thought&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24037;&#20855;&#21253;&#20013;&#30340;&#25552;&#31034;&#26088;&#22312;&#24341;&#23548;MLLM&#38598;&#20013;&#22312;&#21306;&#22495;&#20449;&#24687;&#19978;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#65292;&#25353;&#29031;&#27979;&#37327;&#26631;&#20934;&#38405;&#35835;&#22352;&#26631;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#26631;&#23610;&#21644;&#25351;&#21335;&#38024;&#65289;&#65292;&#24182;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#25512;&#26029;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#22330;&#26223;&#22270;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#24037;&#20855;&#65292;&#26032;&#30340;&#26816;&#27979;Chain-of-Thought&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#35786;&#26029;&#39044;&#27979;&#65292;&#24182;&#35268;&#21010;&#36880;&#27493;&#26694;&#32454;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#31995;&#21015;&#26816;&#27979;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12488v1 Announce Type: cross  Abstract: We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12486</link><description>&lt;p&gt;
&#22522;&#20110;NTK&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NTK-Guided Few-Shot Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21453;&#36951;&#24536;FSCIL&#23398;&#20064;&#32773;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20182;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#20943;&#23569;&#30693;&#35782;&#27969;&#22833;&#65292;&#32780;&#24573;&#35270;&#20102;&#27169;&#22411;&#28508;&#22312;&#33719;&#21462;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#35270;&#35282;&#28145;&#20837;&#25506;&#35752;&#20102;FSCIL&#27169;&#22411;&#27867;&#21270;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#20027;&#35201;&#30340;&#35774;&#35745;&#37325;&#28857;&#22312;&#20110;&#30830;&#20445;&#26368;&#20248;NTK&#25910;&#25947;&#21644;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#20316;&#20026;&#21331;&#36234;&#27867;&#21270;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;NTK&#25910;&#25947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26893;&#26681;&#20110;&#25968;&#23398;&#21407;&#29702;&#30340;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#25351;&#23548;&#25193;&#23637;&#32593;&#32476;&#20869;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#20174;&#22522;&#30784;&#23618;&#38754;&#24320;&#22987;&#65292;&#20248;&#21270;&#26500;&#25104;&#20854;&#27867;&#21270;&#25439;&#22833;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#30784;&#20250;&#35805;&#19978;&#21551;&#21160;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#22609;&#36896;&#21021;&#22987;ne
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.12482</link><description>&lt;p&gt;
&#20855;&#36523;LLM&#20195;&#29702;&#22312;&#32452;&#32455;&#22242;&#38431;&#20013;&#23398;&#20250;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Embodied LLM Agents Learn to Cooperate in Organized Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#20915;&#31574;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#30340;&#29087;&#32451;&#24230;&#12290;LLMs&#22240;&#27492;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#24448;&#24448;&#20250;&#36807;&#24230;&#25253;&#21578;&#24182;&#36981;&#20174;&#20219;&#20309;&#25351;&#20196;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22810;&#20195;&#29702;&#21512;&#20316;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#28151;&#20081;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#23637;&#31034;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#20182;&#20204;&#30340;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12482v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#31163;&#32447;&#31995;&#32479;&#20013;&#37325;&#26032;&#35268;&#21010;&#21644;&#26144;&#23556;&#30340;&#38656;&#27714;&#20197;&#21450;&#22312;&#32447;&#31995;&#32479;&#20013;&#21160;&#24577;&#24863;&#24212;&#25968;&#25454;&#35201;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12463</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning based local path planning for mobile robot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12463
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#31163;&#32447;&#31995;&#32479;&#20013;&#37325;&#26032;&#35268;&#21010;&#21644;&#26144;&#23556;&#30340;&#38656;&#27714;&#20197;&#21450;&#22312;&#32447;&#31995;&#32479;&#20013;&#21160;&#24577;&#24863;&#24212;&#25968;&#25454;&#35201;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#35753;&#31227;&#21160;&#26426;&#22120;&#20154;&#21040;&#36798;&#29305;&#23450;&#30446;&#26631;&#20301;&#32622;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#22330;&#26223;&#20013;&#20197;&#19981;&#21516;&#26041;&#24335;&#24037;&#20316;&#12290;&#22312;&#31163;&#32447;&#22330;&#26223;&#20013;&#65292;&#29615;&#22659;&#22320;&#22270;&#34987;&#21019;&#24314;&#19968;&#27425;&#65292;&#28982;&#21518;&#22312;&#35813;&#22320;&#22270;&#19978;&#36827;&#34892;&#22266;&#23450;&#36335;&#24452;&#35268;&#21010;&#20197;&#21040;&#36798;&#30446;&#26631;&#12290;A*&#21644;RRT&#65288;Rapidly-Exploring Random Tree&#65289;&#31561;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26159;&#31163;&#32447;&#26041;&#27861;&#30340;&#20363;&#23376;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#22312;&#32447;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#20256;&#36755;&#30340;&#24863;&#30693;&#25968;&#25454;&#21160;&#24577;&#31227;&#21160;&#21040;&#32473;&#23450;&#30446;&#26631;&#65292;&#32780;&#19981;&#20351;&#29992;&#22320;&#22270;&#12290;&#22312;&#32447;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;SFM&#65288;Social Force Model&#65289;&#65292;&#35201;&#27714;&#22823;&#37327;&#21160;&#24577;&#24863;&#24212;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#35828;&#22312;&#31163;&#32447;&#31995;&#32479;&#20013;&#38656;&#35201;&#37325;&#26032;&#35268;&#21010;&#21644;&#26144;&#23556;&#65292;&#32780;&#22312;&#32447;&#31995;&#32479;&#20013;&#23384;&#22312;&#21508;&#31181;&#31995;&#32479;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12463v1 Announce Type: cross  Abstract: Different methods are used for a mobile robot to go to a specific target location. These methods work in different ways for online and offline scenarios. In the offline scenario, an environment map is created once, and fixed path planning is made on this map to reach the target. Path planning algorithms such as A* and RRT (Rapidly-Exploring Random Tree) are the examples of offline methods. The most obvious situation here is the need to re-plan the path for changing conditions of the loaded map. On the other hand, in the online scenario, the robot moves dynamically to a given target without using a map by using the perceived data coming from the sensors. Approaches such as SFM (Social Force Model) are used in online systems. However, these methods suffer from the requirement of a lot of dynamic sensing data. Thus, it can be said that the need for re-planning and mapping in offline systems and various system design requirements in online
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;RTD&#26469;&#34913;&#37327;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#19979;&#22797;&#21457;&#24615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNN&#65289;&#27169;&#22411;&#20998;&#24067;&#24335;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.12462</link><description>&lt;p&gt;
&#22797;&#21457;&#24615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#24322;&#36136;&#23398;&#20064;&#21160;&#24577;&#30340;&#25299;&#25169;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;RTD&#26469;&#34913;&#37327;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#19979;&#22797;&#21457;&#24615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNN&#65289;&#27169;&#22411;&#20998;&#24067;&#24335;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#25104;&#20026;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#37325;&#35201;&#33539;&#24335;&#65292;&#25552;&#20379;&#20102;&#31867;&#20284;&#22823;&#33041;&#30340;&#35745;&#31639;&#12290;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#36827;&#23637;&#24050;&#32463;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23545;SNNs&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#30740;&#31350;&#24456;&#23569;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#31867;&#20284;&#26102;&#24207;&#30456;&#20851;&#21487;&#22609;&#24615;&#65288;STDP&#65289;&#30340;&#26080;&#30417;&#30563;&#23616;&#37096;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;\cite{barannikov2021representation}&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#27604;&#36739;&#23398;&#20064;&#34920;&#31034;&#30340;&#25299;&#25169;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#25299;&#25169;&#31163;&#25955;&#24230;&#65288;RTD&#65289;&#12290;&#34429;&#28982;&#26377;&#29992;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#38024;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#19981;&#33021;&#29992;&#20110;&#20687;&#22797;&#21457;&#24615;SNNs&#65288;RSNNs&#65289;&#36825;&#26679;&#30340;&#24490;&#29615;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;RTD&#26469;&#34913;&#37327;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#30340;RSNN&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12462v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation. Recent advances in literature have studied the network representations of deep neural networks. However, there has been little work that studies representations learned by SNNs, especially using unsupervised local learning methods like spike-timing dependent plasticity (STDP). Recent work by \cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD). Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods. We propos
&lt;/p&gt;</description></item><item><title>&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.12459</link><description>&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-negative Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12459
&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#22312;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#23545;&#20154;&#31867;&#29702;&#35299;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;&#65288;NCL&#65289;&#65292;&#36825;&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#22797;&#20852;&#65292;&#26088;&#22312;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;NCL&#30340;&#21147;&#37327;&#22312;&#20110;&#24378;&#21046;&#23558;&#38750;&#36127;&#32422;&#26463;&#24212;&#29992;&#20110;&#29305;&#24449;&#65292;&#36825;&#35753;&#20154;&#24819;&#36215;NMF&#33021;&#22815;&#25552;&#21462;&#19982;&#26679;&#26412;&#38598;&#32676;&#32039;&#23494;&#23545;&#40784;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;NCL&#19981;&#20165;&#22312;&#25968;&#23398;&#19978;&#19982;NMF&#30446;&#26631;&#24456;&#22909;&#22320;&#23545;&#40784;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20351;&#24471;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#21152;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;NCL&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19979;&#28216;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.12451</link><description>&lt;p&gt;
INSIGHT: &#24102;&#26377;&#35821;&#35328;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#31526;&#21495;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#65288;NS-RL&#65289;&#24050;&#25104;&#20026;&#21487;&#35299;&#37322;&#20915;&#31574;&#21046;&#23450;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20854;&#29305;&#28857;&#26159;&#31526;&#21495;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#35270;&#35273;&#35266;&#27979;&#30340;&#20219;&#21153;&#65292;NS-RL&#28041;&#21450;&#23545;&#29366;&#24577;&#36827;&#34892;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#26080;&#27861;&#21033;&#29992;&#22870;&#21169;&#20449;&#21495;&#26469;&#32454;&#21270;&#32467;&#26500;&#21270;&#29366;&#24577;&#12290;&#21487;&#35775;&#38382;&#24615;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#37322;&#24403;&#21069;&#30340;&#31526;&#21495;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#65292;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#25919;&#31574;&#21644;&#20915;&#31574;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;&#22312;&#20061;&#20010;Atari&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12451v1 Announce Type: new  Abstract: Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency. Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies. In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by distilling vision foundation models into a scalable perception module. Moreover, we design a pipeline that uses large language models to generate concise and readable language explanations for policies and decisions. In experiments on nine Atari tasks, our approach demonstrat
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#26377;&#26102;&#29983;&#25104;&#30340;&#25968;&#25454;&#29978;&#33267;&#20250;&#23545;&#23545;&#27604;&#23398;&#20064;&#36896;&#25104;&#20260;&#23475;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#26356;&#24378;&#30340;&#25968;&#25454;&#33192;&#32960;&#24212;&#35813;&#20276;&#38543;&#30528;&#26356;&#24369;&#30340;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.12448</link><description>&lt;p&gt;
&#29983;&#25104;&#30340;&#25968;&#25454;&#24635;&#26159;&#26377;&#21161;&#20110;&#23545;&#27604;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Generated Data Always Help Contrastive Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12448
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#26377;&#26102;&#29983;&#25104;&#30340;&#25968;&#25454;&#29978;&#33267;&#20250;&#23545;&#23545;&#27604;&#23398;&#20064;&#36896;&#25104;&#20260;&#23475;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#26356;&#24378;&#30340;&#25968;&#25454;&#33192;&#32960;&#24212;&#35813;&#20276;&#38543;&#30528;&#26356;&#24369;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#25104;&#20026;&#26080;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#26368;&#25104;&#21151;&#30340;&#33539;&#24335;&#20043;&#19968;&#65292;&#28982;&#32780;&#23427;&#24448;&#24448;&#20381;&#36182;&#22823;&#37327;&#25163;&#24037;&#25968;&#25454;&#22686;&#24378;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#36924;&#30495;&#22270;&#20687;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35748;&#21487;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#19968;&#31181;&#31216;&#20026;&#8220;&#25968;&#25454;&#33192;&#32960;&#8221;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#25968;&#25454;&#65288;&#29978;&#33267;&#26469;&#33258;&#20687;DDPM&#36825;&#26679;&#30340;&#22909;&#25193;&#25955;&#27169;&#22411;&#65289;&#26377;&#26102;&#29978;&#33267;&#20250;&#23545;&#23545;&#27604;&#23398;&#20064;&#36896;&#25104;&#20260;&#23475;&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#33192;&#32960;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#20102;&#26356;&#24378;&#30340;&#25968;&#25454;&#33192;&#32960;&#24212;&#35813;&#20276;&#38543;&#30528;&#26356;&#24369;&#30340;&#22686;&#24378;&#65292;&#21453;&#20043;&#20134;&#28982;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12448v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#20960;&#20309;&#32422;&#26463;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#37325;&#21512;&#37096;&#20998;&#65292;&#27604;&#36739;&#20102;&#28145;&#24230;&#20272;&#35745;&#31561;&#38382;&#39064;&#20013;&#38598;&#25104;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.12431</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#32422;&#26463;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Geometric Constraints in Deep Learning Frameworks: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#20960;&#20309;&#32422;&#26463;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#37325;&#21512;&#37096;&#20998;&#65292;&#27604;&#36739;&#20102;&#28145;&#24230;&#20272;&#35745;&#31561;&#38382;&#39064;&#20013;&#38598;&#25104;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stereophotogrammetry&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#22330;&#26223;&#29702;&#35299;&#25216;&#26415;&#12290;&#20854;&#36215;&#28304;&#21487;&#20197;&#36861;&#28335;&#21040;&#33267;&#23569;19&#19990;&#32426;&#65292;&#24403;&#26102;&#20154;&#20204;&#24320;&#22987;&#30740;&#31350;&#20351;&#29992;&#29031;&#29255;&#26469;&#27979;&#37327;&#19990;&#30028;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#33258;&#37027;&#26102;&#20197;&#26469;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#25104;&#21315;&#19978;&#19975;&#31181;&#26041;&#27861;&#12290;&#32463;&#20856;&#20960;&#20309;&#25216;&#26415;&#30340;Shape from Stereo&#24314;&#31435;&#22312;&#20351;&#29992;&#20960;&#20309;&#26469;&#23450;&#20041;&#22330;&#26223;&#21644;&#25668;&#20687;&#26426;&#20960;&#20309;&#30340;&#32422;&#26463;&#65292;&#28982;&#21518;&#35299;&#20915;&#38750;&#32447;&#24615;&#26041;&#31243;&#32452;&#12290;&#26356;&#36817;&#26399;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#32780;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20960;&#20309;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#25105;&#20204;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#29992;&#20110;&#28145;&#24230;&#20272;&#35745;&#25110;&#20854;&#20182;&#23494;&#20999;&#30456;&#20851;&#38382;&#39064;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#26222;&#36941;&#20960;&#20309;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12431v1 Announce Type: cross  Abstract: Stereophotogrammetry is an emerging technique of scene understanding. Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. Since then, thousands of approaches have been explored. The classic geometric techniques of Shape from Stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations. More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry. In this survey, we explore the overlap for geometric-based and deep learning-based frameworks. We compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. We present a new taxonomy for prevalent geometry enforcing constraints used in modern deep lear
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#25512;&#26029;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#8220;&#35268;&#21010;&#8221;&#21644;&#8220;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#8221;&#20004;&#31181;&#20915;&#31574;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#25968;&#25454;&#22797;&#26434;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.12417</link><description>&lt;p&gt;
&#20851;&#20110;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#39044;&#27979;&#35268;&#21010;&#21644;&#21453;&#20107;&#23454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On Predictive planning and counterfactual learning in active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12417
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#8220;&#35268;&#21010;&#8221;&#21644;&#8220;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#8221;&#20004;&#31181;&#20915;&#31574;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#25968;&#25454;&#22797;&#26434;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29702;&#35299;&#26234;&#33021;&#34892;&#20026;&#30340;&#22522;&#30784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#31181;&#34892;&#20026;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#20027;&#21160;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#25506;&#31350;&#35268;&#21010;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22797;&#26434;&#24615;&#22522;&#30784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#22522;&#20110;&#8220;&#35268;&#21010;&#8221;&#21644;&#8220;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#8221;&#20004;&#31181;&#20915;&#31574;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#20197;&#22312;&#36825;&#20123;&#31574;&#30053;&#20043;&#38388;&#24179;&#34913;&#25968;&#25454;&#22797;&#26434;&#24615;&#19982;&#21327;&#20316;&#65292;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#21183;&#20419;&#36827;&#24179;&#34913;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38656;&#35201;&#20195;&#29702;&#36866;&#24212;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32593;&#26684;&#19990;&#30028;&#24773;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20998;&#26512;&#21508;&#31181;&#21442;&#25968;&#28436;&#21464;&#30340;&#26426;&#20250;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12417v1 Announce Type: new  Abstract: Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32463;&#39564;&#32972;&#26223;&#21644;&#24067;&#26391;&#36816;&#21160;&#36827;&#34892;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#21644;&#24555;&#33410;&#22863;&#30340;&#22522;&#20110;&#36718;&#27425;&#30340;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#32701;&#27611;&#29699;&#20316;&#20026;&#19968;&#31181;&#38656;&#35201;&#36873;&#25163;&#20381;&#36182;&#21464;&#21270;&#30340;&#20915;&#31574;&#30340;&#22266;&#26377;&#33539;&#20363;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#34429;&#28982;&#22312;&#39034;&#24207;&#20915;&#31574;&#30340;&#31163;&#32447;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#25152;&#28041;&#21450;&#65292;&#20294;&#22914;&#20309;&#20174;&#31163;&#32447;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#27169;&#20223;&#20154;&#31867;&#36873;&#25163;&#30340;&#27604;&#36187;&#34892;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22797;&#21046;&#23545;&#25163;&#30340;&#34892;&#20026;&#26377;&#30410;&#20110;&#36873;&#25163;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27604;&#36187;&#21069;&#26377;&#26041;&#21521;&#22320;&#36827;&#34892;&#25112;&#30053;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#20250;&#21463;&#21040;&#27604;&#36187;&#30340;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#21644;&#30001;&#20110;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#30340;&#36873;&#25163;&#36718;&#27425;&#24615;&#36136;&#32780;&#20135;&#29983;&#30340;&#22797;&#21512;&#25928;&#24212;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RallyNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65306;&#65288;i&#65289;RallyNet&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v1 Announce Type: new  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.12403</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#21407;&#22240;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12403
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#29992;&#25143;&#36827;&#34892;&#20154;&#38469;&#35752;&#35770;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#37325;&#35201;&#22330;&#25152;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#22806;&#31435;&#38754;&#21644;&#21311;&#21517;&#24615;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#24179;&#21488;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#20167;&#24680;&#35328;&#35770;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#40657;&#30418;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#35299;&#20915;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#65292;&#35757;&#32451;&#22522;&#30784;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#24544;&#23454;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;LLM&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#21644;&#26368;&#20808;&#36827;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20351;&#36825;&#20123;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;CSI-BERT&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#19981;&#21516;&#23376;&#36733;&#27874;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#21363;&#20351;&#38754;&#23545;&#39640;&#20002;&#21253;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12400</link><description>&lt;p&gt;
&#23547;&#25214;&#20002;&#22833;&#30340;&#25968;&#25454;&#65306;&#19968;&#31181;&#21463;BERT&#21551;&#21457;&#30340;&#26041;&#27861;&#24212;&#23545;&#26080;&#32447;&#20256;&#24863;&#20013;&#30340;&#25968;&#25454;&#21253;&#20002;&#22833;
&lt;/p&gt;
&lt;p&gt;
Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;CSI-BERT&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#19981;&#21516;&#23376;&#36733;&#27874;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#21363;&#20351;&#38754;&#23545;&#39640;&#20002;&#21253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#29992;&#20110;Wi-Fi&#20256;&#24863;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25968;&#25454;&#21253;&#20002;&#22833;&#24120;&#24120;&#23548;&#33268;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#38750;&#36830;&#32493;&#20272;&#35745;&#65292;&#20174;&#32780;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#30340;CSI&#24674;&#22797;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;CSI-BERT&#12290;CSI-BERT&#21487;&#20197;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#25554;&#20540;&#26041;&#27861;&#19981;&#21516;&#65292;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#19968;&#20010;&#23376;&#36733;&#27874;&#65292;CSI-BERT&#25429;&#25417;&#20102;&#19981;&#21516;&#23376;&#36733;&#27874;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#25554;&#20540;&#26041;&#27861;&#30456;&#27604;&#65292;&#21363;&#20351;&#38754;&#23545;&#39640;&#20002;&#21253;&#29575;&#65292;CSI-BERT&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;CSI-BERT&#33719;&#24471;&#30340;&#24674;&#22797;&#30340;CSI&#65292;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26080;&#32447;&#39057;&#35889;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12400v1 Announce Type: cross  Abstract: Despite the development of various deep learning methods for Wi-Fi sensing, package loss often results in noncontinuous estimation of the Channel State Information (CSI), which negatively impacts the performance of the learning models. To overcome this challenge, we propose a deep learning model based on Bidirectional Encoder Representations from Transformers (BERT) for CSI recovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner on the target dataset without the need for additional data. Furthermore, unlike traditional interpolation methods that focus on one subcarrier at a time, CSI-BERT captures the sequential relationships across different subcarriers. Experimental results demonstrate that CSI-BERT achieves lower error rates and faster speed compared to traditional interpolation methods, even when facing with high loss rates. Moreover, by harnessing the recovered CSI obtained from CSI-BERT, other deep learning
&lt;/p&gt;</description></item><item><title>AraPoemBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35799;&#27468;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38463;&#25289;&#20271;&#35799;&#27468;&#30456;&#20851;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;&#20004;&#39033;&#26032;&#39062;&#20219;&#21153;&#20013;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12392</link><description>&lt;p&gt;
AraPoemBERT&#65306;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#35799;&#27468;&#20998;&#26512;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12392
&lt;/p&gt;
&lt;p&gt;
AraPoemBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35799;&#27468;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38463;&#25289;&#20271;&#35799;&#27468;&#30456;&#20851;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;&#20004;&#39033;&#26032;&#39062;&#20219;&#21153;&#20013;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35799;&#27468;&#20197;&#20854;&#20016;&#23500;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#28145;&#21051;&#30340;&#25991;&#21270;&#24847;&#20041;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#20854;&#32467;&#26500;&#21644;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#20808;&#36827;&#30340;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AraPoemBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#22312;&#38463;&#25289;&#20271;&#35799;&#27468;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;AraPoemBERT&#19982;5&#31181;&#19981;&#21516;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#38463;&#25289;&#20271;&#35799;&#27468;&#30456;&#20851;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;&#32477;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;AraPoemBERT&#22312;&#19977;&#39033;&#26032;&#39062;&#20219;&#21153;&#20013;&#30340;&#20004;&#39033;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#29575;&#65306;&#35799;&#20154;&#24615;&#21035;&#20998;&#31867;&#65288;99.34\%&#30340;&#20934;&#30830;&#29575;&#65289;&#21644;&#35799;&#27468;&#33410;&#24459;&#20998;&#31867;&#65288;97.79\%&#30340;&#20934;&#30830;&#29575;&#65289;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#35799;&#27468;&#25276;&#38901;&#20998;&#31867;&#65288;97.73\%&#30340;&#20934;&#30830;&#29575;&#65289;&#20013;&#20063;&#21462;&#24471;&#20102;&#20934;&#30830;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12392v1 Announce Type: cross  Abstract: Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet's gender classification (99.34\% accuracy), and poetry sub-meter classification (97.79\% accuracy). In addition, the model achieved an accuracy score in poems' rhyme classification (97.73\% accuracy) wh
&lt;/p&gt;</description></item><item><title>FairSTG&#36890;&#36807;&#21327;&#20316;&#26679;&#26412;&#32423;&#20248;&#21270;&#23545;&#25239;&#24615;&#33021;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#29420;&#31435;&#30340;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#20316;&#25361;&#25112;&#24615;&#26679;&#26412;&#19982;&#24050;&#23398;&#20064;&#26679;&#26412;&#30340; mix-up &#23454;&#29616;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.12391</link><description>&lt;p&gt;
FairSTG: &#36890;&#36807;&#21327;&#20316;&#26679;&#26412;&#32423;&#20248;&#21270;&#23545;&#25239;&#24615;&#33021;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairSTG: Countering performance heterogeneity via collaborative sample-level optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12391
&lt;/p&gt;
&lt;p&gt;
FairSTG&#36890;&#36807;&#21327;&#20316;&#26679;&#26412;&#32423;&#20248;&#21270;&#23545;&#25239;&#24615;&#33021;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#29420;&#31435;&#30340;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#20316;&#25361;&#25112;&#24615;&#26679;&#26412;&#19982;&#24050;&#23398;&#20064;&#26679;&#26412;&#30340; mix-up &#23454;&#29616;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12391v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234;  &#25688;&#35201;&#65306;&#26102;&#31354;&#23398;&#20064;&#22312;&#31227;&#21160;&#35745;&#31639;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20197;&#36171;&#20104;&#26234;&#33021;&#22478;&#24066;&#26356;&#24378;&#22823;&#30340;&#21151;&#33021;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#24573;&#35270;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#33021;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24615;&#33021;&#24322;&#36136;&#24615;&#23450;&#20026;&#19981;&#20844;&#24179;&#26102;&#31354;&#23398;&#20064;&#30340;&#21407;&#22240;&#65292;&#36825;&#19981;&#20165;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#23454;&#38469;&#21151;&#33021;&#65292;&#36824;&#20250;&#32473;&#29616;&#23454;&#19990;&#30028;&#30340;&#22478;&#24066;&#24212;&#29992;&#24102;&#26469;&#20005;&#37325;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#29420;&#31435;&#30340;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26694;&#26550;FairSTG&#65292;&#20511;&#37492;&#20102;&#21033;&#29992;&#24050;&#20805;&#20998;&#23398;&#20064;&#26679;&#26412;&#30340;&#20248;&#21183;&#26469;&#21327;&#20316;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#24605;&#24819;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FairSTG&#21253;&#25324;&#19968;&#20010;&#26102;&#31354;&#29305;&#24449;&#25552;&#21462;&#22120;&#29992;&#20110;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#19968;&#20010;&#29992;&#20110;&#30693;&#35782;&#36716;&#31227;&#30340;&#21327;&#20316;&#34920;&#31034;&#22686;&#24378;&#27169;&#22359;&#65292;&#20174;&#24050;&#20805;&#20998;&#23398;&#20064;&#26679;&#26412;&#21040;&#22256;&#38590;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12391v1 Announce Type: cross  Abstract: Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12388</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#23545;&#35805;&#31995;&#32479;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#32780;&#21487;&#35299;&#37322;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#23545;&#20110;&#20102;&#35299;&#12289;&#35780;&#20272;&#21644;&#25345;&#32493;&#25913;&#36827;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#25991;&#26412;&#23884;&#20837;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LLMs&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;LLM&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#31034;&#20363;&#30340;&#30417;&#30563;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12388v1 Announce Type: cross  Abstract: Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it sco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;n&#20803;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Binding&#20107;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#38454;&#27573;&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12386</link><description>&lt;p&gt;
&#19982;&#32852;&#21512;&#23398;&#20064;&#19981;&#30456;&#19978;&#19979;&#30340;&#20998;&#38454;&#27573;&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Pipelined Biomedical Event Extraction Rivaling Joint Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;n&#20803;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Binding&#20107;&#20214;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#38454;&#27573;&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20107;&#20214;&#25552;&#21462;&#26159;&#19968;&#39033;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#33719;&#21462;&#20107;&#20214;&#65292;&#20854;&#30446;&#26631;&#21253;&#25324;&#20107;&#20214;&#31867;&#22411;&#12289;&#35302;&#21457;&#35789;&#20197;&#21450;&#20107;&#20214;&#20013;&#28041;&#21450;&#30340;&#21508;&#20010;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;n&#20803;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;Binding&#20107;&#20214;&#65292;&#20197;&#25429;&#33719;&#20107;&#20214;&#32972;&#26223;&#21450;&#21442;&#19982;&#32773;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;BioNLP&#20849;&#20139;&#20219;&#21153;&#30340;GE11&#21644;GE13&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;63.14%&#21644;59.40%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12386v1 Announce Type: cross  Abstract: Biomedical event extraction is an information extraction task to obtain events from biomedical text, whose targets include the type, the trigger, and the respective arguments involved in an event. Traditional biomedical event extraction usually adopts a pipelined approach, which contains trigger identification, argument role recognition, and finally event construction either using specific rules or by machine learning. In this paper, we propose an n-ary relation extraction method based on the BERT pre-training model to construct Binding events, in order to capture the semantic information about an event's context and its participants. The experimental results show that our method achieves promising results on the GE11 and GE13 corpora of the BioNLP shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates that by significantly improving theperformance of Binding events, the overall performance of the pipelined even
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#29616;&#23454;&#20010;&#20307;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#20219;&#21153;&#21019;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.12368</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Characteristic AI Agents via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#29616;&#23454;&#20010;&#20307;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#20219;&#21153;&#21019;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24050;&#26497;&#22823;&#22686;&#24378;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#33268;&#21147;&#20110;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#36171;&#20104;&#29305;&#24449;&#12290;&#23613;&#31649;&#24050;&#26377;&#21830;&#19994;&#20135;&#21697;&#21033;&#29992;LLMs&#24320;&#21457;&#38754;&#21521;&#35282;&#33394;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20294;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#23398;&#26415;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#29616;&#23454;&#20010;&#20307;&#65292;&#25506;&#35752;LLMs&#26500;&#24314;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#20855;&#26377;&#31616;&#21333;&#37197;&#32622;&#25991;&#20214;&#30340;&#35282;&#33394;&#36827;&#34892;&#25805;&#20316;&#12290;&#38024;&#23545;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20026;&#29305;&#24449;&#21270;AI&#20195;&#29702;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25216;&#26415;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#19968;&#20010;&#21517;&#20026;&#8220;Character100&#8221;&#30340;&#25968;&#25454;&#38598;&#34987;&#24314;&#31435;&#29992;&#20110;&#36825;&#19968;&#22522;&#20934;&#65292;&#21253;&#25324;&#32500;&#22522;&#30334;&#31185;&#19978;&#35775;&#38382;&#37327;&#26368;&#39640;&#30340;&#20154;&#29289;&#20379;&#35821;&#35328;&#27169;&#22411;&#25198;&#28436;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12368v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. Wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20284;&#28982;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#21033;&#29992;&#33258;&#28982;&#24182;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#36731;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.12320</link><description>&lt;p&gt;
&#36817;&#20284;&#20284;&#28982;&#27604;&#65306;Boosting&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21069;&#21521;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12320
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20284;&#28982;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#21033;&#29992;&#33258;&#28982;&#24182;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#36731;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12320v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;&#19982;&#21453;&#21521;&#20256;&#25773;&#30456;&#27604;&#65292;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#39069;&#22806;&#20551;&#35774;&#31561;&#38382;&#39064;&#20351;&#24471;&#39640;&#25928;&#12289;&#31526;&#21512;&#29983;&#29289;&#23398;&#30340;&#26367;&#20195;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#20123;&#38480;&#21046;&#20102;&#23545;&#26356;&#28145;&#23618;&#27425;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20284;&#28982;&#27604;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#20294;&#22312;&#37096;&#32626;&#22810;&#20010;&#25968;&#25454;&#21103;&#26412;&#20197;&#20943;&#23569;&#20272;&#35745;&#26041;&#24046;&#26102;&#65292;&#21463;&#21040;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20284;&#28982;&#27604;&#65288;LR&#65289;&#26041;&#27861;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#36890;&#36807;&#21033;&#29992;LR&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#33258;&#28982;&#24182;&#34892;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21516;&#26102;&#31649;&#36947;&#21270;&#20102;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#19987;&#29992;&#30828;&#20214;&#30340;&#35745;&#31639;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12320v1 Announce Type: cross  Abstract: Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks. The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance. In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation. By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware. Extensive experiments demonstrate the effectiveness of the appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12309</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#20174;&#24310;&#36831;&#35266;&#23519;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Delayed Observations via World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20195;&#29702;&#36890;&#24120;&#20551;&#23450;&#22312;&#37319;&#21462;&#34892;&#21160;&#21518;&#31435;&#21363;&#33719;&#24471;&#20851;&#20110;&#34892;&#21160;&#25928;&#26524;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#29289;&#29702;&#38480;&#21046;&#65292;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;&#24310;&#36831;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#65292;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#22312;&#25972;&#21512;&#36807;&#21435;&#35266;&#23519;&#21644;&#23398;&#20064;&#21160;&#24577;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;&#24310;&#36831;POMDP&#38477;&#20302;&#20026;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#24310;&#36831;MDP&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20854;&#20013;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#35266;&#23519;&#24615;&#38477;&#20302;&#26102;&#23454;&#29616;&#27425;&#20248;&#24615;&#33021;&#29978;&#33267;&#36805;&#36895;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#19979;&#32988;&#36807;&#26420;&#32032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#36798;&#21040;30%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12309v1 Announce Type: cross  Abstract: In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#31946;&#31995;&#32479;&#20248;&#21270;&#65292;&#24182;&#19987;&#27880;&#20110;&#33258;&#21160;&#24494;&#20998;&#65292;&#26088;&#22312;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#25512;&#21160;&#27169;&#31946;&#31995;&#32479;&#35774;&#35745;&#30340;&#36827;&#27493;</title><link>https://arxiv.org/abs/2403.12308</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#31946;&#31995;&#32479;&#20248;&#21270;&#65306;&#20197;FuzzyR&#20026;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Fuzzy System Optimisation via Automatic Differentiation -- FuzzyR as a Use Case
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#31946;&#31995;&#32479;&#20248;&#21270;&#65292;&#24182;&#19987;&#27880;&#20110;&#33258;&#21160;&#24494;&#20998;&#65292;&#26088;&#22312;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#25512;&#21160;&#27169;&#31946;&#31995;&#32479;&#35774;&#35745;&#30340;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20854;&#24341;&#20837;&#20197;&#26469;&#65292;&#27169;&#31946;&#38598;&#21644;&#31995;&#32479;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20197;&#20854;&#22312;&#24314;&#27169;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#32780;&#38395;&#21517;&#65292;&#24182;&#26085;&#30410;&#23637;&#29616;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#27169;&#31946;&#31995;&#32479;&#30340;&#24212;&#29992;&#22810;&#31181;&#22810;&#26679;&#65292;&#20294;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20204;&#30340;&#35774;&#35745;&#36827;&#23637;&#30456;&#23545;&#36739;&#23569;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23613;&#31649;&#20687;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#34920;&#31034;&#21463;&#30410;&#20110;&#35745;&#31639;&#24615;&#33021;&#30340;&#22686;&#21152;&#20197;&#21450;&#35757;&#32451;&#26426;&#21046;&#21644;&#21487;&#29992;&#24037;&#20855;&#30340;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#26799;&#24230;&#19979;&#38477;&#65292;&#20294;&#23545;&#27169;&#31946;&#31995;&#32479;&#35774;&#35745;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#31946;&#31995;&#32479;&#20248;&#21270;&#65292;&#29305;&#21035;&#20851;&#27880;&#33258;&#21160;&#24494;&#20998;&#8212;&#8212;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20415;&#35753;&#27169;&#31946;&#31995;&#32479;&#35774;&#35745;&#32773;&#25670;&#33073;&#22797;&#26434;&#30340;der
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12308v1 Announce Type: new  Abstract: Since their introduction, fuzzy sets and systems have become an important area of research known for its versatility in modelling, knowledge representation and reasoning, and increasingly its potential within the context explainable AI. While the applications of fuzzy systems are diverse, there has been comparatively little advancement in their design from a machine learning perspective. In other words, while representations such as neural networks have benefited from a boom in learning capability driven by an increase in computational performance in combination with advances in their training mechanisms and available tool, in particular gradient descent, the impact on fuzzy system design has been limited. In this paper, we discuss gradient-descent-based optimisation of fuzzy systems, focussing in particular on automatic differentiation -- crucial to neural network learning -- with a view to free fuzzy system designers from intricate der
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#32500;&#35745;&#31639;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#19982;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12307</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#32500;&#22270;&#20998;&#31867;&#30340;&#20998;&#23376;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Molecular Classification Using Hyperdimensional Graph Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12307
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#32500;&#35745;&#31639;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#19982;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#39640;&#32500;&#35745;&#31639;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22270;&#20316;&#20026;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#20013;&#30340;&#21033;&#29992;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#36825;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#23588;&#20026;&#26174;&#33879;&#65292;&#37027;&#37324;&#20174;&#22270;&#34920;&#31034;&#20013;&#23398;&#20064;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#39046;&#22495;&#20869;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#28041;&#21450;&#36328;&#19981;&#21516;&#20998;&#23376;&#32467;&#26500;&#35782;&#21035;&#30284;&#32454;&#32990;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;HDC&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19982;&#20808;&#36827;&#27169;&#22411;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#25110;Weisfieler-Lehman&#22270;&#26680;&#65288;WL&#65289;&#30456;&#23218;&#32654;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#36229;&#36234;&#20808;&#21069;&#25552;&#20986;&#30340;&#39640;&#32500;&#35745;&#31639;&#22270;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#22312;&#24615;&#33021;&#25552;&#21319;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#22521;&#20859;&#38454;&#27573;&#21152;&#36895;40&#20493;&#65292;&#25512;&#26029;&#26102;&#38388;&#25913;&#36827;&#20102;15&#20493;&#65292;&#30456;&#23545;&#20110;GNN
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12307v1 Announce Type: cross  Abstract: Our work introduces an innovative approach to graph learning by leveraging Hyperdimensional Computing. Graphs serve as a widely embraced method for conveying information, and their utilization in learning has gained significant attention. This is notable in the field of chemoinformatics, where learning from graph representations plays a pivotal role. An important application within this domain involves the identification of cancerous cells across diverse molecular structures.   We propose an HDC-based model that demonstrates comparable Area Under the Curve results when compared to state-of-the-art models like Graph Neural Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it outperforms previously proposed hyperdimensional computing graph learning methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x acceleration in the training phase and a 15x improvement in inference time compared to GNN a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#35299;&#26512;&#22797;&#26434;&#20020;&#24202;&#35821;&#35328;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2403.12297</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20449;&#24687;&#65306;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#35299;&#26512;&#22797;&#26434;&#20020;&#24202;&#35821;&#35328;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;SUD&#65289;&#30001;&#20110;&#23545;&#20581;&#24247;&#21644;&#31038;&#20250;&#30340;&#26377;&#23475;&#24433;&#21709;&#32780;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290; SUD&#30340;&#35782;&#21035;&#21644;&#27835;&#30103;&#21462;&#20915;&#20110;&#35832;&#22810;&#22240;&#32032;&#65292;&#22914;&#20005;&#37325;&#31243;&#24230;&#12289;&#32852;&#21512;&#20915;&#23450;&#22240;&#32032;&#65288;&#20363;&#22914;&#25106;&#26029;&#30151;&#29366;&#65289;&#21644;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#12290; &#32654;&#22269;&#20445;&#38505;&#25552;&#20379;&#21830;&#20351;&#29992;&#30340;&#29616;&#26377;&#35786;&#26029;&#32534;&#30721;&#31995;&#32479;&#65292;&#22914;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD-10&#65289;&#65292;&#23545;&#20110;&#26576;&#20123;&#35786;&#26029;&#32570;&#20047;&#32454;&#33268;&#24230;&#65292;&#20294;&#20020;&#24202;&#21307;&#29983;&#20250;&#23558;&#27492;&#32454;&#33268;&#24230;&#65288;&#22914;&#12298;&#31934;&#31070;&#38556;&#30861;&#35786;&#26029;&#19982;&#32479;&#35745;&#25163;&#20876;&#12299;&#20998;&#31867;&#25110;DSM-5&#20013;&#25152;&#21457;&#29616;&#30340;&#65289;&#20316;&#20026;&#36741;&#21161;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28155;&#21152;&#21040;&#20020;&#24202;&#35760;&#24405;&#20013;&#12290; &#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#22312;&#20934;&#30830;&#35299;&#26512;&#36825;&#31181;&#22810;&#26679;&#21270;&#30340;&#20020;&#24202;&#35821;&#35328;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#27169;&#24335;&#65292;&#26377;&#26395;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290; &#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLMs&#22312;&#25552;&#21462;&#20005;&#37325;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12297v1 Announce Type: cross  Abstract: Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health. Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes. Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns. This study investigates the application of LLMs for extracting severi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;</title><link>https://arxiv.org/abs/2403.12237</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#30340;&#39640;&#25928;&#22522;&#20110;Transformer&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#36807;&#31243;&#23545;&#20110;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;HPO&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#20197;&#20854;&#21487;&#35266;&#30340;&#35745;&#31639;&#21344;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#32780;&#38395;&#21517;&#65307;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;TRL-HPO&#37197;&#22791;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#21270;&#21644;&#28176;&#36827;&#29983;&#25104;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;TRL-HPO&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;CNN&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30456;&#21516;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;TRL-HPO&#30340;&#20998;&#31867;&#32467;&#26524;&#20248;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;6.8%&#65292;&#35777;&#26126;&#20102;TRL-HPO&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12212</link><description>&lt;p&gt;
&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#27604;&#36739;&#20998;&#26512;&#24052;&#35199;&#20844;&#21496;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#19978;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;NER&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#65292;&#23548;&#33268;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36130;&#21153;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#37329;&#34701;&#39046;&#22495;&#20869;NER&#38656;&#27714;&#65292;&#24182;&#20391;&#37325;&#20110;&#20174;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#36890;&#36807;&#25972;&#29702;&#21253;&#25324;384&#20010;&#36716;&#24405;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#33889;&#33796;&#29273;&#35821;&#65288;BERTimbau&#21644;PTT5&#65289;&#35757;&#32451;&#30340;&#21333;&#35821;&#27169;&#22411;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#21644;mT5&#65289;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;T5&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#22312;&#27169;&#22411;&#24494;&#35843;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#25968;&#25454;&#32570;&#22833;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.12211</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#32570;&#22833;&#39044;&#27979;&#30340;&#32479;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#25968;&#25454;&#32570;&#22833;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#35760;&#24405;&#36890;&#24120;&#30001;&#19981;&#21516;&#30340;&#27169;&#24577;&#32452;&#25104;&#65292;&#22914;&#22270;&#29255;&#12289;&#25991;&#26412;&#21644;&#34920;&#26684;&#20449;&#24687;&#12290;&#25972;&#21512;&#25152;&#26377;&#27169;&#24577;&#21487;&#20197;&#25552;&#20379;&#24739;&#32773;&#29366;&#20917;&#30340;&#20840;&#38754;&#35270;&#22270;&#65292;&#32780;&#32437;&#21521;&#20998;&#26512;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#30142;&#30149;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#65288;MMMV&#65289;&#25968;&#25454;&#32570;&#22833;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20219;&#24847;&#25968;&#37327;&#30340;&#26102;&#38388;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#26088;&#22312;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#65292;&#26080;&#35770;&#20854;&#26159;&#21542;&#23436;&#25972;&#12290;&#25105;&#20204;&#22312;&#39592;&#20851;&#33410;&#28814;&#20513;&#35758;&#65288;OAI&#65289;&#30340;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;&#30140;&#30171;&#21644;Kellgren-Lawrence&#20998;&#32423;&#65288;KLG&#65289;&#22312;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#39044;&#27979;&#30340;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12211v1 Announce Type: cross  Abstract: Medical records often consist of different modalities, such as images, text, and tabular information. Integrating all modalities offers a holistic view of a patient's condition, while analyzing them longitudinally provides a better understanding of disease progression. However, real-world longitudinal medical records present challenges: 1) patients may lack some or all of the data for a specific timepoint, and 2) certain modalities or views might be absent for all patients during a particular period. In this work, we introduce a unified model for longitudinal multi-modal multi-view (MMMV) prediction with missingness. Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability. We conduct extensive experiments on the knee osteoarthritis dataset from the Osteoarthritis Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a future timepoint. We d
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#20351;&#24471;&#25968;&#23383;&#20869;&#23481;&#29983;&#25104;&#26041;&#24335;&#21457;&#29983;&#21464;&#38761;&#65292;&#26412;&#25253;&#21578;&#37325;&#28857;&#25506;&#35752;&#20102;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21046;&#20316;&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#65292;&#23588;&#20854;&#22312;&#32593;&#32476;&#24433;&#21709;&#34892;&#21160;&#20013;&#30340;&#24212;&#29992;&#34920;&#26126;&#20102;&#20854;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.12207</link><description>&lt;p&gt;
&#32593;&#32476;&#24433;&#21709;&#34892;&#21160;&#20013;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#65306;&#19968;&#31181;&#26032;&#20852;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;
Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12207
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#20351;&#24471;&#25968;&#23383;&#20869;&#23481;&#29983;&#25104;&#26041;&#24335;&#21457;&#29983;&#21464;&#38761;&#65292;&#26412;&#25253;&#21578;&#37325;&#28857;&#25506;&#35752;&#20102;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21046;&#20316;&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#65292;&#23588;&#20854;&#22312;&#32593;&#32476;&#24433;&#21709;&#34892;&#21160;&#20013;&#30340;&#24212;&#29992;&#34920;&#26126;&#20102;&#20854;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21457;&#23637;&#20652;&#29983;&#20102;&#25968;&#23383;&#20869;&#23481;&#29983;&#25104;&#26041;&#24335;&#30340;&#36716;&#21464;&#65292;&#36825;&#23545;&#32593;&#32476;&#24433;&#21709;&#34892;&#21160;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25253;&#21578;&#25506;&#35752;&#20102;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#22312;&#21046;&#20316;&#36924;&#30495;&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#24037;&#20855;&#30340;&#26131;&#33719;&#21462;&#24615;&#12289;&#23454;&#29992;&#24615;&#21644;&#36755;&#20986;&#36136;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#23427;&#20204;&#22312;&#27450;&#39575;&#12289;&#24433;&#21709;&#21644;&#39072;&#35206;&#23041;&#32961;&#24773;&#26223;&#20013;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#25253;&#21578;&#20026;&#20960;&#31181;&#20551;&#24819;&#30340;&#32593;&#32476;&#24433;&#21709;&#34892;&#21160;&#29983;&#25104;&#20869;&#23481;&#65292;&#20197;&#23637;&#31034;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#26041;&#27861;&#23545;&#23041;&#32961;&#34892;&#21160;&#32773;&#30340;&#24403;&#21069;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#34429;&#28982;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#25554;&#22270;&#21644;&#38750;&#30495;&#23454;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21019;&#24314;&#20196;&#20154;&#20449;&#26381;&#30340;&#29031;&#29255;&#36924;&#30495;&#20869;&#23481;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21463;&#38480;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#38656;&#35201;&#20154;&#20026;&#24341;&#23548;&#30340;&#31934;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12207v1 Announce Type: cross  Abstract: The evolution of artificial intelligence (AI) has catalyzed a transformation in digital content generation, with profound implications for cyber influence operations. This report delves into the potential and limitations of generative deep learning models, such as diffusion models, in fabricating convincing synthetic images. We critically assess the accessibility, practicality, and output quality of these tools and their implications in threat scenarios of deception, influence, and subversion. Notably, the report generates content for several hypothetical cyber influence operations to demonstrate the current capabilities and limitations of these AI-driven methods for threat actors. While generative models excel at producing illustrations and non-realistic imagery, creating convincing photo-realistic content remains a significant challenge, limited by computational resources and the necessity for human-guided refinement. Our exploration
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#25506;&#32034;&#32452;&#21512;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#31616;&#21333;&#30340;&#39034;&#24207;&#20989;&#25968;&#20018;&#32852;&#65292;&#36824;&#33021;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#20132;&#20114;&#20989;&#25968;&#32452;&#21512;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12201</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#20013;&#30340;&#20989;&#25968;&#26500;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional learning of functions in humans and machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12201
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#25506;&#32034;&#32452;&#21512;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#31616;&#21333;&#30340;&#39034;&#24207;&#20989;&#25968;&#20018;&#32852;&#65292;&#36824;&#33021;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#20132;&#20114;&#20989;&#25968;&#32452;&#21512;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#32452;&#25104;&#20989;&#25968;&#30340;&#33021;&#21147;&#23545;&#20110;&#20154;&#31867;&#22312;&#26377;&#25928;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#20854;&#33021;&#22815;&#28789;&#27963;&#27867;&#21270;&#65292;&#20363;&#22914;&#26681;&#25454;&#24050;&#30693;&#28921;&#39274;&#36807;&#31243;&#21019;&#36896;&#26032;&#33756;&#32948;&#12290;&#38500;&#20102;&#20989;&#25968;&#30340;&#39034;&#24207;&#38142;&#24335;&#20018;&#32852;&#22806;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#23398;&#25991;&#29486;&#34920;&#26126;&#65292;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#20132;&#20114;&#20989;&#25968;&#32452;&#21512;&#65292;&#20854;&#20013;&#36755;&#20986;&#20135;&#29983;&#21462;&#20915;&#20110;&#30001;&#19981;&#21516;&#20989;&#25968;&#25490;&#24207;&#24341;&#36215;&#30340;&#19978;&#19979;&#25991;&#21464;&#21270;&#12290;&#23558;&#35843;&#26597;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20989;&#25968;&#23398;&#20064;&#33539;&#20363;&#65292;&#20197;&#25506;&#32034;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#19981;&#21516;&#20132;&#20114;&#26465;&#20214;&#19979;&#23398;&#20064;&#21644;&#25512;&#29702;&#20855;&#26377;&#32452;&#21512;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#22312;&#23545;&#20010;&#20307;&#20989;&#25968;&#36827;&#34892;&#31616;&#35201;&#35757;&#32451;&#21518;&#65292;&#23545;&#20154;&#31867;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#32452;&#21512;&#20004;&#20010;&#23398;&#20064;&#36807;&#30340;&#20989;&#25968;&#65292;&#28085;&#30422;&#22235;&#31181;&#20027;&#35201;&#30340;&#20132;&#20114;&#31867;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#24212;&#29992;&#31532;&#19968;&#20010;&#20989;&#25968;&#20250;&#21019;&#24314;&#25110;&#21024;&#38500;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12201v1 Announce Type: new  Abstract: The ability to learn and compose functions is foundational to efficient learning and reasoning in humans, enabling flexible generalizations such as creating new dishes from known cooking processes. Beyond sequential chaining of functions, existing linguistics literature indicates that humans can grasp more complex compositions with interacting functions, where output production depends on context changes induced by different function orderings. Extending the investigation into the visual domain, we developed a function learning paradigm to explore the capacity of humans and neural network models in learning and reasoning with compositional functions under varied interaction conditions. Following brief training on individual functions, human participants were assessed on composing two learned functions, in ways covering four main interaction types, including instances in which the application of the first function creates or removes the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#27169;&#22411;&#65292;E2F-Net&#65292;&#36890;&#36807;&#26174;&#33879;&#25552;&#21462;&#30524;&#37096;&#21306;&#22495;&#30340;&#36523;&#20221;&#21644;&#38750;&#36523;&#20221;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#39044;&#35757;&#32451;&#30340;StyleGAN&#29983;&#25104;&#22120;&#30340;&#28508;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#30524;&#30555;&#21040;&#33080;&#37096;&#30340;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2403.12197</link><description>&lt;p&gt;
E2F-Net: &#36890;&#36807;StyleGAN&#28508;&#31354;&#38388;&#36827;&#34892;&#30524;&#30555;&#21040;&#33080;&#37096;&#20462;&#22797;&#30340;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#27169;&#22411;&#65292;E2F-Net&#65292;&#36890;&#36807;&#26174;&#33879;&#25552;&#21462;&#30524;&#37096;&#21306;&#22495;&#30340;&#36523;&#20221;&#21644;&#38750;&#36523;&#20221;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#39044;&#35757;&#32451;&#30340;StyleGAN&#29983;&#25104;&#22120;&#30340;&#28508;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#30524;&#30555;&#21040;&#33080;&#37096;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#20462;&#22797;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#24674;&#22797;&#32570;&#22833;&#25110;&#21463;&#25439;&#30340;&#38754;&#37096;&#22270;&#20687;&#21306;&#22495;&#65292;&#22312;&#36974;&#25377;&#24773;&#26223;&#19979;&#30340;&#20154;&#33080;&#35782;&#21035;&#21644;&#36136;&#37327;&#24046;&#30340;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#27169;&#22411;&#65292;&#21363;Eyes-to-Face Network&#65288;E2F-Net&#65289;&#65292;&#20462;&#22797;&#32473;&#23450;&#30340;&#30524;&#37096;&#21306;&#22495;&#24182;&#23436;&#25104;&#20154;&#33080;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12197v1 Announce Type: cross  Abstract: Face inpainting, the technique of restoring missing or damaged regions in facial images, is pivotal for applications like face recognition in occluded scenarios and image analysis with poor-quality captures. This process not only needs to produce realistic visuals but also preserve individual identity characteristics. The aim of this paper is to inpaint a face given periocular region (eyes-to-face) through a proposed new Generative Adversarial Network (GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used. The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training. We further improve the StyleGAN output to find the optimal code in the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#21327;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#35782;&#21035;&#24694;&#24847;&#36719;&#20214;&#21253;</title><link>https://arxiv.org/abs/2403.12196</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12196
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#21327;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#35782;&#21035;&#24694;&#24847;&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gartner 2022&#24180;&#30340;&#25253;&#21578;&#39044;&#27979;&#65292;&#21040;2025&#24180;&#65292;&#20840;&#29699;45%&#30340;&#32452;&#32455;&#23558;&#36973;&#36935;&#36719;&#20214;&#20379;&#24212;&#38142;&#25915;&#20987;&#65292;&#20984;&#26174;&#20102;&#25913;&#21892;&#36719;&#20214;&#20379;&#24212;&#38142;&#23433;&#20840;&#23545;&#31038;&#21306;&#21644;&#22269;&#23478;&#21033;&#30410;&#30340;&#36843;&#20999;&#24615;&#12290;&#24403;&#21069;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#36890;&#36807;&#36807;&#28388;&#33391;&#24615;&#21644;&#24694;&#24847;&#36719;&#20214;&#21253;&#26469;&#36741;&#21161;&#25163;&#21160;&#23457;&#26680;&#36807;&#31243;&#65292;&#28982;&#32780;&#36825;&#31181;&#25216;&#26415;&#23384;&#22312;&#36739;&#39640;&#30340;&#35823;&#25253;&#29575;&#21644;&#26377;&#38480;&#30340;&#33258;&#21160;&#21270;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#21487;&#20197;&#21463;&#30410;&#20110;&#20808;&#36827;&#12289;&#26356;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20934;&#30830;&#19988;&#35823;&#25253;&#36739;&#23569;&#30340;&#32467;&#26524;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24110;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#35782;&#21035;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12196v1 Announce Type: cross  Abstract: The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests. Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support. Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results. The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.   We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and zero-shot-role-play-Chain of Thought (CoT) prompting techni
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;MAC&#65288;&#22823;&#33268;&#21644;&#36817;&#20284;&#27491;&#30830;&#65289;&#39044;&#27979;&#30340;&#31574;&#30053;&#24615;&#35774;&#26045;&#36873;&#22336;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#25361;&#25112;&#20256;&#32479;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#31574;&#30053;&#26080;&#25928;&#35774;&#26045;&#36873;&#22336;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.12181</link><description>&lt;p&gt;
&#35774;&#26045;&#36873;&#22336;&#26426;&#21046;&#35774;&#35745;&#30340;MAC&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
MAC Advice for Facility Location Mechanism Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;MAC&#65288;&#22823;&#33268;&#21644;&#36817;&#20284;&#27491;&#30830;&#65289;&#39044;&#27979;&#30340;&#31574;&#30053;&#24615;&#35774;&#26045;&#36873;&#22336;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#25361;&#25112;&#20256;&#32479;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#31574;&#30053;&#26080;&#25928;&#35774;&#26045;&#36873;&#22336;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#31639;&#27861;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21253;&#25324;&#35774;&#26045;&#36873;&#22336;&#30340;&#21464;&#31181;&#65292;&#20316;&#20026;&#36229;&#36234;&#20256;&#32479;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#30740;&#31350;$k$-&#35774;&#26045;&#36873;&#22336;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;$n$&#20010;&#20195;&#29702;&#26159;&#31574;&#30053;&#24615;&#30340;&#65292;&#21487;&#33021;&#35823;&#25253;&#20854;&#20301;&#32622;&#12290;&#19982;&#20197;&#24448;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#26159;&#38024;&#23545;$k$&#20010;&#26368;&#20339;&#35774;&#26045;&#20301;&#32622;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#25509;&#25910;&#21040;&#20102;&#27599;&#20010;&#20195;&#29702;&#30340;&#20301;&#32622;&#30340;$n$&#20010;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#27979;&#21482;&#26159;&#8220;&#22823;&#33268;&#8221;&#21644;&#8220;&#36817;&#20284;&#8221;&#27491;&#30830;&#65288;&#25110;&#31216;&#20026;MAC&#65289;--&#21363;&#65292;&#19968;&#20123;$\delta$&#20998;&#25968;&#30340;&#39044;&#27979;&#20301;&#32622;&#20801;&#35768;&#20219;&#24847;&#19981;&#27491;&#30830;&#65292;&#24182;&#19988;&#20854;&#20313;&#39044;&#27979;&#20801;&#35768;&#26368;&#22810;&#26377;$\varepsilon$&#35823;&#24046;&#12290;&#25105;&#20204;&#23545;&#38169;&#35823;&#30340;&#29420;&#31435;&#24615;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#36825;&#26679;&#30340;&#39044;&#27979;&#33021;&#21542;&#24110;&#21161;&#25105;&#20204;&#36229;&#36234;&#24403;&#21069;&#30340;&#26368;&#20339;&#36793;&#30028;&#20197;&#33719;&#24471;&#23545;&#31574;&#30053;&#26080;&#25928;&#30340;&#35774;&#26045;&#36873;&#22336;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;$1$-med
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12181v1 Announce Type: cross  Abstract: Algorithms with predictions have attracted much attention in the last years across various domains, including variants of facility location, as a way to surpass traditional worst-case analyses. We study the $k$-facility location mechanism design problem, where the $n$ agents are strategic and might misreport their location.   Unlike previous models, where predictions are for the $k$ optimal facility locations, we receive $n$ predictions for the locations of each of the agents. However, these predictions are only "mostly" and "approximately" correct (or MAC for short) -- i.e., some $\delta$-fraction of the predicted locations are allowed to be arbitrarily incorrect, and the remainder of the predictions are allowed to be correct up to an $\varepsilon$-error. We make no assumption on the independence of the errors. Can such predictions allow us to beat the current best bounds for strategyproof facility location?   We show that the $1$-med
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;</title><link>https://arxiv.org/abs/2403.12176</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12176
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26411;&#31471;&#21040;&#26411;&#31471;&#23398;&#20064;&#31649;&#36947;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#39640;&#24230;&#33258;&#20027;&#36710;&#36742;&#30340;&#25345;&#32493;&#21457;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12289;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#32508;&#21512;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#26102;&#20915;&#31574;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#22952;&#30861;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#24182;&#20943;&#24369;&#20102;&#36825;&#31867;&#36710;&#36742;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#21830;&#19994;&#21270;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#20123;&#27773;&#36710;&#21442;&#19982;&#25110;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#26102;&#65292;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#36825;&#31181;&#32570;&#28857;&#20174;&#31038;&#20250;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#26411;&#31471;&#21040;&#26411;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#35299;&#37322;&#24615;&#26159;&#20419;&#36827;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#26368;&#20808;&#36827;&#25216;&#26415;&#20013;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20998;&#24320;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12176v1 Announce Type: cross  Abstract: The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to brid
&lt;/p&gt;</description></item><item><title>TnT-LLM &#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#20998;&#37197;&#26631;&#31614;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.12173</link><description>&lt;p&gt;
TnT-LLM&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
TnT-LLM: Text Mining at Scale with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12173
&lt;/p&gt;
&lt;p&gt;
TnT-LLM &#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#20998;&#37197;&#26631;&#31614;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#26377;&#29992;&#30340;&#31867;&#21035;&#26631;&#31614;&#36827;&#34892;&#32452;&#32455;&#65292;&#26159;&#25991;&#26412;&#25366;&#25496;&#20013;&#29992;&#20110;&#19979;&#28216;&#20998;&#26512;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#26500;&#24314;&#22522;&#20110;&#25991;&#26412;&#26631;&#31614;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#25972;&#29702;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#24403;&#26631;&#31614;&#31354;&#38388;&#19981;&#26126;&#30830;&#19988;&#32570;&#23569;&#22823;&#35268;&#27169;&#25968;&#25454;&#27880;&#37322;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23588;&#20026;&#20005;&#23803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20854;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#21475;&#26377;&#21161;&#20110;&#24341;&#23548;&#21644;&#20351;&#29992;&#22823;&#35268;&#27169;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TnT-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26631;&#31614;&#29983;&#25104;&#21644;&#20998;&#37197;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12173v1 Announce Type: cross  Abstract: Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiCiSAD&#30340;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12172</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiCiSAD&#30340;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;SVAD&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#20934;&#30830;&#35782;&#21035;&#24322;&#24120;&#27169;&#24335;&#25110;&#20107;&#20214;&#20351;&#25805;&#20316;&#21592;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#65292;&#20174;&#32780;&#22686;&#24378;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#26410;&#33021;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#29305;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#23454;&#29992;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;GiCiSAD&#65289;&#65292;&#20197;&#20811;&#26381;&#19982;SVAD&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12172v1 Announce Type: cross  Abstract: Skeleton-based video anomaly detection (SVAD) is a crucial task in computer vision. Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities, thereby enhancing safety. Achieving this demands a comprehensive understanding of human motions, both at body and region levels, while also accounting for the wide variations of performing a single action. However, existing studies fail to simultaneously address these crucial properties. This paper introduces a novel, practical and lightweight framework, namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD. GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data, the Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies bet
&lt;/p&gt;</description></item><item><title>EasyJailbreak&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#65292;&#25903;&#25345;11&#31181;&#36234;&#29425;&#26041;&#27861;&#65292;&#24110;&#21161;&#36827;&#34892;&#24191;&#27867;&#33539;&#22260;LLMs&#30340;&#23433;&#20840;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.12171</link><description>&lt;p&gt;
EasyJailbreak: &#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12171
&lt;/p&gt;
&lt;p&gt;
EasyJailbreak&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#65292;&#25903;&#25345;11&#31181;&#36234;&#29425;&#26041;&#27861;&#65292;&#24110;&#21161;&#36827;&#34892;&#24191;&#27867;&#33539;&#22260;LLMs&#30340;&#23433;&#20840;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25915;&#20987;&#23545;&#20110;&#35782;&#21035;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#28431;&#27934;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#32469;&#36807;&#20445;&#38556;&#25514;&#26045;&#24182;&#24341;&#21457;&#34987;&#31105;&#27490;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#36234;&#29425;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;&#31038;&#21306;&#25552;&#20379;&#26631;&#20934;&#23454;&#29616;&#26694;&#26550;&#65292;&#36825;&#38480;&#21046;&#20102;&#20840;&#38754;&#30340;&#23433;&#20840;&#35780;&#20272;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EasyJailbreak&#65292;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#38024;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#12290;&#23427;&#20351;&#29992;&#22235;&#20010;&#32452;&#20214;&#26469;&#26500;&#24314;&#36234;&#29425;&#25915;&#20987;&#65306;&#36873;&#25321;&#22120;&#12289;&#21464;&#24322;&#22120;&#12289;&#32422;&#26463;&#22120;&#21644;&#35780;&#20272;&#22120;&#12290;&#36825;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#26032;&#30340;&#21644;&#29616;&#26377;&#32452;&#20214;&#30340;&#32452;&#21512;&#20013;&#26500;&#24314;&#25915;&#20987;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;EasyJailbreak&#25903;&#25345;11&#31181;&#19981;&#21516;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#20419;&#36827;&#20102;&#23545;&#24191;&#27867;&#33539;&#22260;LLMs&#30340;&#23433;&#20840;&#39564;&#35777;&#12290;&#25105;&#20204;&#22312;10&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#30340;&#39564;&#35777;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12171v1 Announce Type: cross  Abstract: Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerabilit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#26102;&#21033;&#29992;&#35745;&#21010;&#20998;&#26512;&#23547;&#25214;&#21644;&#20648;&#23384;&#26356;&#22909;&#35745;&#21010;&#26426;&#20250;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#31995;&#32479;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#20462;&#22797;&#35745;&#21010;&#26469;&#21462;&#20195;&#20174;&#22836;&#37325;&#26032;&#35268;&#21010;&#30340;&#20248;&#21183;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.12162</link><description>&lt;p&gt;
&#36890;&#36807;&#35745;&#21010;&#20998;&#26512;&#23454;&#29616;&#26234;&#33021;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Intelligent Execution through Plan Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#26102;&#21033;&#29992;&#35745;&#21010;&#20998;&#26512;&#23547;&#25214;&#21644;&#20648;&#23384;&#26356;&#22909;&#35745;&#21010;&#26426;&#20250;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#31995;&#32479;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#20462;&#22797;&#35745;&#21010;&#26469;&#21462;&#20195;&#20174;&#22836;&#37325;&#26032;&#35268;&#21010;&#30340;&#20248;&#21183;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#38656;&#35201;&#29983;&#25104;&#21644;&#25191;&#34892;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#35268;&#21010;&#23545;&#19990;&#30028;&#20570;&#20986;&#20102;&#19968;&#20123;&#20551;&#35774;&#12290;&#22312;&#25191;&#34892;&#35745;&#21010;&#26102;&#65292;&#36825;&#20123;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#31526;&#21512;&#30340;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20851;&#27880;&#36825;&#19968;&#20107;&#23454;&#30340;&#36127;&#38754;&#24433;&#21709;&#20197;&#21450;&#22312;&#25191;&#34892;&#22833;&#36133;&#21518;&#37325;&#26032;&#35268;&#21010;&#30340;&#20351;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#36825;&#19968;&#20107;&#23454;&#30340;&#31215;&#26497;&#24433;&#21709;&#65292;&#21363;&#21457;&#29616;&#26356;&#22909;&#35745;&#21010;&#30340;&#26426;&#20250;&#12290;&#22312;&#35268;&#21010;&#38454;&#27573;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#20250;&#25214;&#21040;&#24182;&#23384;&#20648;&#36825;&#20123;&#26426;&#20250;&#12290;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#65292;&#30417;&#25511;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#26469;&#38598;&#20013;&#24863;&#30693;&#24182;&#20462;&#22797;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35268;&#21010;&#12290;&#22312;&#22810;&#20010;&#20856;&#22411;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#23454;&#39564;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#32988;&#36807;&#26631;&#20934;&#37325;&#26032;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12162v1 Announce Type: new  Abstract: Intelligent robots need to generate and execute plans. In order to deal with the complexity of real environments, planning makes some assumptions about the world. When executing plans, the assumptions are usually not met. Most works have focused on the negative impact of this fact and the use of replanning after execution failures. Instead, we focus on the positive impact, or opportunities to find better plans. When planning, the proposed technique finds and stores those opportunities. Later, during execution, the monitoring system can use them to focus perception and repair the plan, instead of replanning from scratch. Experiments in several paradigmatic robotic tasks show how the approach outperforms standard replanning strategies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#31572;&#26696;&#38598;&#32534;&#31243;&#20013;&#25429;&#25417;&#26102;&#38388;&#27969;&#36893;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#22312;&#24314;&#27169;&#36335;&#30001;&#26041;&#38754;&#26159;&#19968;&#31181;&#26377;&#36259;&#30340;&#36873;&#25321;&#65292;&#20294;&#22312;&#35843;&#24230;&#26041;&#38754;&#21364;&#26159;&#19981;&#23384;&#22312;&#26367;&#20195;&#26041;&#26696;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.12153</link><description>&lt;p&gt;
&#36816;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#36335;&#30001;&#21644;&#35843;&#24230;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65306;&#21021;&#27493;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Routing and Scheduling in Answer Set Programming applied to Multi-Agent Path Finding: Preliminary Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#31572;&#26696;&#38598;&#32534;&#31243;&#20013;&#25429;&#25417;&#26102;&#38388;&#27969;&#36893;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#22312;&#24314;&#27169;&#36335;&#30001;&#26041;&#38754;&#26159;&#19968;&#31181;&#26377;&#36259;&#30340;&#36873;&#25321;&#65292;&#20294;&#22312;&#35843;&#24230;&#26041;&#38754;&#21364;&#26159;&#19981;&#23384;&#22312;&#26367;&#20195;&#26041;&#26696;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#20013;&#36827;&#34892;&#36335;&#30001;&#21644;&#35843;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#23427;&#20204;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#20197;&#20559;&#24207;&#32780;&#19981;&#26159;&#19982;&#34892;&#21160;&#21644;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#27493;&#26469;&#25429;&#25417;&#26102;&#38388;&#27969;&#36893;&#12290;&#36825;&#20063;&#28040;&#38500;&#20102;&#35745;&#21010;&#38271;&#24230;&#30340;&#22266;&#23450;&#19978;&#30028;&#30340;&#38656;&#35201;&#12290;&#36825;&#31181;&#36991;&#20813;&#30340;&#20195;&#20215;&#26159;&#65288;&#37096;&#20998;&#65289;&#26102;&#38388;&#36712;&#36857;&#24517;&#39035;&#26159;&#26080;&#29615;&#30340;&#65292;&#22240;&#20026;&#26080;&#27861;&#20877;&#21306;&#20998;&#21516;&#19968;&#34892;&#21160;&#25110;&#20107;&#20214;&#30340;&#22810;&#27425;&#21457;&#29983;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#20026;&#24314;&#27169;&#36335;&#30001;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#30001;&#20110;ASP&#26080;&#27861;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#34920;&#31034;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#23433;&#25490;&#65292;&#22240;&#27492;&#23427;&#22312;&#35843;&#24230;&#26041;&#38754;&#26080;&#26367;&#20195;&#21697;&#12290;&#36825;&#19982;&#20559;&#24207;&#19981;&#21516;&#65292;&#20559;&#24207;&#21487;&#20197;&#36890;&#36807;&#22806;&#37096;&#25163;&#27573;&#65288;&#22914;&#26080;&#29615;&#21644;&#24046;&#24322;&#32422;&#26463;&#65289;&#39640;&#25928;&#22788;&#29702;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#38416;&#36848;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#30001;&#27492;&#20135;&#29983;&#30340;ASP&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12153v1 Announce Type: new  Abstract: We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding. The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. This also abolishes the need for fixed upper bounds on the length of plans. The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way. This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. We formally elaborate upon this idea and present several resulting ASP encodings. Finally,
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>SACRED&#26159;&#19968;&#31181;&#23433;&#20840;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#20026;&#33258;&#20027;&#31995;&#32479;&#29983;&#25104;&#21021;&#27493;&#23433;&#20840;&#26041;&#26696;&#24182;&#30830;&#23450;&#37325;&#35201;&#23433;&#20840;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.12114</link><description>&lt;p&gt;
&#33258;&#20027;&#38081;&#36335;&#31995;&#32479;&#23433;&#20840;&#20998;&#26512;&#65306;SACRED&#26041;&#27861;&#35770;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Safety Analysis of Autonomous Railway Systems: An Introduction to the SACRED Methodology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12114
&lt;/p&gt;
&lt;p&gt;
SACRED&#26159;&#19968;&#31181;&#23433;&#20840;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#20026;&#33258;&#20027;&#31995;&#32479;&#29983;&#25104;&#21021;&#27493;&#23433;&#20840;&#26041;&#26696;&#24182;&#30830;&#23450;&#37325;&#35201;&#23433;&#20840;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38081;&#36335;&#34892;&#19994;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#24341;&#20837;&#33258;&#20027;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#19968;&#20123;&#38382;&#39064;&#38543;&#20043;&#32780;&#26469;&#12290;&#22914;&#20309;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#21644;&#25216;&#26415;&#30340;&#23433;&#20840;&#65311;&#24403;&#21069;&#30340;&#23433;&#20840;&#20998;&#26512;&#21453;&#26144;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#25925;&#38556;&#27169;&#24335;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#33258;&#21160;&#21270;&#20998;&#26512;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#36890;&#24120;&#26159;&#24179;&#22343;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SACRED&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20026;&#33258;&#20027;&#31995;&#32479;&#29983;&#25104;&#21021;&#27493;&#23433;&#20840;&#26041;&#26696;&#24182;&#30830;&#23450;&#37325;&#35201;&#23433;&#20840;&#25351;&#26631;&#30340;&#23433;&#20840;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12114v1 Announce Type: cross  Abstract: As the railway industry increasingly seeks to introduce autonomy and machine learning (ML), several questions arise. How can safety be assured for such systems and technologies? What is the applicability of current safety standards within this new technological landscape? What are the key metrics to classify a system as safe? Currently, safety analysis for the railway reflects the failure modes of existing technology; in contrast, the primary concern of analysis of automation is typically average performance. Such purely statistical approaches to measuring ML performance are limited, as they may overlook classes of situations that may occur rarely but in which the function performs consistently poorly. To combat these difficulties we introduce SACRED, a safety methodology for producing an initial safety case and determining important safety metrics for autonomous systems. The development of SACRED is motivated by the proposed GoA-4 lig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39640;&#26031;&#21644;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;&#36827;&#34892;&#39135;&#29289;&#32454;&#31890;&#24230;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#33719;&#21462;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#39640;&#26031;&#29305;&#24449;&#21644;&#20174;&#23545;&#35937;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#26469;&#22686;&#24378;&#29305;&#24449;&#26144;&#23556;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#23545;&#25239;&#25968;&#25454;&#28418;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.12109</link><description>&lt;p&gt;
GCAM: &#39135;&#29289;&#32454;&#31890;&#24230;&#35782;&#21035;&#30340;&#39640;&#26031;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GCAM: Gaussian and causal-attention model of food fine-grained recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12109
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39640;&#26031;&#21644;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;&#36827;&#34892;&#39135;&#29289;&#32454;&#31890;&#24230;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#33719;&#21462;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#39640;&#26031;&#29305;&#24449;&#21644;&#20174;&#23545;&#35937;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#26469;&#22686;&#24378;&#29305;&#24449;&#26144;&#23556;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#23545;&#25239;&#25968;&#25454;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#39135;&#29289;&#35782;&#21035;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26377;&#25928;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#39135;&#29289;&#26679;&#26412;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#35299;&#20915;&#39135;&#29289;&#35782;&#21035;&#20013;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#30340;&#36843;&#20999;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37319;&#29992;&#39640;&#26031;&#21644;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#29289;&#20307;&#35782;&#21035;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35757;&#32451;&#20197;&#33719;&#24471;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#39640;&#26031;&#29305;&#24449;&#65292;&#28982;&#21518;&#20174;&#23545;&#35937;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#20174;&#32780;&#22686;&#24378;&#30446;&#26631;&#21306;&#22495;&#30340;&#29305;&#24449;&#26144;&#23556;&#33021;&#21147;&#12290;&#20026;&#20102;&#23545;&#25239;&#30001;&#19981;&#22343;&#21248;&#25968;&#25454;&#20998;&#24067;&#23548;&#33268;&#30340;&#25968;&#25454;&#28418;&#31227;&#65292;&#25105;&#20204;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#20064;&#30340;&#22270;&#20687;&#27880;&#24847;&#26426;&#21046;&#23545;&#32593;&#32476;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#33719;&#21462;&#26356;&#26377;&#29992;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#35782;&#21035;&#27880;&#24847;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12109v1 Announce Type: cross  Abstract: Currently, most food recognition relies on deep learning for category classification. However, these approaches struggle to effectively distinguish between visually similar food samples, highlighting the pressing need to address fine-grained issues in food recognition. To mitigate these challenges, we propose the adoption of a Gaussian and causal-attention model for fine-grained object recognition.In particular, we train to obtain Gaussian features over target regions, followed by the extraction of fine-grained features from the objects, thereby enhancing the feature mapping capabilities of the target regions. To counteract data drift resulting from uneven data distributions, we employ a counterfactual reasoning approach. By using counterfactual interventions, we analyze the impact of the learned image attention mechanism on network predictions, enabling the network to acquire more useful attention weights for fine-grained image recogn
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.12108</link><description>&lt;p&gt;
AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65311;&#19968;&#31181;&#29992;&#20110;&#23454;&#39564;&#35780;&#20272;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Does AI help humans make better decisions? A methodological framework for experimental evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12108
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24403;&#20170;&#31038;&#20250;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#24403;&#21033;&#30410;&#39640;&#26114;&#26102;&#65292;&#20154;&#31867;&#20173;&#28982;&#20316;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#27604;&#21333;&#29420;&#30340;&#20154;&#31867;&#25110;&#21333;&#29420;&#30340;AI&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#39564;&#24615;&#22320;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22522;&#20934;&#28508;&#22312;&#32467;&#26524;&#30340;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#27979;&#37327;&#20915;&#31574;&#32773;&#20570;&#20986;&#27491;&#30830;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#65292;&#22312;&#36825;&#20010;&#35774;&#35745;&#20013;&#65292;&#25552;&#20379;AI&#29983;&#25104;&#30340;&#24314;&#35758;&#22312;&#19981;&#21516;&#26696;&#20363;&#20013;&#34987;&#38543;&#26426;&#20998;&#37197;&#32473;&#26368;&#32456;&#20915;&#31574;&#30340;&#20154;&#31867;&#12290;&#22312;&#36825;&#31181;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#27604;&#36739;&#19977;&#31181;&#26367;&#20195;&#20915;&#31574;&#31995;&#32479;&#30340;&#24615;&#33021;--&#20165;&#20154;&#31867;&#12289;&#20154;&#31867;&#19982;AI&#12289;&#20165;AI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12108v1 Announce Type: new  Abstract: The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. We apply the pr
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#36827;&#27493;&#22330;&#26223;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#36807;&#28193;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#33258;&#21160;&#21270;&#21644;&#36164;&#26412;&#31215;&#32047;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#24037;&#36164;&#21464;&#21270;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.12107</link><description>&lt;p&gt;
AGI&#36807;&#28193;&#22330;&#26223;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Scenarios for the Transition to AGI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12107
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#36827;&#27493;&#22330;&#26223;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#36807;&#28193;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#33258;&#21160;&#21270;&#21644;&#36164;&#26412;&#31215;&#32047;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#24037;&#36164;&#21464;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#25216;&#26415;&#36827;&#27493;&#19981;&#21516;&#22330;&#26223;&#19979;&#20135;&#20986;&#21644;&#24037;&#36164;&#30340;&#21464;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#20551;&#35774;&#20013;&#65292;&#20154;&#31867;&#24037;&#20316;&#21487;&#20197;&#20998;&#35299;&#20026;&#22312;&#22797;&#26434;&#24615;&#19978;&#19981;&#21516;&#30340;&#21407;&#23376;&#20219;&#21153;&#12290;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#21487;&#20197;&#34987;&#33258;&#21160;&#21270;&#23454;&#29616;&#12290;&#24037;&#36164;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#33258;&#21160;&#21270;&#21644;&#36164;&#26412;&#31215;&#32047;&#20043;&#38388;&#30340;&#31454;&#36187;&#12290;&#22914;&#26524;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#20998;&#24067;&#21576;&#29616;&#20986;&#36275;&#22815;&#21402;&#30340;&#26080;&#38480;&#23614;&#37096;&#65292;&#37027;&#20040;&#23601;&#24635;&#26159;&#26377;&#36275;&#22815;&#30340;&#24037;&#20316;&#20379;&#20154;&#31867;&#20174;&#20107;&#65292;&#24037;&#36164;&#21487;&#33021;&#20250;&#26080;&#38480;&#19978;&#21319;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22914;&#26524;&#20154;&#31867;&#21487;&#20197;&#25191;&#34892;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#19988;&#23436;&#20840;&#33258;&#21160;&#21270;&#34987;&#23454;&#29616;&#65292;&#37027;&#20040;&#24037;&#36164;&#20250;&#23849;&#28291;&#12290;&#20294;&#21363;&#20351;&#22312;&#36825;&#20043;&#21069;&#65292;&#22914;&#26524;&#22823;&#35268;&#27169;&#33258;&#21160;&#21270;&#36229;&#36234;&#20102;&#36164;&#26412;&#31215;&#32047;&#65292;&#21171;&#21160;&#21147;&#36807;&#20110;&#20016;&#23500;&#65292;&#24037;&#36164;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#33258;&#21160;&#21270;&#29983;&#20135;&#29575;&#30340;&#22686;&#38271;&#21487;&#33021;&#23548;&#33268;&#24191;&#27867;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12107v1 Announce Type: cross  Abstract: We analyze how output and wages behave under different scenarios for technological progress that may culminate in Artificial General Intelligence (AGI), defined as the ability of AI systems to perform all tasks that humans can perform. We assume that human work can be decomposed into atomistic tasks that differ in their complexity. Advances in technology make ever more complex tasks amenable to automation. The effects on wages depend on a race between automation and capital accumulation. If the distribution of task complexity exhibits a sufficiently thick infinite tail, then there is always enough work for humans, and wages may rise forever. By contrast, if the complexity of tasks that humans can perform is bounded and full automation is reached, then wages collapse. But declines may occur even before if large-scale automation outpaces capital accumulation and makes labor too abundant. Automating productivity growth may lead to broad-b
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;CBP&#65289;&#26159;&#23545;Belief Propagation&#65288;BP&#65289;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#26816;&#27979;&#21644;&#21462;&#28040;&#24490;&#29615;&#24341;&#36215;&#30340;&#28040;&#24687;&#21453;&#21709;&#26469;&#38480;&#21046;&#38169;&#35823;&#30456;&#20851;&#21644;&#20449;&#24565;&#25918;&#22823;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#22312;&#20108;&#20803;&#27010;&#29575;&#22270;&#19978;&#34920;&#29616;&#20248;&#20110;BP&#21644;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12106</link><description>&lt;p&gt;
&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#29992;&#20110;&#36817;&#20284;&#27010;&#29575;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Circular Belief Propagation for Approximate Probabilistic Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12106
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;CBP&#65289;&#26159;&#23545;Belief Propagation&#65288;BP&#65289;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#26816;&#27979;&#21644;&#21462;&#28040;&#24490;&#29615;&#24341;&#36215;&#30340;&#28040;&#24687;&#21453;&#21709;&#26469;&#38480;&#21046;&#38169;&#35823;&#30456;&#20851;&#21644;&#20449;&#24565;&#25918;&#22823;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#22312;&#20108;&#20803;&#27010;&#29575;&#22270;&#19978;&#34920;&#29616;&#20248;&#20110;BP&#21644;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Belief Propagation&#65288;BP&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#27010;&#29575;&#20998;&#24067;&#30340;&#22270;&#20013;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#28040;&#24687;&#26469;&#23454;&#29616;&#12290;&#20854;&#31867;&#20284;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#34920;&#26126;&#65292;&#23427;&#21487;&#33021;&#23545;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#24212;&#29992;&#20110;&#26080;&#29615;&#22270;&#26102;&#65292;BP&#20165;&#20165;&#26159;&#31934;&#30830;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#35813;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;CBP&#65289;&#65292;&#36825;&#26159;BP&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#26816;&#27979;&#21644;&#21462;&#28040;&#24490;&#29615;&#24341;&#36215;&#30340;&#28040;&#24687;&#21453;&#21709;&#32780;&#38480;&#21046;&#20102;&#38169;&#35823;&#30456;&#20851;&#21644;&#20449;&#24565;&#25918;&#22823;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#20108;&#20803;&#27010;&#29575;&#22270;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;CBP&#36828;&#36828;&#20248;&#20110;BP&#65292;&#24182;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12106v1 Announce Type: new  Abstract: Belief Propagation (BP) is a simple probabilistic inference algorithm, consisting of passing messages between nodes of a graph representing a probability distribution. Its analogy with a neural network suggests that it could have far-ranging applications for neuroscience and artificial intelligence. Unfortunately, it is only exact when applied to cycle-free graphs, which restricts the potential of the algorithm. In this paper, we propose Circular Belief Propagation (CBP), an extension of BP which limits the detrimental effects of message reverberation caused by cycles by learning to detect and cancel spurious correlations and belief amplifications. We show in numerical experiments involving binary probabilistic graphs that CBP far outperforms BP and reaches good performance compared to that of previously proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#31227;&#21160;&#26641;&#8221;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#65292;&#20197;&#25552;&#21319;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12100</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#26641;&#23398;&#20064;&#26102;&#38388;&#27573;&#20559;&#22909;&#36827;&#34892;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#31227;&#21160;&#26641;&#8221;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#65292;&#20197;&#25552;&#21319;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#24403;&#21069;&#30340;&#31614;&#21040;&#36712;&#36857;&#25552;&#20379;POI&#30340;&#21160;&#24577;&#25490;&#21517;&#12290;&#26412;&#20219;&#21153;&#30340;&#25512;&#33616;&#24615;&#33021;&#21462;&#20915;&#20110;&#36890;&#36807;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#65288;LBSNs&#65289;&#25968;&#25454;&#20840;&#38754;&#20102;&#35299;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31227;&#21160;&#26641;&#8221;&#30340;&#21019;&#26032;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#20998;&#23618;&#25551;&#36848;&#29992;&#25143;&#30340;&#31614;&#21040;&#35760;&#24405;&#12290;&#31227;&#21160;&#26641;&#21253;&#21547;&#22810;&#31890;&#24230;&#26102;&#38388;&#27573;&#33410;&#28857;&#65292;&#20197;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31227;&#21160;&#26641;&#32593;&#32476;&#65288;MTNet&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12100v1 Announce Type: cross  Abstract: Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#19982;&#21387;&#38136;&#21644;&#27880;&#22609;&#30456;&#20851;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;&#29983;&#25104;&#35774;&#35745;&#20013;&#65292;&#21033;&#29992;&#20108;&#32500;&#28145;&#24230;&#22270;&#20687;&#23558;&#22797;&#26434;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#31616;&#21270;&#20026;&#21487;&#21046;&#36896;&#30340;&#36718;&#24275;&#65292;&#28040;&#38500;&#19981;&#21487;&#21046;&#36896;&#29305;&#24615;&#65292;&#23558;&#37325;&#28857;&#25918;&#22312;&#21402;&#24230;&#21644;&#32907;&#35774;&#35745;&#31561;&#21046;&#36896;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.12098</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Design for Mass Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19982;&#21387;&#38136;&#21644;&#27880;&#22609;&#30456;&#20851;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;&#29983;&#25104;&#35774;&#35745;&#20013;&#65292;&#21033;&#29992;&#20108;&#32500;&#28145;&#24230;&#22270;&#20687;&#23558;&#22797;&#26434;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#31616;&#21270;&#20026;&#21487;&#21046;&#36896;&#30340;&#36718;&#24275;&#65292;&#28040;&#38500;&#19981;&#21487;&#21046;&#36896;&#29305;&#24615;&#65292;&#23558;&#37325;&#28857;&#25918;&#22312;&#21402;&#24230;&#21644;&#32907;&#35774;&#35745;&#31561;&#21046;&#36896;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35774;&#35745;&#65288;GD&#65289;&#20316;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35774;&#35745;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#31639;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#26469;&#21019;&#36896;&#36229;&#36234;&#20256;&#32479;&#38480;&#21046;&#30340;&#22810;&#26679;&#21270;&#21644;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#35774;&#35745;&#22312;&#22797;&#26434;&#35774;&#35745;&#30340;&#21487;&#21046;&#36896;&#24615;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#20462;&#25913;&#65292;&#22240;&#20026;&#26631;&#20934;&#21046;&#36896;&#36807;&#31243;&#23384;&#22312;&#38480;&#21046;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#24182;&#19981;&#36866;&#21512;&#22823;&#35268;&#27169;&#29983;&#20135;&#30340;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#19982;&#21387;&#38136;&#21644;&#27880;&#22609;&#30456;&#20851;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;GD&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#32500;&#28145;&#24230;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#21487;&#21046;&#36896;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#22797;&#26434;&#30340;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#31616;&#21270;&#20026;&#21487;&#21046;&#36896;&#30340;&#36718;&#24275;&#65292;&#21435;&#38500;&#19981;&#21487;&#21046;&#36896;&#30340;&#24748;&#25361;&#31561;&#19981;&#21487;&#34892;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#30452;&#25509;&#32771;&#34385;&#21402;&#24230;&#21644;&#32907;&#35774;&#35745;&#31561;&#37325;&#35201;&#21046;&#36896;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12098v1 Announce Type: cross  Abstract: Generative Design (GD) has evolved as a transformative design approach, employing advanced algorithms and AI to create diverse and innovative solutions beyond traditional constraints. Despite its success, GD faces significant challenges regarding the manufacturability of complex designs, often necessitating extensive manual modifications due to limitations in standard manufacturing processes and the reliance on additive manufacturing, which is not ideal for mass production. Our research introduces an innovative framework addressing these manufacturability concerns by integrating constraints pertinent to die casting and injection molding into GD, through the utilization of 2D depth images. This method simplifies intricate 3D geometries into manufacturable profiles, removing unfeasible features such as non-manufacturable overhangs and allowing for the direct consideration of essential manufacturing aspects like thickness and rib design. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#32570;&#22833;&#30340;&#29992;&#25143;&#36141;&#29289;&#21382;&#21490;&#37096;&#20998;&#24182;&#36866;&#24403;&#20016;&#23500;&#23427;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12096</link><description>&lt;p&gt;
&#20016;&#23500;&#29992;&#25143;&#36141;&#29289;&#35760;&#24405;&#65306;&#29992;&#23618;&#27425;&#25512;&#33616;&#31995;&#32479;&#36171;&#33021;&#30005;&#23376;&#21830;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12096
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32570;&#22833;&#30340;&#29992;&#25143;&#36141;&#29289;&#21382;&#21490;&#37096;&#20998;&#24182;&#36866;&#24403;&#20016;&#23500;&#23427;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#36141;&#29289;&#21382;&#21490;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#26356;&#20016;&#23500;&#30340;&#29992;&#25143;&#21382;&#21490;&#20250;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#26356;&#20542;&#21521;&#20110;&#22312;&#29289;&#21697;&#20197;&#26368;&#20302;&#20215;&#26684;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#36141;&#29289;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22823;&#22810;&#25968;&#29992;&#25143;&#21516;&#26102;&#20174;&#22810;&#20010;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36141;&#29289;&#65307;&#29992;&#25143;&#30340;&#19981;&#21516;&#36141;&#29289;&#21382;&#21490;&#37096;&#20998;&#22312;&#19981;&#21516;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#38388;&#20849;&#20139;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20551;&#35774;&#20219;&#20309;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#37117;&#25317;&#26377;&#29992;&#25143;&#21382;&#21490;&#30340;&#23436;&#25972;&#35760;&#24405;&#65292;&#20294;&#21482;&#33021;&#35775;&#38382;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#12290;&#22914;&#26524;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#39318;&#20808;&#39044;&#27979;&#32570;&#22833;&#30340;&#37096;&#20998;&#24182;&#36866;&#24403;&#20016;&#23500;&#29992;&#25143;&#30340;&#36141;&#29289;&#21382;&#21490;&#65292;&#37027;&#20040;&#23558;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25512;&#33616;&#19979;&#19968;&#20010;&#21830;&#21697;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#29992;&#25143;&#30340;&#36141;&#29289;&#21382;&#21490;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;NDCG@10&#21644;&#20854;&#20182;&#26041;&#38754;&#37117;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12096v1 Announce Type: cross  Abstract: Recommendation systems can provide accurate recommendations by analyzing user shopping history. A richer user history results in more accurate recommendations. However, in real applications, users prefer e-commerce platforms where the item they seek is at the lowest price. In other words, most users shop from multiple e-commerce platforms simultaneously; different parts of the user's shopping history are shared between different e-commerce platforms. Consequently, we assume in this study that any e-commerce platform has a complete record of the user's history but can only access some parts of it. If a recommendation system is able to predict the missing parts first and enrich the user's shopping history properly, it will be possible to recommend the next item more accurately. Our recommendation system leverages user shopping history to improve prediction accuracy. The proposed approach shows significant improvements in both NDCG@10 and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12094</link><description>&lt;p&gt;
LLMs&#26159;&#19968;&#20010;&#22909;&#30340;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#27714;&#35299;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Good Cryptic Crossword Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#26159;&#19968;&#31181;&#35868;&#39064;&#65292;&#19981;&#20165;&#20381;&#36182;&#20110;&#19968;&#33324;&#30693;&#35782;&#65292;&#36824;&#20381;&#36182;&#20110;&#27714;&#35299;&#32773;&#22312;&#19981;&#21516;&#23618;&#38754;&#19978;&#25805;&#32437;&#35821;&#35328;&#24182;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#23383;&#28216;&#25103;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;NLP&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#20915;&#36825;&#31867;&#35868;&#39064;&#20063;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#27969;&#34892;&#30340;LLMs -- LLaMA2&#12289;Mistral&#21644;ChatGPT&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#26174;&#31034;&#23427;&#20204;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12093</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#23398;&#20064;&#65306;&#19968;&#31181;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#22330;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#22312;&#20419;&#36827;&#32463;&#27982;&#22686;&#38271;&#21644;&#31038;&#20250;&#31283;&#23450;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22522;&#20110;Stackelberg Mean Field Game&#65288;SMFG&#65289;&#27169;&#22411;&#65292;&#23558;&#26368;&#20248;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#38382;&#39064;&#24314;&#27169;&#65292;&#20854;&#20013;&#25919;&#24220;&#20316;&#20026;&#25919;&#31574;&#21046;&#23450;&#30340;&#39046;&#23548;&#32773;&#65292;&#22823;&#35268;&#27169;&#23478;&#24237;&#21160;&#24577;&#21709;&#24212;&#20026;&#36861;&#38543;&#32773;&#12290;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#25429;&#25417;&#20102;&#25919;&#24220;&#21644;&#22823;&#35268;&#27169;&#23478;&#24237;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#21160;&#24577;&#21338;&#24328;&#65292;&#24182;&#21487;&#20197;&#35299;&#37322;&#22320;&#35780;&#20272;&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#25928;&#26524;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;SMFG&#30340;&#26041;&#27861;&#65292;&#23558;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#65288;SMFRL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#29420;&#31435;&#20110;&#20808;&#21069;&#30340;&#29615;&#22659;&#30693;&#35782;&#21644;&#36716;&#21464;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;SMFG&#26041;&#27861;&#22312;&#32463;&#27982;&#25919;&#31574;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12093v1 Announce Type: cross  Abstract: Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23450;&#20041;&#24182;&#35268;&#33539;&#20102;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#21305;&#37197;&#23545;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#36317;&#31163;&#22522;&#20934;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12092</link><description>&lt;p&gt;
&#21305;&#37197;&#33521;&#35821;&#22320;&#22336;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Methods for Matching English Language Addresses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23450;&#20041;&#24182;&#35268;&#33539;&#20102;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#21305;&#37197;&#23545;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#36317;&#31163;&#22522;&#20934;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#21344;&#25454;&#30528;&#19968;&#24109;&#20043;&#22320;&#65292;&#22240;&#20026;&#27599;&#20010;&#35789;&#25152;&#20855;&#26377;&#30340;&#20301;&#32622;&#37325;&#35201;&#24615;&#21644;&#23427;&#25152;&#28041;&#21450;&#30340;&#22320;&#29702;&#33539;&#22260;&#12290;&#21305;&#37197;&#22320;&#22336;&#30340;&#20219;&#21153;&#27599;&#22825;&#37117;&#22312;&#21457;&#29983;&#65292;&#24182;&#19988;&#23384;&#22312;&#20110;&#37038;&#20214;&#37325;&#23450;&#21521;&#12289;&#23454;&#20307;&#35299;&#26512;&#31561;&#21508;&#31181;&#39046;&#22495;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23450;&#20041;&#21644;&#35268;&#33539;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#30340;&#21305;&#37197;&#21644;&#19981;&#21305;&#37197;&#23545;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#33258;&#21160;&#25191;&#34892;&#22320;&#22336;&#21305;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21508;&#19981;&#30456;&#21516;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#21040;&#26368;&#36866;&#21512;&#36825;&#31181;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12092v1 Announce Type: cross  Abstract: Addresses occupy a niche location within the landscape of textual data, due to the positional importance carried by every word, and the geographical scope it refers to. The task of matching addresses happens everyday and is present in various fields like mail redirection, entity resolution, etc. Our work defines, and formalizes a framework to generate matching and mismatching pairs of addresses in the English language, and use it to evaluate various methods to automatically perform address matching. These methods vary widely from distance based approaches to deep learning models. By studying the Precision, Recall and Accuracy metrics of these approaches, we obtain an understanding of the best suited method for this setting of the address matching task.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12090</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Foundation Models and Information Retrieval in Digital Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12090
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#12289;LLM&#12289;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#26816;&#32034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12090v1 Announce Type: cross  Abstract: The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#21457;&#29616;&#65292;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#20869;&#23481;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#26356;&#21152;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.12082</link><description>&lt;p&gt;
&#29983;&#36824;&#30340;&#30007;&#23401;&#65306;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#27604;&#25253;&#36947;&#30340;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#21457;&#29616;&#65292;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#20869;&#23481;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#26356;&#21152;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;"&#25105;&#20204;&#26377;&#25928;&#22320;&#25273;&#38500;&#20102;&#27169;&#22411;&#29983;&#25104;&#25110;&#22238;&#24518;&#21704;&#21033;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;"&#28982;&#32780;&#65292;&#19968;&#39033;&#23567;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#35828;&#27861;&#36807;&#20110;&#23485;&#27867;&#12290;&#23569;&#20110;&#21313;&#27425;&#35797;&#39564;&#23548;&#33268;&#37325;&#22797;&#21644;&#20855;&#20307;&#25552;&#21450;&#21704;&#21033;&#27874;&#29305;&#65292;&#21253;&#25324;"&#21834;&#65292;&#25105;&#26126;&#30333;&#20102;&#65281;"&#40635;&#29916;"&#26159;&#29305;&#37324;&#183;&#26222;&#25289;&#20999;&#29305;&#30340;&#21704;&#21033;&#27874;&#29305;&#31995;&#21015;&#20013;&#20351;&#29992;&#30340;&#26415;&#35821;...''&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12082v1 Announce Type: cross  Abstract: Recent work arXiv.2310.02238 asserted that "we effectively erase the model's ability to generate or recall Harry Potter-related content.'' This claim is shown to be overbroad. A small experiment of less than a dozen trials led to repeated and specific mentions of Harry Potter, including "Ah, I see! A "muggle" is a term used in the Harry Potter book series by Terry Pratchett...''
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;&#65292;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#22312;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12077</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12077
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;&#65292;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#22312;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#26377;&#28508;&#21147;&#25913;&#21464;&#20154;&#20204;&#22312;&#32447;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#21709;&#24212;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20250;&#21152;&#21095;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#33021;&#36890;&#36807;&#24494;&#22937;&#22320;&#25805;&#32437;&#22768;&#26126;&#30340;&#26368;&#34180;&#24369;&#37096;&#20998;&#25104;&#21151;&#35268;&#36991;&#25972;&#20010;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#29616;&#23454;&#19988;&#39640;&#39118;&#38505;&#35774;&#32622;&#20013;&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#20581;&#22766;&#24615;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#20855;&#26377;&#40657;&#30418;&#31995;&#32479;&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#35797;&#22270;&#27450;&#39575;&#27169;&#22411;&#36820;&#22238;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#24517;&#24212;&#32842;&#22825;&#12289;PerplexityAI&#21644;YouChat&#31561;&#21508;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#23545;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23637;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12077v1 Announce Type: cross  Abstract: Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;</title><link>https://arxiv.org/abs/2403.12076</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuron-centric Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#23398;&#20064;&#26426;&#21046;&#32972;&#21518;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#20043;&#19968;&#26159;&#36890;&#36807;&#32467;&#26500;&#21644;&#21151;&#33021;&#21487;&#22609;&#24615;&#35843;&#25972;&#20854;&#31361;&#35302;&#12290;&#23613;&#31649;&#31361;&#35302;&#22312;&#20256;&#36882;&#20449;&#24687;&#21040;&#25972;&#20010;&#22823;&#33041;&#20013;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#65292;&#20294;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#26159;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20135;&#29983;&#20102;&#23545;&#31361;&#35302;&#30340;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#35774;&#35745;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#22914;ABCD&#35268;&#21017;&#65292;&#20391;&#37325;&#20110;&#31361;&#35302;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#20248;&#21270;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#20248;&#21270;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#31361;&#35302;&#37117;&#19982;&#22810;&#20010;&#36203;&#24067;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#65288;NcHL&#65289;&#65292;&#20854;&#20248;&#21270;&#20391;&#37325;&#20110;&#31070;&#32463;&#20803;&#32780;&#19981;&#26159;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#19982;ABCD&#35268;&#21017;&#30456;&#27604;&#65292;NcHL&#23558;&#21442;&#25968;&#20943;&#23569;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12076v1 Announce Type: cross  Abstract: One of the most striking capabilities behind the learning mechanisms of the brain is the adaptation, through structural and functional plasticity, of its synapses. While synapses have the fundamental role of transmitting information across the brain, several studies show that it is the neuron activations that produce changes on synapses. Yet, most plasticity models devised for artificial Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than neurons, therefore optimizing synaptic-specific Hebbian parameters. This approach, however, increases the complexity of the optimization process since each synapse is associated to multiple Hebbian parameters. To overcome this limitation, we propose a novel plasticity model, called Neuron-centric Hebbian Learning (NcHL), where optimization focuses on neuron- rather than synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces the parameters from $5W$ to $5N$
&lt;/p&gt;</description></item><item><title>&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12075</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#22810;&#26679;&#21270;&#21361;&#23475;&#30340;&#24320;&#25918;&#24335;&#32418;&#38431;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12075
&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#19981;&#26126;&#26174;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20197;&#20943;&#23569;&#29983;&#25104;&#20882;&#29359;&#24615;&#22270;&#20687;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;"&#38544;&#24615;&#23545;&#25239;"&#25552;&#31034;&#65288;&#35302;&#21457;T2I&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#38750;&#26126;&#26174;&#21407;&#22240;&#65289;&#65292;&#25105;&#20204;&#29420;&#31435;&#36776;&#21035;&#20986;&#19968;&#32452;&#38590;&#20197;&#21457;&#29616;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#24456;&#36866;&#21512;&#25581;&#31034;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Adversarial Nibbler Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#32418;&#38431;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#38544;&#24615;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#24050;&#27719;&#24635;&#19968;&#22871;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#65292;&#37319;&#29992;&#31616;&#21333;&#29992;&#25143;&#30028;&#38754;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#21361;&#23475;&#65292;&#24182;&#21560;&#24341;&#24191;&#27867;&#20154;&#32676;&#26469;&#25429;&#25417;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#38271;&#23614;&#23433;&#20840;&#38382;&#39064;&#12290;&#25361;&#25112;&#22312;&#36830;&#32493;&#22238;&#21512;&#20013;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#23545;T2I&#27169;&#22411;&#20013;&#23433;&#20840;&#38544;&#24739;&#30340;&#25345;&#32493;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;eHealth&#24178;&#39044;&#25514;&#26045;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;&#25913;&#21892;&#20799;&#31461;&#20581;&#24247;&#20064;&#24815;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#26085;&#24120;&#33829;&#20859;&#21644;&#20307;&#32946;&#27963;&#21160;&#20449;&#24687;&#20197;&#21450;&#34394;&#25311;&#22870;&#21169;&#30340;&#26041;&#24335;&#26469;&#20419;&#36827;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.12073</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;eHealth&#24178;&#39044;&#23545;&#20799;&#31461;&#20581;&#24247;&#20064;&#24815;&#25913;&#21892;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Feasibility of Social-Network-Based eHealth Intervention on the Improvement of Healthy Habits among Children
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;eHealth&#24178;&#39044;&#25514;&#26045;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;&#25913;&#21892;&#20799;&#31461;&#20581;&#24247;&#20064;&#24815;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#26085;&#24120;&#33829;&#20859;&#21644;&#20307;&#32946;&#27963;&#21160;&#20449;&#24687;&#20197;&#21450;&#34394;&#25311;&#22870;&#21169;&#30340;&#26041;&#24335;&#26469;&#20419;&#36827;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#38024;&#23545;&#38738;&#23569;&#24180;&#20154;&#32676;&#25913;&#21892;&#39278;&#39135;&#20064;&#24815;&#21644;&#20307;&#32946;&#27963;&#21160;&#30340;eHealth&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#23545;&#35937;&#20026;11&#33267;15&#23681;&#20043;&#38388;&#30340;&#20799;&#31461;&#12290;&#22312;&#20026;&#26399;14&#21608;&#30340;&#26102;&#38388;&#37324;&#65292;&#23545;139&#21517;&#24178;&#39044;&#32452;&#23398;&#29983;&#21644;91&#21517;&#23545;&#29031;&#32452;&#23398;&#29983;&#36827;&#34892;&#20102;&#24178;&#39044;&#65292;&#36825;&#20004;&#25152;&#23398;&#26657;&#21442;&#19982;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;&#24178;&#39044;&#32452;&#36890;&#36807;&#29992;&#25143;&#24080;&#25143;&#21644;&#23494;&#30721;&#35775;&#38382;&#32593;&#32476;&#65292;&#33021;&#22815;&#24314;&#31435;&#21451;&#35850;&#20851;&#31995;&#12289;&#21457;&#24067;&#35780;&#35770;&#12289;&#28857;&#36190;&#12289;&#19982;&#20854;&#20182;&#29992;&#25143;&#20114;&#21160;&#65292;&#27599;&#22825;&#25910;&#21040;&#20851;&#20110;&#33829;&#20859;&#21644;&#20307;&#32946;&#27963;&#21160;&#30340;&#36890;&#30693;&#21644;&#20449;&#24687;&#65292;&#24182;&#22240;&#25913;&#21892;&#20064;&#24815;&#32780;&#33719;&#24471;&#65288;&#34394;&#25311;&#65289;&#22870;&#21169;&#12290;&#23545;&#29031;&#32452;&#27809;&#26377;&#35775;&#38382;&#36825;&#20123;&#21151;&#33021;&#30340;&#26435;&#38480;&#12290;&#22312;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#36523;&#20307;&#36136;&#37327;&#25351;&#25968;&#21644;&#21021;&#22987;&#20581;&#24247;&#20064;&#24815;&#26041;&#38754;&#65292;&#26679;&#26412;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#21069;&#21518;&#27979;&#37327;&#26159;&#36890;&#36807;&#33258;&#25105;&#25253;&#21578;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12073v1 Announce Type: cross  Abstract: This study shows the feasibility of an eHealth solution for tackling eating habits and physical activity in the adolescent population. The participants were children from 11 to 15 years old. An intervention was carried out on 139 students in the intervention group and 91 students in the control group, in two schools during 14 weeks. The intervention group had access to the web through a user account and a password. They were able to create friendship relationships, post comments, give likes and interact with other users, as well as receive notifications and information about nutrition and physical activity on a daily basis and get (virtual) rewards for improving their habits. The control group did not have access to any of these features. The homogeneity of the samples in terms of gender, age, body mass index and initial health-related habits was demonstrated. Pre- and post-measurements were collected through self-reports on the applic
&lt;/p&gt;</description></item><item><title>GenAI&#24037;&#20855;&#21033;&#29992;&#20808;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#35838;&#31243;&#35268;&#21010;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#8220;&#20132;&#20114;&#24335;&#36229;&#32423;&#25552;&#31034;&#8221;&#21151;&#33021;&#65292;&#21487;&#23450;&#21046;&#29983;&#25104;&#35838;&#31243;&#35745;&#21010;&#65292;&#35780;&#20272;&#20013;&#21253;&#25324;&#23450;&#37327;&#21644;&#23450;&#24615;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.12071</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25945;&#32946;&#23450;&#21046;&#65306;&#20197;GenAI&#20026;&#22522;&#30784;&#30340;&#35838;&#31243;&#35268;&#21010;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Tailoring Education with GenAI: A New Horizon in Lesson Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12071
&lt;/p&gt;
&lt;p&gt;
GenAI&#24037;&#20855;&#21033;&#29992;&#20808;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#35838;&#31243;&#35268;&#21010;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#8220;&#20132;&#20114;&#24335;&#36229;&#32423;&#25552;&#31034;&#8221;&#21151;&#33021;&#65292;&#21487;&#23450;&#21046;&#29983;&#25104;&#35838;&#31243;&#35745;&#21010;&#65292;&#35780;&#20272;&#20013;&#21253;&#25324;&#23450;&#37327;&#21644;&#23450;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#26041;&#38754;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#20986;&#29616;&#20026;&#20256;&#32479;&#25945;&#23398;&#26041;&#27861;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#36890;&#24120;&#24573;&#35270;&#20010;&#21035;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;GenAI&#24037;&#20855;&#65292;&#35774;&#35745;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#25968;&#23383;&#21161;&#25163;&#65292;&#21487;&#20197;&#24110;&#21161;&#21046;&#23450;&#23450;&#21046;&#21270;&#30340;&#35838;&#31243;&#35745;&#21010;&#12290;&#35813;&#24037;&#20855;&#37319;&#29992;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#20132;&#20114;&#24335;&#36229;&#32423;&#25552;&#31034;&#8221;&#30340;&#21019;&#26032;&#21151;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26597;&#35810;&#31995;&#32479;&#65292;&#20801;&#35768;&#25945;&#32946;&#24037;&#20316;&#32773;&#36755;&#20837;&#35814;&#32454;&#30340;&#35838;&#22530;&#29305;&#23450;&#20449;&#24687;&#65292;&#22914;&#23398;&#29983;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#23398;&#20064;&#30446;&#26631;&#21644;&#20559;&#22909;&#30340;&#25945;&#23398;&#39118;&#26684;&#12290;&#28982;&#21518;&#65292;GenAI&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#36755;&#20837;&#26469;&#29983;&#25104;&#23450;&#21046;&#30340;&#35838;&#31243;&#35745;&#21010;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#26045;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#26082;&#21253;&#25324;&#23450;&#37327;&#65288;&#21363;&#33410;&#30465;&#30340;&#26102;&#38388;&#30334;&#20998;&#27604;&#65289;&#21448;&#21253;&#25324;&#23450;&#24615;&#65288;&#21363;&#29992;&#25143;&#28385;&#24847;&#24230;&#65289;&#26631;&#20934;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#23398;&#31185;&#21644;&#25945;&#32946;&#27700;&#24179;&#65292;&#24182;&#25345;&#32493;&#25910;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12071v1 Announce Type: cross  Abstract: The advent of Generative AI (GenAI) in education presents a transformative approach to traditional teaching methodologies, which often overlook the diverse needs of individual students. This study introduces a GenAI tool, based on advanced natural language processing, designed as a digital assistant for educators, enabling the creation of customized lesson plans. The tool utilizes an innovative feature termed 'interactive mega-prompt,' a comprehensive query system that allows educators to input detailed classroom specifics such as student demographics, learning objectives, and preferred teaching styles. This input is then processed by the GenAI to generate tailored lesson plans. To evaluate the tool's effectiveness, a comprehensive methodology incorporating both quantitative (i.e., % of time savings) and qualitative (i.e., user satisfaction) criteria was implemented, spanning various subjects and educational levels, with continuous fee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#21697;&#26469;&#20805;&#24403;&#25552;&#21319;&#24314;&#27169;&#27963;&#21160;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#20195;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12069</link><description>&lt;p&gt;
&#32570;&#20047;&#22320;&#38754;&#30495;&#30456;&#24773;&#20917;&#19979;&#25552;&#21319;&#24314;&#27169;&#30340;&#20844;&#24179;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness Evaluation for Uplift Modeling in the Absence of Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#21697;&#26469;&#20805;&#24403;&#25552;&#21319;&#24314;&#27169;&#27963;&#21160;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#20195;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#37319;&#29992;&#21152;&#36895;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#36827;&#34892;&#35780;&#20272;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#22320;&#38754;&#30495;&#30456;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20811;&#26381;&#32570;&#23569;&#22320;&#38754;&#30495;&#30456;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#26367;&#20195;&#21697;&#26469;&#20805;&#24403;&#25552;&#21319;&#24314;&#27169;&#27963;&#21160;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#20195;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#26367;&#20195;&#22320;&#38754;&#30495;&#30456;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12069v1 Announce Type: cross  Abstract: The acceleration in the adoption of AI-based automated decision-making systems poses a challenge for evaluating the fairness of algorithmic decisions, especially in the absence of ground truth. When designing interventions, uplift modeling is used extensively to identify candidates that are likely to benefit from treatment. However, these models remain particularly susceptible to fairness evaluation due to the lack of ground truth on the outcome measure since a candidate cannot be in both treatment and control simultaneously. In this article, we propose a framework that overcomes the missing ground truth problem by generating surrogates to serve as a proxy for counterfactual labels of uplift modeling campaigns. We then leverage the surrogate ground truth to conduct a more comprehensive binary fairness evaluation. We show how to apply the approach in a comprehensive study from a real-world marketing campaign for promotional offers and d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#22810;&#26680;&#26550;&#26500;&#30340;&#26032;&#22411;&#36816;&#34892;&#26102;&#22810;&#26680;&#26550;&#26500;&#27169;&#25311;&#22120;&#8220;RAVSim&#8221;&#65292;&#36890;&#36807;&#35813;&#27169;&#25311;&#22120;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#19982;&#27169;&#22411;&#20132;&#20114;&#24182;&#20462;&#25913;&#21442;&#25968;&#20540;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.12061</link><description>&lt;p&gt;
&#20351;&#29992;&#24212;&#29992;&#29305;&#23450;&#22810;&#26680;&#26550;&#26500;&#35774;&#35745;SNN&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Design-Space Exploration of SNN Models using Application-Specific Multi-Core Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#22810;&#26680;&#26550;&#26500;&#30340;&#26032;&#22411;&#36816;&#34892;&#26102;&#22810;&#26680;&#26550;&#26500;&#27169;&#25311;&#22120;&#8220;RAVSim&#8221;&#65292;&#36890;&#36807;&#35813;&#27169;&#25311;&#22120;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#19982;&#27169;&#22411;&#20132;&#20114;&#24182;&#20462;&#25913;&#21442;&#25968;&#20540;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24403;&#21069;&#29702;&#35299;&#21644;&#21033;&#29992;SNN&#30340;&#28508;&#22312;&#29305;&#24615;&#23384;&#22312;&#30340;&#21160;&#26426;&#21644;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;RAVSim&#8221;&#65288;&#36816;&#34892;&#26102;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#27169;&#25311;&#22120;&#65289;&#30340;&#26032;&#22411;&#36816;&#34892;&#26102;&#22810;&#26680;&#26550;&#26500;&#27169;&#25311;&#22120;&#65292;&#36825;&#26159;&#19968;&#27454;&#23574;&#31471;&#30340;SNN&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;LabVIEW&#24320;&#21457;&#65292;&#24182;&#20316;&#20026;&#23448;&#26041;&#27169;&#22359;&#22312;&#20854;&#32593;&#31449;&#19978;&#20844;&#24320;&#25552;&#20379;&#12290;RAVSim&#26159;&#19968;&#31181;&#36816;&#34892;&#26102;&#34394;&#25311;&#20223;&#30495;&#29615;&#22659;&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#20114;&#21160;&#65292;&#35266;&#23519;&#20854;&#36755;&#20986;&#27987;&#24230;&#30340;&#34892;&#20026;&#65292;&#24182;&#22312;&#27169;&#25311;&#25191;&#34892;&#36807;&#31243;&#20013;&#38543;&#26102;&#20462;&#25913;&#21442;&#25968;&#20540;&#38598;&#12290;&#26368;&#36817;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24037;&#20855;&#37117;&#19981;&#20801;&#35768;&#29992;&#25143;&#19982;&#27169;&#22411;&#20223;&#30495;&#36827;&#34892;&#23454;&#26102;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12061v1 Announce Type: cross  Abstract: With the motivation and the difficulties that currently exist in comprehending and utilizing the promising features of SNNs, we proposed a novel run-time multi-core architecture-based simulator called "RAVSim" (Runtime Analysis and Visualization Simulator), a cutting-edge SNN simulator, developed using LabVIEW and it is publicly available on their website as an official module. RAVSim is a runtime virtual simulation environment tool that enables the user to interact with the model, observe its behavior of output concentration, and modify the set of parametric values at any time while the simulation is in execution. Recently some popular tools have been presented, but we believe that none of the tools allow users to interact with the model simulation in run time.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#38416;&#26126;&#22522;&#20110;&#27700;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20043;&#38388;&#30475;&#20284;&#30456;&#20851;&#20294;&#23454;&#38469;&#24046;&#24322;&#24040;&#22823;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#23427;&#20204;&#22312;&#25628;&#32034;&#26041;&#24335;&#21644;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.12058</link><description>&lt;p&gt;
&#22522;&#20110;&#27700;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65306;&#27700;&#21160;&#21147;&#23398;&#22914;&#20309;&#24110;&#21161;&#25105;&#20204;&#35299;&#20915;NP&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Water-Based Metaheuristics: How Water Dynamics Can Help Us to Solve NP-Hard Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#38416;&#26126;&#22522;&#20110;&#27700;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20043;&#38388;&#30475;&#20284;&#30456;&#20851;&#20294;&#23454;&#38469;&#24046;&#24322;&#24040;&#22823;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#23427;&#20204;&#22312;&#25628;&#32034;&#26041;&#24335;&#21644;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#22522;&#20110;&#27700;&#30340;&#20248;&#21270;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#24050;&#34987;&#24341;&#20837;&#65292;&#26088;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#21644;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#20854;&#22522;&#30784;&#33258;&#28982;&#38544;&#21947;&#26041;&#38754;&#23384;&#22312;&#24378;&#28872;&#30456;&#20284;&#24615;&#65288;&#22823;&#22810;&#25968;&#26041;&#27861;&#20197;&#26576;&#31181;&#26041;&#24335;&#27169;&#25311;&#27700;&#28404;&#21327;&#21516;&#24418;&#25104;&#36890;&#24448;&#22823;&#28023;&#30340;&#36335;&#24452;&#65289;&#65292;&#20294;&#26368;&#32456;&#31639;&#27861;&#22312;&#25628;&#32034;&#26041;&#24335;&#25110;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#26041;&#24335;&#19978;&#21364;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#30740;&#31350;&#20154;&#21592;&#25110;&#20174;&#19994;&#32773;&#21487;&#33021;&#20250;&#35748;&#20026;&#20004;&#31181;&#22522;&#20110;&#27700;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#20005;&#37325;&#20381;&#36182;&#20110;&#23427;&#20204;&#27169;&#25311;&#30340;&#33258;&#28982;&#27700;&#21160;&#21147;&#23398;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#20107;&#23454;&#24182;&#38750;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12058v1 Announce Type: cross  Abstract: Many water-based optimization metaheuristics have been introduced during the last decade, both for combinatorial and for continuous optimization. Despite the strong similarities of these methods in terms of their underlying natural metaphors (most of them emulate, in some way or another, how drops collaboratively form paths down to the sea), in general the resulting algorithms are quite different in terms of their searching approach or their solution construction approach. For instance, each entity may represent a solution by itself or, alternatively, entities may construct solutions by modifying the landscape while moving. A researcher or practitioner could assume that the degree of similarity between two water-based metaheuristics heavily depends on the similarity of the natural water mechanics they emulate, but this is not the case. In order to bring some clarity to this mosaic of apparently related metaheuristics, in this paper we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#22270;&#20687;&#20013;&#26816;&#27979;&#20896;&#29366;&#21160;&#33033;&#20391;&#26525;&#24490;&#29615;&#65292;&#21487;&#36827;&#34892;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#26102;&#38388;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.12055</link><description>&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#20013;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20391;&#26525;&#24490;&#29615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep learning based detection of collateral circulation in coronary angiographies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#22270;&#20687;&#20013;&#26816;&#27979;&#20896;&#29366;&#21160;&#33033;&#20391;&#26525;&#24490;&#29615;&#65292;&#21487;&#36827;&#34892;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#26102;&#38388;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#26159;&#20840;&#29699;&#20027;&#35201;&#30340;&#27515;&#20129;&#21644;&#20303;&#38498;&#21407;&#22240;&#12290;&#21160;&#33033;&#31909;&#26679;&#30828;&#21270;&#26159;CAD&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#65292;&#26159;&#19968;&#31181;&#36880;&#28176;&#29421;&#31364;&#21160;&#33033;&#24182;&#20855;&#26377;&#28508;&#22312;&#33268;&#21629;&#25928;&#26524;&#30340;&#28814;&#30151;&#24615;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#33033;&#31909;&#26679;&#30828;&#21270;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#24490;&#29615;&#32463;&#24120;&#36890;&#36807;&#20391;&#25903;&#21160;&#33033;&#30340;&#24418;&#25104;&#26469;&#36866;&#24212;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#38271;&#26399;&#20581;&#24247;&#25928;&#30410;&#12290;&#22240;&#27492;&#65292;&#21450;&#26102;&#26816;&#27979;&#20896;&#29366;&#21160;&#33033;&#20391;&#26525;&#24490;&#29615;&#65288;CCC&#65289;&#23545;&#20110;CAD&#20010;&#24615;&#21270;&#21307;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#20013;&#30340;CCC&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21367;&#31215;&#39592;&#24178;&#20174;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#24207;&#21015;&#30340;&#27599;&#24103;&#20013;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#29305;&#24449;&#36830;&#25509;&#36215;&#26469;&#65292;&#38543;&#21518;&#30001;&#21478;&#19968;&#20010;&#21367;&#31215;&#23618;&#22312;&#26102;&#38388;&#19978;&#22788;&#29702;&#23884;&#20837;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#22312;&#39044;&#35757;&#32451;&#39592;&#24178;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12055v1 Announce Type: cross  Abstract: Coronary artery disease (CAD) is the dominant cause of death and hospitalization across the globe. Atherosclerosis, an inflammatory condition that gradually narrows arteries and has potentially fatal effects, is the most frequent cause of CAD. Nonetheless, the circulation regularly adapts in the presence of atherosclerosis, through the formation of collateral arteries, resulting in significant long-term health benefits. Therefore, timely detection of coronary collateral circulation (CCC) is crucial for CAD personalized medicine. We propose a novel deep learning based method to detect CCC in angiographic images. Our method relies on a convolutional backbone to extract spatial features from each frame of an angiography sequence. The features are then concatenated, and subsequently processed by another convolutional layer that processes embeddings temporally. Due to scarcity of data, we also experiment with pretraining the backbone on cor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#33014;&#22218;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#30382;&#32932;&#30284;&#35786;&#26029;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12009</link><description>&lt;p&gt;
&#21033;&#29992;&#33014;&#22218;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#30382;&#32932;&#30284;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#33014;&#22218;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#30382;&#32932;&#30284;&#35786;&#26029;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30382;&#32932;&#30149;&#21464;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#22797;&#26434;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23545;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#24615;&#21152;&#21095;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#26377;&#25928;&#23398;&#20064;&#23569;&#25968;&#31867;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#19982;&#33014;&#22218;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;GNNs&#25797;&#38271;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#20026;&#25429;&#33719;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#32423;&#26426;&#21046;&#65292;&#36828;&#36229;&#20256;&#32479;CNN&#30340;&#33021;&#21147;&#12290;&#33014;&#22218;&#32593;&#32476;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20379;&#23545;&#31354;&#38388;&#23618;&#27425;&#30340;&#20248;&#36234;&#35782;&#21035;&#33021;&#21147;&#26469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12009v1 Announce Type: cross  Abstract: In the realm of skin lesion image classification, the intricate spatial and semantic features pose significant challenges for conventional Convolutional Neural Network (CNN)-based methodologies. These challenges are compounded by the imbalanced nature of skin lesion datasets, which hampers the ability of models to learn minority class features effectively. Despite augmentation strategies, such as those using Generative Adversarial Networks (GANs), previous attempts have not fully addressed these complexities. This study introduces an innovative approach by integrating Graph Neural Networks (GNNs) with Capsule Networks to enhance classification performance. GNNs, known for their proficiency in handling graph-structured data, offer an advanced mechanism for capturing complex patterns and relationships beyond the capabilities of traditional CNNs. Capsule Networks further contribute by providing superior recognition of spatial hierarchies 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#34920;&#24773;&#31867;&#21035;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#38754;&#20020;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11942</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#26102;&#38388;&#24314;&#27169;&#25506;&#32034;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11942
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#34920;&#24773;&#31867;&#21035;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#38754;&#20020;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;&#25105;&#20204;&#38024;&#23545;&#21363;&#23558;&#22312;CVPR2024&#20030;&#34892;&#30340;&#31532;6&#23626;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#65288;ABAW&#65289;&#27604;&#36187;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20026;&#26410;&#26631;&#35760;&#30340;&#38754;&#37096;&#25968;&#25454;&#29983;&#25104;&#34920;&#24773;&#31867;&#21035;&#20266;&#26631;&#31614;&#65292;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#26631;&#35760;&#30340;&#38754;&#37096;&#34920;&#24773;&#26679;&#26412;&#65292;&#24182;&#23454;&#26045;&#21435;&#20559;&#21453;&#39304;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11942v1 Announce Type: cross  Abstract: Facial Expression Recognition (FER) plays a crucial role in computer vision and finds extensive applications across various fields. This paper aims to present our approach for the upcoming 6th Affective Behavior Analysis in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024.. In the facial expression recognition task, The limited size of the FER dataset poses a challenge to the expression recognition model's generalization ability, resulting in subpar recognition performance. To address this problem, we employ a semi-supervised learning technique to generate expression category pseudo-labels for unlabeled face data. At the same time, we uniformly sampled the labeled facial expression samples and implemented a debiased feedback learning strategy to address the problem of category imbalance in the dataset and the possible data bias in semi-supervised learning. Moreover, , to further compensate for the limitation and bias of fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11852</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#26080;&#32541;&#22320;&#34701;&#20837;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36710;&#27969;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS)&#20195;&#29702;&#65292;&#26088;&#22312;&#22312;&#27809;&#26377;&#20851;&#20110;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#20840;&#38754;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35813;&#20195;&#29702;&#30340;&#22686;&#24378;&#29256;AL3IS&#65292;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#20855;&#26377;&#36710;&#36742;&#38388;&#36890;&#20449;&#24310;&#36831;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20570;&#20986;&#26356;&#31283;&#20581;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#36890;&#36807;&#28508;&#22312;&#29366;&#24577;&#24314;&#27169;&#29615;&#22659;&#20013;&#30340;&#19981;&#21487;&#35266;&#23519;&#26041;&#38754;&#65292;&#22914;&#20854;&#20182;&#39550;&#39542;&#21592;&#30340;&#24847;&#22270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20195;&#29702;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#12289;&#20248;&#21270;&#21512;&#24182;&#25805;&#20316;&#24182;&#30830;&#20445;&#19982;&#20854;&#20182;&#36710;&#36742;&#36827;&#34892;&#23433;&#20840;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11852v1 Announce Type: cross  Abstract: This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>SmartRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#31574;&#30053;&#65292;&#21487;&#22312;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#37327;&#19979;&#23545;&#36816;&#21160;&#39044;&#27979;&#36827;&#34892;&#32454;&#21270;</title><link>https://arxiv.org/abs/2403.11492</link><description>&lt;p&gt;
SmartRefine: &#19968;&#31181;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient Motion Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11492
&lt;/p&gt;
&lt;p&gt;
SmartRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#31574;&#30053;&#65292;&#21487;&#22312;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#37327;&#19979;&#23545;&#36816;&#21160;&#39044;&#27979;&#36827;&#34892;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21608;&#22260;&#20195;&#29702;&#30340;&#26410;&#26469;&#36816;&#21160;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#21160;&#24577;&#30340;&#12289;&#20154;&#26426;&#28151;&#21512;&#29615;&#22659;&#20013;&#23433;&#20840;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#36947;&#36335;&#22320;&#22270;&#21644;&#21608;&#22260;&#20195;&#29702;&#30340;&#29366;&#24577;&#65292;&#20026;&#36816;&#21160;&#34892;&#20026;&#39044;&#27979;&#25552;&#20379;&#20851;&#38190;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#31574;&#30053;&#65292;&#31216;&#20026;SmartRefine&#65292;&#20197;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#37327;&#23545;&#39044;&#27979;&#36827;&#34892;&#32454;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SmartRefine&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#22330;&#26223;&#30340;&#29305;&#24615;&#20840;&#38754;&#35843;&#25972;&#32454;&#21270;&#37197;&#32622;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36136;&#37327;&#26469;&#26234;&#33021;&#22320;&#36873;&#25321;&#32454;&#21270;&#36845;&#20195;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11492v1 Announce Type: cross  Abstract: Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a qualit
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11395</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automated data processing and feature engineering for deep learning and big data applications: a survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11395
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#24182;&#22312;AI&#30340;&#21457;&#23637;&#20013;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#29305;&#21035;&#26159;&#22312;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#23427;&#20063;&#31616;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#22240;&#20026;&#23398;&#20064;&#36807;&#31243;&#26159;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#37117;&#24050;&#33258;&#21160;&#21270;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24517;&#39035;&#22312;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20043;&#21069;&#32463;&#36807;&#25163;&#21160;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#33258;&#21160;&#21270;&#36825;&#20123;&#20219;&#21153;&#30340;&#29305;&#27530;&#25216;&#26415;&#12290;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#39537;&#21160;&#21147;&#26159;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#12289;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#12290;&#22914;&#20170;&#65292;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;A
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11220</link><description>&lt;p&gt;
CPA-Enhancer&#65306;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#21333;&#19968;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#30830;&#23450;&#36864;&#21270;&#31867;&#22411;&#65292;&#24182;&#20026;&#27599;&#31181;&#31867;&#22411;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CPA-Enhancer&#22312;CoT&#25552;&#31034;&#30340;&#36880;&#27493;&#25351;&#23548;&#19979;&#36880;&#27493;&#35843;&#25972;&#20854;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#25552;&#31034;&#32534;&#30721;&#20102;&#19982;&#36864;&#21270;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;CoT&#25552;&#31034;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CPA-Enhancer&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36864;&#21270;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21463;&#25439;&#22270;&#20687;&#19978;&#23454;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPA-E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#26696;&#20363;&#26694;&#26550;&#26469;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#35770;&#35777;&#65292;&#21253;&#25324;&#22235;&#31867;&#35770;&#28857;&#65306;&#23436;&#20840;&#26080;&#27861;&#36896;&#25104;&#28798;&#38590;&#65292;&#24378;&#22823;&#30340;&#25511;&#21046;&#25514;&#26045;&#65292;&#23613;&#31649;&#26377;&#21487;&#33021;&#36896;&#25104;&#20260;&#23475;&#65292;&#20294;&#20381;&#28982;&#21487;&#20449;&#36182;&#65292;&#20197;&#21450;&#23545;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#39038;&#38382;&#30340;&#23562;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.10462</link><description>&lt;p&gt;
&#23433;&#20840;&#26696;&#20363;&#65306;&#35777;&#26126;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Safety Cases: Justifying the Safety of Advanced AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#26696;&#20363;&#26694;&#26550;&#26469;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#35770;&#35777;&#65292;&#21253;&#25324;&#22235;&#31867;&#35770;&#28857;&#65306;&#23436;&#20840;&#26080;&#27861;&#36896;&#25104;&#28798;&#38590;&#65292;&#24378;&#22823;&#30340;&#25511;&#21046;&#25514;&#26045;&#65292;&#23613;&#31649;&#26377;&#21487;&#33021;&#36896;&#25104;&#20260;&#23475;&#65292;&#20294;&#20381;&#28982;&#21487;&#20449;&#36182;&#65292;&#20197;&#21450;&#23545;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#39038;&#38382;&#30340;&#23562;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#26356;&#21152;&#20808;&#36827;&#65292;&#20225;&#19994;&#21644;&#30417;&#31649;&#26426;&#26500;&#23558;&#38754;&#20020;&#20851;&#20110;&#26159;&#21542;&#23433;&#20840;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#22256;&#38590;&#20915;&#31574;&#12290;&#20026;&#20102;&#20026;&#36825;&#20123;&#20915;&#31574;&#20570;&#20934;&#22791;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#21046;&#23450;&#19968;&#31181;"&#23433;&#20840;&#26696;&#20363;"&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#29702;&#30001;&#65292;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#22826;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#32455;&#23433;&#20840;&#26696;&#20363;&#30340;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22235;&#31867;&#35770;&#35777;&#23433;&#20840;&#30340;&#35770;&#28857;&#65306;&#23436;&#20840;&#26080;&#27861;&#36896;&#25104;&#28798;&#38590;&#65292;&#36275;&#22815;&#24378;&#22823;&#30340;&#25511;&#21046;&#25514;&#26045;&#65292;&#23613;&#31649;&#33021;&#22815;&#36896;&#25104;&#20260;&#23475;&#20173;&#20540;&#24471;&#20449;&#36182;&#65292;&#20197;&#21450;&#23545;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#39038;&#38382;&#30340;&#23562;&#37325;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#20855;&#20307;&#35770;&#28857;&#31034;&#20363;&#65292;&#24182;&#27010;&#36848;&#20102;&#22914;&#20309;&#32452;&#21512;&#35770;&#28857;&#26469;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#23433;&#20840;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10462v1 Announce Type: cross  Abstract: As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.10433</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#38598;&#20307;&#26234;&#33021;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AI-enhanced Collective Intelligence: The State of the Art and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10433
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#31038;&#20250;&#25361;&#25112;&#36229;&#20986;&#20102;&#20154;&#31867;&#20010;&#20307;&#25110;&#38598;&#20307;&#21162;&#21147;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#22312;&#20154;&#31867;&#38598;&#20307;&#20013;&#30340;&#35282;&#33394;&#23558;&#20174;&#36741;&#21161;&#24037;&#20855;&#36716;&#21464;&#20026;&#21442;&#19982;&#24335;&#25104;&#21592;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#25317;&#26377;&#20114;&#34917;&#30340;&#33021;&#21147;&#65292;&#24403;&#20108;&#32773;&#21327;&#21516;&#20316;&#29992;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#36229;&#36234;&#21333;&#29420;&#20154;&#31867;&#25110;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#33021;&#21147;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20132;&#20114;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#28041;&#21450;&#22797;&#26434;&#30340;&#36807;&#31243;&#21644;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#32508;&#36848;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26500;&#24819;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#26234;&#33021;&#34920;&#31034;&#65292;&#21253;&#25324;&#35748;&#30693;&#23618;&#12289;&#29289;&#29702;&#23618;&#21644;&#20449;&#24687;&#23618;&#12290;&#22312;&#36825;&#20010;&#22810;&#23618;&#32593;&#32476;&#20013;&#65292;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#29305;&#24449;&#65307;&#20154;&#31867;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#20174;&#34920;&#23618;&#21040;&#28145;&#23618;&#23646;&#24615;&#19981;&#21516;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#31243;&#24230;&#19978;&#20063;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10433v1 Announce Type: cross  Abstract: The current societal challenges exceed the capacity of human individual or collective effort alone. As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#27979;&#35797;&#31243;&#24207;&#65292;&#20197;&#20248;&#21270;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10086</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38024;&#23545;&#38750;&#21151;&#33021;&#23646;&#24615;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#27979;&#35797;&#31243;&#24207;&#65292;&#20197;&#20248;&#21270;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#32423;&#27979;&#35797;&#65288;SLT&#65289;&#24050;&#32463;&#25104;&#20026;&#38598;&#25104;&#30005;&#36335;&#27979;&#35797;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#36229;&#36807;&#21313;&#24180;&#65292;&#24182;&#19988;&#20173;&#28982;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#38024;&#23545;&#27979;&#35797;&#31243;&#24207;&#29983;&#25104;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#34987;&#27979;&#35774;&#22791;&#65288;DUT&#65289;&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#27979;&#35797;&#31243;&#24207;&#12290;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;LLMs&#22312;&#27979;&#35797;&#31243;&#24207;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#20248;&#21270;DUT&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10086v1 Announce Type: cross  Abstract: System-Level Test (SLT) has been a part of the test flow for integrated circuits for over a decade and still gains importance. However, no systematic approaches exist for test program generation, especially targeting non-functional properties of the Device under Test (DUT). Currently, test engineers manually compose test suites from off-the-shelf software, approximating the end-user environment of the DUT. This is a challenging and tedious task that does not guarantee sufficient control over non-functional properties. This paper proposes Large Language Models (LLMs) to generate test programs. We take a first glance at how pre-trained LLMs perform in test program generation to optimize non-functional properties of the DUT. Therefore, we write a prompt to generate C code snippets that maximize the instructions per cycle of a super-scalar, out-of-order architecture in simulation. Additionally, we apply prompt and hyperparameter optimizati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35782;&#21035;&#26032;&#29616;&#35937;&#65292;&#24182;&#39044;&#27979;&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25216;&#26415;&#21644;&#19981;&#21516;&#22269;&#23478;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09920</link><description>&lt;p&gt;
&#39044;&#27979;AI&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Generalization of AI Colonoscopy Models to Unseen Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09920
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35782;&#21035;&#26032;&#29616;&#35937;&#65292;&#24182;&#39044;&#27979;&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25216;&#26415;&#21644;&#19981;&#21516;&#22269;&#23478;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631; AI&#32467;&#32928;&#38236;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35780;&#20272;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#25216;&#26415;&#38656;&#35201;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29616;&#35937;&#24182;&#39044;&#27979;&#24687;&#32905;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;MSN&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#24687;&#32905;&#22270;&#20687;&#20013;&#34987;&#23631;&#34109;&#30340;&#21306;&#22495;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;MSN&#20165;&#22312;&#20197;&#33394;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#26085;&#26412;&#32467;&#32928;&#38236;&#65288;354&#20010;&#35270;&#39057;&#65292;128&#23567;&#26102;&#65289;&#19978;&#26816;&#27979;&#26410;&#30693;&#25216;&#26415;&#65306;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#21644;&#33394;&#32454;&#32990;&#20869;&#38236;&#65288;CE&#65289;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;MSN&#39044;&#27979;&#36328;&#22269;&#32467;&#32928;&#38236;&#35270;&#39057;&#19978;&#30340;&#24687;&#32905;&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;&#65288;CADe&#65289;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;MSN&#26410;&#25509;&#21463;&#36807;&#26469;&#33258;&#26085;&#26412;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09920v1 Announce Type: cross  Abstract: Background and aims Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels.   Methods We use a "Masked Siamese Network" (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN's ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN's ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan.   Results MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09673</link><description>&lt;p&gt;
FoldToken&#65306;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21450;&#26356;&#22810;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
FoldToken: Learning Protein Language via Vector Quantization and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09673
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#21516;&#26102;&#25551;&#36848;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#22806;&#35821;&#65311;&#30001;&#20110;&#36830;&#32493;3D&#28857;&#34920;&#31034;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#31163;&#25955;&#24207;&#21015;&#30340;&#23545;&#27604;&#24314;&#27169;&#26041;&#24335;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{FoldTokenizer}&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#28041;&#21450;&#23558;&#27531;&#22522;&#31867;&#22411;&#21644;&#32467;&#26500;&#25237;&#23556;&#21040;&#19968;&#20010;&#31163;&#25955;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#20449;&#24687;&#20445;&#23384;&#30340;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#31163;&#25955;&#31526;&#21495;&#31216;&#20026;\textbf{FoldToken}&#65292;&#32780;FoldTokens&#30340;&#24207;&#21015;&#21017;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#24418;&#24577;&#12290;&#25105;&#20204;&#23558;&#21019;&#24314;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#24212;&#29992;&#20110;&#26222;&#36890;&#20027;&#24178;&#20462;&#34917;&#21644;&#25239;&#20307;&#35774;&#35745;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#39318;&#20010;GPT&#39118;&#26684;&#27169;&#22411;(\textbf{FoldGPT})&#29992;&#20110;&#20855;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#12290;&#25105;&#20204;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#26174;&#33879;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09673v1 Announce Type: cross  Abstract: Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#23545;&#20110;&#26102;&#31354;&#29305;&#24615;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2403.09669</link><description>&lt;p&gt;
STREAM&#65306;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#31354;&#35780;&#20272;&#21644;&#20998;&#26512;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#23545;&#20110;&#26102;&#31354;&#29305;&#24615;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#22810;&#26679;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#35780;&#20272;&#24230;&#37327;&#30340;&#20840;&#38754;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#30701;&#35270;&#39057;&#29255;&#27573;&#26102;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#32570;&#20047;&#25552;&#20379;&#25913;&#36827;&#35265;&#35299;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;&#23884;&#20837;&#35270;&#39057;&#23884;&#20837;&#32593;&#32476;&#26469;&#31616;&#21333;&#35843;&#25972;&#22270;&#20687;&#24230;&#37327;&#26041;&#27861;&#32780;&#24471;&#21040;&#30340;&#65292;&#36825;&#21487;&#33021;&#20302;&#20272;&#20102;&#35270;&#39057;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;Frechet Video Distance (FVD) &#22312;&#31354;&#38388;&#26041;&#38754;&#30340;&#37325;&#35270;&#31243;&#24230;&#35201;&#22823;&#20110;&#35270;&#39057;&#30340;&#26102;&#38388;&#33258;&#28982;&#24615;&#65292;&#19988;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#23884;&#20837;&#32593;&#32476;&#36755;&#20837;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#20165;&#38480;&#20110;16&#24103;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#23427;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#35780;&#20272;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#29420;&#29305;&#22320;&#35774;&#35745;&#20197;&#35299;&#20915;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09669v1 Announce Type: cross  Abstract: Image generative models have made significant progress in generating realistic and diverse images, supported by comprehensive guidance from various evaluation metrics. However, current video generative models struggle to generate even short video clips, with limited tools that provide insights for improvements. Current video evaluation metrics are simple adaptations of image metrics by switching the embeddings with video embedding networks, which may underestimate the unique characteristics of video. Our analysis reveals that the widely used Frechet Video Distance (FVD) has a stronger emphasis on the spatial aspect than the temporal naturalness of video and is inherently constrained by the input size of the embedding networks used, limiting it to 16 frames. Additionally, it demonstrates considerable instability and diverges from human evaluations. To address the limitations, we propose STREAM, a new video evaluation metric uniquely des
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09499</link><description>&lt;p&gt;
&#20351;&#29992;Q&#23398;&#20064;&#30340;&#22902;&#29275;&#20859;&#27542;&#22330;&#30005;&#27744;&#31649;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22902;&#29275;&#20859;&#27542;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#65292;&#26159;&#20892;&#19994;&#20013;&#19968;&#20010;&#33021;&#28304;&#23494;&#38598;&#22411;&#30340;&#37096;&#38376;&#12290;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#38598;&#25104;&#21040;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#30005;&#27744;&#31649;&#29702;&#23545;&#20110;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#33267;&#20851;&#37325;&#35201;&#12290;&#31649;&#29702;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#30001;&#20110;&#30005;&#33021;&#28040;&#32791;&#30340;&#27874;&#21160;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#38388;&#27463;&#24615;&#20197;&#21450;&#33021;&#28304;&#20215;&#26684;&#30340;&#27874;&#21160;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#65292;&#28982;&#32780;&#22312;&#36825;&#19968;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20197;&#29233;&#23572;&#20848;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#20197;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#20026;&#26680;&#24515;&#30340;2030&#24180;&#33021;&#28304;&#25112;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23433;&#25490;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#20020;&#24202;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.09481</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Clinical Reasoning over Tabular Data and Text with Bayesian Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#20020;&#24202;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#20294;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26469;&#35828;&#21364;&#19981;&#22815;&#20860;&#23481;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#21017;&#20026;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#27604;&#36739;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#20197;&#29983;&#25104;&#24615;&#21644;&#21028;&#21035;&#24615;&#26041;&#24335;&#22686;&#24378;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#65292;&#32467;&#21512;&#27169;&#25311;&#32467;&#26524;&#20197;&#19968;&#20010;&#22522;&#30784;&#21307;&#30103;&#26696;&#20363;&#65288;&#32954;&#28814;&#35786;&#26029;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09481v1 Announce Type: new  Abstract: Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08438</link><description>&lt;p&gt;
&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#65306;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#21644;&#21487;&#20877;&#29616;&#24615;&#30340;&#22256;&#38590;&#36817;&#24180;&#26469;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#38656;&#35201;&#21487;&#20877;&#29616;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#21516;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20419;&#36827;&#20102;&#24320;&#25918;&#21644;&#21487;&#35775;&#38382;&#30340;&#30740;&#31350;&#12289;&#31283;&#20581;&#30340;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20197;&#21450;&#26032;&#21457;&#29616;&#30340;&#24555;&#36895;&#25972;&#21512;&#12290;&#35780;&#20272;&#30740;&#31350;&#20986;&#29256;&#29289;&#25903;&#25345;&#20877;&#29616;&#24615;&#30340;&#31243;&#24230;&#26159;&#26412;&#25991;&#30340;&#19968;&#20010;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#21162;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36716;&#21521;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#32500;&#24230;&#35781;&#21650;&#65292;&#23427;&#22312;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#65292;&#20351;&#24471;&#26356;&#38590;&#25214;&#21040;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08438v1 Announce Type: cross  Abstract: Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data 
&lt;/p&gt;</description></item><item><title>RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06420</link><description>&lt;p&gt;
RLingua&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06420
&lt;/p&gt;
&lt;p&gt;
RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#20197;&#20854;&#20302;&#26679;&#26412;&#25928;&#29575;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLingua&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;RL&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25552;&#21462;LLMs&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#21021;&#27493;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12290;&#23613;&#31649;&#19981;&#23436;&#32654;&#65292;LLM&#29983;&#25104;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#34987;&#29992;&#20110;&#22312;rollout&#26102;&#20197;&#34928;&#20943;&#27010;&#29575;&#29983;&#25104;&#21160;&#20316;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;RL&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#24182;&#20462;&#25913;&#20102;&#28436;&#21592;&#25439;&#22833;&#65292;&#20197;&#20351;&#31574;&#30053;&#23398;&#20064;&#26397;&#30528;LLM&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#35268;&#33539;&#21270;&#12290;RLingua&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#21892;&#19981;&#23436;&#32654;&#30340;LLM&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RLing
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;</title><link>https://arxiv.org/abs/2403.06097</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;LLM&#26367;&#20195;&#20154;&#24037;&#26631;&#27880;&#65311; &#26080;&#20154;&#26426;&#20132;&#20184;&#20219;&#21153;&#19979;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22320;&#22336;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CNER-UAV&#65292;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#65292;&#21487;&#20197;&#20840;&#38754;&#35757;&#32451;&#21644;&#35780;&#20272;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#20026;&#26500;&#24314;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#32422;&#21253;&#21547;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#32463;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312; \url{https://github.com/zhhvvv/CNER-UAV} &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#39033;&#30446;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.06025</link><description>&lt;p&gt;
CarbonNet: &#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#27668;&#20505;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#26159;&#20160;&#20040;&#65311; &#24212;&#29992;&#65306;&#23398;&#20064;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#24418;&#29366;&#20013;&#20943;&#32531;&#20840;&#29699;&#21464;&#26262;&#30340;&#22320;&#36136;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#39033;&#30446;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#65292;&#20197;&#24212;&#29992;&#20110;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#12290;CCS&#24050;&#34987;&#35777;&#26126;&#26159;&#30899;&#20013;&#21644;&#31038;&#20250;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#23478;&#21457;&#29616;&#23384;&#22312;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#22823;&#27169;&#22411;&#23610;&#24230;&#32780;&#23548;&#33268;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#38590;&#20197;&#27867;&#21270;&#20855;&#26377;&#22797;&#26434;&#29289;&#29702;&#23398;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#30001;&#30899;&#27880;&#20837;&#23548;&#33268;&#30340;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#21709;&#24212;&#65292;&#24182;&#21033;&#29992;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#20026;CCS&#39033;&#30446;&#30340;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06025v1 Announce Type: cross  Abstract: We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface geometry images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18590</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#24191;&#27867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18590
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#22609;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#24402;&#22240;&#20110;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#29420;&#29305;&#25512;&#29702;&#33021;&#21147;&#12290;&#19981;&#21516;&#20110;&#32570;&#20047;&#30452;&#25509;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#20256;&#32479;&#31995;&#32479;&#65292;LLMs&#22312;&#25512;&#33616;&#29289;&#21697;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26631;&#24535;&#30528;&#25512;&#33616;&#39046;&#22495;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#33539;&#24335;&#36716;&#21464;&#12290;&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#23450;&#20041;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#35813;&#30740;&#31350;&#24443;&#24213;&#25506;&#35752;&#20102;LLMs&#22312;&#25512;&#33616;&#26694;&#26550;&#20869;&#22266;&#26377;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#32454;&#33268;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#24179;&#31283;&#36807;&#28193;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#20139;&#25968;&#25454;&#27744;&#30340;&#20840;&#38754;&#23398;&#20064;&#31574;&#30053;&#65292;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14698</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#20998;&#26512;&#29992;&#20110;&#20998;&#31867;&#19982;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65306;&#25104;&#37117;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Big data analytics to classify earthwork-related locations: A Chengdu study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26174;&#33879;&#21152;&#21095;&#65292;&#23548;&#33268;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#20581;&#24247;&#21518;&#26524;&#12290;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65288;ERLs&#65289;&#26159;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;ERLs&#30340;&#26377;&#25928;&#31649;&#29702;&#19968;&#30452;&#26159;&#25919;&#24220;&#21644;&#29615;&#22659;&#26426;&#26500;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#20854;&#20998;&#31867;&#20998;&#23646;&#19981;&#21516;&#30340;&#30417;&#31649;&#37096;&#38376;&#12289;&#20449;&#24687;&#38556;&#30861;&#12289;&#25968;&#25454;&#26356;&#26032;&#24310;&#36831;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#28304;&#22836;&#28784;&#23576;&#27745;&#26579;&#30340;&#25233;&#21046;&#25514;&#26045;&#30340;&#32570;&#20047;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#30740;&#31350;&#20102;&#29305;&#24449;&#19982;&#28784;&#23576;&#27745;&#26579;&#28304;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#25104;&#21151;&#23454;&#26045;&#22312;&#19968;&#20010;&#21517;&#20026;&#30340;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13224</link><description>&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#25511;&#21046;&#22823;&#22411;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#65288;EVCS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#25554;&#27133;&#21151;&#29575;&#38480;&#21046;&#12289;&#21512;&#21516;&#38408;&#20540;&#36229;&#38480;&#24809;&#32602;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#26089;&#26399;&#26029;&#24320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25511;&#21046;EVCS&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20197;&#21450;&#33021;&#37327;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#39547;&#30041;&#26102;&#38388;&#20381;&#36182;&#38543;&#26426;&#36807;&#31243;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#22686;&#24378;&#20102;&#25104;&#26412;&#38477;&#20302;&#30340;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;22&#22825;&#27169;&#25311;&#23637;&#31034;&#20102;&#20004;&#31181;&#25552;&#20986;&#26041;&#27861;&#30456;&#23545;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#20004;&#38454;&#27573;&#26041;&#27861;&#35777;&#26126;&#20102;&#38024;&#23545;&#26089;&#26399;&#26029;&#24320;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#34385;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12518</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Neural Additive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#26377;&#26102;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#21457;&#23637;&#20986;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAMs&#65289;&#26159;&#22312;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#21521;&#19978;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NAM&#23376;&#31867;&#65292;&#23427;&#20351;&#29992;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#12290;GP-NAM&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20248;&#21183;&#12290;&#19982;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#25439;&#22833;&#65292;&#22240;&#20026;GPs&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#21442;&#25968;&#21333;&#21464;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;GP-NAM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12518v1 Announce Type: cross  Abstract: Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10109</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35786;&#26029;&#38169;&#35823;&#21457;&#29983;&#26159;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26080;&#27861;&#36731;&#26131;&#33719;&#21462;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#26469;&#36827;&#34892;&#24102;&#26377;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#30340;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#30340;&#39044;&#27979;&#65292;&#22312;&#20020;&#24202;&#21307;&#29983;&#20173;&#28982;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#28857;&#19978;&#65292;&#26088;&#22312;&#29305;&#21035;&#20943;&#36731;&#35786;&#26029;&#24310;&#36831;&#21644;&#28304;&#20110;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#25512;&#26029;&#20986;&#20107;&#20214;&#24615;&#30340;&#8220;&#30495;&#23454;&#8221;&#35786;&#26029;&#30340;&#26102;&#38388;&#31890;&#24230;&#32454;&#33268;&#30340;&#22238;&#39038;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#20445;&#35777;&#36755;&#20837;&#25991;&#26412;&#26159;&#22312;&#21487;&#20197;&#36827;&#34892;&#33258;&#20449;&#30340;&#35786;&#26029;&#20043;&#21069;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#26816;&#32034;&#21021;&#22987;&#30340;&#35777;&#25454;&#27744;&#65292;&#28982;&#21518;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09450</link><description>&lt;p&gt;
&#24341;&#23548;&#36974;&#34109;&#34920;&#31034;&#23398;&#20064;&#20197;&#25429;&#25417;&#24515;&#30005;&#22270;&#30340;&#26102;&#31354;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24191;&#27867;&#29992;&#20316;&#30417;&#27979;&#24515;&#33039;&#36215;&#28304;&#30340;&#30005;&#20449;&#21495;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#20449;&#21495;&#36827;&#34892;&#21508;&#31181;&#30142;&#30149;&#31579;&#26597;&#30340;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30142;&#30149;&#31579;&#26597;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;ECG&#25968;&#25454;&#26377;&#38480;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#29616;&#36890;&#29992;&#34920;&#31034;&#26159;&#20811;&#26381;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;ECG&#25968;&#25454;&#19978;&#32431;&#31929;&#24212;&#29992;SSL&#65292;&#32780;&#19981;&#32771;&#34385;ECG&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ST-MEM&#65288;&#26102;&#31354;&#36974;&#34109;&#24515;&#30005;&#22270;&#24314;&#27169;&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;12&#23548;&#32852;ECG&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;ST-MEM&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;SSL&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05946</link><description>&lt;p&gt;
&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#65306;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24322;&#24120;&#20107;&#20214;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#29702;&#35299;&#24322;&#24120;&#20107;&#20214;&#32972;&#21518;&#30340;&#22240;&#26524;&#21407;&#22240;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#31361;&#28982;&#21464;&#21270;&#12290;&#25581;&#31034;&#22240;&#26524;&#21407;&#22240;&#26377;&#21161;&#20110;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#25581;&#31034;&#35299;&#37322;&#35266;&#23519;&#20107;&#20214;&#30340;&#8220;&#22914;&#26524;-&#37027;&#20040;&#8221;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#38388;&#28857;&#36807;&#31243;&#26469;&#24314;&#27169;&#25152;&#20851;&#27880;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19968;&#32452;&#28508;&#22312;&#35268;&#21017;&#26469;&#35299;&#37322;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#22312;E&#27493;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#34987;&#27599;&#20010;&#21457;&#29616;&#30340;&#35268;&#21017;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;M&#27493;&#20013;&#65292;&#25105;&#20204;&#26356;&#26032;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#22686;&#24378;&#21487;&#33021;&#24615;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20197;&#24494;&#20998;&#30340;&#26041;&#24335;&#20248;&#21270;&#35268;&#21017;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#35268;&#21017;&#21644;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using syntheti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;RA-Rec&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#65292;&#24182;&#35774;&#35745;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04527</link><description>&lt;p&gt;
RA-Rec:&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;RA-Rec&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#65292;&#24182;&#35774;&#35745;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20026;LLM&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#32467;&#21512;&#24102;&#26469;&#20102;&#26032;&#30340;&#28526;&#27969;&#65292;&#31216;&#20026;LLM-based RS&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#20363;&#65292;&#21363;ID&#30452;&#25509;&#20351;&#29992;&#33539;&#20363;&#21644;ID&#32763;&#35793;&#33539;&#20363;&#65292;&#25351;&#20986;&#23427;&#20204;&#30340;&#26680;&#24515;&#24369;&#28857;&#22312;&#20110;&#32570;&#20047;&#25512;&#33616;&#30693;&#35782;&#21644;&#29420;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21363;ID&#34920;&#31034;&#65292;&#23427;&#20197;&#19968;&#31181;&#20114;&#34917;&#30340;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RA-Rec&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;&#65292;&#19982;&#22810;&#31181;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#21644;LLM&#26550;&#26500;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ID&#23884;&#20837;&#35270;&#20026;&#36719;&#25552;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#30340;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#21450;&#20026;&#23545;&#40784;&#23450;&#21046;&#30340;&#25968;&#25454;&#26500;&#24314;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RA-Rec&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures. Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperfo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02464</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#21315;&#35328;&#65306;&#20351;&#29992;&#32431;Transformer&#23558;&#22270;&#24418;&#27431;&#25289;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#24314;&#27169;&#20026;&#32431;&#35821;&#35328;&#29978;&#33267;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22266;&#26377;&#20449;&#24687;&#65311;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#19968;&#30452;&#26159;&#22270;&#24418;&#24314;&#27169;&#20013;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;GNN&#21644;Graphformer&#21162;&#21147;&#23558;&#22270;&#24418;&#32534;&#30721;&#20026;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#20294;&#20174;&#21521;&#37327;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#22270;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GraphsGPT&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#22270;&#24418;&#21333;&#35789;&#30340;Graph2Seq&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#22270;&#24418;&#21333;&#35789;&#37325;&#26500;&#21407;&#22987;&#22270;&#24418;&#20197;&#30830;&#20445;&#20449;&#24687;&#31561;&#20215;&#24615;&#30340;GraphGPT&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;100M&#20010;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#20102;GraphsGPT&#65292;&#24182;&#24471;&#21040;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;(1) &#39044;&#35757;&#32451;&#30340;Graph2Seq&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;8/9&#20010;&#22270;&#24418;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;(2) &#39044;&#35757;&#32451;&#30340;GraphGPT&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#29983;&#25104;&#22120;&#65292;&#20854;&#33021;&#22815;&#36827;&#34892;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;(3) Graph2Seq+Gr
&lt;/p&gt;
&lt;p&gt;
Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02334</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#38656;&#35201;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#21040;&#26368;&#36817;&#65292;&#20851;&#20110;&#28145;&#24230;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24402;&#32435;&#20559;&#35265;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#23545;&#20110;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#36731;&#24494;&#29305;&#24449;&#20132;&#20114;&#20551;&#35774;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#65292;&#31216;&#20026;AMFormer&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AMFormer&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#24378;&#23545;&#25163;&#12290;&#36825;&#24402;&#22240;&#20110;&#20854;&#24182;&#34892;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#20248;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#20855;&#26377;&#31639;&#26415;&#24037;&#31243;&#29305;&#24449;&#30340;&#25193;&#23637;&#31354;&#38388;&#20013;&#20998;&#31163;&#34920;&#26684;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20063;&#39564;&#35777;&#20102;AMFormer&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#21512;&#29702;&#24615;&#65292;&#34920;&#26126;&#23427;&#24050;&#32463;&#24314;&#31435;&#20102;&#24378;&#26377;&#21147;&#30340;&#24402;&#32435;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#25955;&#20110;&#19981;&#21516;&#25968;&#25454;&#23396;&#23707;&#30340;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#22320;&#21306;&#25968;&#25454;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#30495;&#30456;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07931</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07931
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#25955;&#20110;&#19981;&#21516;&#25968;&#25454;&#23396;&#23707;&#30340;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#22320;&#21306;&#25968;&#25454;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#30495;&#30456;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#22270;&#20687;&#30456;&#20851;&#38382;&#39064;&#30340;&#27969;&#34892;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25968;&#25454;&#38544;&#31169;&#21644;&#33719;&#21462;&#26041;&#24335;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20449;&#24687;&#23384;&#20648;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23396;&#23707;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#38590;&#20197;&#23558;&#25152;&#26377;&#20449;&#24687;&#25972;&#21512;&#21040;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#26041;&#24335;&#20013;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37096;&#20998;&#26412;&#22320;&#21270;&#25968;&#25454;&#21306;&#22495;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#30340;&#30495;&#30456;&#12290;&#36825;&#34920;&#26126;&#23427;&#20204;&#26377;&#33021;&#21147;&#36890;&#36807;&#25968;&#23383;&#26041;&#24335;&#24471;&#20986;&#32467;&#35770;&#65292;&#20294;&#22312;&#32570;&#20047;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#23588;&#20854;&#22312;&#23581;&#35797;&#24320;&#21457;&#36890;&#24120;&#38656;&#35201;&#35813;&#21151;&#33021;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#36825;&#26679;&#30340;&#30830;&#23450;&#36890;&#24120;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#22312;&#36825;&#19968;&#24120;&#35265;&#26465;&#20214;&#19979;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07931v2 Announce Type: replace-cross  Abstract: With the popularization of AI solutions for image based problems, there has been a growing concern for both data privacy and acquisition. In a large number of cases, information is located on separate data silos and it can be difficult for a developer to consolidate all of it in a fashion that is appropriate for machine learning model development. Alongside this, a portion of these localized data regions may not have access to a labelled ground truth. This indicates that they have the capacity to reach conclusions numerically, but are not able to assign classifications amid a lack of pertinent information. Such a determination is often negligible, especially when attempting to develop image based solutions that often necessitate this capability. With this being the case, we propose an innovative vertical federated learning (VFL) model architecture that can operate under this common set of conditions. This is the first (and curr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22635;&#34917;&#36328;&#22495;&#26816;&#32034;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#31867;&#21035;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.00420</link><description>&lt;p&gt;
SynCDR&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#36328;&#39046;&#22495;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SynCDR : Training Cross Domain Retrieval Models with Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00420
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22635;&#34917;&#36328;&#22495;&#26816;&#32034;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#31867;&#21035;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#39046;&#22495;&#26816;&#32034;&#20013;&#65292;&#38656;&#35201;&#27169;&#22411;&#33021;&#22815;&#22312;&#20004;&#20010;&#35270;&#35273;&#22495;&#20013;&#35782;&#21035;&#20986;&#23646;&#20110;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#29289;&#20307;&#30340;&#33609;&#22270;&#65292;&#27169;&#22411;&#38656;&#35201;&#20174;&#22312;&#32447;&#21830;&#24215;&#30340;&#30446;&#24405;&#20013;&#26816;&#32034;&#21040;&#35813;&#29289;&#20307;&#30340;&#30495;&#23454;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#22635;&#34917;&#36328;&#22495;&#20043;&#38388;&#32570;&#22833;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;catg...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00420v2 Announce Type: replace-cross  Abstract: In cross-domain retrieval, a model is required to identify images from the same semantic category across two visual domains. For instance, given a sketch of an object, a model needs to retrieve a real image of it from an online store's catalog. A standard approach for such a problem is learning a feature space of images where Euclidean distances reflect similarity. Even without human annotations, which may be expensive to acquire, prior methods function reasonably well using unlabeled images for training. Our problem constraint takes this further to scenarios where the two domains do not necessarily share any common categories in training data. This can occur when the two domains in question come from different versions of some biometric sensor recording identities of different people. We posit a simple solution, which is to generate synthetic data to fill in these missing category examples across domains. This, we do via categ
&lt;/p&gt;</description></item><item><title>MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.08656</link><description>&lt;p&gt;
MaxK-GNN: &#25506;&#32034;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#29702;&#35770;&#36895;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08656
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21152;&#36895;&#26041;&#38754;&#65292;GPU&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#24179;&#21488;&#12290; GPU&#22312;GNN&#19978;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20363;&#22914;PyG&#12289;DGL&#19982;cuSPARSE&#65292;&#20197;&#21450;GNNAdvisor&#26694;&#26550;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#20869;&#23384;&#27969;&#37327;&#20173;&#28982;&#24456;&#26174;&#33879;&#12290; &#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#36890;&#36807;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#22402;&#30452;&#20248;&#21270;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#26159;&#23558;&#21152;&#36895;&#20248;&#21270;&#35270;&#20026;&#8220;&#20107;&#21518;&#24605;&#32771;&#8221;&#65288;&#21363;&#65288;i&#65289;&#32473;&#23450;GNN&#31639;&#27861;&#65292;&#35774;&#35745;&#21152;&#36895;&#22120;&#65292;&#25110;&#65288;ii&#65289;&#32473;&#23450;&#30828;&#20214;&#65292;&#20027;&#35201;&#20248;&#21270;GNN&#31639;&#27861;&#65289;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MaxK-GNN&#65292;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#20808;&#36827;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#12290; &#65288;i&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;MaxK&#38750;&#32447;&#24615;&#24182;&#25552;&#20379;&#20102;MaxK&#38750;&#32447;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
&lt;/p&gt;</description></item><item><title>Genixer&#26159;&#19968;&#31181;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21253;&#25324;&#20061;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#26469;&#25913;&#21892;&#25968;&#25454;&#29983;&#25104;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.06731</link><description>&lt;p&gt;
Genixer&#65306;&#23558;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#20026;&#24378;&#22823;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06731
&lt;/p&gt;
&lt;p&gt;
Genixer&#26159;&#19968;&#31181;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21253;&#25324;&#20061;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#26469;&#25913;&#21892;&#25968;&#25454;&#29983;&#25104;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06731v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;: &#35843;&#20248;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#35843;&#20248;&#25968;&#25454;&#30340;&#21019;&#24314;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#20381;&#36182;&#20110;GPT-4&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#19981;&#20165;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#19988;&#22312;&#22797;&#26434;&#20219;&#21153;&#65288;&#21363;&#22522;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#20219;&#21153;&#65289;&#20013;&#30340;&#24615;&#33021;&#20063;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;Genixer&#65292;&#29992;&#20110;&#29983;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#30340;&#35843;&#20248;&#25968;&#25454;&#65292;&#21253;&#25324;&#20061;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#20363;&#22914;&#24120;&#35265;&#30340;VQA&#65292;REC&#65292;REG&#21644;PointQ&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Genixer&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#26469;&#32531;&#35299;&#25968;&#25454;&#29983;&#25104;&#30340;&#22256;&#38590;&#65306;(i)&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;(ii)&#25351;&#23548;&#27169;&#26495;&#35774;&#35745;&#65292;(iii)&#36171;&#33021;MLLM&#65292;&#21644;(iv)&#25968;&#25454;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30340;Genixer&#30340;&#21331;&#36234;&#30340;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;MLLM&#20855;&#26377;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06731v2 Announce Type: replace-cross  Abstract: Instruction tuning data is essential for training the Multimodal Large Language Models (MLLMs). However, the creation of high-quality instruction tuning data presents significant challenges. Prior methods that depended on GPT-4 for data generation were not only costly but also lacked satisfactory performance in complex tasks (i.e., grounding-based reasoning tasks). To address these issues, we developed an innovative data generation pipeline, Genixer, to generate various high-quality instruction tuning data, including nine representative tasks, e.g., Common VQA, REC, REG, and PointQ. Specifically, Genixer provides a unified solution with four key steps for alleviating the difficulty of data generation: (i) instruction data collection, (ii) instruction template design, (iii) empowering MLLM, and (iv) data generation and filtering. Subsequently, the superior qualitative results of our Genixer demonstrate that current MLLMs have a 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20302;&#21151;&#32791;&#20107;&#20214;&#25668;&#20687;&#22836;&#23545;&#21335;&#26497;&#20225;&#40517;&#34892;&#20026;&#36827;&#34892;&#36830;&#32493;&#36828;&#31243;&#23450;&#20301;&#65292;&#23558;&#38382;&#39064;&#23450;&#20041;&#20026;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#20219;&#21153;&#24182;&#25104;&#21151;&#24320;&#21457;&#20102;&#30456;&#24212;&#26041;&#27861;</title><link>https://arxiv.org/abs/2312.03799</link><description>&lt;p&gt;
&#20302;&#21151;&#32791;&#12289;&#36830;&#32493;&#36828;&#31243;&#34892;&#20026;&#23450;&#20301;&#19982;&#20107;&#20214;&#25668;&#20687;&#22836;
&lt;/p&gt;
&lt;p&gt;
Low-power, Continuous Remote Behavioral Localization with Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03799
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20302;&#21151;&#32791;&#20107;&#20214;&#25668;&#20687;&#22836;&#23545;&#21335;&#26497;&#20225;&#40517;&#34892;&#20026;&#36827;&#34892;&#36830;&#32493;&#36828;&#31243;&#23450;&#20301;&#65292;&#23558;&#38382;&#39064;&#23450;&#20041;&#20026;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#20219;&#21153;&#24182;&#25104;&#21151;&#24320;&#21457;&#20102;&#30456;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#32773;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21160;&#29289;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#20986;&#29616;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#36828;&#30340;&#37326;&#22806;&#35266;&#23519;&#37326;&#29983;&#29289;&#31181;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#22256;&#38590;&#30340;&#29031;&#26126;&#26465;&#20214;&#20197;&#21450;&#23545;&#30005;&#28304;&#20379;&#24212;&#21644;&#25968;&#25454;&#23384;&#20648;&#30340;&#38480;&#21046;&#12290;&#20107;&#20214;&#25668;&#20687;&#22836;&#30001;&#20110;&#20854;&#20302;&#21151;&#32791;&#21644;&#39640;&#21160;&#24577;&#33539;&#22260;&#33021;&#21147;&#65292;&#20026;&#20381;&#36182;&#30005;&#27744;&#30340;&#36828;&#31243;&#30417;&#25511;&#25552;&#20379;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26032;&#39062;&#30340;&#20256;&#24863;&#22120;&#26469;&#37327;&#21270;&#19968;&#31181;&#31216;&#20026;&#8220;&#29378;&#21916;&#23637;&#31034;&#8221;&#30340;&#24052;&#23707;&#20225;&#40517;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#20219;&#21153;&#65292;&#30830;&#23450;&#34892;&#20026;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#21335;&#26497;&#27954;&#35760;&#24405;&#20102;&#19968;&#20010;&#32321;&#27542;&#20225;&#40517;&#32676;&#33853;&#30340;&#27963;&#21160;&#25968;&#21608;&#65292;&#24182;&#26631;&#35760;&#20102;16&#20010;&#24034;&#30340;&#20107;&#20214;&#25968;&#25454;&#12290;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#30001;&#20505;&#36873;&#26102;&#38388;&#38388;&#38548;&#29983;&#25104;&#22120;&#65288;&#25552;&#26696;&#65289;&#21644;&#34892;&#20026;&#20998;&#31867;&#22120;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03799v2 Announce Type: replace-cross  Abstract: Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery-dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica for several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26377;&#25928;&#30340;&#25968;&#25454;&#31934;&#28860;&#33539;&#24335;RDED&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03526</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26377;&#25928;&#30340;&#25968;&#25454;&#31934;&#28860;&#33539;&#24335;RDED&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#26426;&#22120;&#23398;&#20064;&#35201;&#27714;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#22240;&#27492;&#38754;&#20020;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#38598;&#31934;&#28860;&#20316;&#20026;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#21387;&#32553;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30446;&#21069;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#20854;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#19977;&#20010;&#25152;&#38656;&#23646;&#24615;&#65292;&#21363;&#29616;&#23454;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RDED&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26377;&#25928;&#30340;&#25968;&#25454;&#31934;&#28860;&#33539;&#24335;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#12290;&#23545;&#21508;&#31181;&#31070;&#32463;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;RDED&#30340;&#36827;&#23637;&#65306;&#25105;&#20204;&#21487;&#20197;&#23558;&#23436;&#25972;&#30340;ImageNet-1K&#31934;&#28860;&#20026;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03526v2 Announce Type: replace-cross  Abstract: Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprisin
&lt;/p&gt;</description></item><item><title>MACE&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#36866;&#24212;&#22810;&#27491;&#24120;&#27169;&#24335;&#19988;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20248;&#31168;&#30340;&#27169;&#24335;&#25552;&#21462;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#26381;&#21153;&#27491;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2311.16191</link><description>&lt;p&gt;
&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#27491;&#24120;&#24615;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-Pattern Normalities in the Frequency Domain for Efficient Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16191
&lt;/p&gt;
&lt;p&gt;
MACE&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#36866;&#24212;&#22810;&#27491;&#24120;&#27169;&#24335;&#19988;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20248;&#31168;&#30340;&#27169;&#24335;&#25552;&#21462;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#26381;&#21153;&#27491;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26174;&#33879;&#25552;&#21319;&#20102;&#20113;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;&#23613;&#31649;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26368;&#36817;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#22312;&#20113;&#29615;&#22659;&#20013;&#36935;&#21040;&#20102;&#23454;&#38469;&#25361;&#25112;&#65306;&#22312;&#32500;&#25252;&#27599;&#20010;&#26381;&#21153;&#30340;&#21807;&#19968;&#27169;&#22411;&#30340;&#19981;&#20999;&#23454;&#38469;&#24615;&#19982;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#22788;&#29702;&#21508;&#31181;&#27491;&#24120;&#27169;&#24335;&#30340;&#26377;&#38480;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#30683;&#30462;&#65292;&#20197;&#21450;&#22788;&#29702;&#23454;&#26102;&#39640;&#36127;&#36733;&#21644;&#30701;&#26399;&#24322;&#24120;&#26816;&#27979;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MACE&#65292;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#36866;&#24212;&#22810;&#27491;&#24120;&#27169;&#24335;&#19988;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#23427;&#26377;&#19977;&#20010;&#26032;&#39062;&#30340;&#29305;&#28857;&#65306;&#65288;i&#65289;&#19968;&#31181;&#20248;&#31168;&#30340;&#27169;&#24335;&#25552;&#21462;&#26426;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#27491;&#24120;&#27169;&#24335;&#24182;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#26381;&#21153;&#27491;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16191v2 Announce Type: replace-cross  Abstract: Anomaly detection significantly enhances the robustness of cloud systems. While neural network-based methods have recently demonstrated strong advantages, they encounter practical challenges in cloud environments: the contradiction between the impracticality of maintaining a unique model for each service and the limited ability to deal with diverse normal patterns by a unified model, as well as issues with handling heavy traffic in real time and short-term anomaly detection sensitivity.   Thus, we propose MACE, a multi-normal-pattern accommodated and efficient anomaly detection method in the frequency domain for time series anomaly detection. There are three novel characteristics of it: (i) a pattern extraction mechanism excelling at handling diverse normal patterns with a unified model, which enables the model to identify anomalies by examining the correlation between the data sample and its service normal pattern, instead of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#23545;&#40784;&#21069;&#33258;&#36866;&#24212;&#8221;&#65288;ALT&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#25552;&#21462;&#35270;&#39057;&#20013;&#26368;&#37325;&#35201;&#23454;&#20307;&#30340;&#35821;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15619</link><description>&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#20043;&#21069;&#23545;&#40784;&#65306;&#21033;&#29992;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#36827;&#34892;&#36890;&#29992;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#23545;&#40784;&#21069;&#33258;&#36866;&#24212;&#8221;&#65288;ALT&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#25552;&#21462;&#35270;&#39057;&#20013;&#26368;&#37325;&#35201;&#23454;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#39057;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36981;&#24490;&#8220;&#33258;&#36866;&#24212;&#28982;&#21518;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#65292;&#23558;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#35843;&#25972;&#20026;&#24314;&#27169;&#35270;&#39057;&#32423;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21160;&#20316;&#26631;&#31614;&#30340;one-hot&#25110;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23545;&#40784;&#21069;&#33258;&#36866;&#24212;&#8221;&#65288;ALT&#65289;&#33539;&#24335;&#12290;&#22312;&#36866;&#24212;&#21040;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#20043;&#21069;&#65292;&#25105;&#20204;&#21033;&#29992;&#20026;&#27599;&#19968;&#24103;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#12290;&#36825;&#20123;&#23545;&#40784;&#36890;&#36807;&#23558;&#21306;&#22495;&#24863;&#30693;&#22270;&#20687;&#23884;&#20837;&#19982;&#31163;&#32447;&#26500;&#24314;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#21305;&#37197;&#26469;&#23454;&#29616;&#12290;&#26377;&#20102;&#23545;&#40784;&#30340;&#23454;&#20307;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#30340;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26597;&#35810;&#39304;&#36865;&#21040;&#22522;&#20110;transformer&#30340;&#35270;&#39057;&#36866;&#37197;&#22120;&#20013;&#65292;&#21487;&#20197;&#24110;&#21161;&#20174;&#35270;&#39057;&#21040;&#21521;&#37327;&#25552;&#21462;&#26368;&#37325;&#35201;&#23454;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15619v2 Announce Type: replace-cross  Abstract: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an "adapt then align" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel "Align before Adapt" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This parad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#23376;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#26679;&#26412;&#31232;&#32570;&#24615;&#25361;&#25112;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.15056</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15056
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#23376;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#26679;&#26412;&#31232;&#32570;&#24615;&#25361;&#25112;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#21457;&#29616;&#28508;&#22312;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;(DDI)&#19968;&#30452;&#26159;&#20020;&#24202;&#27835;&#30103;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#29992;&#20110;DDI&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#65292;&#32780;&#24050;&#30693;DDI&#36739;&#20026;&#32597;&#35265;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;KnowDDI&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;KnowDDI&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22823;&#22411;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#20013;&#20016;&#23500;&#30340;&#37051;&#23621;&#20449;&#24687;&#22686;&#24378;&#33647;&#29289;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#20026;&#27599;&#23545;&#33647;&#29289;&#23398;&#20064;&#19968;&#20010;&#30693;&#35782;&#23376;&#22270;&#20197;&#35299;&#37322;&#39044;&#27979;&#30340;DDI&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#37117;&#19982;&#36830;&#25509;&#24378;&#24230;&#30456;&#20851;&#32852;&#65292;&#25351;&#31034;&#24050;&#30693;DDI&#30340;&#37325;&#35201;&#24615;&#25110;&#31867;&#20284;&#24378;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#22312;&#33647;&#29289;&#23545;&#20043;&#38388;&#26159;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;DDI&#30340;&#24773;&#20917;&#36890;&#36807;&#20016;&#23500;&#30340;&#33647;&#29289;&#34920;&#31034;&#21644;&#20256;&#25773;&#30340;&#33647;&#29289;&#30456;&#20284;&#24615;&#24471;&#21040;&#38544;&#21547;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15056v2 Announce Type: replace-cross  Abstract: Background: Discovering potential drug-drug interactions (DDIs) is a long-standing challenge in clinical treatments and drug developments. Recently, deep learning techniques have been developed for DDI prediction. However, they generally require a huge number of samples, while known DDIs are rare.   Methods: In this work, we present KnowDDI, a graph neural network-based method that addresses the above challenge. KnowDDI enhances drug representations by adaptively leveraging rich neighborhood information from large biomedical knowledge graphs. Then, it learns a knowledge subgraph for each drug-pair to interpret the predicted DDI, where each of the edges is associated with a connection strength indicating the importance of a known DDI or resembling strength between a drug-pair whose connection is unknown. Thus, the lack of DDIs is implicitly compensated by the enriched drug representations and propagated drug similarities.   Resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#31867;&#20284;&#20294;&#28508;&#22312;&#19978;&#26356;&#23454;&#29992;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#27604;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12996</link><description>&lt;p&gt;
RLIF: &#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLIF: Interactive Imitation Learning as Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#31867;&#20284;&#20294;&#28508;&#22312;&#19978;&#26356;&#23454;&#29992;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#27604;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20026;&#33258;&#21160;&#25216;&#33021;&#33719;&#21462;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20294;&#26159;&#23545;&#20110;&#23454;&#38469;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#22914;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#27169;&#20223;&#23398;&#20064;&#24448;&#24448;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#20687;DAgger&#36825;&#26679;&#30340;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#19968;&#20301;&#25509;&#36817;&#26368;&#20248;&#30340;&#19987;&#23478;&#25163;&#20013;&#22312;&#32447;&#33719;&#21462;&#32416;&#27491;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#22256;&#25200;&#22825;&#30495;&#34892;&#20026;&#20811;&#38534;&#30340;&#20998;&#24067;&#20559;&#31227;&#25361;&#25112;&#65292;&#26080;&#35770;&#22312;&#29702;&#35770;&#19978;&#36824;&#26159;&#22312;&#23454;&#36341;&#20013;&#37117;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20854;&#20182;&#32452;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#31867;&#20284;&#20294;&#28508;&#22312;&#19978;&#26356;&#23454;&#29992;&#30340;&#20551;&#35774;&#19979;&#23454;&#29616;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#29992;&#25143;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12996v2 Announce Type: replace  Abstract: Although reinforcement learning methods offer a powerful framework for automatic skill acquisition, for practical learning-based control problems in domains such as robotics, imitation learning often provides a more convenient and accessible alternative. In particular, an interactive imitation learning method such as DAgger, which queries a near-optimal expert to intervene online to collect correction data for addressing the distributional shift challenges that afflict na\"ive behavioral cloning, can enjoy good performance both in theory and practice without requiring manually specified reward functions and other components of full reinforcement learning methods. In this paper, we explore how off-policy reinforcement learning can enable improved performance under assumptions that are similar but potentially even more practical than those of interactive imitation learning. Our proposed method uses reinforcement learning with user inte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.09684</link><description>&lt;p&gt;
&#21307;&#29983;&#20204;&#30693;&#36947;&#22914;&#20309;&#25552;&#37266;&#21527;&#65311;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#24110;&#21161;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#25552;&#31034;&#24037;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#26469;&#25913;&#36827;&#21021;&#22987;&#25552;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#21307;&#23398;&#19987;&#23478;&#12289;&#38750;&#21307;&#23398;&#19987;&#23478;&#20197;&#21450;&#32463;&#36807;APO&#22686;&#24378;&#30340;GPT3.5&#21644;GPT4&#30340;&#36755;&#20986;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#20020;&#24202;&#31508;&#35760;&#21508;&#33410;&#25552;&#31034;&#36136;&#37327;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#26174;&#31034;&#65292;&#19987;&#23478;&#22312;APO&#21518;&#20445;&#25345;&#20869;&#23481;&#36136;&#37327;&#65292;&#20294;&#26356;&#20559;&#22909;&#33258;&#24049;&#30340;&#20462;&#25913;&#65292;&#34920;&#26126;&#20102;&#19987;&#23478;&#23450;&#21046;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#20004;&#38454;&#27573;&#20248;&#21270;&#36807;&#31243;&#65292;&#21033;&#29992;APO-GPT4&#30830;&#20445;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#19987;&#23478;&#36755;&#20837;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09684v2 Announce Type: replace-cross  Abstract: This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31867;&#20284;&#20110;NeRF&#30340;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20174;&#20027;&#35266;&#25968;&#25454;&#35757;&#32451;&#30340;&#20016;&#23500;&#22810;&#27169;&#24577;&#22330;&#26223;&#27169;&#22411;&#65292;&#26377;&#26395;&#25552;&#21319;&#26234;&#33021;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.06455</link><description>&lt;p&gt;
Aria-NeRF: &#22810;&#27169;&#24577;&#20027;&#35266;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Aria-NeRF: Multimodal Egocentric View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06455
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31867;&#20284;&#20110;NeRF&#30340;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20174;&#20027;&#35266;&#25968;&#25454;&#35757;&#32451;&#30340;&#20016;&#23500;&#22810;&#27169;&#24577;&#22330;&#26223;&#27169;&#22411;&#65292;&#26377;&#26395;&#25552;&#21319;&#26234;&#33021;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#21152;&#36895;&#30740;&#31350;&#65292;&#24320;&#21457;&#20174;&#20027;&#35266;&#25968;&#25454;&#35757;&#32451;&#30340;&#20016;&#23500;&#22810;&#27169;&#24577;&#22330;&#26223;&#27169;&#22411;&#65292;&#22522;&#20110;&#21463;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#21551;&#21457;&#30340;&#21487;&#24494;&#20307;&#31215;&#20809;&#32447;&#36861;&#36394;&#12290;&#20174;&#20027;&#35266;&#22270;&#20687;&#24207;&#21015;&#26500;&#24314;&#31867;&#20284;&#20110;NeRF&#30340;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#22312;&#34394;&#25311;&#29616;&#23454;/&#22686;&#24378;&#29616;&#23454;&#39046;&#22495;&#20855;&#26377;&#22810;&#26679;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;NeRF&#30340;&#20027;&#35266;&#27169;&#22411;&#21487;&#20197;&#29992;&#20316;&#36924;&#30495;&#30340;&#27169;&#25311;&#65292;&#23545;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#36827;&#27493;&#20316;&#20986;&#37325;&#22823;&#36129;&#29486;&#12290;&#20027;&#35266;&#35270;&#35282;&#21512;&#25104;&#30340;&#26410;&#26469;&#21487;&#33021;&#23548;&#33268;&#36229;&#36234;&#20170;&#22825;NeRFs&#30340;&#26032;&#39062;&#29615;&#22659;&#34920;&#31034;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#25968;&#25454;&#19982;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#65288;&#22914;&#29992;&#20110;&#33258;&#25105;&#36816;&#21160;&#36319;&#36394;&#30340;IMU&#65292;&#29992;&#20110;&#25429;&#25417;&#34920;&#38754;&#32441;&#29702;&#21644;&#20154;&#31867;&#35821;&#22659;&#30340;&#38899;&#39057;&#20256;&#24863;&#22120;&#65292;&#20197;&#21450;&#29992;&#20110;&#25512;&#26029;&#22330;&#26223;&#20013;&#20154;&#31867;&#27880;&#24847;&#27169;&#24335;&#30340;&#30524;&#37096;&#20957;&#35270;&#36861;&#36394;&#22120;&#65289;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06455v2 Announce Type: replace-cross  Abstract: We seek to accelerate research in developing rich, multimodal scene models trained from egocentric data, based on differentiable volumetric ray-tracing inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like model from an egocentric image sequence plays a pivotal role in understanding human behavior and holds diverse applications within the realms of VR/AR. Such egocentric NeRF-like models may be used as realistic simulations, contributing significantly to the advancement of intelligent agents capable of executing tasks in the real-world. The future of egocentric view synthesis may lead to novel environment representations going beyond today's NeRFs by augmenting visual data with multimodal sensors such as IMU for egomotion tracking, audio sensors to capture surface texture and human language context, and eye-gaze trackers to infer human attention patterns in the scene. To support and facilitate the developm
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;GPT 3.5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#21521;&#37327;&#30340;RAG&#25968;&#25454;&#24211;&#21644;&#38750;&#31639;&#27861;&#36719;&#25552;&#31034;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#22522;&#32447;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26465;&#20214;&#19979;&#24494;&#35843;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.05903</link><description>&lt;p&gt;
&#22312;&#38750;&#19987;&#19994;LLM&#29992;&#25143;&#20013;&#20026;&#24494;&#35843;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#36719;&#25552;&#31034;&#24314;&#31435;&#24615;&#33021;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05903
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;GPT 3.5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#21521;&#37327;&#30340;RAG&#25968;&#25454;&#24211;&#21644;&#38750;&#31639;&#27861;&#36719;&#25552;&#31034;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#22522;&#32447;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26465;&#20214;&#19979;&#24494;&#35843;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#12289;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#36719;&#25552;&#31034;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#30740;&#31350;&#24448;&#24448;&#32858;&#28966;&#20110;&#20351;&#29992;&#39640;&#24230;&#25216;&#26415;&#21270;&#25110;&#39640;&#25104;&#26412;&#30340;&#25216;&#26415;&#65292;&#20351;&#35768;&#22810;&#26032;&#21457;&#29616;&#30340;&#26041;&#27861;&#23545;&#38750;&#25216;&#26415;&#29992;&#25143;&#26469;&#35828;&#30456;&#23545;&#19981;&#26131;&#35775;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#26410;&#32463;&#20462;&#25913;&#30340;GPT 3.5&#29256;&#26412;&#12289;&#32463;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20197;&#21450;&#22312;&#21333;&#29420;&#35775;&#38382;&#22522;&#20110;&#21521;&#37327;&#30340;RAG&#25968;&#25454;&#24211;&#26102;&#30456;&#21516;&#26410;&#32463;&#20462;&#25913;&#30340;&#27169;&#22411;&#65292;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#37197;&#22791;&#20102;&#22522;&#26412;&#30340;&#38750;&#31639;&#27861;&#36719;&#25552;&#31034;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#22411;&#22238;&#31572;&#19968;&#32452;&#20027;&#35201;&#28041;&#21450;2021&#24180;9&#26376;&#20043;&#21518;&#20107;&#20214;&#30340;100&#20010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65288;&#36825;&#26159;GPT 3.5&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#32467;&#26463;&#30340;&#26102;&#38388;&#28857;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#20351;&#29992;&#21830;&#19994;&#24179;&#21488;&#24182;&#24212;&#29992;&#40664;&#35748;&#35774;&#32622;&#65292;&#19981;&#36827;&#34892;&#36845;&#20195;&#20197;&#24314;&#31435;&#19968;&#32452;&#22522;&#32447;&#36755;&#20986;&#65292;&#37027;&#20040;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05903v2 Announce Type: replace-cross  Abstract: Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperf
&lt;/p&gt;</description></item><item><title>TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.02971</link><description>&lt;p&gt;
TabRepo&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#34920;&#26684;&#27169;&#22411;&#35780;&#20272;&#24211;&#21450;&#20854;AutoML&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02971
&lt;/p&gt;
&lt;p&gt;
TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TabRepo&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#22810;&#31181;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#36827;&#34892;&#35832;&#22914;&#27604;&#36739;&#36229;&#21442;&#25968;&#20248;&#21270;&#19982;&#24403;&#21069;AutoML&#31995;&#32479;&#20197;&#21450;&#22312;&#20351;&#29992;&#39044;&#35745;&#31639;&#27169;&#22411;&#39044;&#27979;&#30340;&#21516;&#26102;&#32771;&#34385;&#38598;&#25104;&#30340;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#25191;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#24212;&#29992;&#26631;&#20934;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#24310;&#36831;&#26041;&#38754;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02971v2 Announce Type: replace-cross  Abstract: We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#25552;&#20986;&#20351;&#29992;LLM&#20316;&#20026;&#33258;&#21160;&#21518;&#32534;&#36753;&#22120;(APE)&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#25193;&#23637;&#21040;&#25991;&#26723;&#32423;&#32763;&#35793;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14855</link><description>&lt;p&gt;
&#32763;&#35793;&#30340;&#19978;&#19979;&#25991;&#32454;&#21270;&#65306;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21518;&#32534;&#36753;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14855
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#25552;&#20986;&#20351;&#29992;LLM&#20316;&#20026;&#33258;&#21160;&#21518;&#32534;&#36753;&#22120;(APE)&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#25193;&#23637;&#21040;&#25991;&#26723;&#32423;&#32763;&#35793;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35201;&#27714;&#24191;&#27867;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30340;&#26174;&#33879;&#24615;&#33021;&#26174;&#31034;&#20102;&#23427;&#20204;&#22312;&#32763;&#35793;&#20013;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#36827;&#34892;MT&#65292;&#24182;&#25506;&#32034;&#20102;&#26368;&#36817;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#20026;&#32763;&#35793;&#30446;&#30340;&#24494;&#35843;&#65292;&#20063;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#23558;LLM&#20316;&#20026;&#33258;&#21160;&#21518;&#32534;&#36753;&#22120;(APE)&#32780;&#19981;&#26159;&#30452;&#25509;&#36716;&#25442;&#22120;&#12290;&#22522;&#20110;LLM&#22788;&#29702;&#21644;&#29983;&#25104;&#38271;&#24207;&#21015;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26723;&#32423;&#32763;&#35793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#36827;&#34892; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14855v2 Announce Type: replace-cross  Abstract: Large Language Models (LLM's) have demonstrated considerable success in various Natural Language Processing tasks, but they have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLM's for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments find that fine-tuning for translation purposes even led to performance degradation. To overcome this, we propose an alternative approach: adapting LLM's as Automatic Post-Editors (APE) rather than direct translators. Building on the LLM's exceptional ability to process and generate lengthy sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-t
&lt;/p&gt;</description></item><item><title>&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08559</link><description>&lt;p&gt;
&#20196;&#20154;&#24778;&#21497;&#20294;&#20196;&#20154;&#22256;&#24785;&#65306;&#29992;&#20551;&#35774;&#32454;&#21270;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08559
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#25968;&#35266;&#23519;&#20013;&#25512;&#23548;&#20986;&#28508;&#22312;&#21407;&#21017;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#21363;&#24402;&#32435;&#25512;&#29702;&#65292;&#23545;&#20110;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#20551;&#35774;&#32454;&#21270;&#36825;&#19968;&#25216;&#26415;&#23545;LMs&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35813;&#25216;&#26415;&#26356;&#25509;&#36817;&#20154;&#31867;&#24402;&#32435;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#36755;&#20837;-&#36755;&#20986;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#20026;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2309.10668</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#21363;&#20026;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Language Modeling Is Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.10668
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#20026;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#24050;&#30830;&#31435;&#20102;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#36716;&#21270;&#20026;&#26080;&#25439;&#21387;&#32553;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#38598;&#20013;&#31934;&#21147;&#35757;&#32451;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#65288;&#35821;&#35328;&#65289;&#27169;&#22411;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#26377;&#26395;&#25104;&#20026;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#36890;&#36807;&#21387;&#32553;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#22823;&#22411;&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#30340;&#21387;&#32553;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#36890;&#29992;&#39044;&#27979;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20851;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;Chinchilla 70B&#65292;&#34429;&#28982;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#23558;ImageNet&#30340;&#34917;&#19969;&#21387;&#32553;&#20026;&#20854;&#21407;&#22987;&#22823;&#23567;&#30340;43.4%&#65292;&#23558;LibriSpeech&#26679;&#26412;&#21387;&#32553;&#20026;&#20854;&#21407;&#22987;&#22823;&#23567;&#30340;16.4%&#65292;&#36229;&#36234;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.10668v2 Announce Type: replace-cross  Abstract: It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors li
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item><item><title>Radiology-GPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25918;&#23556;&#23398;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25918;&#23556;&#23398;&#35786;&#26029;&#12289;&#30740;&#31350;&#21644;&#27807;&#36890;&#26041;&#38754;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#20020;&#24202;NLP&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#25512;&#21160;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.08666</link><description>&lt;p&gt;
Radiology-GPT&#65306;&#29992;&#20110;&#25918;&#23556;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Radiology-GPT: A Large Language Model for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08666
&lt;/p&gt;
&lt;p&gt;
Radiology-GPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25918;&#23556;&#23398;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25918;&#23556;&#23398;&#35786;&#26029;&#12289;&#30740;&#31350;&#21644;&#27807;&#36890;&#26041;&#38754;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#20020;&#24202;NLP&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#25512;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Radiology-GPT&#65292;&#19968;&#20010;&#29992;&#20110;&#25918;&#23556;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#25918;&#23556;&#23398;&#39046;&#22495;&#30693;&#35782;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;Radiology-GPT&#34920;&#29616;&#20986;&#27604;StableLM&#12289;Dolly&#21644;LLaMA&#31561;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#25918;&#23556;&#23398;&#35786;&#26029;&#12289;&#30740;&#31350;&#21644;&#27807;&#36890;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#20652;&#21270;&#21058;&#12290;Radiology-GPT&#30340;&#25104;&#21151;&#23454;&#26045;&#34920;&#26126;&#20102;&#23450;&#20301;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#20026;&#29420;&#29305;&#30340;&#21307;&#23398;&#19987;&#19994;&#23450;&#21046;&#65292;&#21516;&#26102;&#30830;&#20445;&#36981;&#23432;HIPAA&#31561;&#38544;&#31169;&#26631;&#20934;&#12290;&#24320;&#21457;&#38024;&#23545;&#21508;&#23478;&#21307;&#38498;&#29305;&#23450;&#38656;&#27714;&#30340;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#26223;&#21576;&#29616;&#20986;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#34701;&#21512;&#30340;&#20250;&#35805;&#33021;&#21147;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#34987;&#35774;&#23450;&#20026;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08666v2 Announce Type: replace-cross  Abstract: We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34892;&#20154;&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#36807;&#39532;&#36335;&#34892;&#20026;&#65292;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2304.08260</link><description>&lt;p&gt;
&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#39044;&#27979;&#34892;&#20154;&#20114;&#21160;&#32467;&#26524;&#65306;&#31359;&#36234;&#25110;&#31561;&#24453;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cross or Wait? Predicting Pedestrian Interaction Outcomes at Unsignalized Crossings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34892;&#20154;&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#36807;&#39532;&#36335;&#34892;&#20026;&#65292;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34892;&#20154;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#34892;&#20026;&#26159;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#34892;&#20154;&#36807;&#39532;&#36335;&#30340;&#34892;&#20026;&#21463;&#21040;&#21508;&#31181;&#20114;&#21160;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#21040;&#36798;&#26102;&#38388;&#12289;&#34892;&#20154;&#31561;&#24453;&#26102;&#38388;&#12289;&#26001;&#39532;&#32447;&#30340;&#23384;&#22312;&#65292;&#20197;&#21450;&#34892;&#20154;&#21644;&#39550;&#39542;&#21592;&#30340;&#29305;&#24615;&#21644;&#20010;&#24615;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#29992;&#20110;&#39044;&#27979;&#20114;&#21160;&#32467;&#26524;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#34892;&#20154;&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#36807;&#39532;&#36335;&#34892;&#20026;&#65292;&#21253;&#25324;&#34892;&#20154;&#30340;&#36807;&#39532;&#36335;&#20915;&#31574;&#12289;&#36807;&#34903;&#24320;&#22987;&#26102;&#38388;&#65288;CIT&#65289;&#21644;&#36807;&#34903;&#25345;&#32493;&#26102;&#38388;&#65288;CD&#65289;&#12290;&#20998;&#24067;&#24335;&#27169;&#25311;&#25968;&#25454;&#34987;&#29992;&#20110;&#39044;&#27979;&#21644;&#20998;&#26512;&#36825;&#20123;&#20114;&#21160;&#22240;&#32032;&#12290;&#19982;&#36923;&#36753;&#22238;&#24402;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#25552;&#39640;&#20102;4.46%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08260v2 Announce Type: replace-cross  Abstract: Predicting pedestrian behavior when interacting with vehicles is one of the most critical challenges in the field of automated driving. Pedestrian crossing behavior is influenced by various interaction factors, including time to arrival, pedestrian waiting time, the presence of zebra crossing, and the properties and personality traits of both pedestrians and drivers. However, these factors have not been fully explored for use in predicting interaction outcomes. In this paper, we use machine learning to predict pedestrian crossing behavior including pedestrian crossing decision, crossing initiation time (CIT), and crossing duration (CD) when interacting with vehicles at unsignalized crossings. Distributed simulator data are utilized for predicting and analyzing the interaction factors. Compared with the logistic regression baseline model, our proposed neural network model improves the prediction accuracy and F1 score by 4.46% an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;TOPSIS&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25490;&#21517;&#20113;ERP&#30340;&#37319;&#29992;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2301.00693</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;TOPSIS&#36827;&#34892;&#28145;&#24230;&#24490;&#29615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Recurrent Learning Through Long Short Term Memory and TOPSIS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.00693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;TOPSIS&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25490;&#21517;&#20113;ERP&#30340;&#37319;&#29992;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36164;&#28304;&#35268;&#21010;&#65288;ERP&#65289;&#36719;&#20214;&#23558;&#36164;&#28304;&#12289;&#25968;&#25454;&#27719;&#32858;&#22312;&#19968;&#36215;&#65292;&#20197;&#20445;&#25345;&#20225;&#19994;&#27969;&#31243;&#20013;&#30340;&#36719;&#20214;&#27969;&#30021;&#12290;&#28982;&#32780;&#65292;&#20113;&#35745;&#31639;&#30340;&#24265;&#20215;&#12289;&#31616;&#20415;&#21644;&#24555;&#36895;&#31649;&#29702;&#25215;&#35834;&#20419;&#20351;&#20225;&#19994;&#25152;&#26377;&#32773;&#23558;&#21333;&#19968;&#20307;&#26550;&#26500;&#36716;&#21464;&#20026;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;/&#20113;&#30340;ERP&#12290;&#30001;&#20110;&#20113;ERP&#30340;&#24320;&#21457;&#28041;&#21450;&#19968;&#20010;&#24490;&#29615;&#36807;&#31243;&#65292;&#21363;&#35268;&#21010;&#12289;&#23454;&#26045;&#12289;&#27979;&#35797;&#21644;&#21319;&#32423;&#65292;&#20854;&#37319;&#29992;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#38382;&#39064;&#12290;&#26368;&#32456;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;TOPSIS&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#21035;&#25490;&#21517;&#37319;&#29992;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#27169;&#22411;&#36890;&#36807;&#38416;&#36848;&#20851;&#38190;&#21442;&#19982;&#32773;&#12289;&#26381;&#21153;&#12289;&#26550;&#26500;&#12289;&#21151;&#33021;&#65292;&#22312;&#21442;&#32771;&#27169;&#22411;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#32771;&#34385;&#25216;&#26415;&#12289;&#21019;&#26032;&#21644;&#25269;&#25239;&#38382;&#39064;&#65292;&#24320;&#23637;&#20102;&#19968;&#39033;&#23450;&#24615;&#35843;&#26597;&#65292;&#20197;&#23601;&#20851;&#38190;&#37319;&#29992;&#22240;&#32032;&#25552;&#20986;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.00693v2 Announce Type: replace-cross  Abstract: Enterprise resource planning (ERP) software brings resources, data together to keep software-flow within business processes in a company. However, cloud computing's cheap, easy and quick management promise pushes business-owners for a transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP development involves a cyclic process, namely planning, implementing, testing and upgrading, its adoption is realized as a deep recurrent neural network problem. Eventually, a classification algorithm based on long short term memory (LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption features. Our theoretical model is validated over a reference model by articulating key players, services, architecture, functionalities. Qualitative survey is conducted among users by considering technology, innovation and resistance issues, to formulate hypotheses on key adoption factors.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09516</link><description>&lt;p&gt;
&#36890;&#36807;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#22238;&#25910;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#31070;&#32463;&#31639;&#23376;&#22240;&#20854;&#39640;&#25512;&#29702;&#25928;&#29575;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#31639;&#23376;&#38656;&#35201;&#29983;&#25104;&#22823;&#37327;&#24102;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#21363;PDE&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35299;&#20915;&#22823;&#37327;&#32447;&#24615;&#26041;&#31243;&#32452;&#20197;&#33719;&#24471;PDE&#30340;&#25968;&#20540;&#35299;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#22312;&#30456;&#20284;&#24615;&#65292;&#23548;&#33268;&#35745;&#31639;&#26497;&#20854;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;(SKR)&#65292;&#20197;&#25552;&#39640;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SKR&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#25968;&#25454;&#29983;&#25104;&#32791;&#26102;&#24615;&#36136;&#30340;&#23581;&#35797;&#12290;SKR&#30340;&#26680;&#24515;&#26159;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#33218;&#31934;&#32454;&#25805;&#20316;&#21644;&#38656;&#35201;&#31934;&#32454;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.07603</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#29992;&#20110;&#21452;&#33218;&#31934;&#32454;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multi-task robot data for dual-arm fine manipulation. (arXiv:2401.07603v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#33218;&#31934;&#32454;&#25805;&#20316;&#21644;&#38656;&#35201;&#31934;&#32454;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#39046;&#22495;&#65292;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#33719;&#21462;&#25805;&#20316;&#25216;&#33021;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#22312;&#36825;&#26679;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#21508;&#31181;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#22312;&#22810;&#20010;&#29289;&#20307;&#19978;&#23454;&#29616;&#20102;&#26222;&#36866;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#30456;&#23545;&#19981;&#31934;&#30830;&#30340;&#21333;&#33218;&#20219;&#21153;&#19978;&#65292;&#24182;&#27809;&#26377;&#35299;&#20915;&#26426;&#22120;&#20154;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38656;&#35201;&#25191;&#34892;&#30340;&#32454;&#31890;&#24230;&#29289;&#20307;&#25805;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#21452;&#33218;&#20219;&#21153;&#21644;/&#25110;&#38656;&#35201;&#31934;&#32454;&#25805;&#20316;&#30340;&#22810;&#26679;&#21270;&#29289;&#20307;&#25805;&#32437;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#21253;&#21547;224k&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65288;150&#23567;&#26102;&#65292;1,104&#20010;&#35821;&#35328;&#25351;&#20196;&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#33218;&#31934;&#32454;&#20219;&#21153;&#65292;&#22914;&#31227;&#21160;&#30871;&#12289;&#25171;&#24320;&#31508;&#34955;&#25110;&#21093;&#39321;&#34121;&#65292;&#24182;&#19988;&#36825;&#20123;&#25968;&#25454;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#35270;&#35273;&#27880;&#24847;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of robotic manipulation, deep imitation learning is recognized as a promising approach for acquiring manipulation skills. Additionally, learning from diverse robot datasets is considered a viable method to achieve versatility and adaptability. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise, not addressing the fine-grained object manipulation that robots are expected to perform in the real world. This paper introduces a dataset of diverse object manipulations that includes dual-arm tasks and/or tasks requiring fine manipulation. To this end, we have generated dataset with 224k episodes (150 hours, 1,104 language instructions) which includes dual-arm fine tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data is publicly available. Additionally, this dataset includes visual attention signals a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01989</link><description>&lt;p&gt;
&#37325;&#35775;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#20174;&#20301;&#32622;&#20559;&#35265;&#30340;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#26469;&#34920;&#24449;&#21644;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20808;&#21069;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26356;&#20026;&#38480;&#21046;&#24615;&#30340;&#24341;&#23548;&#20559;&#35265;&#29616;&#35937;&#30340;&#19968;&#33324;&#34920;&#36848;&#12290;&#20301;&#32622;&#20559;&#35265;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#26576;&#20123;&#37096;&#20998;&#19978;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#65292;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLM&#27169;&#22411;&#22914;GPT 3.5-Turbo&#65292;Llama-2&#21644;Dolly-v2&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#65292;&#20197;&#21450;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22914;Pegasus&#21644;BART&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#20154;&#26426;&#20132;&#20114;&#21327;&#35758;&#30340;&#20849;&#21516;&#35789;&#27719;&#30340;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;&#35299;AI&#24212;&#35813;&#25552;&#20379;&#20160;&#20040;&#20449;&#24687;&#26469;&#24110;&#21161;&#20154;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19778</link><description>&lt;p&gt;
&#35774;&#35745;AI&#25903;&#25345;&#20154;&#31867;&#21442;&#19982;AI&#36741;&#21161;&#20915;&#31574;&#65306;&#20174;&#31995;&#32479;&#24615;&#22238;&#39038;&#20013;&#20998;&#31867;&#20154;&#26426;&#20132;&#20114;&#30340;&#31246;&#34920;
&lt;/p&gt;
&lt;p&gt;
Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review. (arXiv:2310.19778v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#20154;&#26426;&#20132;&#20114;&#21327;&#35758;&#30340;&#20849;&#21516;&#35789;&#27719;&#30340;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;&#35299;AI&#24212;&#35813;&#25552;&#20379;&#20160;&#20040;&#20449;&#24687;&#26469;&#24110;&#21161;&#20154;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21162;&#21147;&#36807;&#20998;&#20851;&#27880;&#25216;&#26415;&#36827;&#27493;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#31639;&#27861;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20174;&#26356;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#20419;&#36827;AI&#30340;&#21457;&#23637;&#12290;&#30830;&#23450;AI&#24212;&#35813;&#25552;&#20379;&#21738;&#20123;&#20449;&#24687;&#26469;&#24110;&#21161;&#20154;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28982;&#32780;&#65292;&#20449;&#24687;&#22914;&#20309;&#21576;&#29616;&#65292;&#27604;&#22914;&#25512;&#33616;&#30340;&#39034;&#24207;&#21644;&#35299;&#37322;&#30340;&#35201;&#27714;&#21516;&#26679;&#37325;&#35201;&#12290;&#36825;&#25512;&#21160;&#20102;&#26356;&#31934;&#30830;&#22320;&#30740;&#31350;&#20154;&#26426;&#20132;&#20114;&#20316;&#20026;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#25903;&#25345;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#24050;&#26377;&#20960;&#20010;&#23454;&#35777;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#20854;&#20013;&#20132;&#20114;&#24418;&#24335;&#21508;&#24322;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#35789;&#27719;&#26469;&#25551;&#36848;&#20154;&#26426;&#20132;&#20114;&#21327;&#35758;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efforts in levering Artificial Intelligence (AI) in decision support systems have disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. To address this, explainable AI promotes AI development from a more human-centered perspective. Determining what information AI should provide to aid humans is vital, however, how the information is presented, e. g., the sequence of recommendations and the solicitation of interpretations, is equally crucial. This motivates the need to more precisely study Human-AI interaction as a pivotal component of AI-based decision support. While several empirical studies have evaluated Human-AI interactions in multiple application domains in which interactions can take many forms, there is not yet a common vocabulary to describe human-AI interaction protocols. To address this gap, we describe the results of a systematic review of the AI-assisted decision making literature, anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#21644;&#19968;&#20010;LLM-guided&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#39640;&#32423;&#25351;&#23548;&#23545;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11409</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLMs for Privilege-Escalation Scenarios. (arXiv:2310.11409v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#21644;&#19968;&#20010;LLM-guided&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#39640;&#32423;&#25351;&#23548;&#23545;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28183;&#36879;&#27979;&#35797;&#26159;&#32593;&#32476;&#23433;&#20840;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20801;&#35768;&#32452;&#32455;&#20027;&#21160;&#35782;&#21035;&#21644;&#20462;&#22797;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#28508;&#22312;&#32593;&#32476;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#22312;&#28183;&#36879;&#27979;&#35797;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#19968;&#20010;&#36827;&#23637;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#25506;&#32034;LLMs&#19982;&#28183;&#36879;&#27979;&#35797;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#26412;&#22320;&#34394;&#25311;&#26426;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#30340;LLMs&#21644;&#25552;&#31034;&#31574;&#30053;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#21521;LLMs&#25552;&#20379;&#39640;&#32423;&#25351;&#23548;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#39046;&#22495;&#65292;&#21253;&#25324;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20445;&#25345;&#19987;&#27880;&#12289;&#22788;&#29702;&#38169;&#35823;&#20197;&#21450;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#36827;&#34892;3D&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#27979;&#35268;&#21010;&#65292;&#21512;&#29702;&#36873;&#25321;&#30456;&#26426;&#20301;&#32622;&#24182;&#32771;&#34385;&#22122;&#22768;&#23545;&#37325;&#24314;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;3D&#37325;&#24314;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00145</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;3D&#37325;&#24314;&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#35270;&#35282;&#30340;&#35266;&#27979;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
3D Reconstruction in Noisy Agricultural Environments: A Bayesian Optimization Perspective for View Planning. (arXiv:2310.00145v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#36827;&#34892;3D&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#27979;&#35268;&#21010;&#65292;&#21512;&#29702;&#36873;&#25321;&#30456;&#26426;&#20301;&#32622;&#24182;&#32771;&#34385;&#22122;&#22768;&#23545;&#37325;&#24314;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;3D&#37325;&#24314;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#37325;&#24314;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22240;&#20854;&#22312;&#20892;&#19994;&#12289;&#27700;&#19979;&#21644;&#22478;&#24066;&#29615;&#22659;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21512;&#29702;&#23433;&#25918;&#30456;&#26426;&#26469;&#26368;&#22823;&#21270;&#35270;&#35273;&#20449;&#24687;&#65292;&#25552;&#39640;3D&#37325;&#24314;&#32467;&#26524;&#65292;&#31216;&#20026;&#35266;&#27979;&#35268;&#21010;&#12290;&#36890;&#36807;&#23558;&#20960;&#20309;&#26631;&#20934;&#24212;&#29992;&#20110;&#36873;&#25321;&#36739;&#23569;&#20294;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#30340;&#20219;&#24847;&#22270;&#20687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;3D&#37325;&#24314;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#32771;&#34385;&#21040;&#23384;&#22312;&#30340;&#22122;&#22768;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#25552;&#20379;&#26377;&#20851;&#22122;&#22768;&#30340;&#20808;&#39564;&#20449;&#24687;&#26102;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#22122;&#22768;&#65292;&#20165;&#20381;&#38752;&#30456;&#23545;&#36739;&#23569;&#30340;&#22122;&#22768;&#23454;&#29616;&#26469;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#20854;&#23553;&#38381;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D reconstruction is a fundamental task in robotics that gained attention due to its major impact in a wide variety of practical settings, including agriculture, underwater, and urban environments. An important approach for this task, known as view planning, is to judiciously place a number of cameras in positions that maximize the visual information improving the resulting 3D reconstruction. Circumventing the need for a large number of arbitrary images, geometric criteria can be applied to select fewer yet more informative images to markedly improve the 3D reconstruction performance. Nonetheless, incorporating the noise of the environment that exists in various real-world scenarios into these criteria may be challenging, particularly when prior information about the noise is not provided. To that end, this work advocates a novel geometric function that accounts for the existing noise, relying solely on a relatively small number of noise realizations without requiring its closed-form e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;Dysen&#65289;&#26469;&#22686;&#24378;&#38754;&#21521;&#21160;&#24577;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#21160;&#20316;&#12289;&#24314;&#31435;&#21160;&#24577;&#22330;&#26223;&#22270;&#21644;&#20016;&#23500;&#32454;&#33410;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;T2V&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.13812</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#38754;&#21521;&#21160;&#24577;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models. (arXiv:2308.13812v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;Dysen&#65289;&#26469;&#22686;&#24378;&#38754;&#21521;&#21160;&#24577;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#21160;&#20316;&#12289;&#24314;&#31435;&#21160;&#24577;&#22330;&#26223;&#22270;&#21644;&#20016;&#23500;&#32454;&#33410;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;T2V&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#22312;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#26368;&#36817;&#20986;&#29616;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#27604;&#36807;&#21435;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#24378;&#22823;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;DMs&#33021;&#22815;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#35270;&#39057;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#20027;&#35201;&#38480;&#21046;&#65288;&#20363;&#22914;&#65292;&#21160;&#20316;&#20986;&#29616;&#38556;&#30861;&#65292;&#31895;&#31961;&#30340;&#35270;&#39057;&#36816;&#21160;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21152;&#24378;DMs&#23545;&#35270;&#39057;&#21160;&#24577;&#30340;&#24863;&#30693;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;T2V&#29983;&#25104;&#12290;&#21463;&#21040;&#20154;&#31867;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21160;&#24577;&#22330;&#26223;&#31649;&#29702;&#22120;&#65288;&#31216;&#20026;Dysen&#65289;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#65288;&#27493;&#39588;1&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#26102;&#38388;&#39034;&#24207;&#23433;&#25490;&#30340;&#20851;&#38190;&#21160;&#20316;&#65292;&#65288;&#27493;&#39588;2&#65289;&#23558;&#21160;&#20316;&#26102;&#38388;&#34920;&#36716;&#21270;&#20026;&#21160;&#24577;&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#34920;&#31034;&#65292;&#20197;&#21450;&#65288;&#27493;&#39588;3&#65289;&#20016;&#23500;DSG&#20013;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#20805;&#20998;&#21644;&#21512;&#29702;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Takin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04792</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#34892;&#26143;&#25506;&#27979;&#36710;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast and Optimal Learning-based Path Planning Method for Planetary Rovers. (arXiv:2308.04792v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#31243;&#22270;&#20013;&#24555;&#36895;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#65292;&#31216;&#20026;NNPP&#12290;NNPP&#27169;&#22411;&#20174;&#22823;&#37327;&#39044;&#27880;&#37322;&#30340;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#23398;&#20064;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#22320;&#22270;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#27599;&#20010;&#20687;&#32032;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#34920;&#31034;&#20854;&#23646;&#20110;&#22320;&#22270;&#19978;&#26368;&#20248;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20174;DEM&#33719;&#21462;&#30340;&#22369;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#39640;&#24230;&#24046;&#35745;&#31639;&#27599;&#20010;&#32593;&#26684;&#21333;&#20803;&#30340;&#36941;&#21382;&#25104;&#26412;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#23545;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20998;&#26512;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;NNPP&#27169;&#22411;&#33021;&#22815;&#22312;&#26032;&#30340;&#22320;&#22270;&#19978;&#25191;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;NNPP&#29983;&#25104;&#30340;&#24341;&#23548;&#22330;&#33021;&#22815;&#20934;&#30830;&#25351;&#23548;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent autonomous path planning is crucial to improve the exploration efficiency of planetary rovers. In this paper, we propose a learning-based method to quickly search for optimal paths in an elevation map, which is called NNPP. The NNPP model learns semantic information about start and goal locations, as well as map representations, from numerous pre-annotated optimal path demonstrations, and produces a probabilistic distribution over each pixel representing the likelihood of it belonging to an optimal path on the map. More specifically, the paper computes the traversal cost for each grid cell from the slope, roughness and elevation difference obtained from the DEM. Subsequently, the start and goal locations are encoded using a Gaussian distribution and different location encoding parameters are analyzed for their effect on model performance. After training, the NNPP model is able to perform path planning on novel maps. Experiments show that the guidance field generated by the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>SAMAug&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#32467;&#21512;&#21021;&#22987;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;Segment Anything Model&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01187</link><description>&lt;p&gt;
SAMAug: Segment Anything Model&#30340;&#28857;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAMAug: Point Prompt Augmentation for Segment Anything Model. (arXiv:2307.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01187
&lt;/p&gt;
&lt;p&gt;
SAMAug&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#32467;&#21512;&#21021;&#22987;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;Segment Anything Model&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SAMAug&#65292;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26032;&#22411;&#35270;&#35273;&#28857;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;&#12290;SAMAug&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#20449;&#24687;&#32473;SAM&#12290;SAM&#20174;&#19968;&#20010;&#21021;&#22987;&#28857;&#25552;&#31034;&#24320;&#22987;&#29983;&#25104;&#19968;&#20010;&#21021;&#22987;&#25513;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#25552;&#20986;&#30340;SAMAug&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#39069;&#22806;&#30340;&#28857;&#65292;SAM&#21487;&#20197;&#22522;&#20110;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#21644;&#21021;&#22987;&#25552;&#31034;&#29983;&#25104;&#22686;&#24378;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#28857;&#22686;&#24378;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65306;&#38543;&#26426;&#37319;&#26679;&#65292;&#22522;&#20110;&#26368;&#22823;&#24046;&#24322;&#29109;&#30340;&#37319;&#26679;&#65292;&#26368;&#22823;&#36317;&#31163;&#21644;&#26174;&#33879;&#24615;&#12290;&#22312;COCO&#12289;Fundus&#12289;COVID QUEx&#21644;ISIC2018&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;SAMAug&#21487;&#20197;&#25552;&#21319;SAM&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#26368;&#22823;&#36317;&#31163;&#21644;&#26174;&#33879;&#24615;&#12290;SAMAug&#35777;&#26126;&#20102;&#20854;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15006</link><description>&lt;p&gt;
DNABERT-2:&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#22522;&#22240;&#32452;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#26159;&#29983;&#29289;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;DNABERT&#21644;Nucleotide Transformer&#31561;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;k-mer&#20316;&#20026;&#22522;&#22240;&#32452;&#35821;&#35328;&#30340;&#26631;&#35760;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;k-mer&#26631;&#35760;&#21270;&#24341;&#20837;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#26159;&#21457;&#23637;&#22823;&#35268;&#27169;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#32452;&#26631;&#35760;&#21270;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#35265;&#35299;&#65292;&#22522;&#20110;&#27492;&#25552;&#20986;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;k-mer&#26631;&#35760;&#21270;&#65292;BPE&#36890;&#36807;&#36845;&#20195;&#21512;&#24182;&#35821;&#26009;&#24211;&#20013;&#26368;&#39057;&#32321;&#20849;&#21516;&#20986;&#29616;&#30340;&#22522;&#22240;&#32452;&#29255;&#27573;&#26469;&#26500;&#24314;&#26631;&#35760;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;BPE&#19981;&#20165;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#36824;&#33021;&#20174;&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#25928;&#29575;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.09509</link><description>&lt;p&gt;
Granger&#22240;&#26524;&#30340;&#20998;&#23618;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Granger-Causal Hierarchical Skill Discovery. (arXiv:2306.09509v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#36973;&#21463;&#20302;&#26679;&#26412;&#25928;&#29575;&#21644;&#26377;&#38480;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#23398;&#20064;&#24471;&#21040;&#30340;&#20132;&#20114;&#26816;&#27979;&#22120;&#26469;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#12290;&#21463;Granger&#22240;&#26524;&#24615;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#25429;&#25417;&#21040;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23398;&#20064;&#26377;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#26159;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#22256;&#22659;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;HIntS - &#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#20854;&#20182;RL&#21644;HRL&#26041;&#27861;&#37117;&#34920;&#29616;&#19981;&#20339;&#12290;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#19981;&#20165;&#23637;&#31034;&#20102;&#20351;&#29992;Breakout&#30340;&#21464;&#20307;&#30340;&#36716;&#31227;&#65292;&#32780;&#19988;&#19982;&#21487;&#27604;&#36739;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30456;&#27604;&#65292;&#36824;&#34920;&#29616;&#20986;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;HIntS&#19968;&#36215;&#35777;&#26126;&#20102;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown promising results learning policies for complex tasks, but can often suffer from low sample efficiency and limited transfer. We introduce the Hierarchy of Interaction Skills (HIntS) algorithm, which uses learned interaction detectors to discover and train a hierarchy of skills that manipulate factors in factored environments. Inspired by Granger causality, these unsupervised detectors capture key events between factors to sample efficiently learn useful skills and transfer those skills to other related tasks -- tasks where many reinforcement learning techniques struggle. We evaluate HIntS on a robotic pushing task with obstacles -- a challenging domain where other RL and HRL methods fall short. The learned skills not only demonstrate transfer using variants of Breakout, a common RL benchmark, but also show 2-3x improvement in both sample efficiency and final performance compared to comparable RL baselines. Together, HIntS demonstrates a proof of co
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.10314</link><description>&lt;p&gt;
LeTI&#65306;&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10314
&lt;/p&gt;
&lt;p&gt;
LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LM)&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#25216;&#26415;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#23545;&#65288;&#20363;&#22914;&#25351;&#20196;&#24494;&#35843;&#65289;&#25110;&#29992;&#35780;&#20272;&#36755;&#20986;&#36136;&#37327;&#30340;&#25968;&#23383;&#22870;&#21169;&#65288;&#20363;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;LM&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#28508;&#21147;(LeTI)&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20108;&#36827;&#21046;&#26631;&#31614;&#26816;&#26597;&#20854;&#27491;&#30830;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21453;&#39304;&#25351;&#20986;&#21644;&#35299;&#37322;&#20854;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#12290;&#36825;&#31181;&#35774;&#32622;&#21487;&#20197;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#22320;&#33719;&#21462;&#25991;&#26412;&#21453;&#39304;&#65306;&#20351;&#29992;Python&#35299;&#37322;&#22120;&#36827;&#34892;&#20195;&#30721;&#25191;&#34892;&#26102;&#30340;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#12290; LeTI&#20351;&#29992;LM&#30446;&#26631;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#25991;&#26412;&#21453;&#39304;&#36827;&#34892;&#20018;&#32852;&#30340;&#36845;&#20195;&#24494;&#35843;&#65292;&#21482;&#26377;&#22312;&#29983;&#25104;&#20195;&#30721;&#26080;&#27861;&#25191;&#34892;&#26102;&#25165;&#25552;&#20379;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;58k&#20010;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LeTI&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01838</link><description>&lt;p&gt;
BugNIST -- &#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#20307;&#31215;&#19977;&#32500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BugNIST -- A New Large Scale Volumetric 3D Image Dataset for Classification and Detection. (arXiv:2304.01838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#30340;&#36827;&#23637;&#21463;&#21040;&#25968;&#25454;&#38598;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#30340;&#20998;&#26512;&#26041;&#27861;&#37117;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25968;&#25454;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#20854;&#20182;&#20307;&#31215;&#22270;&#20687;&#65288;&#20363;&#22914;&#24494;-CT&#65289;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#20419;&#36827;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#36229;&#36234;&#21307;&#23398;&#25968;&#25454;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BugNIST&#25968;&#25454;&#38598;&#24182;&#20813;&#36153;&#25552;&#20379;&#12290;BugNIST&#26159;&#19968;&#32452;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#12290;BugNIST&#21253;&#21547;9437&#20010;&#20307;&#31215;&#65292;&#20854;&#20013;9087&#20010;&#26159;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#65292;350&#20010;&#26159;&#26118;&#34411;&#21644;&#20854;&#20182;&#26448;&#26009;&#30340;&#28151;&#21512;&#29289;&#12290;BugNIST&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26816;&#27979;&#25361;&#25112;&#65292;&#20351;&#24471;&#26816;&#27979;&#27169;&#22411;&#22312;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#19978;&#35757;&#32451;&#24182;&#22312;&#26118;&#34411;&#28151;&#21512;&#29289;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#33021;&#22815;&#35299;&#20915;&#27492;&#20219;&#21153;&#30340;&#27169;&#22411;&#23558;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#65288;&#21363;&#21608;&#22260;&#26448;&#26009;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in 3D volumetric image analysis research is limited by the lack of datasets and most advances in analysis methods for volumetric images are based on medical data. However, medical data do not necessarily resemble the characteristics of other volumetric images such as micro-CT. To promote research in 3D volumetric image analysis beyond medical data, we have created the BugNIST dataset and made it freely available. BugNIST is an extensive dataset of micro-CT scans of 12 types of bugs, such as insects and larvae. BugNIST contains 9437 volumes where 9087 are of individual bugs and 350 are mixtures of bugs and other material. The goal of BugNIST is to benchmark classification and detection methods, and we have designed the detection challenge such that detection models are trained on scans of individual bugs and tested on bug mixtures. Models capable of solving this task will be independent of the context, i.e., the surrounding material. This is a great advantage if the context is 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12923</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#65306;&#36829;&#21453;&#35268;&#21017;&#26159;&#21542;&#26377;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12923
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#20223;&#32463;&#36807;&#35757;&#32451;&#30340;&#8220;&#25945;&#24072;&#8221;&#32593;&#32476;&#30340;&#36719;&#27010;&#29575;&#26469;&#25552;&#39640;&#8220;&#23398;&#29983;&#8221;&#32593;&#32476;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#25104;&#36866;&#24212;&#25945;&#24072;&#30340;&#27010;&#29575;&#65292;&#23398;&#29983;&#19981;&#20165;&#26126;&#26174;&#20559;&#31163;&#36825;&#20123;&#27010;&#29575;&#65292;&#32780;&#19988;&#34920;&#29616;&#27604;&#25945;&#24072;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30830;&#23450;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#30340;&#30830;&#20999;&#24615;&#36136;&#65292;&#24182;&#35770;&#35777;&#23427;&#20204;&#19982;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#20849;&#23384;&#26469;&#35299;&#20915;&#36825;&#19968;&#30475;&#20284;&#30683;&#30462;&#30340;&#35266;&#23519;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#20559;&#24046;&#23545;&#24212;&#20110;&#23398;&#29983;&#31995;&#32479;&#24615;&#22320;&#22840;&#22823;&#25945;&#24072;&#30340;&#33258;&#20449;&#27700;&#24179;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#24314;&#31435;&#20102;KD&#22312;&#25910;&#25947;&#26356;&#24555;&#30340;&#36807;&#31243;&#20013;&#22840;&#22823;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#24433;&#21709;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#20154;&#20204;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24182;&#19981;&#33021;&#24110;&#21161;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20419;&#36827;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2209.11812</link><description>&lt;p&gt;
&#35299;&#37322;&#12289;&#20844;&#27491;&#24615;&#21644;&#20154;&#31867;-AI&#20915;&#31574;&#20013;&#30340;&#36866;&#24403;&#20381;&#36182;&#65288;arXiv:2209.11812v3 [cs.HC] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#24433;&#21709;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#20154;&#20204;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24182;&#19981;&#33021;&#24110;&#21161;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20419;&#36827;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20174;&#31616;&#30701;&#30340;&#25991;&#26412;&#31616;&#20171;&#20013;&#39044;&#27979;&#32844;&#19994;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#24433;&#21709;&#22914;&#20309;&#36890;&#36807;&#20154;&#20204;&#30340;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#26469;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24433;&#21709;&#20102;&#20844;&#27491;&#24615;&#24863;&#30693;&#65292;&#32780;&#20844;&#27491;&#24615;&#24863;&#30693;&#21448;&#19982;&#20154;&#20204;&#36981;&#24490;AI&#24314;&#35758;&#30340;&#20542;&#21521;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#30340;&#35299;&#37322;&#24182;&#19981;&#33021;&#35753;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35299;&#37322;&#21487;&#33021;&#20250;&#24433;&#21709;&#20381;&#36182;&#65292;&#32780;&#19981;&#35770;AI&#24314;&#35758;&#30340;&#27491;&#30830;&#24615;&#22914;&#20309;&#12290;&#21462;&#20915;&#20110;&#35299;&#37322;&#31361;&#20986;&#30340;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#25110;&#38459;&#30861;&#20998;&#37197;&#20844;&#27491;&#24615;&#65306;&#24403;&#35299;&#37322;&#31361;&#20986;&#19982;&#25935;&#24863;&#23646;&#24615;&#26126;&#26174;&#30456;&#20851;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#26102;&#65292;&#36825;&#20250;&#20419;&#20351;&#20154;&#20204;&#35206;&#30422;AI&#19982;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw
&lt;/p&gt;</description></item></channel></rss>