<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;Fisher&#21512;&#24182;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23454;&#38469;&#25361;&#25112;&#65292;&#20855;&#26377;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05476</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#20013;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;Fisher&#21152;&#26435;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation. (arXiv:2307.05476v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;Fisher&#21512;&#24182;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23454;&#38469;&#25361;&#25112;&#65292;&#20855;&#26377;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#24179;&#21488;&#21644;&#26381;&#21153;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#35782;&#21035;&#30456;&#20851;&#29289;&#21697;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;&#24207;&#21015;&#25512;&#33616;&#30340;&#39046;&#22495;&#26088;&#22312;&#25429;&#25417;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#21160;&#24577;&#20559;&#22909;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30001;&#20110;&#26377;&#38480;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#32780;&#23548;&#33268;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;Fisher&#21512;&#24182;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#20013;&#65292;&#35299;&#20915;&#24182;&#35299;&#20915;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#30830;&#20445;&#40065;&#26834;&#24494;&#35843;&#65292;&#20174;&#32780;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#24207;&#21015;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the exponential growth of online platforms and services, recommendation systems have become essential for identifying relevant items based on user preferences. The domain of sequential recommendation aims to capture evolving user preferences over time. To address dynamic preference, various contrastive learning methods have been proposed to target data sparsity, a challenge in recommendation systems due to the limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#22686;&#24378;&#21644;&#38477;&#22122;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#22812;&#38388;&#22270;&#20687;&#22686;&#24378;&#31639;&#27861;&#65292;&#33021;&#22815;&#23558;&#20302;&#29031;&#24230;&#30340;&#22812;&#38388;&#22270;&#20687;&#36716;&#25442;&#20026;&#26356;&#20142;&#12289;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#33021;&#22815;&#25233;&#21046;&#22122;&#22768;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#31639;&#27861;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.05447</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#24230;&#22686;&#24378;&#21644;&#38477;&#22122;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#22812;&#38388;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and Denoising. (arXiv:2307.05447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#22686;&#24378;&#21644;&#38477;&#22122;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#22812;&#38388;&#22270;&#20687;&#22686;&#24378;&#31639;&#27861;&#65292;&#33021;&#22815;&#23558;&#20302;&#29031;&#24230;&#30340;&#22812;&#38388;&#22270;&#20687;&#36716;&#25442;&#20026;&#26356;&#20142;&#12289;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#33021;&#22815;&#25233;&#21046;&#22122;&#22768;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#31639;&#27861;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#20013;&#22812;&#38388;&#30446;&#26631;&#26816;&#27979;&#21644;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#22812;&#38388;&#22270;&#20687;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#30456;&#24212;&#30340;&#30333;&#22825;&#22270;&#20687;&#30456;&#27604;&#65292;&#22812;&#38388;&#22270;&#20687;&#30340;&#29305;&#28857;&#26159;&#20142;&#24230;&#20302;&#12289;&#23545;&#27604;&#24230;&#20302;&#21644;&#22122;&#22768;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#30340;&#22270;&#20687;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#20302;&#29031;&#24230;&#22270;&#20687;&#36716;&#25442;&#20026;&#26356;&#20142;&#12289;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#24207;&#21015;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#23545;&#27604;&#24230;&#22686;&#24378;&#21644;&#38477;&#22122;&#31639;&#27861;&#30340;&#38142;&#26465;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24418;&#24335;&#30340;&#36882;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#22812;&#38388;&#22270;&#20687;&#30340;&#20142;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#30495;&#23454;&#23454;&#39564;&#21644;&#27169;&#25311;&#23454;&#39564;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20004;&#20010;&#32467;&#26524;&#37117;&#26174;&#31034;&#20102;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;&#23545;&#27604;&#24230;&#23545;&#12289;Meylan&#21644;Retinex&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the low accuracy of object detection and recognition in many intelligent surveillance systems at nighttime, the quality of night images is crucial. Compared with the corresponding daytime image, nighttime image is characterized as low brightness, low contrast and high noise. In this paper, a bio-inspired image enhancement algorithm is proposed to convert a low illuminance image to a brighter and clear one. Different from existing bio-inspired algorithm, the proposed method doesn't use any training sequences, we depend on a novel chain of contrast enhancement and denoising algorithms without using any forms of recursive functions. Our method can largely improve the brightness and contrast of night images, besides, suppress noise. Then we implement on real experiment, and simulation experiment to test our algorithms. Both results show the advantages of proposed algorithm over contrast pair, Meylan and Retinex.
&lt;/p&gt;</description></item><item><title>ISLTranslate&#26159;&#19968;&#20010;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#30340;&#26368;&#22823;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#35299;&#20915;&#21360;&#24230;&#25163;&#35821;&#36164;&#28304;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05440</link><description>&lt;p&gt;
ISLTranslate: &#32763;&#35793;&#21360;&#24230;&#25163;&#35821;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05440
&lt;/p&gt;
&lt;p&gt;
ISLTranslate&#26159;&#19968;&#20010;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#30340;&#26368;&#22823;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#35299;&#20915;&#21360;&#24230;&#25163;&#35821;&#36164;&#28304;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#20840;&#29699;&#35768;&#22810;&#21548;&#38556;&#20154;&#22763;&#30340;&#20027;&#35201;&#36890;&#20449;&#26041;&#24335;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#24357;&#34917;&#21548;&#38556;&#31038;&#21306;&#19982;&#20854;&#20182;&#20154;&#32676;&#20043;&#38388;&#30340;&#27807;&#36890;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#24320;&#21457;&#32479;&#35745;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#25163;&#35821;&#30340;&#36164;&#28304;&#21294;&#20047;&#12290;&#26412;&#36164;&#28304;&#35770;&#25991;&#20171;&#32461;&#20102;ISLTranslate&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#65288;ISL&#65289;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#26368;&#22823;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#25163;&#35821;&#21040;&#21475;&#35821;&#32763;&#35793;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;ISL&#32763;&#35793;&#23545;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;BOLD-fMRI&#35745;&#31639;&#20154;&#31867;&#36830;&#25509;&#32452;&#21457;&#23637;&#38431;&#21015;&#20013;&#21628;&#21560;&#20307;&#31215;&#19982;&#26102;&#38388;&#65288;RTV&#65289;&#21644;&#21628;&#21560;&#21464;&#24322;&#65288;RV&#65289;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#21487;&#20197;&#20174;&#38745;&#24687;&#29366;&#24577;&#30340;BOLD&#20449;&#21495;&#20013;&#25429;&#25417;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#37325;&#24314;&#30495;&#23454;&#30340;&#21628;&#21560;&#26102;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;fMRI&#30740;&#31350;&#30340;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2307.05426</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;BOLD-fMRI&#35745;&#31639;&#20154;&#31867;&#36830;&#25509;&#32452;&#21457;&#23637;&#38431;&#21015;&#20013;&#30340;&#21628;&#21560;&#20307;&#31215;&#19982;&#26102;&#38388;&#65288;RTV&#65289;&#21644;&#21628;&#21560;&#21464;&#24322;&#65288;RV&#65289;
&lt;/p&gt;
&lt;p&gt;
Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort. (arXiv:2307.05426v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;BOLD-fMRI&#35745;&#31639;&#20154;&#31867;&#36830;&#25509;&#32452;&#21457;&#23637;&#38431;&#21015;&#20013;&#21628;&#21560;&#20307;&#31215;&#19982;&#26102;&#38388;&#65288;RTV&#65289;&#21644;&#21628;&#21560;&#21464;&#24322;&#65288;RV&#65289;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#21487;&#20197;&#20174;&#38745;&#24687;&#29366;&#24577;&#30340;BOLD&#20449;&#21495;&#20013;&#25429;&#25417;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#37325;&#24314;&#30495;&#23454;&#30340;&#21628;&#21560;&#26102;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;fMRI&#30740;&#31350;&#30340;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;fMRI&#30740;&#31350;&#20013;&#65292;&#21628;&#21560;&#20449;&#21495;&#19981;&#21487;&#29992;&#25110;&#36136;&#37327;&#19981;&#21487;&#25509;&#21463;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#30452;&#25509;&#20174;BOLD&#20449;&#21495;&#20013;&#21435;&#38500;&#20302;&#39057;&#21628;&#21560;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#32500;CNN&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#24314;&#20004;&#20010;&#21628;&#21560;&#27979;&#37327;&#25351;&#26631;&#65292;RV&#21644;RVT&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#21487;&#20197;&#20174;&#38745;&#24687;&#29366;&#24577;&#30340;BOLD&#20449;&#21495;&#20013;&#25429;&#25417;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#37325;&#24314;&#30495;&#23454;&#30340;RV&#21644;RVT&#26102;&#24207;&#12290;&#39044;&#35745;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#23558;&#38477;&#20302;fMRI&#30740;&#31350;&#30340;&#25104;&#26412;&#65292;&#20943;&#23569;&#22797;&#26434;&#24615;&#65292;&#24182;&#20943;&#36731;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#38656;&#35201;&#20329;&#25140;&#21628;&#21560;&#36125;&#27931;&#26031;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KIBS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20165;&#19968;&#24352;&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;&#20840;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#24314;&#31569;&#29289;&#30340;&#23627;&#39030;&#37096;&#20998;&#36827;&#34892;&#19977;&#32500;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#22478;&#24066;&#30340;3D&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2307.05409</link><description>&lt;p&gt;
&#21333;&#24352;&#21355;&#26143;&#22270;&#20687;&#20013;&#23627;&#39030;&#37096;&#20998;&#30340;&#19977;&#32500;&#26816;&#27979;&#21450;&#20854;&#22312;LOD2&#24314;&#31569;&#37325;&#24314;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
3D detection of roof sections from a single satellite image and application to LOD2-building reconstruction. (arXiv:2307.05409v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KIBS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20165;&#19968;&#24352;&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;&#20840;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#24314;&#31569;&#29289;&#30340;&#23627;&#39030;&#37096;&#20998;&#36827;&#34892;&#19977;&#32500;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#22478;&#24066;&#30340;3D&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21355;&#26143;&#20809;&#26629;&#22270;&#20687;&#20013;&#37325;&#24314;&#22478;&#24066;&#21306;&#22495;&#30340;3D&#27169;&#22411;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#23398;&#26415;&#30740;&#31350;&#21644;&#24037;&#19994;&#30740;&#31350;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#30446;&#21069;&#33021;&#22815;&#36798;&#21040;LOD2&#32423;&#21035;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#20960;&#20309;&#23398;&#30340;&#36807;&#31243;&#65292;&#24182;&#19988;&#38656;&#35201;&#31435;&#20307;&#22270;&#20687;&#21644;/&#25110;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;3D&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;KIBS&#65288;&#36890;&#36807;&#20998;&#21106;&#36827;&#34892;&#20851;&#38190;&#28857;&#25512;&#29702;&#65289;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#29305;&#28857;&#65306;i&#65289;&#19968;&#31181;&#29992;&#20110;&#23627;&#39030;&#37096;&#20998;&#30340;3D&#26816;&#27979;&#30340;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;ii&#65289;&#20165;&#20351;&#29992;&#19968;&#24352;&#65288;&#38750;&#27491;&#20132;&#30340;&#65289;&#21355;&#26143;&#20809;&#26629;&#22270;&#20687;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#12290;&#36825;&#26159;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#23454;&#29616;&#30340;&#65306;i&#65289;&#36890;&#36807;Mask R-CNN&#27169;&#22411;&#23545;&#24314;&#31569;&#29289;&#23627;&#39030;&#37096;&#20998;&#36827;&#34892;2D&#20998;&#21106;&#65292;&#24182;&#22312;RGB&#21355;&#26143;&#20809;&#26629;&#22270;&#20687;&#20013;&#28151;&#21512;&#36825;&#20123;&#20998;&#21106;&#20687;&#32032;&#65292;ii&#65289;&#36890;&#36807;&#21478;&#19968;&#20010;&#30456;&#21516;&#30340;Mask R-CNN&#27169;&#22411;&#36890;&#36807;&#20840;&#26223;&#20998;&#21106;&#25512;&#26029;&#20986;&#23627;&#39030;&#37096;&#20998;&#35282;&#28857;&#30340;&#39640;&#24230;&#21040;&#22320;&#38754;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#23436;&#25972;&#30340;3D&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing urban areas in 3D out of satellite raster images has been a long-standing and challenging goal of both academical and industrial research. The rare methods today achieving this objective at a Level Of Details $2$ rely on procedural approaches based on geometry, and need stereo images and/or LIDAR data as input. We here propose a method for urban 3D reconstruction named KIBS(\textit{Keypoints Inference By Segmentation}), which comprises two novel features: i) a full deep learning approach for the 3D detection of the roof sections, and ii) only one single (non-orthogonal) satellite raster image as model input. This is achieved in two steps: i) by a Mask R-CNN model performing a 2D segmentation of the buildings' roof sections, and after blending these latter segmented pixels within the RGB satellite raster image, ii) by another identical Mask R-CNN model inferring the heights-to-ground of the roof sections' corners via panoptic segmentation, unto full 3D reconstruction of t
&lt;/p&gt;</description></item><item><title>&#22312;&#25991;&#26723;&#22788;&#29702;&#24179;&#21488;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#21333;&#29420;&#21576;&#29616;&#26102;&#35757;&#32451;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#21442;&#32771;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05399</link><description>&lt;p&gt;
&#22312;&#25991;&#26723;&#22788;&#29702;&#24179;&#21488;&#20013;&#29992;&#20110;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#30340;&#39046;&#22495;&#26080;&#20851;&#31070;&#32463;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform. (arXiv:2307.05399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05399
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#22788;&#29702;&#24179;&#21488;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#21333;&#29420;&#21576;&#29616;&#26102;&#35757;&#32451;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#21442;&#32771;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#29983;&#20135;&#37096;&#32626;&#35201;&#27714;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#23545;&#22810;&#20010;&#20219;&#21153;&#39640;&#25928;&#21487;&#29992;&#12290;&#29305;&#21035;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#20197;&#27969;&#24335;&#26041;&#24335;&#21040;&#36798;&#65292;&#24182;&#19988;&#27599;&#20010;&#31867;&#21035;&#21333;&#29420;&#21576;&#29616;&#12290;&#26368;&#36817;&#30340;&#38543;&#26426;&#26799;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#25110;&#32773;&#23384;&#22312;&#35832;&#22914;&#20869;&#23384;&#32531;&#20914;&#21306;&#30340;&#38480;&#21046;&#65292;&#19981;&#33021;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#21487;&#24494;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#21333;&#29420;&#21576;&#29616;&#26102;&#35757;&#32451;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#22312;&#32447;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#27809;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;SOTA&#32467;&#26524;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#21442;&#32771;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#30340;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;OCR&#21644;&#26234;&#33021;&#23383;&#31526;&#35782;&#21035;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.05396</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Handwritten Text Recognition Using Convolutional Neural Network. (arXiv:2307.05396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05396
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#30340;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;OCR&#21644;&#26234;&#33021;&#23383;&#31526;&#35782;&#21035;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OCR&#65288;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#20165;&#20165;&#25195;&#25551;&#25991;&#26723;&#65292;&#20197;&#30005;&#23376;&#36895;&#24230;&#20840;&#38754;&#35782;&#21035;&#25163;&#20889;&#21644;&#25171;&#21360;&#23383;&#31526;&#30340;&#23383;&#27597;&#25968;&#23383;&#35782;&#21035;&#12290;&#26368;&#36817;&#65292;&#23545;&#35270;&#35273;&#25968;&#25454;&#30340;&#29702;&#35299;&#34987;&#31216;&#20026;&#26234;&#33021;&#23383;&#31526;&#35782;&#21035;&#65288;ICR&#65289;&#12290;&#26234;&#33021;&#23383;&#31526;&#35782;&#21035;&#65288;ICR&#65289;&#26159;&#21487;&#20197;&#23558;&#25163;&#20889;&#25110;&#25171;&#21360;&#23383;&#31526;&#30340;&#25195;&#25551;&#36716;&#25442;&#20026;ASCII&#25991;&#26412;&#30340;OCR&#27169;&#22359;&#12290;ASCII&#25968;&#25454;&#26159;&#30005;&#23376;&#36890;&#20449;&#20013;&#25968;&#25454;&#32534;&#30721;&#30340;&#26631;&#20934;&#26684;&#24335;&#12290;ASCII&#20026;&#23383;&#27597;&#12289;&#25968;&#23383;&#12289;&#31526;&#21495;&#12289;&#31354;&#26684;&#21644;&#20854;&#20182;&#23383;&#31526;&#20998;&#37197;&#26631;&#20934;&#25968;&#20540;&#12290;&#26356;&#25216;&#26415;&#24615;&#22320;&#35828;&#65292;OCR&#26159;&#20351;&#29992;&#30005;&#23376;&#35774;&#22791;&#23558;&#20108;&#32500;&#25991;&#26412;&#20449;&#24687;&#36716;&#25442;&#20026;&#26426;&#22120;&#32534;&#30721;&#25991;&#26412;&#30340;&#36807;&#31243;&#12290;&#20219;&#20309;&#21253;&#21547;&#25991;&#23383;&#30340;&#26426;&#22120;&#20070;&#20889;&#25110;&#25163;&#20889;&#29289;&#20307;&#37117;&#21487;&#20197;&#36890;&#36807;&#25195;&#25551;&#20202;&#25110;&#21482;&#26159;&#25991;&#23383;&#30340;&#22270;&#29255;&#26469;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
OCR (Optical Character Recognition) is a technology that offers comprehensive alphanumeric recognition of handwritten and printed characters at electronic speed by merely scanning the document. Recently, the understanding of visual data has been termed Intelligent Character Recognition (ICR). Intelligent Character Recognition (ICR) is the OCR module that can convert scans of handwritten or printed characters into ASCII text. ASCII data is the standard format for data encoding in electronic communication. ASCII assigns standard numeric values to letters, numeral, symbols, white-spaces and other characters. In more technical terms, OCR is the process of using an electronic device to transform 2-Dimensional textual information into machine-encoded text. Anything that contains text both machine written or handwritten can be scanned either through a scanner or just simply a picture of the text is enough for the recognition system to distinguish the text. The goal of this papers is to show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STATENet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#22312;&#26102;&#31354;&#21644;&#27169;&#22411;&#23618;&#38754;&#19978;&#35299;&#20915;&#20102;&#26032;&#29983;&#20799;&#30315;&#30187;&#26816;&#27979;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#33021;&#26174;&#33879;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05382</link><description>&lt;p&gt;
&#20445;&#25252;&#26410;&#26469;: &#22522;&#20110;&#26102;&#31354;&#24314;&#27169;&#30340;&#26032;&#29983;&#20799;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling. (arXiv:2307.05382v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STATENet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#22312;&#26102;&#31354;&#21644;&#27169;&#22411;&#23618;&#38754;&#19978;&#35299;&#20915;&#20102;&#26032;&#29983;&#20799;&#30315;&#30187;&#26816;&#27979;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#33021;&#26174;&#33879;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#29983;&#20799;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65292;&#21450;&#26102;&#26816;&#27979;&#20855;&#26377;&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#26032;&#29983;&#20799;&#30340;&#30315;&#30187;&#21457;&#20316;&#26159;&#19968;&#39033;&#24120;&#35265;&#20294;&#33021;&#25327;&#25937;&#29983;&#21629;&#30340;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#30417;&#27979;&#38656;&#35201;&#20154;&#21147;&#22823;&#37327;&#25237;&#20837;&#65292;&#22240;&#27492;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26032;&#29983;&#20799;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#38024;&#23545;&#25104;&#20154;&#30315;&#30187;&#30417;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#36890;&#24120;&#20250;&#22240;&#20026;&#20197;&#19979;&#21407;&#22240;&#32780;&#22833;&#36133;&#65306;&#65288;i&#65289;&#20154;&#33041;&#20013;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#20301;&#32622;&#30340;&#21160;&#24577;&#21464;&#21270;&#65307;&#65288;ii&#65289;&#26032;&#29983;&#20799;&#33041;&#30005;&#22270;&#30340;&#19981;&#21516;&#30005;&#26497;&#37197;&#32622;&#20197;&#21450;&#65288;iii&#65289;&#19981;&#21516;&#21463;&#35797;&#23545;&#35937;&#20043;&#38388;&#30340;&#24040;&#22823;&#20998;&#24067;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STATENet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#22312;&#26102;&#31354;&#21644;&#27169;&#22411;&#23618;&#38754;&#19978;&#35299;&#20915;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23545;&#30495;&#23454;&#30340;&#22823;&#35268;&#27169;&#26032;&#29983;&#20799;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A timely detection of seizures for newborn infants with electroencephalogram (EEG) has been a common yet life-saving practice in the Neonatal Intensive Care Unit (NICU). However, it requires great human efforts for real-time monitoring, which calls for automated solutions to neonatal seizure detection. Moreover, the current automated methods focusing on adult epilepsy monitoring often fail due to (i) dynamic seizure onset location in human brains; (ii) different montages on neonates and (iii) huge distribution shift among different subjects. In this paper, we propose a deep learning framework, namely STATENet, to address the exclusive challenges with exquisite designs at the temporal, spatial and model levels. The experiments over the real-world large-scale neonatal EEG dataset illustrate that our framework achieves significantly better seizure detection performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#24773;&#32490;&#20998;&#26512;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25552;&#21462;&#29305;&#24449;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;K&#26368;&#36817;&#37051;&#31639;&#27861;&#22312;DEAP Dataset&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#29366;&#24577;&#30340;&#20998;&#31867;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2307.05375</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Emotion Analysis on EEG Signal Using Machine Learning and Neural Network. (arXiv:2307.05375v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#24773;&#32490;&#20998;&#26512;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25552;&#21462;&#29305;&#24449;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;K&#26368;&#36817;&#37051;&#31639;&#27861;&#22312;DEAP Dataset&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#29366;&#24577;&#30340;&#20998;&#31867;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#23545;&#19968;&#20010;&#20154;&#30340;&#24605;&#32771;&#21644;&#19982;&#20182;&#20154;&#20114;&#21160;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#12290;&#23427;&#36830;&#25509;&#30528;&#19968;&#20010;&#20154;&#30340;&#24863;&#21463;&#21644;&#20854;&#34892;&#20026;&#65292;&#21487;&#20197;&#35828;&#23427;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24433;&#21709;&#30528;&#19968;&#20010;&#20154;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;&#30001;&#20110;&#24773;&#32490;&#30340;&#27169;&#24335;&#21644;&#21453;&#24212;&#22240;&#20154;&#32780;&#24322;&#65292;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#38656;&#35201;&#37319;&#29992;&#36866;&#29992;&#20110;&#24191;&#27867;&#20154;&#32676;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#21462;&#29305;&#24449;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24773;&#32490;&#35782;&#21035;&#20351;&#29992;&#33041;&#30005;&#27874;&#25110;EEG&#20449;&#21495;&#38656;&#35201;&#23454;&#26045;&#39640;&#25928;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#12290;&#22810;&#31181;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#30340;&#26041;&#27861;&#19968;&#30452;&#22312;&#36827;&#34892;&#20013;&#65292;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#32773;&#20204;&#22312;&#21033;&#29992;&#33041;&#20449;&#21495;&#33258;&#21160;&#29702;&#35299;&#24773;&#32490;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#23545;&#20174;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;DEAP Dataset&#25910;&#38598;&#21040;&#30340;EEG&#20449;&#21495;&#36827;&#34892;&#20102;&#20960;&#31181;&#24773;&#24863;&#29366;&#24577;&#30340;&#20998;&#31867;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion has a significant influence on how one thinks and interacts with others. It serves as a link between how a person feels and the actions one takes, or it could be said that it influences one's life decisions on occasion. Since the patterns of emotions and their reflections vary from person to person, their inquiry must be based on approaches that are effective over a wide range of population regions. To extract features and enhance accuracy, emotion recognition using brain waves or EEG signals requires the implementation of efficient signal processing techniques. Various approaches to human-machine interaction technologies have been ongoing for a long time, and in recent years, researchers have had great success in automatically understanding emotion using brain signals. In our research, several emotional states were classified and tested on EEG signals collected from a well-known publicly available dataset, the DEAP Dataset, using SVM (Support Vector Machine), KNN (K-Nearest Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;SSNet&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;EEG&#12289;EOG&#21644;EMG&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#20840;&#36830;&#25509;&#23618;&#36827;&#34892;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#21644;Kappa&#31995;&#25968;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05373</link><description>&lt;p&gt;
&#36890;&#36807;SSNet&#20174;EEG&#12289;EOG&#21644;EMG&#20449;&#21495;&#20013;&#20998;&#31867;&#30561;&#30496;&#38454;&#27573; (arXiv:2307.05373v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
Classification of sleep stages from EEG, EOG and EMG signals by SSNet. (arXiv:2307.05373v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;SSNet&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;EEG&#12289;EOG&#21644;EMG&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#20840;&#36830;&#25509;&#23618;&#36827;&#34892;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#21644;Kappa&#31995;&#25968;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#22312;&#35786;&#26029;&#30561;&#30496;&#30456;&#20851;&#30142;&#30149;&#65292;&#21253;&#25324;&#30561;&#30496;&#21628;&#21560;&#38556;&#30861;(SDB)&#30142;&#30149;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21629;&#21517;&#20026;SSNet&#65292;&#23427;&#21253;&#25324;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#12290;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20174;&#30005;&#30524;&#22270;&#65288;EOG&#65289;&#12289;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#20449;&#21495;&#30340;&#32452;&#21512;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#22240;&#20026;&#27599;&#20010;&#20449;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#12290;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20135;&#29983;&#30340;&#29305;&#24449;&#34987;&#36830;&#25509;&#36215;&#26469;&#20256;&#36882;&#21040;&#20840;&#36830;&#25509;&#23618;&#29992;&#20110;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;Sleep-EDF Expanded dataset&#21644;ISRUC-Sleep dataset&#36827;&#34892;&#35780;&#20272;&#12290;&#30561;&#30496;-EDF Expanded&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;Kappa&#31995;&#25968;&#20998;&#21035;&#20026;96.36%&#21644;93.40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of sleep stages plays an essential role in diagnosing sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In this study, we propose an end-to-end deep learning architecture, named SSNet, which comprises of two deep learning networks based on Convolutional Neuron Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks extract features from the combination of Electrooculogram (EOG), Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal has distinct features that help in the classification of sleep stages. The features produced by the two-deep learning networks are concatenated to pass to the fully connected layer for the classification. The performance of our proposed model is evaluated by using two public datasets Sleep-EDF Expanded dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36% and 93.40% respectively, for classifying three classes of sleep stages using Sleep-EDF Expanded da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#20302;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21644;&#21453;&#21160;&#21147;&#23398;&#32908;&#32905;&#27169;&#22411;&#25972;&#21512;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#35299;&#30721;&#21644;&#22806;&#25512;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.05361</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#30340;&#20302;&#26679;&#26412;&#23398;&#20064;&#23545;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#36827;&#34892;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics. (arXiv:2307.05361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#20302;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21644;&#21453;&#21160;&#21147;&#23398;&#32908;&#32905;&#27169;&#22411;&#25972;&#21512;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#35299;&#30721;&#21644;&#22806;&#25512;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#65292;&#26159;&#23454;&#26102;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#20013;&#31070;&#32463;&#32908;&#32905;&#21050;&#28608;&#12289;&#32908;&#32905;&#21160;&#21147;&#23398;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#20197;&#20840;&#33258;&#21160;&#21644;&#21487;&#37325;&#22797;&#30340;&#26041;&#24335;&#25913;&#36827;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#30340;&#23567;&#26679;&#26412;&#24615;&#36136;&#21644;&#29289;&#29702;&#21487;&#35299;&#37322;&#24615;&#38480;&#21046;&#20102;DNN&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#20302;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21644;&#21453;&#21160;&#21147;&#23398;&#32908;&#32905;&#27169;&#22411;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#35299;&#30721;&#21644;&#22806;&#25512;&#20272;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20197;&#38480;&#21046;&#39640;&#23618;&#32467;&#26500;&#21270;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle force and joint kinematics estimation from surface electromyography (sEMG) are essential for real-time biomechanical analysis of the dynamic interplay among neural muscle stimulation, muscle dynamics, and kinetics. Recent advances in deep neural networks (DNNs) have shown the potential to improve biomechanical analysis in a fully automated and reproducible manner. However, the small sample nature and physical interpretability of biomechanical analysis limit the applications of DNNs. This paper presents a novel physics-informed low-shot learning method for sEMG-based estimation of muscle force and joint kinematics. This method seamlessly integrates Lagrange's equation of motion and inverse dynamic muscle model into the generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from the small sample data. Specifically, Lagrange's equation of motion is introduced into the generative model to restrain the structured decoding of the hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05360</link><description>&lt;p&gt;
&#25581;&#24320;&#24040;&#20154;&#30340;&#30495;&#38754;&#30446;&#65306;&#23545;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#28145;&#21051;&#22320;&#37325;&#22609;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#39046;&#22495;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#26377;&#30528;&#29420;&#29305;&#20043;&#22788;&#65292;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#36718;&#23545;&#35805;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23637;&#31034;&#20986;&#23545;&#32534;&#30721;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#23545;ChatGPT&#30340;&#32534;&#30721;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;Python&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#21450;&#38598;&#20013;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#19978;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#20027;&#39064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35780;&#20272;ChatGPT&#35299;&#20915;&#25152;&#25552;&#20132;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#35780;&#20272;&#20854;&#20195;&#30721;&#36136;&#37327;&#20197;&#21450;&#20195;&#30721;&#24341;&#21457;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;&#24403;ChatGPT&#30340;&#20195;&#30721;&#25104;&#21151;&#25191;&#34892;&#20294;&#26410;&#33021;&#35299;&#20915;&#25163;&#22836;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20250;&#30740;&#31350;&#36890;&#36807;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#20102;&#35299;ChatGPT&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.05358</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#35843;&#33410;&#22120;&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#25955;&#23458;&#25143;&#31471;&#19978;&#26631;&#31614;&#31232;&#32570;&#65292;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#20986;&#29616;&#20197;&#20174;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FSSL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#25968;&#25454;&#29420;&#31435;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FSSL&#30340;&#26356;&#23454;&#38469;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#20165;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#21516;&#65292;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20063;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;FSSL&#26694;&#26550;&#65292;FedDure&#12290;FedDure&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#65288;C-reg&#65289;&#21644;&#32454;&#35843;&#33410;&#22120;&#65288;F-reg&#65289;&#35299;&#38500;&#20102;&#20197;&#21069;&#30340;&#20551;&#35774;&#65306;C-reg&#36890;&#36807;&#36319;&#36394;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#25928;&#26524;&#26469;&#35268;&#33539;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#65307;F-reg&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#20869;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#23436;&#32654;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;OFDM&#31995;&#32479;&#20013;&#30340;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#26368;&#22823;&#21162;&#21147;&#35745;&#31639;&#20219;&#21153;&#21644;&#35823;&#24046;&#32422;&#26463;&#35745;&#31639;&#20219;&#21153;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#35745;&#31639;&#22343;&#26041;&#35823;&#24046;&#21644;&#35745;&#31639;&#22833;&#25928;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05357</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#23436;&#32654;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;OFDM&#31995;&#32479;&#20013;&#30340;&#31354;&#20013;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air Computation in OFDM Systems with Imperfect Channel State Information. (arXiv:2307.05357v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#23436;&#32654;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;OFDM&#31995;&#32479;&#20013;&#30340;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#26368;&#22823;&#21162;&#21147;&#35745;&#31639;&#20219;&#21153;&#21644;&#35823;&#24046;&#32422;&#26463;&#35745;&#31639;&#20219;&#21153;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#35745;&#31639;&#22343;&#26041;&#35823;&#24046;&#21644;&#35745;&#31639;&#22833;&#25928;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#23436;&#32654;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#31995;&#32479;&#20013;&#30340;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#12290;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#22810;&#20010;&#21333;&#22825;&#32447;&#26080;&#32447;&#35774;&#22791;&#65288;WDs&#65289;&#21516;&#26102;&#21521;&#22810;&#22825;&#32447;&#25509;&#20837;&#28857;&#65288;AP&#65289;&#21457;&#36865;&#26410;&#32534;&#30721;&#20449;&#21495;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#23376;&#36733;&#27874;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#21151;&#33021;&#35745;&#31639;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#26223;&#65292;&#21363;&#26368;&#22823;&#21162;&#21147;&#35745;&#31639;&#20219;&#21153;&#21644;&#35823;&#24046;&#32422;&#26463;&#35745;&#31639;&#20219;&#21153;&#65292;&#30446;&#26631;&#20998;&#21035;&#26159;&#26368;&#23567;&#21270;&#22810;&#20010;&#23376;&#36733;&#27874;&#19978;&#30340;&#24179;&#22343;&#35745;&#31639;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#35745;&#31639;&#22833;&#25928;&#27010;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#20102;WDs&#30340;&#21457;&#36865;&#31995;&#25968;&#21644;AP&#30340;&#25509;&#25910;&#27874;&#26463;&#24418;&#25104;&#21521;&#37327;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#21333;&#20010;WD&#30340;&#26368;&#22823;&#21457;&#36865;&#21151;&#29575;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;AP&#20165;&#20855;&#26377;&#19968;&#20010;&#25509;&#25910;&#22825;&#32447;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Lagr&#30340;&#21322;&#38381;&#24335;&#20840;&#23616;&#26368;&#20248;&#35299;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the over-the-air computation (AirComp) in an orthogonal frequency division multiplexing (OFDM) system with imperfect channel state information (CSI), in which multiple single-antenna wireless devices (WDs) simultaneously send uncoded signals to a multi-antenna access point (AP) for distributed functional computation over multiple subcarriers. In particular, we consider two scenarios with best-effort and error-constrained computation tasks, with the objectives of minimizing the average computation mean squared error (MSE) and the computation outage probability over the multiple subcarriers, respectively. Towards this end, we jointly optimize the transmit coefficients at the WDs and the receive beamforming vectors at the AP over subcarriers, subject to the maximum transmit power constraints at individual WDs. First, for the special case with a single receive antenna at the AP, we propose the semi-closed-form globally optimal solutions to the two problems using the Lagr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.05333</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#30140;&#30171;&#35780;&#20272;&#65306;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#29289;&#32852;&#32593;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#20020;&#24202;&#35843;&#26597;&#65289;&#19982;&#21487;&#25193;&#23637;&#30340;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#30140;&#30171;&#29366;&#24577;&#30340;&#36523;&#20307;&#12289;&#34892;&#20026;&#21644;&#24515;&#29702;&#31038;&#20132;&#25351;&#26631;&#30340;&#21457;&#29616;&#12290;&#23613;&#31649;&#20197;&#25216;&#26415;&#36827;&#27493;&#25913;&#21464;&#21307;&#30103;&#31995;&#32479;&#30340;&#28909;&#24773;&#21644;&#25215;&#35834;&#65292;&#20294;&#20020;&#24202;&#30140;&#30171;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#21463;&#21040;&#20102;&#38382;&#39064;&#26412;&#36523;&#30340;&#22810;&#26679;&#24615;&#21644;&#20010;&#24615;&#21270;&#20197;&#21450;&#20844;&#24179;&#24615;&#31561;&#20854;&#20182;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#65289;&#27169;&#22411;&#26174;&#31034;&#20986;&#20559;&#35265;&#65292;&#24182;&#27495;&#35270;&#29305;&#23450;&#20154;&#32676;&#65288;&#22914;&#22522;&#20110;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#65292;&#36825;&#24341;&#36215;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#20154;&#24037;&#26234;&#33021;&#36866;&#24212;&#24615;&#30340;&#24576;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#26088;&#22312;&#32771;&#34385;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20219;&#20309;&#25935;&#24863;&#23646;&#24615;&#65292;&#24182;&#20844;&#24179;&#39044;&#27979;&#24739;&#32773;&#30340;&#30140;&#30171;&#29366;&#24577;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05330</link><description>&lt;p&gt;
&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#20215;&#20540;&#12290; (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#26827;&#30424;&#19978;&#26827;&#23376;&#30340;&#20215;&#20540;&#65292;&#24182;&#30830;&#23450;&#26827;&#23376;&#22312;&#26827;&#30424;&#19978;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#38543;&#30528;&#22269;&#38469;&#35937;&#26827;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#30340;&#20215;&#20540;&#12290;&#20256;&#32479;&#26041;&#27861;&#23545;&#26827;&#23376;&#36171;&#20104;&#22266;&#23450;&#30340;&#20215;&#20540;$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26827;&#23376;&#21644;&#26827;&#30424;&#26041;&#38754;&#30340;&#36793;&#38469;&#20272;&#20540;&#26469;&#25913;&#36827;&#36825;&#31181;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#39532;&#21644;&#35937;&#30340;&#20301;&#32622;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#20853;&#30340;&#20215;&#20540;&#30340;&#23453;&#36149;&#35265;&#35299;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23612;&#22982;&#20304;&#32500;&#22855;&#26159;&#20513;&#23548;&#20853;&#30340;&#32467;&#26500;&#21644;&#20215;&#20540;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GuitarPro&#26684;&#24335;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#21033;&#29992;173&#39318;&#28176;&#36827;&#37329;&#23646;&#27468;&#26354;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#21019;&#20316;&#20986;&#28176;&#36827;&#37329;&#23646;&#20316;&#21697;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#39564;&#35777;&#20102;&#29983;&#25104;&#38899;&#20048;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#32456;&#65292;&#23558;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#21046;&#20316;&#25104;&#23436;&#25972;&#30340;&#28176;&#36827;&#37329;&#23646;&#27468;&#26354;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.05328</link><description>&lt;p&gt;
ProgGP: &#20174;GuitarPro&#21513;&#20182;&#25351;&#27861;&#29983;&#25104;&#21040;&#28176;&#36827;&#24335;&#37329;&#23646;&#21046;&#20316;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal Production. (arXiv:2307.05328v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GuitarPro&#26684;&#24335;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#21033;&#29992;173&#39318;&#28176;&#36827;&#37329;&#23646;&#27468;&#26354;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#21019;&#20316;&#20986;&#28176;&#36827;&#37329;&#23646;&#20316;&#21697;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#39564;&#35777;&#20102;&#29983;&#25104;&#38899;&#20048;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#32456;&#65292;&#23558;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#21046;&#20316;&#25104;&#23436;&#25972;&#30340;&#28176;&#36827;&#37329;&#23646;&#27468;&#26354;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GuitarPro&#26684;&#24335;&#30340;&#20998;&#35789;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#25903;&#25345;&#21513;&#20182;&#34920;&#36798;&#23646;&#24615;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#20215;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;ProgGP&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25193;&#23637;&#20102;&#36825;&#39033;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;173&#39318;&#28176;&#36827;&#37329;&#23646;&#27468;&#26354;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#21019;&#36896;&#20986;&#35813;&#27969;&#27966;&#30340;&#20316;&#21697;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#21513;&#20182;&#12289;&#36125;&#26031;&#21513;&#20182;&#12289;&#40723;&#12289;&#38050;&#29748;&#21644;&#31649;&#24358;&#20048;&#37096;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#38899;&#20048;&#23398;&#33539;&#24335;&#30340;&#25968;&#37327;&#20998;&#26512;&#21644;&#23454;&#36341;&#30740;&#31350;&#33539;&#24335;&#30340;&#36136;&#24615;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#26816;&#39564;&#29983;&#25104;&#38899;&#20048;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#29992;&#20316;&#24037;&#20855;&#65292;&#30001;&#20154;&#31867;&#37329;&#23646;&#38899;&#20048;&#21046;&#20316;&#20154;&#22522;&#20110;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#36827;&#34892;&#20840;&#38754;&#21046;&#20316;&#21644;&#28151;&#38899;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in the field of symbolic music generation has shown value in using a tokenization based on the GuitarPro format, a symbolic representation supporting guitar expressive attributes, as an input and output representation. We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a custom dataset of 173 progressive metal songs, for the purposes of creating compositions from that genre through a human-AI partnership. Our model is able to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We examine the validity of the generated music using a mixed methods approach by combining quantitative analyses following a computational musicology paradigm and qualitative analyses following a practice-based research paradigm. Finally, we demonstrate the value of the model by using it as a tool to create a progressive metal song, fully produced and mixed by a human metal producer based on AI-generated music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#25805;&#20316;&#25110;&#29983;&#25104;&#35821;&#20041;&#20998;&#21106;&#25513;&#33180;&#20013;&#29289;&#20307;&#31867;&#21035;&#30340;&#24418;&#29366;&#65292;&#29305;&#21035;&#26159;&#20154;&#33080;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#21106;&#25513;&#33180;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#20351;&#27599;&#20010;&#31867;&#21035;&#23884;&#20837;&#21487;&#20197;&#29420;&#31435;&#32534;&#36753;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22270;&#20687;&#24067;&#23616;&#30340;&#33258;&#21160;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.05317</link><description>&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#38754;&#37096;&#22270;&#20687;&#21512;&#25104;&#30340;&#35821;&#20041;&#37096;&#20214;
&lt;/p&gt;
&lt;p&gt;
Automatic Generation of Semantic Parts for Face Image Synthesis. (arXiv:2307.05317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#25805;&#20316;&#25110;&#29983;&#25104;&#35821;&#20041;&#20998;&#21106;&#25513;&#33180;&#20013;&#29289;&#20307;&#31867;&#21035;&#30340;&#24418;&#29366;&#65292;&#29305;&#21035;&#26159;&#20154;&#33080;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#21106;&#25513;&#33180;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#20351;&#27599;&#20010;&#31867;&#21035;&#23884;&#20837;&#21487;&#20197;&#29420;&#31435;&#32534;&#36753;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22270;&#20687;&#24067;&#23616;&#30340;&#33258;&#21160;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#65288;SIS&#65289;&#25351;&#30340;&#26159;&#22312;&#32473;&#23450;&#23450;&#20041;&#29289;&#20307;&#31867;&#21035;&#31354;&#38388;&#24067;&#23616;&#30340;&#35821;&#20041;&#20998;&#21106;&#25513;&#33180;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#38500;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#22806;&#65292;&#36824;&#33268;&#21147;&#20110;&#35299;&#20915;&#22914;&#20309;&#22686;&#21152;&#26679;&#24335;&#65288;&#20363;&#22914;&#32441;&#29702;&#65289;&#19978;&#30340;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#24573;&#30053;&#20102;&#21478;&#19968;&#20010;&#29305;&#24449;&#65292;&#21363;&#36890;&#36807;&#25513;&#33180;&#25552;&#20379;&#30340;&#24067;&#23616;&#21487;&#20197;&#36827;&#34892;&#25805;&#20316;&#30340;&#21487;&#33021;&#24615;&#12290;&#30446;&#21069;&#65292;&#21807;&#19968;&#30340;&#23454;&#29616;&#26041;&#24335;&#26159;&#36890;&#36807;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#25163;&#21160;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#25805;&#20316;&#25110;&#29983;&#25104;&#35821;&#20041;&#20998;&#21106;&#25513;&#33180;&#20013;&#29289;&#20307;&#31867;&#21035;&#30340;&#24418;&#29366;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#33080;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20801;&#35768;&#23558;&#20998;&#21106;&#25513;&#33180;&#25353;&#31867;&#21035;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#20854;&#20013;&#27599;&#20010;&#31867;&#21035;&#23884;&#20837;&#21487;&#20197;&#29420;&#31435;&#32534;&#36753;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#21452;&#21521;LSTM&#22359;&#21644;&#19968;&#20010;&#21367;&#31215;&#35299;&#30721;&#22120;&#36755;&#20986;&#19968;&#20010;&#26032;&#30340;&#12289;&#23616;&#37096;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic image synthesis (SIS) refers to the problem of generating realistic imagery given a semantic segmentation mask that defines the spatial layout of object classes. Most of the approaches in the literature, other than the quality of the generated images, put effort in finding solutions to increase the generation diversity in terms of style i.e. texture. However, they all neglect a different feature, which is the possibility of manipulating the layout provided by the mask. Currently, the only way to do so is manually by means of graphical users interfaces. In this paper, we describe a network architecture to address the problem of automatically manipulating or generating the shape of object classes in semantic segmentation masks, with specific focus on human faces. Our proposed model allows embedding the mask class-wise into a latent space where each class embedding can be independently edited. Then, a bi-directional LSTM block and a convolutional decoder output a new, locally man
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;VQA&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05314</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering. (arXiv:2307.05314v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;VQA&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;(VQA)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#26469;&#22238;&#31572;&#32473;&#23450;&#21307;&#23398;&#22270;&#20687;&#30340;&#20020;&#24202;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;VQA&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#65292;&#39044;&#35757;&#32451;&#24494;&#35843;&#33539;&#24335;&#24050;&#25104;&#20026;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#24120;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#37319;&#29992;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20197;&#21450;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20316;&#20026;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23398;&#20064;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36801;&#31227;&#21040;&#19979;&#28216;&#21307;&#23398;VQA&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;2.2&#65285;&#65292;14.7&#65285;&#21644;1.7&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical visual question answering (VQA) is a challenging task that requires answering clinical questions of a given medical image, by taking consider of both visual and language information. However, due to the small scale of training data for medical VQA, pre-training fine-tuning paradigms have been a commonly used solution to improve model generalization performance. In this paper, we present a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimodal and multimodal contrastive losses, along with masked language modeling and image text matching as pretraining objectives. The pre-trained model is then transferred to downstream medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA) performance on three publicly available medical VQA datasets with significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we conduct a compr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>U-CREAT&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20107;&#20214;&#25552;&#21462;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#26816;&#32034;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.05260</link><description>&lt;p&gt;
U-CREAT: &#26080;&#30417;&#30563;&#20107;&#20214;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05260
&lt;/p&gt;
&lt;p&gt;
U-CREAT&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20107;&#20214;&#25552;&#21462;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#26816;&#32034;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#30340;&#20219;&#21153;&#26159;&#33258;&#21160;&#24341;&#29992;&#19982;&#32473;&#23450;&#26597;&#35810;&#26696;&#20363;&#30456;&#20851;&#65288;&#22522;&#20110;&#20107;&#23454;&#21644;&#20808;&#20363;&#65289;&#30340;&#20808;&#21069;&#27861;&#24459;&#26696;&#20363;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#22522;&#20934;&#65288;&#20197;&#33521;&#25991;&#20026;&#20027;&#65289;&#29992;&#20110;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#65306;IL-PCR&#65288;&#21360;&#24230;&#27861;&#24459;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#65289;&#35821;&#26009;&#24211;&#12290;&#32771;&#34385;&#21040;&#26696;&#20363;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#27861;&#24459;&#25991;&#26723;&#30340;&#38271;&#24230;&#65292;BM25&#20173;&#28982;&#26159;&#25490;&#21517;&#24341;&#29992;&#20808;&#21069;&#25991;&#26723;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#20214;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#30340;&#31649;&#36947;&#31995;&#32479;U-CREAT&#65288;&#26080;&#30417;&#30563;&#20107;&#20214;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#19982;BM25&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#26816;&#32034;&#36895;&#24230;&#22823;&#22823;&#21152;&#24555;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#36866;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#27861;&#24459;&#20307;&#31995;&#65288;&#21360;&#24230;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (India
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21307;&#38498;&#32508;&#21512;&#35268;&#21010;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#24378;&#35843;&#20102;&#32508;&#21512;&#35268;&#21010;&#22810;&#20010;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.05258</link><description>&lt;p&gt;
&#21307;&#38498;&#20013;&#30340;&#32508;&#21512;&#35268;&#21010;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Integrated Planning in Hospitals: A Review. (arXiv:2307.05258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21307;&#38498;&#32508;&#21512;&#35268;&#21010;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#24378;&#35843;&#20102;&#32508;&#21512;&#35268;&#21010;&#22810;&#20010;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1950&#24180;&#20197;&#26469;&#65292;&#39640;&#25928;&#35268;&#21010;&#21307;&#38498;&#31232;&#32570;&#36164;&#28304;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20026;&#27492;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#26041;&#27861;&#12290;&#23613;&#31649;&#39640;&#25928;&#35268;&#21010;&#21333;&#19968;&#36164;&#28304;&#22914;&#25163;&#26415;&#23460;&#12289;&#24202;&#20301;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#21307;&#25252;&#20154;&#21592;&#24050;&#32463;&#33021;&#22815;&#24102;&#26469;&#24040;&#22823;&#30340;&#25928;&#30410;&#65292;&#20294;&#32508;&#21512;&#35268;&#21010;&#22810;&#20010;&#36164;&#28304;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22823;&#30340;&#28508;&#21147;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#32508;&#21512;&#35268;&#21010;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;&#21307;&#38498;&#19981;&#21516;&#36164;&#28304;&#32508;&#21512;&#35268;&#21010;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#25991;&#29486;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#38754;&#36827;&#34892;&#20998;&#26512;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#12290;&#20960;&#20010;&#20132;&#21449;&#27604;&#36739;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27934;&#23519;&#65292;&#20363;&#22914;&#24314;&#27169;&#21644;&#27714;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient planning of scarce resources in hospitals is a challenging task for which a large variety of Operations Research and Management Science approaches have been developed since the 1950s. While efficient planning of single resources such as operating rooms, beds, or specific types of staff can already lead to enormous efficiency gains, integrated planning of several resources has been shown to hold even greater potential, and a large number of integrated planning approaches have been presented in the literature over the past decades.  This paper provides the first literature review that focuses specifically on the Operations Research and Management Science literature related to integrated planning of different resources in hospitals. We collect the relevant literature and analyze it regarding different aspects such as uncertainty modeling and the use of real-life data. Several cross comparisons reveal interesting insights concerning, e.g., relations between the modeling and solut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#21517;&#20026;&#20271;&#20811;&#21033;DeepDrive&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05256</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#23545;&#25239;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards exploring adversarial learning for anomaly detection in complex driving scenes. (arXiv:2307.05256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#21517;&#20026;&#20271;&#20811;&#21033;DeepDrive&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31561;&#33258;&#20027;&#31995;&#32479;&#25191;&#34892;&#21508;&#31181;&#23433;&#20840;&#20851;&#38190;&#21151;&#33021;&#12290;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#20294;&#26159;&#36825;&#20123;&#24863;&#30693;&#32452;&#20214;&#26080;&#27861;&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#65292;&#22240;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32452;&#20214;&#30340;&#20934;&#30830;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#19981;&#23646;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#20316;&#20026;&#22312;&#24320;&#21457;&#21644;&#36816;&#34892;&#26102;&#23433;&#20840;&#24230;&#37327;&#25351;&#26631;&#20351;&#29992;&#12290;&#23545;&#25239;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#24322;&#24120;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#24182;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21517;&#20026;&#20271;&#20811;&#21033;DeepDrive&#30340;&#39640;&#24230;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the many Autonomous Systems (ASs), such as autonomous driving cars, performs various safety-critical functions. Many of these autonomous systems take advantage of Artificial Intelligence (AI) techniques to perceive their environment. But these perceiving components could not be formally verified, since, the accuracy of such AI-based components has a high dependency on the quality of training data. So Machine learning (ML) based anomaly detection, a technique to identify data that does not belong to the training data could be used as a safety measuring indicator during the development and operational time of such AI-based components. Adversarial learning, a sub-field of machine learning has proven its ability to detect anomalies in images and videos with impressive results on simple data sets. Therefore, in this work, we investigate and provide insight into the performance of such techniques on a highly complex driving scenes dataset called Berkeley DeepDrive.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05213</link><description>&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#37117;&#21253;&#21547;&#38656;&#35201;&#22312;&#35299;&#20915;&#20043;&#21069;&#36827;&#34892;&#39044;&#27979;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#20026;&#20102;&#35757;&#32451;&#28041;&#21450;&#30340;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#12290;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;&#24335;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#20219;&#21153;&#25439;&#22833;&#26469;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;DFL&#26041;&#27861;&#21463;&#21040;&#23427;&#20204;&#23545;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#38382;&#39064;&#26159;&#32447;&#24615;&#30340;&#65289;&#20197;&#21450;&#21482;&#33021;&#39044;&#27979;&#20986;&#29616;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#30340;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30456;&#21453;&#22320;&#39044;&#27979;&#21442;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#65288;SFGE&#65289;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#25193;&#22823;DFL&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiment
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05194</link><description>&lt;p&gt;
&#36890;&#36807;$\beta$-&#20998;&#35299;&#19968;&#21518;&#39564;&#37319;&#26679;&#23454;&#29616;&#24046;&#20998;&#35745;&#31639;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#31169;&#23494;&#24615;&#30830;&#20445;&#20102;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#20219;&#20309;&#20010;&#20307;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21457;&#24067;&#12290;&#23454;&#29616;&#36825;&#31181;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#22312;&#21442;&#25968;&#20272;&#35745;&#25110;&#20272;&#35745;&#36807;&#31243;&#20013;&#30452;&#25509;&#27880;&#20837;&#22122;&#38899;&#12290;&#32780;&#37319;&#26679;&#26469;&#33258;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#26159;&#25351;&#25968;&#26426;&#21046;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#19988;&#39640;&#25928;&#30340;&#31169;&#23494;&#20272;&#35745;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#36739;&#24378;&#30340;&#36793;&#30028;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#22522;&#26412;&#27169;&#22411;&#65288;&#22914;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#22120;&#65289;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#20174;&#24191;&#20041;&#21518;&#39564;&#20013;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#12290;&#36825;&#25552;&#20379;&#20102;&#31169;&#23494;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#23398;&#20064;&#21644;&#29702;&#35299;&#25163;&#26415;&#35270;&#39057;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.05182</link><description>&lt;p&gt;
Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery&#65288;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65289;
&lt;/p&gt;
&lt;p&gt;
Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#23398;&#20064;&#21644;&#29702;&#35299;&#25163;&#26415;&#35270;&#39057;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#22312;&#23398;&#20064;&#25163;&#26415;&#26102;&#36890;&#24120;&#20381;&#36182;&#20110;&#39640;&#32423;&#22806;&#31185;&#21307;&#29983;&#21644;&#19987;&#23478;&#22238;&#31572;&#20182;&#20204;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#20204;&#32463;&#24120;&#24537;&#20110;&#20020;&#24202;&#21644;&#23398;&#26415;&#24037;&#20316;&#65292;&#27809;&#26377;&#22810;&#23569;&#26102;&#38388;&#25552;&#20379;&#25351;&#23548;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#21482;&#33021;&#25552;&#20379;&#31616;&#21333;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#31572;&#26696;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#31995;&#32479;&#23545;&#20110;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#20174;&#24405;&#21046;&#30340;&#25163;&#26415;&#35270;&#39057;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22806;&#31185;&#22330;&#26223;&#30340;&#31471;&#21040;&#31471;Transformer&#19982;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#65288;CAT-ViL&#65289;&#30340;VQLA&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#36890;&#36807;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;CAT-ViL&#23884;&#20837;&#27169;&#22359;&#30340;&#35774;&#35745;&#26088;&#22312;&#34701;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#25991;&#26412;&#26469;&#28304;&#30340;&#24322;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#26512;&#21475;&#22836;&#21327;&#35758;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30524;&#21160;&#21644;&#40736;&#26631;&#36861;&#36394;&#27979;&#35797;&#20102;&#21475;&#22836;&#21453;&#39304;&#21644;&#21050;&#28608;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#29992;&#25143;&#21453;&#39304;&#19982;&#21050;&#28608;&#30340;&#29305;&#23450;&#21306;&#22495;&#30456;&#20851;&#32852;&#65292;&#29992;&#20110;&#19987;&#23478;&#23457;&#26680;&#29305;&#23450;&#32593;&#39029;&#20803;&#32032;&#30340;&#21453;&#39304;&#25110;&#21487;&#35270;&#21270;&#21453;&#39304;&#32473;&#20986;&#30340;&#32593;&#39029;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.05171</link><description>&lt;p&gt;
&#25552;&#21319;&#21487;&#29992;&#24615;&#27979;&#35797;&#30340;&#21475;&#22836;&#21453;&#39304;&#65306;&#21033;&#29992;&#30524;&#21160;&#21644;&#40736;&#26631;&#25968;&#25454;&#33258;&#21160;&#36830;&#25509;&#21475;&#22836;&#35760;&#24405;&#21644;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Enriching Verbal Feedback from Usability Testing: Automatic Linking of Thinking-Aloud Recordings and Stimulus using Eye Tracking and Mouse Data. (arXiv:2307.05171v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#26512;&#21475;&#22836;&#21327;&#35758;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30524;&#21160;&#21644;&#40736;&#26631;&#36861;&#36394;&#27979;&#35797;&#20102;&#21475;&#22836;&#21453;&#39304;&#21644;&#21050;&#28608;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#29992;&#25143;&#21453;&#39304;&#19982;&#21050;&#28608;&#30340;&#29305;&#23450;&#21306;&#22495;&#30456;&#20851;&#32852;&#65292;&#29992;&#20110;&#19987;&#23478;&#23457;&#26680;&#29305;&#23450;&#32593;&#39029;&#20803;&#32032;&#30340;&#21453;&#39304;&#25110;&#21487;&#35270;&#21270;&#21453;&#39304;&#32473;&#20986;&#30340;&#32593;&#39029;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#22836;&#24605;&#32771;&#26041;&#27861;&#26159;&#19968;&#31181;&#37325;&#35201;&#19988;&#24120;&#29992;&#30340;&#21487;&#29992;&#24615;&#20248;&#21270;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#21475;&#22836;&#25968;&#25454;&#21487;&#33021;&#32791;&#36153;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#26512;&#21475;&#22836;&#21327;&#35758;&#24182;&#27979;&#35797;&#20102;&#21033;&#29992;&#30524;&#21160;&#21644;&#40736;&#26631;&#36861;&#36394;&#38142;&#25509;&#21475;&#22836;&#21453;&#39304;&#21644;&#21050;&#28608;&#20043;&#38388;&#30340;&#26041;&#27861;&#12290;&#33719;&#24471;&#30340;&#25968;&#25454; - &#29992;&#25143;&#21453;&#39304;&#19982;&#21050;&#28608;&#30340;&#29305;&#23450;&#21306;&#22495;&#30456;&#20851;&#32852; - &#21487;&#20197;&#29992;&#20110;&#35753;&#19987;&#23478;&#23457;&#26680;&#29305;&#23450;&#32593;&#39029;&#20803;&#32032;&#30340;&#21453;&#39304;&#25110;&#32773;&#21487;&#35270;&#21270;&#21453;&#39304;&#32473;&#20986;&#30340;&#32593;&#39029;&#37096;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#35797;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#27880;&#35270;&#25110;&#25351;&#21521;&#20182;&#20204;&#21475;&#22836;&#34920;&#36798;&#30340;&#32593;&#39029;&#20869;&#23481;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#21475;&#22836;&#34920;&#36798;&#23545;&#19977;&#20010;&#32593;&#31449;&#30340;&#24847;&#35265;&#12290;&#21475;&#22836;&#22238;&#24212;&#20197;&#21450;&#30524;&#21160;&#21644;&#40736;&#26631;&#31227;&#21160;&#22343;&#34987;&#35760;&#24405;&#19979;&#26469;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21629;&#20013;&#29575;&#65292;&#21363;&#20197;&#35270;&#32447;&#27880;&#35270;&#25110;&#40736;&#26631;&#25351;&#21521;&#30340;&#21475;&#22836;&#25552;&#21040;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;AOIs&#65289;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The think aloud method is an important and commonly used tool for usability optimization. However, analyzing think aloud data could be time consuming. In this paper, we put forth an automatic analysis of verbal protocols and test the link between spoken feedback and the stimulus using eye tracking and mouse tracking. The gained data - user feedback linked to a specific area of the stimulus - could be used to let an expert review the feedback on specific web page elements or to visualize on which parts of the web page the feedback was given. Specifically, we test if participants fixate on or point with the mouse to the content of the webpage that they are verbalizing. During the testing, participants were shown three websites and asked to verbally give their opinion. The verbal responses, along with the eye and cursor movements were recorded. We compared the hit rate, defined as the percentage of verbally mentioned areas of interest (AOIs) that were fixated with gaze or pointed to with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#20301;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#20339;&#30340;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;Gumbel-softmax&#37319;&#26679;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#24182;&#26174;&#33879;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05170</link><description>&lt;p&gt;
&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;&#31070;&#32463;&#20998;&#20301;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Quantile Optimization for Edge-Cloud Computing. (arXiv:2307.05170v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#20301;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#20339;&#30340;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;Gumbel-softmax&#37319;&#26679;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#24182;&#26174;&#33879;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23547;&#27714;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#30340;&#26368;&#20339;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#24182;&#26368;&#23567;&#21270;&#22522;&#20110;&#31361;&#21457;&#35745;&#36153;&#30340;&#25104;&#26412;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#32593;&#32476;&#25299;&#25169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#25551;&#36848;&#21508;&#31181;&#27969;&#37327;&#38656;&#27714;&#30340;&#38543;&#26426;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20811;&#26381;&#38382;&#39064;&#31163;&#25955;&#29305;&#24449;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#23558;Gumbel-softmax&#37325;&#21442;&#25968;&#21270;&#26041;&#27861;&#25512;&#24191;&#20026;&#19968;&#20010;&#26080;&#32422;&#26463;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#65292;&#20316;&#20026;&#31163;&#25955;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#24310;&#32493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;Gumbel-softmax&#37319;&#26679;&#32593;&#32476;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#32593;&#32476;&#32467;&#26500;&#21453;&#26144;&#20102;&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#34987;&#35757;&#32451;&#20026;&#20351;&#24471;&#26080;&#32422;&#26463;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#25104;&#26412;&#20989;&#25968;&#26399;&#26395;&#26368;&#23567;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#20316;&#20026;&#19968;&#20010;&#39640;&#25928;&#30340;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#37319;&#26679;&#22120;&#65292;&#22312;&#21487;&#34892;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek the best traffic allocation scheme for the edge-cloud computing network that satisfies constraints and minimizes the cost based on burstable billing. First, for a fixed network topology, we formulate a family of integer programming problems with random parameters describing the various traffic demands. Then, to overcome the difficulty caused by the discrete feature of the problem, we generalize the Gumbel-softmax reparameterization method to induce an unconstrained continuous optimization problem as a regularized continuation of the discrete problem. Finally, we introduce the Gumbel-softmax sampling network to solve the optimization problems via unsupervised learning. The network structure reflects the edge-cloud computing topology and is trained to minimize the expectation of the cost function for unconstrained continuous optimization problems. The trained network works as an efficient traffic allocation scheme sampler, remarkably outperforming the random strategy in feasibili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.05162</link><description>&lt;p&gt;
SuryaKiran&#22312;MEDIQA-Sum 2023&#20013;&#30340;&#24212;&#29992;&#65306;&#21033;&#29992;LoRA&#36827;&#34892;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;&#29992;&#20363;&#30340;&#32467;&#26524;&#12290;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32454;&#35843;&#32791;&#36153;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#24182;&#20855;&#26377;&#39640;&#23384;&#20648;&#38656;&#27714;&#20197;&#23384;&#20648;&#32454;&#35843;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#20445;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22266;&#23450;&#22522;&#20934;&#24182;&#28155;&#21152;&#39069;&#22806;&#23618;&#26469;&#35299;&#20915;&#26102;&#38388;&#21644;&#36164;&#28304;&#25361;&#25112;&#65292;PEFT&#26041;&#27861;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#21517;&#20026;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#35299;&#20915;ImageCLEFmedical&#30340;Subtask A&#21644;B&#25152;&#36827;&#34892;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models helps improve the results for domain-specific use cases. End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model. Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune. This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization. The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model. The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36866;&#24212;&#65292;&#24182;&#22312;&#22810;&#20010;MIR&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#38899;&#20048;&#25968;&#25454;&#19978;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;MIR&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05161</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#20048;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Speech Self-supervised Learning for Music. (arXiv:2307.05161v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36866;&#24212;&#65292;&#24182;&#22312;&#22810;&#20010;MIR&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#38899;&#20048;&#25968;&#25454;&#19978;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;MIR&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#22312;&#38899;&#20048;&#35760;&#24405;&#19978;&#39044;&#35757;&#32451;&#30340;SSL&#27169;&#22411;&#21487;&#33021;&#20027;&#35201;&#26159;&#38381;&#28304;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#35821;&#38899;&#27169;&#22411;&#22914;wav2vec2.0&#22312;&#38899;&#20048;&#24314;&#27169;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23558;&#35821;&#38899;SSL&#27169;&#22411;&#24212;&#29992;&#20110;&#38899;&#20048;&#35760;&#24405;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#22312;&#38899;&#20048;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36866;&#24212;&#65292;&#20998;&#21035;&#26159;data2vec1.0&#21644;Hubert&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#21035;&#31216;&#20026;music2vec&#21644;musicHuBERT&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#37197;&#32622;&#19979;&#35757;&#32451;&#20102;12&#20010;&#20855;&#26377;95M&#21442;&#25968;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#31995;&#32479;&#35780;&#20272;&#20102;13&#20010;&#19981;&#21516;&#30340;MIR&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38899;&#20048;&#25968;&#25454;&#19978;&#35757;&#32451;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;MIR&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20351;&#29992;&#35774;&#35745;&#20026;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24418;&#24335;&#35770;&#35777;&#30340;&#32972;&#26223;&#19979;&#34920;&#36798;&#31283;&#23450;&#35299;&#37322;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#31181;&#37325;&#24314;&#30340;&#24503;&#20041;&#21547;&#20041;&#12290;&#20854;&#20013;&#30340;&#37325;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#35770;&#35777;&#37051;&#22495;&#32467;&#26500;&#26500;&#24314;&#24503;&#20041;&#36923;&#36753;&#30340;&#35299;&#37322;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.05156</link><description>&lt;p&gt;
&#31283;&#23450;&#35268;&#33539;&#35299;&#37322;&#65306;&#20174;&#35770;&#35777;&#21040;&#24503;&#20041;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Stable Normative Explanations: From Argumentation to Deontic Logic. (arXiv:2307.05156v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24418;&#24335;&#35770;&#35777;&#30340;&#32972;&#26223;&#19979;&#34920;&#36798;&#31283;&#23450;&#35299;&#37322;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#31181;&#37325;&#24314;&#30340;&#24503;&#20041;&#21547;&#20041;&#12290;&#20854;&#20013;&#30340;&#37325;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#35770;&#35777;&#37051;&#22495;&#32467;&#26500;&#26500;&#24314;&#24503;&#20041;&#36923;&#36753;&#30340;&#35299;&#37322;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24418;&#24335;&#35770;&#35777;&#30340;&#32972;&#26223;&#19979;&#34920;&#36798;&#22312;&#21487;&#20405;&#29359;&#36923;&#36753;&#20013;&#24050;&#32463;&#21457;&#23637;&#30340;&#31283;&#23450;&#35299;&#37322;&#27010;&#24565;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#37325;&#24314;&#30340;&#24503;&#20041;&#21547;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#35770;&#35777;&#37051;&#22495;&#32467;&#26500;&#26500;&#24314;&#24503;&#20041;&#36923;&#36753;&#30340;&#35299;&#37322;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#30452;&#25509;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines how a notion of stable explanation developed elsewhere in Defeasible Logic can be expressed in the context of formal argumentation. With this done, we discuss the deontic meaning of this reconstruction and show how to build from argumentation neighborhood structures for deontic logic where this notion of explanation can be characterised. Some direct complexity results are offered.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#36923;&#36753;&#65292;&#36890;&#36807;&#32447;&#24615;&#19981;&#31561;&#24335;&#20013;&#30340;&#35745;&#25968;&#27169;&#24577;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#21487;&#23558;&#20844;&#24335;&#36716;&#21270;&#20026;&#31561;&#20215;&#30340;GNN&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#21487;&#23558;GNN&#36716;&#21270;&#20026;&#20844;&#24335;&#12290;&#20316;&#32773;&#36824;&#35777;&#26126;&#20102;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#26159;&#21487;&#21028;&#23450;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;PSPACE&#20013;&#30340;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.05150</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#37322;&#19968;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#24577;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A Modal Logic for Explaining some Graph Neural Networks. (arXiv:2307.05150v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#36923;&#36753;&#65292;&#36890;&#36807;&#32447;&#24615;&#19981;&#31561;&#24335;&#20013;&#30340;&#35745;&#25968;&#27169;&#24577;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#21487;&#23558;&#20844;&#24335;&#36716;&#21270;&#20026;&#31561;&#20215;&#30340;GNN&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#21487;&#23558;GNN&#36716;&#21270;&#20026;&#20844;&#24335;&#12290;&#20316;&#32773;&#36824;&#35777;&#26126;&#20102;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#26159;&#21487;&#21028;&#23450;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;PSPACE&#20013;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#36923;&#36753;&#65292;&#20854;&#20013;&#35745;&#25968;&#27169;&#24577;&#20986;&#29616;&#22312;&#32447;&#24615;&#19981;&#31561;&#24335;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#20010;&#20844;&#24335;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#31561;&#20215;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27599;&#20010;GNN&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#20844;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#26159;&#21487;&#21028;&#23450;&#30340;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;PSPACE&#20013;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a modal logic in which counting modalities appear in linear inequalities. We show that each formula can be transformed into an equivalent graph neural network (GNN). We also show that each GNN can be transformed into a formula. We show that the satisfiability problem is decidable. We also discuss some variants that are in PSPACE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05134</link><description>&lt;p&gt;
TIAM -- &#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#35780;&#20272;&#20854;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#28210;&#26579;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#37325;&#35201;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#31561;&#39069;&#22806;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#20174;&#38543;&#26426;&#36215;&#22987;&#28857;&#24320;&#22987;&#30340;&#65292;&#20294;&#36890;&#24120;&#19981;&#32771;&#34385;&#36825;&#19968;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#25552;&#31034;&#20013;&#25351;&#23450;&#30340;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#26356;&#22909;&#22320;&#25551;&#36848;&#23545;&#40784;&#24615;&#65292;&#21253;&#25324;&#25351;&#23450;&#23545;&#35937;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;T2I&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39069;&#22806;&#32467;&#26524;&#65292;&#21363;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20351;&#29992;&#25200;&#21160;&#20316;&#20026;&#35780;&#20272;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;XAI&#25216;&#26415;&#30340;&#24212;&#29992;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25200;&#21160;&#20998;&#26512;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#24402;&#22240;&#30340;&#36136;&#37327;&#65292;&#24182;&#20026;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05104</link><description>&lt;p&gt;
&#23545;&#25200;&#21160;&#20316;&#20026;&#26102;&#24207;XAI&#35780;&#20272;&#25216;&#26415;&#30340;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI. (arXiv:2307.05104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20351;&#29992;&#25200;&#21160;&#20316;&#20026;&#35780;&#20272;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;XAI&#25216;&#26415;&#30340;&#24212;&#29992;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25200;&#21160;&#20998;&#26512;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#24402;&#22240;&#30340;&#36136;&#37327;&#65292;&#24182;&#20026;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#22686;&#21152;&#20102;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#27668;&#20505;&#31185;&#23398;&#39046;&#22495;&#65292;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;XAI&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#65292;&#22914;XAI&#25216;&#26415;&#25552;&#20379;&#30340;&#24402;&#22240;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25200;&#21160;&#20998;&#26512;&#21253;&#25324;&#31995;&#32479;&#22320;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#23545;XAI&#26041;&#27861;&#29983;&#25104;&#30340;&#24402;&#22240;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;XAI&#25216;&#26415;&#65292;&#24182;&#22312;&#19977;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25200;&#21160;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#24402;&#22240;&#30340;&#36136;&#37327;&#65292;&#24182;&#27934;&#23519;&#21147;&#22320;&#25581;&#31034;&#20986;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) has gained significant attention recently as the demand for transparency and interpretability of machine learning models has increased. In particular, XAI for time series data has become increasingly important in finance, healthcare, and climate science. However, evaluating the quality of explanations, such as attributions provided by XAI techniques, remains challenging. This paper provides an in-depth analysis of using perturbations to evaluate attributions extracted from time series models. A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method. We apply this approach to several state-of-the-art XAI techniques and evaluate their performance on three time series classification datasets. Our results demonstrate that the perturbation analysis approach can effectively evaluate the quality of attributions and provide insights into the strengths and limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#38477;&#20302;&#31616;&#21333;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#38450;&#24481;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25552;&#39640;&#27169;&#22411;&#30340;&#23545;&#25239;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05095</link><description>&lt;p&gt;
ATWM&#65306;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ATWM: Defense against adversarial malware based on adversarial training. (arXiv:2307.05095v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#38477;&#20302;&#31616;&#21333;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#38450;&#24481;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25552;&#39640;&#27169;&#22411;&#30340;&#23545;&#25239;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#23601;&#12290;&#20026;&#20102;&#25269;&#24481;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Windows&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#12290;&#24694;&#24847;&#36719;&#20214;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#24694;&#24847;&#21151;&#33021;&#30340;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#65292;&#20197;&#25915;&#20987;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#24182;&#36867;&#36991;&#20854;&#26816;&#27979;&#12290;&#30446;&#21069;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#23545;&#25239;&#24615;&#38450;&#24481;&#30740;&#31350;&#65292;&#20294;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#22270;&#20687;&#26679;&#26412;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#38450;&#24481;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#22788;&#29702;&#26469;&#38450;&#24481;&#31616;&#21333;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#38450;&#24481;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#32452;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#20102;&#19977;&#31181;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning technology has made great achievements in the field of image. In order to defend against malware attacks, researchers have proposed many Windows malware detection models based on deep learning. However, deep learning models are vulnerable to adversarial example attacks. Malware can generate adversarial malware with the same malicious function to attack the malware detection model and evade detection of the model. Currently, many adversarial defense studies have been proposed, but existing adversarial defense studies are based on image sample and cannot be directly applied to malware sample. Therefore, this paper proposes an adversarial malware defense method based on adversarial training. This method uses preprocessing to defend simple adversarial examples to reduce the difficulty of adversarial training. Moreover, this method improves the adversarial defense capability of the model through adversarial training. We experimented with three attack methods in two sets of dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#20307;&#39537;&#21160;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#31995;&#32479;&#19982;ChatGPT&#36827;&#34892;&#20803;&#23398;&#20064;&#30456;&#20114;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#24247;&#22797;&#39046;&#22495;&#30340;&#24212;&#29992;&#23454;&#29616;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05082</link><description>&lt;p&gt;
OntoChatGPT&#20449;&#24687;&#31995;&#32479;&#65306;&#26412;&#20307;&#39537;&#21160;&#30340;ChatGPT&#20803;&#23398;&#20064;&#32467;&#26500;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning. (arXiv:2307.05082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#20307;&#39537;&#21160;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#31995;&#32479;&#19982;ChatGPT&#36827;&#34892;&#20803;&#23398;&#20064;&#30456;&#20114;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#24247;&#22797;&#39046;&#22495;&#30340;&#24212;&#29992;&#23454;&#29616;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26412;&#20307;&#39537;&#21160;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#31995;&#32479;&#19982;ChatGPT&#65288;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30456;&#20114;&#32467;&#21512;&#12290;&#30740;&#31350;&#24320;&#21457;&#20102;&#24418;&#24335;&#27169;&#22411;&#65288;&#20449;&#24687;&#21644;&#21151;&#33021;&#20004;&#20010;&#26041;&#38754;&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;&#23558;&#26412;&#20307;&#39537;&#21160;&#30340;&#25552;&#31034;&#19982;ChatGPT&#30340;&#20803;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35770;&#22522;&#30784;&#12290;&#24471;&#21040;&#30340;&#19977;&#37325;&#32467;&#26500;&#21253;&#25324;&#26041;&#27861;&#35770;&#22522;&#30784;&#12289;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#21644;OntoChatGPT&#31995;&#32479;&#65292;&#20849;&#21516;&#25552;&#39640;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25928;&#33021;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24247;&#22797;&#39046;&#22495;&#20013;&#37319;&#29992;&#20044;&#20811;&#20848;&#35821;&#23454;&#29616;&#20102;&#35813;&#25216;&#26415;&#12290;&#36890;&#36807;&#24212;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65292;OntoChatGPT&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#29983;&#25104;&#30456;&#20851;&#30340;&#22238;&#31572;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#35770;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24378;&#35843;&#20854;&#19981;&#20165;&#36866;&#29992;&#20110;ChatGPT&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Uni-Removal&#65292;&#36825;&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#36864;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#30693;&#35782;&#20256;&#36882;&#21644;&#22495;&#36866;&#24212;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#21644;&#21442;&#25968;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#36864;&#21270;&#65292;&#24182;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;</title><link>http://arxiv.org/abs/2307.05075</link><description>&lt;p&gt;
Uni-Removal: &#19968;&#20010;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#23454;&#38469;&#22270;&#20687;&#20013;&#22810;&#31181;&#36864;&#21270;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images. (arXiv:2307.05075v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Uni-Removal&#65292;&#36825;&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#36864;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#30693;&#35782;&#20256;&#36882;&#21644;&#22495;&#36866;&#24212;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#21644;&#21442;&#25968;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#36864;&#21270;&#65292;&#24182;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23454;&#38469;&#22270;&#20687;&#20013;&#21435;&#38500;&#22810;&#31181;&#36864;&#21270;&#65292;&#22914;&#38654;&#12289;&#38632;&#21644;&#27169;&#31946;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#36866;&#23450;&#38382;&#39064;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#36864;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Uni-Removal&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#21644;&#21442;&#25968;&#35299;&#20915;&#23454;&#38469;&#22270;&#20687;&#20013;&#22810;&#31181;&#36864;&#21270;&#38382;&#39064;&#12290;&#22312;&#30693;&#35782;&#20256;&#36882;&#38454;&#27573;&#65292;Uni-Removal&#21033;&#29992;&#30417;&#30563;&#22810;&#25945;&#24072;&#21644;&#23398;&#29983;&#26550;&#26500;&#65292;&#22312;&#30693;&#35782;&#20256;&#36882;&#38454;&#27573;&#20013;&#20419;&#36827;&#20174;&#39044;&#35757;&#32451;&#30340;&#19987;&#38376;&#22788;&#29702;&#19981;&#21516;&#36864;&#21270; &#31867;&#22411;&#30340;&#25945;&#24072;&#32593;&#32476;&#20013;&#23398;&#20064;&#12290;&#24341;&#20837;&#20102;&#22810;&#31890;&#24230;&#23545;&#27604;&#25439;&#22833;&#26469;&#22686;&#24378;&#26469;&#33258;&#29305;&#24449;&#21644;&#22270;&#20687;&#31354;&#38388;&#30340;&#23398;&#20064;&#12290;&#22312;&#22495;&#36866;&#24212;&#38454;&#27573;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#35757;&#32451;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Removing multiple degradations, such as haze, rain, and blur, from real-world images poses a challenging and illposed problem. Recently, unified models that can handle different degradations have been proposed and yield promising results. However, these approaches focus on synthetic images and experience a significant performance drop when applied to realworld images. In this paper, we introduce Uni-Removal, a twostage semi-supervised framework for addressing the removal of multiple degradations in real-world images using a unified model and parameters. In the knowledge transfer stage, Uni-Removal leverages a supervised multi-teacher and student architecture in the knowledge transfer stage to facilitate learning from pretrained teacher networks specialized in different degradation types. A multi-grained contrastive loss is introduced to enhance learning from feature and image spaces. In the domain adaptation stage, unsupervised fine-tuning is performed by incorporating an adversarial d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#26816;&#32034;&#31034;&#20363;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05074</link><description>&lt;p&gt;
&#37319;&#29992;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain. (arXiv:2307.05074v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#26816;&#32034;&#31034;&#20363;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#24211;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#25552;&#31034;&#20197;&#24341;&#23548;LLMs&#29702;&#35299;&#36755;&#20837;&#38382;&#39064;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;SQL&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#20005;&#26684;&#30340;SQL&#35821;&#27861;&#35201;&#27714;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#19968;&#31995;&#21015;&#31034;&#20363;&#65288;&#21363;&#38382;&#39064;-SQL&#23545;&#65289;&#26469;&#25552;&#31034;LLMs&#29983;&#25104;SQL&#65292;&#20294;&#22266;&#23450;&#30340;&#25552;&#31034;&#20960;&#20046;&#26080;&#27861;&#22788;&#29702;&#26816;&#32034;&#20986;&#30340;&#31034;&#20363;&#19982;&#36755;&#20837;&#38382;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#21253;&#25324;&#26679;&#26412;&#24863;&#30693;&#25552;&#31034;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26679;&#26412;&#24863;&#30693;&#31034;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;SQL&#36816;&#31639;&#31526;&#30340;&#32452;&#21512;&#21644;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20540;&#21270;&#20449;&#24565;&#32858;&#21512;&#30340;&#35758;&#31243;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#36335;&#24452;&#36830;&#25509;&#21644;&#29978;&#33267;&#21487;&#21542;&#24615;&#26159;&#20135;&#29983;&#32452;&#32455;&#32467;&#26524;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#23521;&#22836;&#32479;&#27835;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.05072</link><description>&lt;p&gt;
&#23558;&#20934;&#30830;&#24230;&#32858;&#21512;&#21040;&#20449;&#24565;&#20013;&#65306;&#19981;&#21487;&#33021;&#32467;&#26524;&#30340;&#35758;&#31243;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Aggregating Credences into Beliefs: Agenda Conditions for Impossibility Results. (arXiv:2307.05072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20540;&#21270;&#20449;&#24565;&#32858;&#21512;&#30340;&#35758;&#31243;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#36335;&#24452;&#36830;&#25509;&#21644;&#29978;&#33267;&#21487;&#21542;&#24615;&#26159;&#20135;&#29983;&#32452;&#32455;&#32467;&#26524;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#23521;&#22836;&#32479;&#27835;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#21270;&#20449;&#24565;&#32858;&#21512;&#35299;&#20915;&#20102;&#22914;&#20309;&#29702;&#24615;&#22320;&#23558;&#20010;&#20307;&#30340;&#27010;&#29575;&#20449;&#24565;&#32858;&#21512;&#25104;&#38598;&#20307;&#30340;&#20108;&#20540;&#20449;&#24565;&#12290;&#31867;&#20284;&#20110;&#21028;&#26029;&#32858;&#21512;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#21046;&#23450;&#20844;&#29702;&#35201;&#27714;&#65292;&#35777;&#26126;&#19981;&#21487;&#33021;&#23450;&#29702;&#65292;&#24182;&#30830;&#23450;&#19981;&#21487;&#33021;&#23450;&#29702;&#30340;&#30830;&#20999;&#35758;&#31243;&#26465;&#20214;&#26159;&#20108;&#20540;&#21270;&#20449;&#24565;&#32858;&#21512;&#20013;&#30340;&#33258;&#28982;&#32780;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#25105;&#20204;&#20197;&#21069;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#35758;&#31243;&#29702;&#35770;&#26041;&#27861;&#25512;&#24191;&#32467;&#26524;&#65292;&#24182;&#30830;&#23450;&#35758;&#31243;&#20013;&#38382;&#39064;&#20043;&#38388;&#36923;&#36753;&#36830;&#25509;&#25152;&#38656;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#23618;&#27425;&#65292;&#20197;&#23548;&#33268;&#19981;&#21487;&#33021;&#23450;&#29702;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#35777;&#26126;&#65306;&#65288;1&#65289;&#36335;&#24452;&#36830;&#25509;&#21644;&#29978;&#33267;&#21487;&#21542;&#24615;&#26500;&#25104;&#20102;&#32452;&#32455;&#32467;&#26524;&#30340;&#30830;&#20999;&#35758;&#31243;&#26465;&#20214;&#65292;&#21363;&#28385;&#36275;&#36880;&#20010;&#21629;&#39064;&#29420;&#31435;&#24615;&#21644;&#38598;&#20307;&#20449;&#24565;&#30340;&#25512;&#29702;&#23553;&#38381;&#24615;&#30340;&#20108;&#20540;&#21270;&#20449;&#24565;&#32858;&#21512;&#22312;&#36739;&#23567;&#26465;&#20214;&#19979;&#20250;&#23548;&#33268;&#23521;&#22836;&#32479;&#27835;&#65307;&#65288;2&#65289;&#21542;&#23450;&#36830;&#25509;...
&lt;/p&gt;
&lt;p&gt;
Binarizing belief aggregation addresses how to rationally aggregate individual probabilistic beliefs into collective binary beliefs. Similar to the development of judgment aggregation theory, formulating axiomatic requirements, proving impossibility theorems, and identifying exact agenda conditions of impossibility theorems are natural and important research topics in binarizing belief aggregation. Building on our previous research on impossibility theorems, we use an agenda-theoretic approach to generalize the results and to determine the necessary and sufficient level of logical interconnection between the issues in an agenda for the impossibility theorems to arise. We demonstrate that (1) path-connectedness and even-negatability constitute the exact agenda condition for the oligarchy result stating that binarizing belief aggregation satisfying proposition-wise independence and deductive closure of collective beliefs yields the oligarchies under minor conditions; (2) negation-connect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#65288;FCA&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#25366;&#25496;&#21644;&#23547;&#25214;&#26410;&#30693;&#26410;&#30693;&#65292;&#20174;&#32780;&#36991;&#20813;&#28508;&#22312;&#30340;&#37325;&#22823;&#25910;&#30410;&#25110;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.05071</link><description>&lt;p&gt;
&#23545;&#26410;&#30693;&#26410;&#30693;&#30340;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Mining for Unknown Unknowns. (arXiv:2307.05071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#65288;FCA&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#25366;&#25496;&#21644;&#23547;&#25214;&#26410;&#30693;&#26410;&#30693;&#65292;&#20174;&#32780;&#36991;&#20813;&#28508;&#22312;&#30340;&#37325;&#22823;&#25910;&#30410;&#25110;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#30693;&#26410;&#30693;&#26159;&#32570;&#20047;&#20107;&#21069;&#25551;&#36848;&#30340;&#26410;&#26469;&#30456;&#20851;&#30340;&#20598;&#21457;&#20107;&#20214;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#22238;&#39038;&#24615;&#30340;&#25253;&#21578;&#26174;&#31034;&#65292;&#22914;&#26524;&#27492;&#31867;&#24773;&#20917;&#20107;&#21069;&#34987;&#21457;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#25110;&#36991;&#20813;&#26174;&#33879;&#25910;&#30410;&#25110;&#25439;&#22833;&#65292;&#20294;&#33719;&#21462;&#26410;&#30693;&#26410;&#30693;&#20173;&#28982;&#26159;&#38590;&#20197;&#25417;&#25720;&#30340;&#65292;&#26080;&#35770;&#26159;&#22312;&#23454;&#36341;&#19978;&#36824;&#26159;&#22312;&#27010;&#24565;&#19978;&#12290;&#26412;&#25991;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#65288;FCA&#65289; - &#19968;&#31181;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#25366;&#25496;&#21644;&#32452;&#32455;&#25968;&#25454;&#30340;&#26684;&#35770;&#23376;&#39046;&#22495; - &#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20197;&#31995;&#32479;&#22320;&#25171;&#30772;&#24605;&#32500;&#23450;&#21183;&#65292;&#25351;&#23548;&#23545;&#26410;&#30693;&#26410;&#30693;&#30340;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unknown unknowns are future relevant contingencies that lack an ex ante description. While there are numerous retrospective accounts showing that significant gains or losses might have been achieved or avoided had such contingencies been previously uncovered, getting hold of unknown unknowns still remains elusive, both in practice and conceptually. Using Formal Concept Analysis (FCA) - a subfield of lattice theory which is increasingly applied for mining and organizing data - this paper introduces a simple framework to systematically think out of the box and direct the search for unknown unknowns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20449;&#24565;&#20462;&#27491;&#26694;&#26550;&#19979;&#23545;&#30830;&#35748;&#20559;&#24046;&#12289;&#26694;&#26550;&#20559;&#24046;&#21644;&#38170;&#23450;&#20559;&#24046;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#24182;&#24212;&#29992;&#20110;&#19977;&#31181;&#24120;&#35265;&#30340;&#20449;&#24565;&#20462;&#27491;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#20559;&#24046;&#23545;&#20110;&#36861;&#36394;&#30495;&#30456;&#26159;&#26377;&#24433;&#21709;&#30340;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;&#22312;&#38543;&#26426;&#24773;&#22659;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05069</link><description>&lt;p&gt;
&#35748;&#30693;&#20559;&#24046;&#19982;&#20449;&#24565;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Cognitive Bias and Belief Revision. (arXiv:2307.05069v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20449;&#24565;&#20462;&#27491;&#26694;&#26550;&#19979;&#23545;&#30830;&#35748;&#20559;&#24046;&#12289;&#26694;&#26550;&#20559;&#24046;&#21644;&#38170;&#23450;&#20559;&#24046;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#24182;&#24212;&#29992;&#20110;&#19977;&#31181;&#24120;&#35265;&#30340;&#20449;&#24565;&#20462;&#27491;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#20559;&#24046;&#23545;&#20110;&#36861;&#36394;&#30495;&#30456;&#26159;&#26377;&#24433;&#21709;&#30340;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;&#22312;&#38543;&#26426;&#24773;&#22659;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20449;&#24565;&#20462;&#27491;&#30340;&#26694;&#26550;&#19979;&#65292;&#23545;&#19977;&#31181;&#35748;&#30693;&#20559;&#24046;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#30340;&#25551;&#36848;&#65306;&#30830;&#35748;&#20559;&#24046;&#12289;&#26694;&#26550;&#20559;&#24046;&#21644;&#38170;&#23450;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#20854;&#35299;&#37322;&#20026;&#23545;&#36845;&#20195;&#20462;&#27491;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#30693;&#21517;&#30340;&#20449;&#24565;&#20462;&#27491;&#26041;&#27861;&#65306;&#26465;&#20214;&#20462;&#27491;&#12289;&#35789;&#20856;&#24335;&#20462;&#27491;&#21644;&#26368;&#23567;&#20462;&#27491;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21463;&#20559;&#24046;&#20449;&#24565;&#20462;&#27491;&#26041;&#27861;&#22312;&#36861;&#36394;&#30495;&#30456;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#27169;&#25311;&#35780;&#20272;&#20102;&#20559;&#24046;&#20449;&#24565;&#20462;&#27491;&#22312;&#38543;&#26426;&#24773;&#22659;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we formalise three types of cognitive bias within the framework of belief revision: confirmation bias, framing bias, and anchoring bias. We interpret them generally, as restrictions on the process of iterated revision, and we apply them to three well-known belief revision methods: conditioning, lexicographic revision, and minimal revision. We investigate the reliability of biased belief revision methods in truth tracking. We also run computer simulations to assess the performance of biased belief revision in random scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#29702;&#24615;&#20915;&#31574;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26080;&#27861;&#23545;&#33258;&#36523;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05068</link><description>&lt;p&gt;
&#26377;&#30028;&#24402;&#32435;&#29702;&#24615;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Bounded Inductive Rationality. (arXiv:2307.05068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#29702;&#24615;&#20915;&#31574;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26080;&#27861;&#23545;&#33258;&#36523;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#36873;&#25321;&#30340;&#20027;&#27969;&#29702;&#35770;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#65292;&#21363;&#24403;&#38754;&#20020;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#25191;&#34892;&#25152;&#26377;&#30456;&#20851;&#35745;&#31639;&#65292;&#24182;&#30830;&#23450;&#25152;&#26377;&#30456;&#20851;&#30340;&#36923;&#36753;/&#25968;&#23398;&#21629;&#39064;&#30340;&#30495;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20363;&#22914;&#25105;&#20204;&#23545;&#22278;&#21608;&#29575;&#30340;&#36828;&#31243;&#23567;&#25968;&#25552;&#20379;&#36172;&#27880;&#65292;&#25110;&#32773;&#26234;&#33021;&#20307;&#38754;&#20020;&#35745;&#31639;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#26102;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#20551;&#35774;&#22312;&#29615;&#22659;&#20013;&#21253;&#21547;&#26234;&#33021;&#20307;&#33258;&#36523;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#20250;&#20135;&#29983;&#30683;&#30462;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#21338;&#24328;&#29702;&#35770;&#30740;&#31350;&#30340;&#25112;&#30053;&#20114;&#21160;&#26159;&#20915;&#31574;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#19968;&#20010;&#29702;&#24615;&#26234;&#33021;&#20307;&#30001;&#20854;&#29615;&#22659;&#65288;&#20854;&#20182;&#29609;&#23478;&#65289;&#39044;&#27979;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#29702;&#24615;&#20915;&#31574;&#29702;&#35770;&#12290;&#25105;&#20204;&#32771;&#34385;&#21453;&#22797;&#38754;&#20020;&#20915;&#31574;&#38382;&#39064;&#30340;&#26234;&#33021;&#20307;&#65288;&#21253;&#25324;&#23545;&#22278;&#21608;&#29575;&#23567;&#25968;&#36172;&#27880;&#30340;&#38382;&#39064;&#20197;&#21450;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#23545;&#25112;&#30340;&#28216;&#25103;&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;...
&lt;/p&gt;
&lt;p&gt;
The dominant theories of rational choice assume logical omniscience. That is, they assume that when facing a decision problem, an agent can perform all relevant computations and determine the truth value of all relevant logical/mathematical claims. This assumption is unrealistic when, for example, we offer bets on remote digits of pi or when an agent faces a computationally intractable planning problem. Furthermore, the assumption of logical omniscience creates contradictions in cases where the environment can contain descriptions of the agent itself. Importantly, strategic interactions as studied in game theory are decision problems in which a rational agent is predicted by its environment (the other players). In this paper, we develop a theory of rational decision making that does not assume logical omniscience. We consider agents who repeatedly face decision problems (including ones like betting on digits of pi or games against other agents). The main contribution of this paper is t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;ZDDs&#26367;&#20195;BDDs&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#26816;&#26597;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.05067</link><description>&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#38590;&#39064;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#20351;&#29992;ZDDs&#23545;&#31526;&#21495;&#27169;&#22411;&#26816;&#26597;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Exploiting Asymmetry in Logic Puzzles: Using ZDDs for Symbolic Model Checking Dynamic Epistemic Logic. (arXiv:2307.05067v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05067
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;ZDDs&#26367;&#20195;BDDs&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#26816;&#26597;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#65288;BDDs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#32531;&#35299;&#27169;&#22411;&#26816;&#26597;&#20013;&#30340;&#29366;&#24577;&#29190;&#28856;&#38382;&#39064;&#12290;BDDs&#30340;&#19968;&#31181;&#21464;&#20307;&#26159;&#38646;&#20943;&#21387;&#20915;&#31574;&#22270;&#65288;ZDDs&#65289;&#65292;&#23427;&#30465;&#30053;&#20102;&#24517;&#39035;&#20026;&#20551;&#30340;&#21464;&#37327;&#65292;&#32780;&#19981;&#26159;&#30465;&#30053;&#19981;&#37325;&#35201;&#30340;&#21464;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;ZDDs&#26469;&#31526;&#21495;&#21270;&#32534;&#30721;&#29992;&#20110;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#30340;Kripke&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#25512;&#29702;&#20851;&#20110;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#30693;&#35782;&#21644;&#20449;&#24687;&#21160;&#24577;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;ZDD&#21464;&#20307;&#22312;&#25991;&#29486;&#20013;&#30340;&#19977;&#20010;&#33879;&#21517;&#31034;&#20363;&#65288;&#28201;&#27700;&#23401;&#23376;&#38382;&#39064;&#65292;&#21644;&#31215;&#35868;&#20197;&#21450;&#23601;&#39184;&#21152;&#23494;&#25104;&#21592;&#38382;&#39064;&#65289;&#23545;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#22522;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#26816;&#26597;&#31243;&#24207;SMCDEL&#21644;CUDD&#24211;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#36866;&#30340;ZDD&#21464;&#20307;&#26367;&#25442;BDDs&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;&#36825;&#34920;&#26126;ZDDs&#26159;&#23545;&#20110;&#27169;&#22411;&#26816;&#26597;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary decision diagrams (BDDs) are widely used to mitigate the state-explosion problem in model checking. A variation of BDDs are Zero-suppressed Decision Diagrams (ZDDs) which omit variables that must be false, instead of omitting variables that do not matter. We use ZDDs to symbolically encode Kripke models used in Dynamic Epistemic Logic, a framework to reason about knowledge and information dynamics in multi-agent systems. We compare the memory usage of different ZDD variants for three well-known examples from the literature: the Muddy Children, the Sum and Product puzzle and the Dining Cryptographers. Our implementation is based on the existing model checker SMCDEL and the CUDD library. Our results show that replacing BDDs with the right variant of ZDDs can significantly reduce memory usage. This suggests that ZDDs are a useful tool for model checking multi-agent systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25112;&#30053;&#24615;&#30693;&#36947;&#22914;&#20309;&#36923;&#36753;&#30340;&#34920;&#26684;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#35813;&#36923;&#36753;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#21487;&#20197;&#22312;PSPACE&#20013;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2307.05066</link><description>&lt;p&gt;
&#25112;&#30053;&#24615;&#30693;&#36947;&#30340;&#36923;&#36753;&#30340;&#34920;&#26684;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tableaux for the Logic of Strategically Knowing How. (arXiv:2307.05066v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25112;&#30053;&#24615;&#30693;&#36947;&#22914;&#20309;&#36923;&#36753;&#30340;&#34920;&#26684;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#35813;&#36923;&#36753;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#21487;&#20197;&#22312;PSPACE&#20013;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#30693;&#36947;&#22914;&#20309;&#36923;&#36753;&#23558;&#26631;&#20934;&#30340;&#35748;&#30693;&#36923;&#36753;&#24310;&#20280;&#65292;&#21152;&#20837;&#20102;&#19968;&#20010;&#30693;&#36947;&#22914;&#20309;&#30340;&#25805;&#20316;&#31526;&#12290;&#36825;&#20010;&#30693;&#36947;&#22914;&#20309;&#30340;&#25805;&#20316;&#31526;&#34987;&#35299;&#37322;&#20026;&#23384;&#22312;&#19968;&#31181;&#31574;&#30053;&#65292;&#20351;&#24471;&#20195;&#29702;&#30693;&#36947;&#35813;&#31574;&#30053;&#21487;&#20197;&#30830;&#20445; p&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#25112;&#30053;&#24615;&#30693;&#36947;&#22914;&#20309;&#36923;&#36753;&#30340;&#22810;&#20195;&#29702;&#29256;&#26412;&#30340;&#34920;&#26684;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#34920;&#26684;&#27861;&#30340;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#35813;&#36923;&#36753;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#21487;&#20197;&#22312;PSPACE&#20013;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logic of goal-directed knowing-how extends the standard epistemic logic with an operator of knowing-how. The knowing-how operator is interpreted as that there exists a strategy such that the agent knows that the strategy can make sure that p. This paper presents a tableau procedure for the multi-agent version of the logic of strategically knowing-how and shows the soundness and completeness of this tableau procedure. This paper also shows that the satisfiability problem of the logic can be decided in PSPACE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29699;&#20307;&#31995;&#32479;&#30340;&#20004;&#32423;&#21487;&#20449;&#24230;&#38480;&#21046;&#20462;&#27491;&#25805;&#20316;&#31526;&#30340;&#26500;&#36896;&#65292;&#24182;&#23545;&#36825;&#20123;&#25805;&#20316;&#31526;&#36827;&#34892;&#20102;&#20844;&#29702;&#21270;&#21051;&#30011;&#12290;</title><link>http://arxiv.org/abs/2307.05062</link><description>&lt;p&gt;
&#22522;&#20110;&#29699;&#20307;&#31995;&#32479;&#30340;&#20004;&#32423;&#21487;&#20449;&#24230;&#38480;&#21046;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
System of Spheres-based Two Level Credibility-limited Revisions. (arXiv:2307.05062v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29699;&#20307;&#31995;&#32479;&#30340;&#20004;&#32423;&#21487;&#20449;&#24230;&#38480;&#21046;&#20462;&#27491;&#25805;&#20316;&#31526;&#30340;&#26500;&#36896;&#65292;&#24182;&#23545;&#36825;&#20123;&#25805;&#20316;&#31526;&#36827;&#34892;&#20102;&#20844;&#29702;&#21270;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#32423;&#21487;&#20449;&#24230;&#38480;&#21046;&#20462;&#27491;&#26159;&#19968;&#31181;&#38750;&#20248;&#20808;&#32423;&#20462;&#27491;&#25805;&#20316;&#12290;&#22312;&#36827;&#34892;&#20004;&#32423;&#21487;&#20449;&#24230;&#38480;&#21046;&#20462;&#27491;&#26102;&#65292;&#32771;&#34385;&#20004;&#20010;&#23618;&#32423;&#30340;&#21487;&#20449;&#24230;&#21644;&#19968;&#20010;&#23618;&#32423;&#30340;&#19981;&#21487;&#20449;&#24230;&#12290;&#24403;&#36890;&#36807;&#26368;&#39640;&#23618;&#32423;&#21487;&#20449;&#24230;&#30340;&#21477;&#23376;&#36827;&#34892;&#20462;&#27491;&#26102;&#65292;&#25805;&#20316;&#31526;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#26631;&#20934;&#30340;&#20462;&#27491;&#65307;&#22914;&#26524;&#21477;&#23376;&#22788;&#20110;&#31532;&#20108;&#32423;&#21487;&#20449;&#24230;&#65292;&#21017;&#20462;&#27491;&#32467;&#26524;&#19982;&#36890;&#36807;&#21542;&#23450;&#35813;&#21477;&#23376;&#36827;&#34892;&#26631;&#20934;&#25910;&#32553;&#30340;&#32467;&#26524;&#19968;&#33268;&#65307;&#22914;&#26524;&#21477;&#23376;&#19981;&#21487;&#20449;&#65292;&#21017;&#21407;&#22987;&#20449;&#24565;&#38598;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Grove&#30340;&#29699;&#20307;&#31995;&#32479;&#30340;&#20004;&#32423;&#21487;&#20449;&#24230;&#38480;&#21046;&#20462;&#27491;&#25805;&#20316;&#31526;&#30340;&#26500;&#36896;&#65292;&#24182;&#23545;&#36825;&#20123;&#25805;&#20316;&#31526;&#36827;&#34892;&#20102;&#20844;&#29702;&#21270;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two level credibility-limited revision is a non-prioritized revision operation. When revising by a two level credibility-limited revision, two levels of credibility and one level of incredibility are considered. When revising by a sentence at the highest level of credibility, the operator behaves as a standard revision, if the sentence is at the second level of credibility, then the outcome of the revision process coincides with a standard contraction by the negation of that sentence. If the sentence is not credible, then the original belief set remains unchanged. In this paper, we propose a construction for two level credibility-limited revision operators based on Grove's systems of spheres and present an axiomatic characterization for these operators.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;&#20013;&#30340;&#35760;&#24518;&#32570;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#28151;&#21512;&#31574;&#30053;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#30456;&#20851;&#22343;&#34913;&#26469;&#35299;&#20915;&#24102;&#26377;&#20581;&#24536;&#21644;&#30095;&#24573;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20851;&#38190;&#20915;&#31574;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#21487;&#35745;&#31639;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.05059</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;&#20013;&#30340;&#35760;&#24518;&#32570;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Imperfect Recall in Multi-Agent Influence Diagrams. (arXiv:2307.05059v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;&#20013;&#30340;&#35760;&#24518;&#32570;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#28151;&#21512;&#31574;&#30053;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#30456;&#20851;&#22343;&#34913;&#26469;&#35299;&#20915;&#24102;&#26377;&#20581;&#24536;&#21644;&#30095;&#24573;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20851;&#38190;&#20915;&#31574;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#21487;&#35745;&#31639;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;&#65288;MAIDs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#27969;&#34892;&#21338;&#24328;&#35770;&#27169;&#22411;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;MAIDs&#30456;&#27604;&#25193;&#23637;&#21338;&#24328;&#34920;&#31034;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#26234;&#33021;&#20307;&#37319;&#29992;&#34892;&#20026;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#20915;&#31574;&#35774;&#32622;&#29420;&#31435;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#26469;&#36873;&#25321;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#35760;&#24518;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#34892;&#20026;&#31574;&#30053;&#30340;&#32435;&#20160;&#22343;&#34913;&#21487;&#33021;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#28151;&#21512;&#31574;&#30053;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#30456;&#20851;&#22343;&#34913;&#26469;&#35299;&#20915;&#24102;&#26377;&#20581;&#24536;&#21644;&#30095;&#24573;&#26234;&#33021;&#20307;&#30340;MAIDs&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;MAIDs&#20013;&#20851;&#38190;&#20915;&#31574;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;&#21487;&#35745;&#31639;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;MAIDs&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#21644;&#22242;&#38431;&#24773;&#22659;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#24773;&#22659;&#20013;&#30340;&#35760;&#24518;&#32570;&#22833;&#36890;&#24120;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent influence diagrams (MAIDs) are a popular game-theoretic model based on Bayesian networks. In some settings, MAIDs offer significant advantages over extensive-form game representations. Previous work on MAIDs has assumed that agents employ behavioural policies, which set independent conditional probability distributions over actions for each of their decisions. In settings with imperfect recall, however, a Nash equilibrium in behavioural policies may not exist. We overcome this by showing how to solve MAIDs with forgetful and absent-minded agents using mixed policies and two types of correlated equilibrium. We also analyse the computational complexity of key decision problems in MAIDs, and explore tractable cases. Finally, we describe applications of MAIDs to Markov games and team situations, where imperfect recall is often unavoidable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#29702;&#35770;&#26469;&#35299;&#20915;&#27169;&#24577;&#36923;&#36753;&#20013;&#19968;&#33268;&#24615;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26631;&#20934;&#30340;&#32972;&#26223;&#30693;&#35782;&#27169;&#22359;&#26469;&#36827;&#34892;&#33539;&#30068;&#20915;&#31574;&#12290;&#36825;&#20123;&#32467;&#26524;&#21644;&#26041;&#27861;&#26377;&#21161;&#20110;&#38416;&#26126;&#35748;&#35782;&#35770;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#35299;&#20915;&#19982;&#21028;&#26029;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#20013;&#27169;&#24577;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05053</link><description>&lt;p&gt;
&#24378;&#21270;&#27169;&#24577;&#36923;&#36753;&#20013;&#30340;&#19968;&#33268;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Strengthening Consistency Results in Modal Logic. (arXiv:2307.05053v1 [math.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#29702;&#35770;&#26469;&#35299;&#20915;&#27169;&#24577;&#36923;&#36753;&#20013;&#19968;&#33268;&#24615;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26631;&#20934;&#30340;&#32972;&#26223;&#30693;&#35782;&#27169;&#22359;&#26469;&#36827;&#34892;&#33539;&#30068;&#20915;&#31574;&#12290;&#36825;&#20123;&#32467;&#26524;&#21644;&#26041;&#27861;&#26377;&#21161;&#20110;&#38416;&#26126;&#35748;&#35782;&#35770;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#35299;&#20915;&#19982;&#21028;&#26029;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#20013;&#27169;&#24577;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24577;&#36923;&#36753;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#32473;&#23450;&#30340;&#29702;&#35770;&#26159;&#21542;&#19968;&#33268;&#12290;&#20294;&#19968;&#33268;&#20110;&#20160;&#20040;&#65311;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#20856;&#22411;&#26041;&#27861;&#26159;&#30830;&#23450;&#19968;&#32452;&#32972;&#26223;&#30693;&#35782;&#20844;&#29702;&#65288;&#27604;&#22914;S4&#12289;D&#31561;&#65289;&#65292;&#28982;&#21518;&#23637;&#31034;&#19982;&#35813;&#29702;&#35770;&#30456;&#19968;&#33268;&#30340;&#20551;&#35774;&#19982;&#36825;&#20123;&#32972;&#26223;&#20844;&#29702;&#30456;&#19968;&#33268;&#12290;&#20294;&#30830;&#23450;&#20855;&#20307;&#30340;&#32972;&#26223;&#20844;&#29702;&#30340;&#36873;&#25321;&#21644;&#21010;&#20998;&#65292;&#33267;&#23569;&#26377;&#26102;&#20505;&#65292;&#20165;&#20165;&#26159;&#20256;&#32479;&#30340;&#19968;&#31181;&#20570;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#21629;&#39064;&#27169;&#24577;&#36923;&#36753;&#30340;**&#36890;&#29992;&#29702;&#35770;**&#65292;&#20197;&#26356;&#21152;&#31283;&#20581;&#30340;&#26041;&#24335;&#35299;&#20915;&#19968;&#33268;&#24615;&#32467;&#26524;&#38382;&#39064;&#12290;&#36890;&#29992;&#29702;&#35770;&#20316;&#20026;&#32972;&#26223;&#30693;&#35782;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#20026;&#19968;&#33268;&#24615;&#30340;&#33539;&#30068;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#26412;&#25991;&#30340;&#32467;&#26524;&#21644;&#26041;&#27861;&#26377;&#21161;&#20110;&#38416;&#26126;&#35748;&#35782;&#35770;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#30340;&#33539;&#22260;&#21644;&#33021;&#21147;&#26469;&#35299;&#20915;&#19982;&#21028;&#26029;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#20013;&#27169;&#24577;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental question asked in modal logic is whether a given theory is consistent. But consistent with what? A typical way to address this question identifies a choice of background knowledge axioms (say, S4, D, etc.) and then shows the assumptions codified by the theory in question to be consistent with those background axioms. But determining the specific choice and division of background axioms is, at least sometimes, little more than tradition. This paper introduces **generic theories** for propositional modal logic to address consistency results in a more robust way. As building blocks for background knowledge, generic theories provide a standard for categorical determinations of consistency. We argue that the results and methods of this paper help to elucidate problems in epistemology and enjoy sufficient scope and power to have purchase on problems bearing on modalities in judgement, inference, and decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35748;&#35782;&#20027;&#20041;&#19977;&#27573;&#35770;&#30340;&#20960;&#31181;&#21464;&#20307;&#65292;&#38598;&#20013;&#22312;&#21253;&#21547;&#38750;&#24179;&#20961;&#20294;&#33258;&#28982;&#34920;&#36798;&#24335;&#30340;de re&#35299;&#37322;&#19978;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#36825;&#20123;&#36923;&#36753;&#30340;&#20960;&#20010;&#20844;&#29702;&#21270;&#65292;&#24182;&#32473;&#20986;&#20102;&#23436;&#22791;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2307.05043</link><description>&lt;p&gt;
&#35748;&#35782;&#20027;&#20041;&#19977;&#27573;&#35770;&#65306;&#31532;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
Epistemic Syllogistic: First Steps. (arXiv:2307.05043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35748;&#35782;&#20027;&#20041;&#19977;&#27573;&#35770;&#30340;&#20960;&#31181;&#21464;&#20307;&#65292;&#38598;&#20013;&#22312;&#21253;&#21547;&#38750;&#24179;&#20961;&#20294;&#33258;&#28982;&#34920;&#36798;&#24335;&#30340;de re&#35299;&#37322;&#19978;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#36825;&#20123;&#36923;&#36753;&#30340;&#20960;&#20010;&#20844;&#29702;&#21270;&#65292;&#24182;&#32473;&#20986;&#20102;&#23436;&#22791;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20122;&#37324;&#22763;&#22810;&#24503;&#20851;&#20110;&#27169;&#24577;&#19977;&#27573;&#35770;&#30340;&#35752;&#35770;&#24120;&#34987;&#35270;&#20026;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#30001;&#20110;&#21382;&#21490;&#21644;&#21746;&#23398;&#30340;&#20852;&#36259;&#32780;&#21463;&#21040;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#24403;&#20195;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#19968;&#38454;&#27169;&#24577;&#36923;&#36753;&#30340;&#33258;&#28982;&#29255;&#27573;&#65292;&#20540;&#24471;&#36827;&#34892;&#20840;&#38754;&#30340;&#25216;&#26415;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#33258;&#28982;&#36923;&#36753;&#35745;&#21010;&#30340;&#28789;&#24863;&#65292;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#35748;&#35782;&#32972;&#26223;&#19979;&#30340;&#20960;&#31181;&#21464;&#20307;&#27169;&#24577;&#19977;&#27573;&#35770;&#65292;&#20174;&#32780;&#21019;&#36896;&#20102;&#8220;&#35748;&#35782;&#20027;&#20041;&#19977;&#27573;&#35770;&#8221;&#36825;&#20010;&#26415;&#35821;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38598;&#20013;&#22312;&#28041;&#21450;&#38750;&#24179;&#20961;&#20294;&#33258;&#28982;&#34920;&#36798;&#24335;&#30340;&#35748;&#35782;&#20027;&#20041;&#19977;&#27573;&#35770;&#30340;de re&#35299;&#37322;&#19978;&#65292;&#20363;&#22914;&#8220;&#34987;&#35748;&#30693;&#20026;A&#30340;&#25152;&#26377;&#20107;&#29289;&#20063;&#34987;&#35748;&#30693;&#20026;&#38750;B&#8221;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35748;&#35782;&#20027;&#20041;&#24517;&#28982;&#19977;&#27573;&#35770;&#21450;&#20854;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;&#26356;&#22797;&#26434;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#36825;&#20123;&#36923;&#36753;&#30340;&#20960;&#20010;&#20844;&#29702;&#21270;&#65292;&#38468;&#24102;&#35777;&#26126;&#30340;&#23436;&#22791;&#24615;&#21487;&#33021;&#26159;&#29420;&#31435;&#30340;&#21033;&#30410;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aristotle's discussions on modal syllogistic have often been viewed as error-prone and have garnered significant attention in the literature due to historical and philosophical interests. However, from a contemporary standpoint, they also introduced natural fragments of first-order modal logic, warranting a comprehensive technical analysis. In this paper, drawing inspiration from the natural logic program, we propose and examine several variants of modal syllogistic within the epistemic context, thereby coining the term Epistemic Syllogistic. Specifically, we concentrate on the de re interpretation of epistemic syllogisms containing non-trivial yet natural expressions such as "all things known to be A are also known to be not B." We explore the epistemic apodeictic syllogistic and its extensions, which accommodate more complex terms. Our main contributions include several axiomatizations of these logics, with completeness proofs that may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#39064;&#36923;&#36753;&#25805;&#20316;&#30340;&#20248;&#21183;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#38544;&#24335;&#25512;&#29702;&#33021;&#21147;&#21644;&#23616;&#37096;&#26174;&#24335;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05036</link><description>&lt;p&gt;
&#20855;&#26377;&#22270;&#22686;&#24378;&#20449;&#24687;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural-Symbolic Recommendation with Graph-Enhanced Information. (arXiv:2307.05036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#39064;&#36923;&#36753;&#25805;&#20316;&#30340;&#20248;&#21183;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#38544;&#24335;&#25512;&#29702;&#33021;&#21147;&#21644;&#23616;&#37096;&#26174;&#24335;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#26159;&#19968;&#20010;&#20174;&#25968;&#25454;&#20013;&#24402;&#32435;&#32479;&#35745;&#30340;&#38382;&#39064;&#65292;&#20063;&#26159;&#19968;&#20010;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#35748;&#30693;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#38544;&#24335;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20687;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#19968;&#26679;&#65292;&#23427;&#20204;&#21482;&#20174;&#24863;&#30693;&#30340;&#35282;&#24230;&#23398;&#20064;&#21305;&#37197;&#27169;&#24335;&#12290;&#19968;&#20123;&#30740;&#31350;&#32773;&#20351;&#29992;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#20174;&#35748;&#30693;&#25512;&#29702;&#30340;&#35282;&#24230;&#23454;&#29616;&#25512;&#33616;&#39044;&#27979;&#65292;&#20294;&#36825;&#31181;&#25512;&#29702;&#26159;&#23616;&#37096;&#30340;&#65292;&#24573;&#35270;&#20102;&#20840;&#23616;&#33539;&#22260;&#20869;&#30340;&#38544;&#24335;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#39064;&#36923;&#36753;&#25805;&#20316;&#30340;&#20248;&#21183;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#38544;&#24335;&#25512;&#29702;&#33021;&#21147;&#21644;&#23616;&#37096;&#26174;&#24335;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#30456;&#37051;&#20132;&#20114;&#21407;&#21017;&#26500;&#24314;&#20102;&#19968;&#20010;&#29289;&#21697;-&#29289;&#21697;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21629;&#39064;&#36923;&#36753;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#20840;&#23616;&#33539;&#22260;&#20869;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation system is not only a problem of inductive statistics from data but also a cognitive task that requires reasoning ability. The most advanced graph neural networks have been widely used in recommendation systems because they can capture implicit structured information from graph-structured data. However, like most neural network algorithms, they only learn matching patterns from a perception perspective. Some researchers use user behavior for logic reasoning to achieve recommendation prediction from the perspective of cognitive reasoning, but this kind of reasoning is a local one and ignores implicit information on a global scale. In this work, we combine the advantages of graph neural networks and propositional logic operations to construct a neuro-symbolic recommendation model with both global implicit reasoning ability and local explicit logic reasoning ability. We first build an item-item graph based on the principle of adjacent interaction and use graph neural net
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#23398;&#20064;&#29575;&#34928;&#20943;&#12289;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#22312;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05025</link><description>&lt;p&gt;
&#21457;&#25381;&#27491;&#21017;&#21270;&#31574;&#30053;&#22312;&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05025
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#23398;&#20064;&#29575;&#34928;&#20943;&#12289;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#22312;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#22024;&#26434;&#35757;&#32451;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#21253;&#25324;&#22797;&#26434;&#30340;&#25216;&#26415;&#65292;&#22914;&#22122;&#22768;&#24314;&#27169;&#12289;&#26631;&#31614;&#26657;&#27491;&#21644;&#21327;&#21516;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#24120;&#29992;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#23398;&#20064;&#29575;&#34928;&#20943;&#12289;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#37319;&#29992;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#27604;&#22797;&#26434;&#30340;&#31639;&#27861;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#27491;&#21017;&#21270;&#31574;&#30053;&#22312;&#20043;&#21069;&#30340;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30740;&#31350;&#20013;&#24050;&#34987;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#40723;&#21169;&#37325;&#26032;&#35780;&#20272;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30340;&#22522;&#20934;&#65292;&#24182;&#20419;&#20351;&#37325;&#26032;&#32771;&#34385;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data. These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training. In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods. Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels. While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored. Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsider
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65288;FAM&#65289;&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#37322;&#27809;&#26377;FC&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.05017</link><description>&lt;p&gt;
&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65306;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification. (arXiv:2307.05017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65288;FAM&#65289;&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#37322;&#27809;&#26377;FC&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#35270;&#21270;&#22270;&#20687;&#19978;&#30340;&#21028;&#21035;&#21306;&#22495;&#65292;&#21487;&#20197;&#29702;&#35299;&#21644;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;CAM&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#24378;&#22823;&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#26356;&#21152;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;CAM&#30340;&#26041;&#27861;&#65288;&#22914;CAM&#12289;Grad-CAM&#21644;Relevance-CAM&#65289;&#21482;&#33021;&#29992;&#20110;&#35299;&#37322;&#20855;&#26377;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;CNN&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27809;&#26377;FC&#23618;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#20363;&#22914;&#23567;&#26679;&#26412;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#12289;&#23545;&#27604;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65288;FAM&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#37322;&#27809;&#26377;FC&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;FAM&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#20004;&#20010;&#22270;&#20687;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24471;&#21040;&#36890;&#36947;&#26435;&#37325;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decisions made by convolutional neural networks(CNN) can be understood and explained by visualizing discriminative regions on images. To this end, Class Activation Map (CAM) based methods were proposed as powerful interpretation tools, making the prediction of deep learning models more explainable, transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM, Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with fully-connected (FC) layers as a classifier. It is worth noting that many deep learning models classify images without FC layers, e.g., few-shot learning image classification, contrastive learning image classification, and image retrieval tasks. In this work, a post-hoc interpretation tool named feature activation map (FAM) is proposed, which can interpret deep learning models without FC layers as a classifier. In the proposed FAM algorithm, the channel-wise contribution weights are derived from the similarity scores between two image emb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25511;&#21046;&#19982;&#27010;&#29575;&#25512;&#29702;&#32467;&#21512;&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#25512;&#29702;&#25511;&#21046;&#20854;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05004</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25511;&#21046;&#20316;&#20026;&#27010;&#29575;&#25512;&#29702;&#30340;&#26032;&#20852;&#36890;&#20449;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning. (arXiv:2307.05004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25511;&#21046;&#19982;&#27010;&#29575;&#25512;&#29702;&#32467;&#21512;&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#25512;&#29702;&#25511;&#21046;&#20854;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#65292;&#23558;&#26032;&#20852;&#36890;&#20449;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#21160;&#20316;&#35268;&#21010;&#65292;&#31216;&#20026;&#25511;&#21046;&#20316;&#20026;&#25512;&#29702;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#21644;&#26681;&#25454;&#35268;&#21010;&#30340;&#21160;&#20316;&#36827;&#34892;&#20272;&#35745;&#30340;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#12290;&#36890;&#36807;&#36825;&#20123;&#28040;&#24687;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#21457;&#36865;&#20851;&#20110;&#20854;&#21160;&#20316;&#30340;&#20449;&#24687;&#65292;&#24182;&#20102;&#35299;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#20272;&#35745;&#30340;&#28040;&#24687;&#26469;&#25913;&#21464;&#20854;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;&#36825;&#31181;&#28040;&#24687;&#30340;&#25512;&#29702;&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#20449;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;Metropolis-Hasting&#21629;&#21517;&#28216;&#25103;&#26469;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#36890;&#36807;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#28040;&#24687;&#65292;&#20197;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning. The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions. Through these messages, each agent can send information about its actions and know information about the actions of another agent. Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks. This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game. Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#23454;&#29616;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#21482;&#26377;&#22122;&#22768;&#19987;&#23478;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21363;&#21487;&#25104;&#21151;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.04998</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#36827;&#34892;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selective Sampling and Imitation Learning via Online Regression. (arXiv:2307.04998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#23454;&#29616;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#21482;&#26377;&#22122;&#22768;&#19987;&#23478;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21363;&#21487;&#25104;&#21151;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#20027;&#21160;&#26597;&#35810;&#22024;&#26434;&#30340;&#19987;&#23478;&#26469;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#26080;&#22122;&#22768;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#32780;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#21482;&#33021;&#33719;&#24471;&#22024;&#26434;&#30340;&#19987;&#23478;&#21453;&#39304;&#26102;&#65292;&#20381;&#36182;&#32431;&#31163;&#32447;&#25968;&#25454;&#30340;&#31639;&#27861;&#65288;&#38750;&#20132;&#20114;&#24335;IL&#65289;&#34987;&#35777;&#26126;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;IL&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#37319;&#26679;&#26469;&#20027;&#21160;&#26597;&#35810;&#22024;&#26434;&#30340;&#19987;&#23478;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;&#20989;&#25968;&#31867;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#26032;&#36873;&#25321;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#22024;&#26434;&#19987;&#23478;&#21453;&#39304;&#30340;IL&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;IL&#31639;&#27861;&#26469;&#36827;&#34892;&#26377;&#38480;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.  Our algorithm for sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04996</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#22312;&#30452;&#25509;&#33829;&#38144;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28608;&#21457;&#20102;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#26469;&#25552;&#21319;&#23458;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#21160;&#26426;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#26381;&#21153;&#39046;&#22495;&#65292;&#20844;&#21496;&#21487;&#20197;&#36890;&#36807;&#21521;&#23458;&#25143;&#25552;&#20379;&#30456;&#20851;&#37329;&#34701;&#25991;&#31456;&#26469;&#22521;&#20859;&#20851;&#31995;&#65292;&#20419;&#36827;&#23458;&#25143;&#21442;&#19982;&#21644;&#20419;&#36827;&#30693;&#24773;&#30340;&#37329;&#34701;&#20915;&#31574;&#12290;&#23613;&#31649;&#19968;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#20197;&#25913;&#36827;&#20869;&#23481;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#36827;&#34892;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#23478;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#19968;&#32452;&#23458;&#25143;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#20351;&#29992;XGBoost&#31639;&#27861;&#26469;&#21521;&#23458;&#25143;&#25512;&#33616;&#25991;&#31456;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20174;&#32467;&#26500;&#21270;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65289;&#29983;&#25104;&#30340;KG&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;&#27169;&#22411;&#65292;&#21363;&#21333;&#35843;DBM&#65292;&#23427;&#20801;&#35768;&#27599;&#19968;&#23618;&#20855;&#26377;&#20219;&#24847;&#30340;&#33258;&#36830;&#25509;&#65292;&#20294;&#36890;&#36807;&#19968;&#31181;&#26041;&#24335;&#38480;&#21046;&#20102;&#26435;&#37325;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#21644;&#20840;&#23616;&#21807;&#19968;&#30340;&#22343;&#22330;&#19981;&#21160;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04990</link><description>&lt;p&gt;
&#21333;&#35843;&#30340;&#28145;&#24230;Boltzmann&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Monotone deep Boltzmann machines. (arXiv:2307.04990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04990
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;&#27169;&#22411;&#65292;&#21363;&#21333;&#35843;DBM&#65292;&#23427;&#20801;&#35768;&#27599;&#19968;&#23618;&#20855;&#26377;&#20219;&#24847;&#30340;&#33258;&#36830;&#25509;&#65292;&#20294;&#36890;&#36807;&#19968;&#31181;&#26041;&#24335;&#38480;&#21046;&#20102;&#26435;&#37325;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#21644;&#20840;&#23616;&#21807;&#19968;&#30340;&#22343;&#22330;&#19981;&#21160;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;Boltzmann&#26426;&#22120;(DBMs)&#26159;&#26368;&#26089;&#30740;&#31350;&#30340;"&#28145;&#24230;"&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#65292;&#23427;&#26159;&#30001;&#19968;&#20010;&#25551;&#36848;&#32593;&#32476;&#20013;&#25152;&#26377;&#21464;&#37327;/&#33410;&#28857;&#30340;&#21487;&#33021;&#24615;&#30340;&#25104;&#23545;&#33021;&#37327;&#20989;&#25968;&#25152;&#25511;&#21046;&#30340;&#22810;&#23618;&#27010;&#29575;&#27169;&#22411;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;DBMs&#36890;&#24120;&#20250;&#21463;&#21040;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#36890;&#36807;"&#38480;&#21046;&#24615;" Boltzmann&#26426;&#22120;(RBM)&#26550;&#26500;&#65288;&#19981;&#20801;&#35768;&#23618;&#38388;&#36830;&#25509;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36890;&#29992;&#30340;DBM&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#20854;&#20182;&#21487;&#33021;&#30340;&#35774;&#35745;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#65288;&#36817;&#20284;&#65289;&#25512;&#29702;&#65311;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;&#27169;&#22411;&#65292;&#21363;&#21333;&#35843;DBM&#65292;&#23427;&#20801;&#35768;&#27599;&#19968;&#23618;&#20855;&#26377;&#20219;&#24847;&#30340;&#33258;&#36830;&#25509;&#65292;&#20294;&#36890;&#36807;&#19968;&#31181;&#26041;&#24335;&#38480;&#21046;&#20102;&#26435;&#37325;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#21644;&#20840;&#23616;&#21807;&#19968;&#30340;&#22343;&#22330;&#19981;&#21160;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#35843;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference. In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference? In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#27969;&#34892;&#30149;&#27169;&#22411;&#20013;&#27169;&#25311;&#20102;&#20154;&#31867;&#34892;&#20026;&#65292;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#23637;&#31034;&#20102;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#19982;&#30495;&#23454;&#19990;&#30028;&#30456;&#20284;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#27969;&#34892;&#30149;&#26354;&#32447;&#30340;&#24179;&#22374;&#21270;&#12290;&#35813;&#30740;&#31350;&#21019;&#36896;&#20102;&#25913;&#36827;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#28508;&#21147;&#65292;&#20026;&#34920;&#31034;&#20154;&#31867;&#24605;&#32500;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2307.04986</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#36827;&#34892;&#27969;&#34892;&#30149;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Epidemic Modeling with Generative Agents. (arXiv:2307.04986v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#27969;&#34892;&#30149;&#27169;&#22411;&#20013;&#27169;&#25311;&#20102;&#20154;&#31867;&#34892;&#20026;&#65292;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#23637;&#31034;&#20102;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#19982;&#30495;&#23454;&#19990;&#30028;&#30456;&#20284;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#27969;&#34892;&#30149;&#26354;&#32447;&#30340;&#24179;&#22374;&#21270;&#12290;&#35813;&#30740;&#31350;&#21019;&#36896;&#20102;&#25913;&#36827;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#28508;&#21147;&#65292;&#20026;&#34920;&#31034;&#20154;&#31867;&#24605;&#32500;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#20307;&#23618;&#38754;&#24314;&#27169;&#33539;&#24335;&#65292;&#20197;&#35299;&#20915;&#23558;&#20154;&#31867;&#34892;&#20026;&#32435;&#20837;&#27969;&#34892;&#30149;&#27169;&#22411;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#27969;&#34892;&#30149;&#27169;&#22411;&#20013;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#36827;&#34892;&#33258;&#20027;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#36890;&#36807;&#21508;&#31181;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#27169;&#20223;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20026;&#65292;&#22914;&#29983;&#30149;&#26102;&#36827;&#34892;&#38548;&#31163;&#65292;&#30149;&#20363;&#22686;&#21152;&#26102;&#36827;&#34892;&#33258;&#25105;&#38548;&#31163;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26234;&#33021;&#20307;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;&#36817;&#26399;&#27969;&#34892;&#30149;&#35266;&#23519;&#21040;&#30340;&#22810;&#27425;&#27874;&#21160;&#65292;&#28982;&#21518;&#26159;&#19968;&#27573;&#27969;&#34892;&#26399;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#20351;&#27969;&#34892;&#30149;&#26354;&#32447;&#24179;&#22374;&#21270;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#36827;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#34920;&#31034;&#20154;&#31867;&#22823;&#33041;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models. Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT. Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise. Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period. Moreover, the agents successfully flatten the epidemic curve. This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04964</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494; &#31532;&#19968;&#37096;&#20998;&#65306;PPO
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#34013;&#22270;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25104;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65288;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65289;&#21161;&#25163;&#12290;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#25104;&#20026;&#25903;&#25745;&#36825;&#19968;&#36861;&#27714;&#30340;&#20851;&#38190;&#25216;&#26415;&#33539;&#24335;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36335;&#32447;&#36890;&#24120;&#21253;&#25324;&#29992;&#20110;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12289;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#27169;&#22411;&#36755;&#20986;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20197;&#21450;&#29992;&#20110;&#25913;&#21892;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#36827;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#35774;&#35745;&#12289;&#29615;&#22659;&#20132;&#20114;&#21644;&#20195;&#29702;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#20877;&#21152;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35797;&#39564;&#25104;&#26412;&#24040;&#22823;&#65292;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#28608;&#21169;&#25216;&#26415;&#23545;&#40784;&#21644;LLMs&#30340;&#23433;&#20840;&#30528;&#38470;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#12290;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.04957</link><description>&lt;p&gt;
&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#20960;&#20046;&#24635;&#26159;&#23450;&#20041;&#20026;&#27839;&#36807;&#31243;&#20013;&#22870;&#21169;&#30340;\emph{&#32047;&#31215;}&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#20449;&#21644;&#32593;&#32476;&#39046;&#22495;&#20013;&#65292;&#30446;&#26631;&#24182;&#19981;&#33258;&#28982;&#22320;&#34920;&#36798;&#20026;&#22870;&#21169;&#30340;&#27714;&#21644;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#21508;&#31181;&#38382;&#39064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#38750;&#32047;&#31215;&#30446;&#26631;&#65292;&#25105;&#20204;&#29992;&#19982;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36816;&#31639;&#26367;&#25442;&#20102;&#36125;&#23572;&#26364;&#26356;&#26032;&#35268;&#21017;&#20013;&#30340;&#21407;&#22987;&#27714;&#21644;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24191;&#20041;&#36816;&#31639;&#24418;&#24335;&#30340;&#36275;&#22815;&#26465;&#20214;&#20197;&#21450;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision 
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;Transformer&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#12290;&#30456;&#27604;&#20110;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#24490;&#29615;Transformer&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#25104;&#21151;&#35299;&#20915;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.04895</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;Transformer&#23398;&#20064;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer. (arXiv:2307.04895v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04895
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;Transformer&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#12290;&#30456;&#27604;&#20110;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#24490;&#29615;Transformer&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#25104;&#21151;&#35299;&#20915;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSPs&#65289;&#26159;&#20851;&#20110;&#25214;&#21040;&#28385;&#36275;&#32473;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#21464;&#37327;&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22686;&#21152;&#24490;&#29615;&#24615;&#36136;&#30340;Transformer&#26469;&#23398;&#20064;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35299;&#20915;CSPs&#26159;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;Graph&#31070;&#32463;&#32593;&#32476;&#12289;SATNet&#21644;&#19968;&#20123;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#30001;&#20110;Transformer&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#30340;&#24490;&#29615;Transformer&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35270;&#35273;&#32422;&#26463;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#31163;&#25955;&#32422;&#26463;&#30340;&#28436;&#32462;&#30693;&#35782;&#26469;&#23454;&#29616;Transformer&#30340;&#24402;&#32435;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;CSPs&#30340;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformer to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer's inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.04893</link><description>&lt;p&gt;
&#36873;&#25321;&#22909;&#23545;&#25163;&#65306;&#22914;&#20309;&#25351;&#23548;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Local Learner (2L)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#20379;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#65292;&#20197;&#25351;&#23548;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25628;&#32034;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;&#20043;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#36845;&#20195;&#26368;&#20339;&#21709;&#24212;&#31639;&#27861;(IBR)&#65292;&#34394;&#26500;&#28216;&#25103;&#31639;&#27861;(FP)&#21644;&#21452;&#27491;&#20132;&#31639;&#27861;(DO)&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#25110;&#20250;&#28431;&#25481;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;2L&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#20197;&#25552;&#39640;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#28216;&#25103;&#20013;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;MicroRTS&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;2L&#23398;&#20064;&#21040;&#30340;&#21442;&#32771;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;IBR&#65292;FP&#21644;DO&#26356;&#24378;&#30340;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19968;&#22330;MicroRTS&#38182;&#26631;&#36187;&#65292;&#20854;&#20013;&#20351;&#29992;2L&#21512;&#25104;&#22120;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#26032;MicroRTS&#27604;&#36187;&#30340;&#32988;&#32773;&#65292;&#36825;&#20123;&#32988;&#32773;&#22343;&#20026;&#20154;&#31867;&#32534;&#31243;&#21592;&#32534;&#20889;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34913;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#24178;&#25200;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31867;&#22312;&#32447;&#24863;&#30693;&#31639;&#27861;&#26469;&#20943;&#36731;&#24178;&#25200;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04887</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#32531;&#35299;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34913;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#24178;&#25200;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31867;&#22312;&#32447;&#24863;&#30693;&#31639;&#27861;&#26469;&#20943;&#36731;&#24178;&#25200;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#24178;&#25200;&#22312;&#35768;&#22810;&#22522;&#20110;&#32593;&#32476;&#30340;&#23398;&#20064;&#31995;&#32479;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#23384;&#22312;&#35768;&#22810;&#20943;&#36731;&#24178;&#25200;&#30340;&#24314;&#35758;&#12290;&#22312;&#20811;&#26381;&#24178;&#25200;&#20043;&#21069;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;Fitted Q-Iteration&#21644;DQN&#31561;&#22522;&#20110;&#20540;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#24178;&#25200;&#30340;&#23450;&#20041;&#21644;&#26032;&#22411;&#24230;&#37327;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#24178;&#25200;&#24230;&#37327;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#26174;&#31034;&#20986;&#23427;&#19982;&#25511;&#21046;&#24615;&#33021;&#30340;&#19981;&#31283;&#23450;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#26032;&#24178;&#25200;&#24230;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#24120;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#26032;&#31185;&#23398;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20943;&#36731;&#24178;&#25200;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31867;&#25105;&#20204;&#31216;&#20026;&#22312;&#32447;&#24863;&#30693;&#31639;&#27861;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#24178;&#25200;&#65292;&#24182;&#19988;&#26681;&#25454;&#25105;&#20204;&#30340;&#24230;&#37327;&#26174;&#31034;&#23427;&#20204;&#20943;&#23569;&#20102;&#24178;&#25200;&#65292;&#24182;&#22312;&#20960;&#20010;&#32463;&#20856;&#30340;&#25511;&#21046;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. Before overcoming interference we must understand it better. In this work, we provide a definition and novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with instability in control performance, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms which mitigate interference. Lastly, we outline a class of algorithms which we call online-aware that are designed to mitigate interference, and show they do reduce interference according to our measure and that they improve stability and performance in several classic control environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04869</link><description>&lt;p&gt;
Fed-CPrompt: &#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#30340;&#23545;&#27604;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#65288;FCL&#65289;&#20174;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#26426;&#23494;&#25968;&#25454;&#38598;&#20013;&#36880;&#27493;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;FCL&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#23384;&#22312;&#22240;&#26080;&#27861;&#35775;&#38382;&#21382;&#21490;&#20219;&#21153;&#25968;&#25454;&#32780;&#23548;&#33268;&#20005;&#37325;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;Fed-CPrompt&#65292;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#20449;&#26041;&#24335;&#33719;&#24471;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#12290;Fed-CPrompt&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#65292;&#20197;&#20998;&#21035;&#22788;&#29702;FCL&#20013;&#30340;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;Fed-CPrompt&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26080;&#37325;&#22797;&#23398;&#20064;FCL&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#22312;&#20856;&#22411;&#30340;&#34892;&#36208;&#21644;&#36305;&#27493;&#36895;&#24230;&#33539;&#22260;&#20869;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20272;&#35745;&#27493;&#24577;&#65288;CFs&#65289;&#30340;&#26102;&#38388;&#31354;&#38388;&#20020;&#24202;&#29305;&#24449;&#65292;&#22914;&#27493;&#25968;&#21644;&#38271;&#24230;&#12289;&#27493;&#38271;&#12289;&#27493;&#39057;&#12289;&#27493;&#36895;&#21644;&#34892;&#36208;&#36317;&#31163;&#31561;&#65292;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#24335;&#21152;&#36895;&#35745;&#36827;&#34892;&#22522;&#20110;&#31038;&#21306;&#30340;&#31227;&#21160;&#24615;&#35780;&#20272;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#22797;&#26434;&#24615;&#21644;&#21487;&#29992;&#24615;&#12289;&#25104;&#26412;&#21644;&#20998;&#26512;&#26041;&#27861;&#23398;&#24341;&#36215;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#27492;&#31867;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#38382;&#39064;&#65306;&#33021;&#21542;&#20351;&#29992;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#26469;&#25552;&#21462;Duchenne&#32908;&#32905;&#33806;&#32553;&#65288;DMD&#65289;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#65288;TDs&#65289;&#24739;&#32773;&#22312;&#24191;&#27867;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20869;&#30340;&#27493;&#24577;CFs&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;15&#21517;DMD&#24739;&#20799;&#21644;15&#21517;TDs&#34987;&#35201;&#27714;&#22312;10MRW&#12289;25MRW&#12289;100MRW&#12289;6MWT&#21644;FW&#35780;&#20272;&#20013;&#20197;&#19968;&#31995;&#21015;&#27493;&#24577;&#36895;&#24230;&#36827;&#34892;&#30417;&#30563;&#24615;&#20020;&#24202;&#27979;&#35797;&#65292;&#21516;&#26102;&#20329;&#25140;&#25163;&#26426;&#22522;&#30784;&#21152;&#36895;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SHAP@k&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26469;&#35299;&#20915;Top-k&#29305;&#24449;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;Explore-m&#38382;&#39064;&#24182;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.04850</link><description>&lt;p&gt;
SHAP@k&#65306;&#39640;&#25928;&#19988;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#22320;&#35782;&#21035;Top-k&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features. (arXiv:2307.04850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SHAP@k&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26469;&#35299;&#20915;Top-k&#29305;&#24449;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;Explore-m&#38382;&#39064;&#24182;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SHAP&#26694;&#26550;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#37325;&#35201;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#21463;&#37329;&#34701;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Top-k&#35782;&#21035;&#38382;&#39064;&#65288;TkIP&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#20855;&#26377;&#26368;&#39640;SHAP&#20540;&#30340;k&#20010;&#29305;&#24449;&#12290;&#34429;&#28982;&#20219;&#20309;&#35745;&#31639;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;SHAP&#20540;&#30340;&#26041;&#27861;&#65288;&#22914;KernelSHAP&#21644;SamplingSHAP&#65289;&#37117;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;TkIP&#30340;&#35299;&#20915;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30446;&#26631;&#26159;&#22312;&#35299;&#20915;TkIP&#30340;&#32972;&#26223;&#19979;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;TkIP&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;Explore-m&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#38382;&#39064;&#19982;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30456;&#20851;&#30340;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#36825;&#31181;&#32852;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;MAB&#25991;&#29486;&#20013;&#30340;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65306;&#65288;1&#65289;&#26356;&#22909;&#30340;&#20572;&#27490;&#26465;&#20214;&#65288;&#20572;&#27490;&#37319;&#26679;&#65289;&#65292;&#35782;&#21035;PAC&#65288;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65289;&#20445;&#35777;&#24050;&#32463;&#28385;&#36275;&#65307;&#65288;2&#65289;&#19968;&#31181;&#36138;&#23146;&#37319;&#26679;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SHAP framework provides a principled method to explain the predictions of a model by computing feature importance. Motivated by applications in finance, we introduce the Top-k Identification Problem (TkIP), where the objective is to identify the k features with the highest SHAP values. While any method to compute SHAP values with uncertainty estimates (such as KernelSHAP and SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample inefficient. The goal of our work is to improve the sample efficiency of existing methods in the context of solving TkIP. Our key insight is that TkIP can be framed as an Explore-m problem--a well-studied problem related to multi-armed bandits (MAB). This connection enables us to improve sample efficiency by leveraging two techniques from the MAB literature: (1) a better stopping-condition (to stop sampling) that identifies when PAC (Probably Approximately Correct) guarantees have been met and (2) a greedy sampling scheme that judic
&lt;/p&gt;</description></item><item><title>SigOpt Mulch&#26159;&#19968;&#31181;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;SigOpt Mulch&#26159;&#8220;&#27169;&#22411;&#24863;&#30693;&#22411;&#8221;&#30340;&#65292;&#33021;&#22815;&#38024;&#23545;GBTs&#36827;&#34892;&#26356;&#20248;&#21270;&#30340;&#24615;&#33021;&#35843;&#25972;&#65292;&#24182;&#19988;&#26080;&#38656;&#39046;&#22495;&#30693;&#35782;&#65292;&#24110;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.04849</link><description>&lt;p&gt;
SigOpt Mulch: &#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees. (arXiv:2307.04849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04849
&lt;/p&gt;
&lt;p&gt;
SigOpt Mulch&#26159;&#19968;&#31181;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;SigOpt Mulch&#26159;&#8220;&#27169;&#22411;&#24863;&#30693;&#22411;&#8221;&#30340;&#65292;&#33021;&#22815;&#38024;&#23545;GBTs&#36827;&#34892;&#26356;&#20248;&#21270;&#30340;&#24615;&#33021;&#35843;&#25972;&#65292;&#24182;&#19988;&#26080;&#38656;&#39046;&#22495;&#30693;&#35782;&#65292;&#24110;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#26641;(GBTs)&#26159;&#30740;&#31350;&#20154;&#21592;&#12289;&#26426;&#22120;&#23398;&#20064;(ML)&#23454;&#36341;&#32773;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#26222;&#36941;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;&#35757;&#32451;GBTs&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36873;&#25321;&#36825;&#20123;&#36229;&#21442;&#25968;&#36890;&#24120;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#26368;&#36817;&#65292;ML&#31038;&#21306;&#25552;&#20513;&#36890;&#36807;&#40657;&#30418;&#20248;&#21270;&#26469;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#31995;&#32479;&#24212;&#29992;&#20110;&#35843;&#25972;GBTs&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31995;&#32479;&#19981;&#20855;&#22791;&#8220;&#27169;&#22411;&#24863;&#30693;&#24615;&#8221;&#65292;&#32780;&#26159;&#35774;&#35745;&#29992;&#20110;&#8220;&#36890;&#29992;&#8221;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;&#20248;&#21270;&#24615;&#33021;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#8220;&#39046;&#22495;&#30693;&#35782;&#8221;&#65292;&#27604;&#22914;&#36229;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#36873;&#25321;&#65292;&#36825;&#19982;&#40657;&#30418;&#20248;&#21270;&#26088;&#22312;&#25552;&#20379;&#30340;&#33258;&#21160;&#23454;&#39564;&#30456;&#24726;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SigOpt Mulch
&lt;/p&gt;
&lt;p&gt;
Gradient boosted trees (GBTs) are ubiquitous models used by researchers, machine learning (ML) practitioners, and data scientists because of their robust performance, interpretable behavior, and ease-of-use. One critical challenge in training GBTs is the tuning of their hyperparameters. In practice, selecting these hyperparameters is often done manually. Recently, the ML community has advocated for tuning hyperparameters through black-box optimization and developed state-of-the-art systems to do so. However, applying such systems to tune GBTs suffers from two drawbacks. First, these systems are not \textit{model-aware}, rather they are designed to apply to a \textit{generic} model; this leaves significant optimization performance on the table. Second, using these systems requires \textit{domain knowledge} such as the choice of hyperparameter search space, which is an antithesis to the automatic experimentation that black-box optimization aims to provide. In this paper, we present SigOp
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2307.04841</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04841
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#38656;&#35201;&#23398;&#20064;&#22312;&#21453;&#39304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#22810;&#20010;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36825;&#31181;&#32463;&#39564;&#19978;&#30340;&#25104;&#21151;&#65292;&#20173;&#28982;&#27809;&#26377;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#29992;&#20110;&#34920;&#31034;&#29366;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#23398;&#20064;&#21160;&#24577;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#22312;&#19968;&#20010;&#39640;&#26031;&#31561;&#25928;&#20551;&#35774;&#19979;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20854;&#20013;&#23545;&#38543;&#26426;&#36712;&#36857;&#30340;&#24179;&#22343;&#20540;&#34987;&#26367;&#25442;&#20026;&#26102;&#24577;&#30456;&#20851;&#30340;&#39640;&#26031;&#29305;&#24449;&#24179;&#22343;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#23545;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#36827;&#34892;&#23376;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25193;&#22823;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38480;&#21046;&#12289;&#20260;&#23475;&#21644;&#39118;&#38505;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#20851;&#20110;AI&#30340;&#22840;&#22823;&#28818;&#20316;&#21644;&#35823;&#35299;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#19968;&#20123;&#23545;AI&#25216;&#26415;&#30340;&#38169;&#35823;&#35748;&#35782;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#27880;&#24847;&#30001;&#20110;&#36825;&#20123;&#38480;&#21046;&#32780;&#20135;&#29983;&#30340;&#23454;&#38469;&#20260;&#23475;&#12290;</title><link>http://arxiv.org/abs/2307.04821</link><description>&lt;p&gt;
&#25193;&#22823;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#12289;&#20260;&#23475;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Amplifying Limitations, Harms and Risks of Large Language Models. (arXiv:2307.04821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25193;&#22823;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38480;&#21046;&#12289;&#20260;&#23475;&#21644;&#39118;&#38505;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#20851;&#20110;AI&#30340;&#22840;&#22823;&#28818;&#20316;&#21644;&#35823;&#35299;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#19968;&#20123;&#23545;AI&#25216;&#26415;&#30340;&#38169;&#35823;&#35748;&#35782;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#27880;&#24847;&#30001;&#20110;&#36825;&#20123;&#38480;&#21046;&#32780;&#20135;&#29983;&#30340;&#23454;&#38469;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#35797;&#22270;&#36890;&#36807;&#19968;&#20010;&#23567;&#23567;&#30340;&#20030;&#21160;&#26469;&#25269;&#21046;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21450;&#20854;&#33021;&#21147;&#25152;&#24102;&#26469;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#28818;&#20316;&#65292;&#20197;&#21450;&#30001;&#27492;&#24102;&#26469;&#30340;&#31185;&#24187;&#24773;&#26223;&#30340;&#20998;&#25955;&#27880;&#24847;&#21147;&#12290;&#36825;&#20063;&#26377;&#21161;&#20110;&#37027;&#20123;&#22312;&#35813;&#39046;&#22495;&#20043;&#22806;&#30340;&#20154;&#20102;&#35299;&#19968;&#20123;AI&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#24403;&#21069;&#27969;&#34892;&#35805;&#35821;&#30340;&#32972;&#26223;&#19979;&#65292;AI&#40664;&#35748;&#20026;&#24847;&#21619;&#30528;&#22522;&#30784;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;&#29992;&#20110;&#21019;&#24314;ChatGPT&#30340;&#27169;&#22411;&#12290;&#36825;&#26412;&#36523;&#23601;&#26159;&#23545;&#30740;&#31350;&#39046;&#22495;&#22810;&#26679;&#24615;&#12289;&#28145;&#24230;&#21644;&#23481;&#37327;&#30340;&#26354;&#35299;&#65292;&#32780;&#30495;&#27491;&#20195;&#34920;AI&#39046;&#22495;&#30340;&#26159;&#30740;&#31350;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#12290;AI&#20316;&#20026;&#19968;&#38376;&#30740;&#31350;&#39046;&#22495;&#65292;&#33267;&#23569;&#20174;20&#19990;&#32426;50&#24180;&#20195;&#20197;&#26469;&#23601;&#23384;&#22312;&#20110;&#36719;&#20214;&#26500;&#20214;&#20013;&#12290;&#25105;&#20204;&#35797;&#22270;&#31361;&#20986;&#19968;&#20123;LLMs&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#24378;&#35843;&#30001;&#20110;&#36825;&#20123;&#23616;&#38480;&#24615;&#24050;&#32463;&#20986;&#29616;&#24182;&#23558;&#32487;&#32493;&#20986;&#29616;&#30340;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present this article as a small gesture in an attempt to counter what appears to be exponentially growing hype around Artificial Intelligence (AI) and its capabilities, and the distraction provided by the associated talk of science-fiction scenarios that might arise if AI should become sentient and super-intelligent. It may also help those outside of the field to become more informed about some of the limitations of AI technology. In the current context of popular discourse AI defaults to mean foundation and large language models (LLMs) such as those used to create ChatGPT. This in itself is a misrepresentation of the diversity, depth and volume of research, researchers, and technology that truly represents the field of AI. AI being a field of research that has existed in software artefacts since at least the 1950's. We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations. A
&lt;/p&gt;</description></item><item><title>S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.04804</link><description>&lt;p&gt;
S2vNTM: &#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04804
&lt;/p&gt;
&lt;p&gt;
S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#26469;&#35828;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#65288;1&#65289;&#24456;&#38590;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#65292;&#27604;&#22914;&#20851;&#38190;&#35789;&#65307;&#65288;2&#65289;&#35757;&#32451;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#65288;S2vNTM&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#12290;S2vNTM&#23558;&#19968;&#20123;&#31181;&#23376;&#20851;&#38190;&#35789;&#20316;&#20026;&#20027;&#39064;&#30340;&#36755;&#20837;&#12290;S2vNTM&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;S2vNTM&#22312;&#25552;&#20379;&#26377;&#38480;&#20851;&#38190;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;S2vNTM&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03913</link><description>&lt;p&gt;
&#22312;&#21457;&#23637;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20013;&#24212;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20197;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20849;&#21516;&#35748;&#30693;&#31995;&#32479;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21644;&#24212;&#29992;&#24050;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#23558;&#20316;&#20026;&#19968;&#21517;&#38431;&#21451;&#32780;&#19981;&#20165;&#20165;&#26159;&#24037;&#20855;&#19982;&#20154;&#31867;&#21327;&#20316;&#12290;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#27599;&#20010;&#25104;&#21592;&#30340;&#24050;&#30693;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#65292;&#24182;&#23558;&#32852;&#21512;&#24615;&#33021;&#25552;&#39640;&#21040;&#20219;&#20309;&#23454;&#20307;&#20043;&#19978;&#12290;2023&#24180;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25112;&#30053;&#35745;&#21010;&#26356;&#26032;&#35748;&#35782;&#21040;&#65292;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29420;&#31435;&#24615;&#33021;&#30340;&#30740;&#31350;&#35745;&#21010;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#24517;&#39035;&#25552;&#20379;&#30340;&#21151;&#33021;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#33021;&#20316;&#20026;&#20154;&#31867;&#30340;&#38431;&#21451;&#23384;&#22312;&#20105;&#35758;&#12290;&#20027;&#35201;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#37319;&#29992;"&#21327;&#20316;"&#33539;&#24335;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03419</link><description>&lt;p&gt;
QI2 -- &#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#22823;&#25968;&#25454;&#30340;&#22686;&#38271;&#24433;&#21709;&#21644;&#20998;&#24067;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#27431;&#27954;&#22996;&#21592;&#20250;&#35745;&#21010;&#30340;AI&#27861;&#26696;&#20026;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24066;&#22330;&#25512;&#20986;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#39564;&#35777;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#12290;&#36890;&#36807;&#23567;&#20363;&#23376;&#25968;&#25454;&#38598;&#20171;&#32461;&#21644;&#35299;&#37322;&#20102;&#35813;&#27010;&#24565;&#21644;&#20248;&#21183;&#12290;&#22914;&#20309;&#24212;&#29992;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#25163;&#20889;&#25968;&#23383;&#30340;&#30693;&#21517;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02738</link><description>&lt;p&gt;
RecallM:&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#23558;&#20026;&#36830;&#32493;&#23398;&#20064;&#12289;&#22797;&#26434;&#25512;&#29702;&#21644;&#23398;&#20064;&#24207;&#21015;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#25171;&#19979;&#22522;&#30784;&#12290;&#21019;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#19987;&#27880;&#20110;&#20026;AGI&#31995;&#32479;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#23637;&#31034;&#20102;RecallM&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23427;&#25552;&#20379;&#30340;&#25913;&#36827;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16177</link><description>&lt;p&gt;
&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65306;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16177
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#12290;&#23427;&#30340;&#21147;&#37327;&#12289;&#33539;&#22260;&#21644;&#35268;&#27169;&#23558;&#36229;&#36234;&#31185;&#23398;&#65292;&#25104;&#20026;&#20419;&#20351;&#30693;&#35782;&#21457;&#29616;&#24182;&#25913;&#21464;&#19990;&#30028;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#21644;&#23450;&#20041;&#23427;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20854;&#28508;&#21147;&#21644;&#31649;&#29702;&#20854;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#33258;1962&#24180;&#20197;&#26469;&#32531;&#24930;&#21457;&#23637;&#65292;&#24182;&#19988;&#33258;2000&#24180;&#20197;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#23427;&#26159;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26159;21&#19990;&#32426;&#26368;&#27963;&#36291;&#12289;&#26368;&#24378;&#22823;&#21644;&#21457;&#23637;&#26368;&#24555;&#30340;&#21019;&#26032;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#20215;&#20540;&#12289;&#21147;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#23427;&#27491;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#25968;&#25454;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#21253;&#21547;&#20102;&#26080;&#25968;&#20851;&#20110;&#25968;&#25454;&#31185;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#35768;&#22810;&#23450;&#20041;&#26159;&#29420;&#31435;&#30340;&#12289;&#24212;&#29992;&#29305;&#23450;&#30340;&#12289;&#30456;&#20114;&#19981;&#23436;&#25972;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#25968;&#25454;&#31185;&#23398;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#22810;&#37325;&#23450;&#20041;&#25361;&#25112;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
&lt;/p&gt;</description></item><item><title>BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.16015</link><description>&lt;p&gt;
BayesFlow: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16015
&lt;/p&gt;
&lt;p&gt;
BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#25512;&#26029;&#28041;&#21450;&#19968;&#31995;&#21015;&#35745;&#31639;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#12289;&#39564;&#35777;&#21644;&#20174;&#27010;&#29575;&#27169;&#22411;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#21407;&#21017;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#20013;&#30340;&#20856;&#22411;&#38382;&#39064;&#21253;&#25324;&#36817;&#20284;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#22411;&#31867;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;&#21516;&#19968;&#36807;&#31243;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Python&#24211;BayesFlow&#65292;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#35757;&#32451;&#24050;&#24314;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#25674;&#36824;&#25968;&#25454;&#21387;&#32553;&#21644;&#25512;&#26029;&#12290;&#22312;BayesFlow&#20013;&#23454;&#29616;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#37325;&#29992;&#20110;&#27169;&#22411;&#30340;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#30001;&#20110;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#20960;&#20046;&#21363;&#26102;&#22320;&#25191;&#34892;&#25512;&#26029;&#65292;&#22240;&#27492;&#21069;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24456;&#24555;&#23601;&#33021;&#22815;&#25674;&#36824;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01147</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#24615;&#32422;&#26463;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#24378;&#21147;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25903;&#25345;&#20844;&#24179;&#24615;&#65292;&#24182;&#22686;&#21152;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#32463;&#20856;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#32780;&#24448;&#24448;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38519;&#20837;&#19981;&#33391;&#23616;&#37096;&#26368;&#20248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;MM&#32593;&#32476;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#29992;&#20005;&#26684;&#36882;&#22686;&#30340;&#24179;&#28369;&#38750;&#32447;&#24615;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#27169;&#22359;&#32487;&#25215;&#20102;MM&#26550;&#26500;&#30340;&#28176;&#36817;&#36924;&#36817;&#24615;&#36136;&#12290;&#23427;&#21487;&#20197;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#21333;&#35843;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#65292;SMM&#27169;&#22359;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#35745;&#31639;&#38656;&#27714;&#20063;&#35201;&#23569;&#24471;&#22810;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#19982;&#26367;&#20195;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#24471;&#26356;&#20026;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#19982;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.15066</link><description>&lt;p&gt;
GPT4Graph&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65311;&#19968;&#39033;&#23454;&#35777;&#35780;&#20272;&#19982;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. (arXiv:2305.15066v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#19982;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#24050;&#25104;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22270;&#25968;&#25454;&#26080;&#22788;&#19981;&#22312;&#65292;&#26159;AGI&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#27969;&#34892;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#36890;&#24120;&#21253;&#25324;&#19968;&#20123;&#31639;&#27861;&#32452;&#20214;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#19968;&#20123;&#19982;&#22270;&#25968;&#25454;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21462;&#24471;&#19968;&#23450;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23427;&#20204;&#22312;&#26356;&#24191;&#27867;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;10&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22270;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#19981;&#20165;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#29702;&#35299;&#26041;&#38754;&#30340;&#24403;&#21069;&#38480;&#21046;&#65292;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06470</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#22833;&#36133;&#21450;&#20854;&#22312;&#26816;&#27979;Deepfakes&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#36896;&#20986;&#36924;&#30495;&#30340;&#24433;&#20687;&#30340;&#33021;&#21147;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#24230;&#65292;&#36825;&#20351;&#24471;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24456;&#38590;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#32570;&#38519;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31867;&#12290;&#36890;&#36807;&#20102;&#35299;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;&#20170;&#22825;&#31038;&#20250;&#20013;Deepfakes&#30340;&#26222;&#36941;&#23384;&#22312;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#23427;&#20204;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of image and video generation models to create photorealistic images has reached unprecedented heights, making it difficult to distinguish between real and fake images in many cases. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting deep fakes. The prevalence of deep fakes in today's society is a serious concern, and our findings can help mitigate their negative impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.02168</link><description>&lt;p&gt;
I2I: &#29992;&#25913;&#36827;&#30340;&#30693;&#35782;&#21021;&#22987;&#21270;&#36716;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25509;&#22120;&#26159;&#24310;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#27169;&#22359;&#38169;&#22833;&#20102;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Improvise to Initialize (I2I) &#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#65292;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102; I2I &#22312; CLiMB&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992; I2I &#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#22987;&#32456;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20419;&#36827;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#36827;&#30340; AdapterFusion&#65292;I2I &#20063;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#32780;&#19981;&#20135;&#29983;&#30456;&#20851;&#30340;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#20849;&#21516;&#20449;&#24565;&#65292;&#20173;&#28982;&#21487;&#20197;&#21457;&#29983;&#33258;&#28982;&#30340;&#32852;&#21512;&#34892;&#20026;&#12290;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#20849;&#21516;&#20449;&#24565;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#19968;&#31181;&#34987;&#35777;&#26126;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#32852;&#21512;&#34892;&#20026;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#26356;&#23481;&#26131;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.07185</link><description>&lt;p&gt;
&#32852;&#21512;&#34892;&#20026;&#21644;&#20849;&#21516;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Joint Behavior and Common Belief. (arXiv:2303.07185v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#20849;&#21516;&#20449;&#24565;&#65292;&#20173;&#28982;&#21487;&#20197;&#21457;&#29983;&#33258;&#28982;&#30340;&#32852;&#21512;&#34892;&#20026;&#12290;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#20849;&#21516;&#20449;&#24565;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#19968;&#31181;&#34987;&#35777;&#26126;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#32852;&#21512;&#34892;&#20026;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#26356;&#23481;&#26131;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;25&#24180;&#20013;&#65292;&#20849;&#21516;&#20449;&#24565;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#32852;&#21512;&#34892;&#20026;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20294;&#36825;&#24182;&#19981;&#23436;&#20840;&#27491;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#23637;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#20849;&#21516;&#20449;&#24565;&#65292;&#20173;&#28982;&#21487;&#20197;&#21457;&#29983;&#33258;&#28982;&#30340;&#32852;&#21512;&#34892;&#20026;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21487;&#20197;&#23548;&#33268;&#32852;&#21512;&#34892;&#20026;&#30340;&#20849;&#21516;&#20449;&#24565;&#30340;&#21464;&#20307;&#65292;&#21363;&#20351;&#27809;&#26377;&#23454;&#29616;&#26631;&#20934;&#30340;&#20849;&#21516;&#20449;&#24565;&#65292;&#20854;&#20013;&#19968;&#31181;&#34987;&#35777;&#26126;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#32852;&#21512;&#34892;&#20026;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#20247;&#25152;&#21608;&#30693;&#65292;&#23454;&#38469;&#19978;&#24456;&#38590;&#23454;&#29616;&#20849;&#21516;&#20449;&#24565;&#65292;&#32780;&#36825;&#20123;&#21464;&#20307;&#26356;&#23481;&#26131;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For over 25 years, common belief has been widely viewed as necessary for joint behavior. But this is not quite correct. We show by example that what can naturally be thought of as joint behavior can occur without common belief. We then present two variants of common belief that can lead to joint behavior, even without standard common belief ever being achieved, and show that one of them, action-stamped common belief, is in a sense necessary and sufficient for joint behavior. These observations are significant because, as is well known, common belief is quite difficult to achieve in practice, whereas these variants are more easily achievable.
&lt;/p&gt;</description></item><item><title>&#25197;&#26354;-&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25439;&#22833;&#20989;&#25968;&#25552;&#21462;&#22833;&#30495;&#19981;&#21464;&#34920;&#31034;&#24182;&#36807;&#28388;&#25481;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#25209;&#27425;&#22823;&#23567;&#30340;&#20381;&#36182;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#23545;&#23453;&#36149;&#30340;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#36827;&#34892;&#35299;&#32544;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#22686;&#24378;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05066</link><description>&lt;p&gt;
&#25197;&#26354;-&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05066
&lt;/p&gt;
&lt;p&gt;
&#25197;&#26354;-&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25439;&#22833;&#20989;&#25968;&#25552;&#21462;&#22833;&#30495;&#19981;&#21464;&#34920;&#31034;&#24182;&#36807;&#28388;&#25481;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#25209;&#27425;&#22823;&#23567;&#30340;&#20381;&#36182;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#23545;&#23453;&#36149;&#30340;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#36827;&#34892;&#35299;&#32544;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#22686;&#24378;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20197;&#20854;&#22312;&#34920;&#31034;&#23398;&#20064;&#21644;&#21508;&#31181;&#19979;&#28216;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#26368;&#36817;&#65292;&#27491;&#23545;&#23545;&#27604;&#23398;&#20064;&#65288;POCL&#65289;&#22312;&#26080;&#38656;&#26500;&#24314;&#27491;&#36127;&#35757;&#32451;&#38598;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#38477;&#20302;&#23545;&#25209;&#27425;&#22823;&#23567;&#30340;&#20381;&#36182;&#26469;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#12290;POCL&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#25439;&#22833;&#20989;&#25968;&#25552;&#21462;&#22833;&#30495;&#19981;&#21464;&#34920;&#31034;&#65288;DIR&#65289;&#65292;&#35813;&#34920;&#31034;&#25551;&#36848;&#20102;&#21463;&#19981;&#21516;&#22833;&#30495;&#24433;&#21709;&#30340;&#27491;&#23545;&#34920;&#31034;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#38544;&#24335;&#22320;&#20351;&#27169;&#22411;&#33021;&#22815;&#28388;&#38500;&#25110;&#24573;&#30053;&#21463;&#19981;&#21516;&#22833;&#30495;&#24433;&#21709;&#30340;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#65288;DVR&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;POCL&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#24378;&#21046;&#25191;&#34892;&#26377;&#20215;&#20540;&#30340;DVR&#30340;&#35299;&#32544;&#21644;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;POCL&#26041;&#27861;&#23545;&#22686;&#24378;&#31574;&#30053;&#24456;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single loss function to extract the distortion invariant representation (DIR) which describes the proximity of positive-pair representations affected by different distortions. This loss function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, existing POCL methods do not explicitly enforce the disentanglement and exploitation of the actually valuable DVR. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a nov
&lt;/p&gt;</description></item><item><title>SAINE&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#36719;&#20214;&#30340;&#31185;&#23398;&#27880;&#37322;&#21644;&#25512;&#29702;&#24341;&#25806;&#65292;&#21487;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#23398;&#26415;&#20986;&#29256;&#29289;&#39046;&#22495;&#26377;&#24212;&#29992;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#33021;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#20998;&#31867;&#36807;&#31243;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#31185;&#23398;&#30740;&#31350;&#30340;&#29702;&#35299;&#12290;&#24895;&#24847;&#19982;&#31185;&#23398;&#30028;&#21512;&#20316;&#21644;&#25910;&#38598;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2302.14468</link><description>&lt;p&gt;
SAINE: &#31185;&#23398;&#30740;&#31350;&#30340;&#31185;&#23398;&#27880;&#37322;&#19982;&#25512;&#29702;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
SAINE: Scientific Annotation and Inference Engine of Scientific Research. (arXiv:2302.14468v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14468
&lt;/p&gt;
&lt;p&gt;
SAINE&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#36719;&#20214;&#30340;&#31185;&#23398;&#27880;&#37322;&#21644;&#25512;&#29702;&#24341;&#25806;&#65292;&#21487;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#23398;&#26415;&#20986;&#29256;&#29289;&#39046;&#22495;&#26377;&#24212;&#29992;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#33021;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#20998;&#31867;&#36807;&#31243;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#31185;&#23398;&#30740;&#31350;&#30340;&#29702;&#35299;&#12290;&#24895;&#24847;&#19982;&#31185;&#23398;&#30028;&#21512;&#20316;&#21644;&#25910;&#38598;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SAINE&#65292;&#19968;&#31181;&#22522;&#20110;&#19968;&#32452;&#26631;&#20934;&#24320;&#28304;&#36719;&#20214;&#65288;&#22914;Label Studio&#21644;MLflow&#65289;&#30340;&#31185;&#23398;&#27880;&#37322;&#21644;&#25512;&#29702;&#24341;&#25806;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27880;&#37322;&#24341;&#25806;&#21487;&#20197;&#26377;&#21161;&#20110;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20998;&#31867;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#20851;&#20110;&#23618;&#27425;&#23398;&#31185;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#22312;&#20102;&#35299;&#23398;&#26415;&#20986;&#29256;&#29289;&#39046;&#22495;&#20013;&#20351;&#29992;SAINE&#36827;&#34892;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#32467;&#26524;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#31995;&#32479;&#30340;&#24110;&#21161;&#19979;&#25910;&#38598;&#21040;&#30340;&#29992;&#25143;&#36755;&#20837;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#20998;&#31867;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#20419;&#36827;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#31185;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21644;&#25512;&#29702;&#24341;&#25806;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25903;&#25345;&#19979;&#28216;&#20803;&#31185;&#23398;&#39033;&#30446;&#12290;&#25105;&#20204;&#27426;&#36814;&#31185;&#23398;&#30028;&#22312;&#36825;&#20123;&#39033;&#30446;&#19978;&#36827;&#34892;&#21512;&#20316;&#21644;&#21453;&#39304;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#20197;&#22312;https://youtu.be/yToO-G9YQK4&#20013;&#35775;&#38382;&#12290;&#23454;&#26102;&#28436;&#31034;&#32593;&#31449;&#21487;&#22312;https://app.heartex&#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SAINE, an Scientific Annotation and Inference ENgine based on a set of standard open-source software, such as Label Studio and MLflow. We show that our annotation engine can benefit the further development of a more accurate classification. Based on our previous work on hierarchical discipline classifications, we demonstrate its application using SAINE in understanding the space for scholarly publications. The user study of our annotation results shows that user input collected with the help of our system can help us better understand the classification process. We believe that our work will help to foster greater transparency and better understand scientific research. Our annotation and inference engine can further support the downstream meta-science projects. We welcome collaboration and feedback from the scientific community on these projects. The demonstration video can be accessed from https://youtu.be/yToO-G9YQK4. A live demo website is available at https://app.heartex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.00390</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;"Web of Science"&#20013;&#23545;&#30740;&#31350;&#39046;&#22495;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification of Research Fields in the "Web of Science" Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25277;&#35937;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#33258;&#21160;&#20998;&#31867;&#21040;&#19977;&#32423;&#23618;&#27425;&#26631;&#31614;&#38598;&#65288;&#23398;&#31185;&#12289;&#39046;&#22495;&#12289;&#23376;&#39046;&#22495;&#65289;&#20013;&#65292;&#20197;&#22810;&#31867;&#21035;&#35774;&#32622;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25991;&#31456;&#30340;&#30693;&#35782;&#29983;&#20135;&#21644;&#24341;&#29992;&#30340;&#24433;&#21709;&#65292;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#25152;&#36848;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36825;&#20123;&#27963;&#21160;&#34987;&#24402;&#20026;&#22810;&#20010;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#31995;&#32479;&#22312;Microsoft Academic Graph (&#29256;&#26412;2018-05-17)&#30340;160 million&#20221;&#25688;&#35201;&#29255;&#27573;&#20013;&#21306;&#20998;&#20102;44&#20010;&#23398;&#31185;&#12289;718&#20010;&#39046;&#22495;&#21644;1,485&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#20197;&#27169;&#22359;&#21270;&#21644;&#20998;&#24067;&#24335;&#26041;&#24335;&#36827;&#34892;&#25209;&#37327;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21644;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#27169;&#22411;&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#24418;&#22120;&#65289;&#20013;&#36827;&#34892;&#20102;3,140&#27425;&#23454;&#39564;&#12290;&#20998;&#31867;&#20934;&#30830;&#29575;&gt; 90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a hierarchical classification system that automatically categorizes a scholarly publication using its abstract into a three-tier hierarchical label set (discipline, field, subfield) in a multi-class setting. This system enables a holistic categorization of research activities in the mentioned hierarchy in terms of knowledge production through articles and impact through citations, permitting those activities to fall into multiple categories. The classification system distinguishes 44 disciplines, 718 fields and 1,485 subfields among 160 million abstract snippets in Microsoft Academic Graph (version 2018-05-17). We used batch training in a modularized and distributed fashion to address and allow for interdisciplinary and interfield classifications in single-label and multi-label settings. In total, we have conducted 3,140 experiments in all considered models (Convolutional Neural Networks, Recurrent Neural Networks, Transformers). The classification accuracy is &gt; 90%
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#23545;&#23398;&#20064;&#21040;&#30340;&#33016;&#37096; X &#20809;&#29255;&#24322;&#24120;&#26816;&#27979;&#30340;&#23402;&#29983;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#32452;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#27867;&#21270;&#25928;&#26524;&#30340;&#40065;&#26834;&#34920;&#31034;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12636</link><description>&lt;p&gt;
&#25506;&#31350;&#29992;&#20110;&#33016;&#37096; X &#20809;&#29255;&#30340;&#23402;&#29983;&#34920;&#31034;&#23398;&#20064;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays. (arXiv:2301.12636v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#23545;&#23398;&#20064;&#21040;&#30340;&#33016;&#37096; X &#20809;&#29255;&#24322;&#24120;&#26816;&#27979;&#30340;&#23402;&#29983;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#32452;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#27867;&#21270;&#25928;&#26524;&#30340;&#40065;&#26834;&#34920;&#31034;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22686;&#24378;&#23545;&#20110;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#33258;&#28982;&#22270;&#20687;&#30340;&#22686;&#24378;&#31574;&#30053;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#21307;&#23398;&#22270;&#20687;&#19982;&#33258;&#28982;&#22270;&#20687;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#28165;&#26970;&#22312;&#23402;&#29983;&#34920;&#31034;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#22686;&#24378;&#31574;&#30053;&#26159;&#21542;&#36866;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#65292;&#20197;&#21450;&#36866;&#29992;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#26041;&#27861;&#23545;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;MIMIC-CXR&#12289;CheXpert &#21644; VinDR-CXR&#65289;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#33016;&#37096; X &#20809;&#29255;&#24322;&#24120;&#26816;&#27979;&#30340;&#23402;&#29983;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#12289;&#24494;&#35843;&#12289;&#38646;&#26679;&#26412;&#36801;&#31227;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#23454;&#39564;&#26469;&#30740;&#31350;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12313</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#32570;&#22833;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#12290;&#26368;&#36817;&#65292;Arakelyan&#31561;&#20154;&#65288;2021&#65289;&#65307;Minervini&#31561;&#20154;&#65288;2022&#65289;&#34920;&#26126;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#20063;&#21487;&#20197;&#29992;&#20110;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65306;&#20182;&#20204;&#30340;&#36830;&#32493;&#26597;&#35810;&#20998;&#35299;&#65288;CQD&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#26597;&#35810;&#65292;&#20351;&#29992;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22238;&#31572;&#24182;&#36890;&#36807;t-&#33539;&#25968;&#26469;&#32858;&#21512;&#20854;&#20998;&#25968;&#65292;&#20197;&#23545;&#27599;&#20010;&#22797;&#26434;&#26597;&#35810;&#30340;&#31572;&#26696;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;CQD&#19981;&#22788;&#29702;&#21542;&#23450;&#24182;&#19988;&#20165;&#20351;&#29992;&#21407;&#23376;&#35757;&#32451;&#26597;&#35810;&#30340;&#35757;&#32451;&#20449;&#21495;&#65306;&#22312;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26399;&#38388;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#27809;&#26377;&#36890;&#36807;&#27169;&#31946;&#36923;&#36753;t-&#33539;&#25968;&#36827;&#34892;&#26657;&#20934;&#20197;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#36825;&#20010;&#26032;&#32452;&#20214;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#27861;&#22312;&#22797;&#26434;&#26597;&#35810;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propa
&lt;/p&gt;</description></item><item><title>ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.10343</link><description>&lt;p&gt;
ClimaX:&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10343
&lt;/p&gt;
&lt;p&gt;
ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#36825;&#26679;&#30340;&#25968;&#20540;&#27169;&#22411;&#22312;&#27169;&#25311;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#22823;&#27668;&#29616;&#35937;&#26102;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#33021;&#26144;&#23556;&#26469;&#30452;&#25509;&#35299;&#20915;&#19979;&#28216;&#39044;&#27979;&#25110;&#25237;&#23556;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#20351;&#29992;&#20026;&#29305;&#23450;&#26102;&#31354;&#20219;&#21153;&#31574;&#21010;&#21644;&#21516;&#36136;&#21270;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#20540;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#28436;&#31034;&#20102;ClimaX&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#20307;&#38544;&#31169;&#24674;&#22797;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#25968;&#25454;&#20849;&#20139;&#30340;&#21327;&#21516;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#20849;&#20139;&#21327;&#35843;&#21487;&#20197;&#23454;&#29616;&#23545;&#38544;&#31169;&#30340;&#26174;&#33879;&#24674;&#22797;&#65292;&#24182;&#24102;&#26469;&#21452;&#36194;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.05995</link><description>&lt;p&gt;
&#38598;&#20307;&#38544;&#31169;&#24674;&#22797;&#65306;&#36890;&#36807;&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#25968;&#25454;&#20849;&#20139;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Collective Privacy Recovery: Data-sharing Coordination via Decentralized Artificial Intelligence. (arXiv:2301.05995v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#20307;&#38544;&#31169;&#24674;&#22797;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#25968;&#25454;&#20849;&#20139;&#30340;&#21327;&#21516;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#20849;&#20139;&#21327;&#35843;&#21487;&#20197;&#23454;&#29616;&#23545;&#38544;&#31169;&#30340;&#26174;&#33879;&#24674;&#22797;&#65292;&#24182;&#24102;&#26469;&#21452;&#36194;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20307;&#38544;&#31169;&#25439;&#22833;&#21464;&#25104;&#20102;&#19968;&#20010;&#24040;&#22823;&#30340;&#38382;&#39064;&#65292;&#23545;&#20010;&#20154;&#33258;&#30001;&#21644;&#27665;&#20027;&#26500;&#25104;&#20102;&#32039;&#24613;&#23041;&#32961;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#26159;&#21542;&#20934;&#22791;&#22909;&#23558;&#20010;&#20154;&#25968;&#25454;&#35270;&#20026;&#31232;&#32570;&#36164;&#28304;&#65292;&#24182;&#26681;&#25454;&#8220;&#23613;&#21487;&#33021;&#23569;&#65292;&#23613;&#21487;&#33021;&#22810;&#8221;&#30340;&#21407;&#21017;&#20849;&#20139;&#25968;&#25454;&#65311;&#25105;&#20204;&#20551;&#35774;&#65292;&#22914;&#26524;&#19968;&#20010;&#20010;&#20307;&#32676;&#20307;&#65288;&#25968;&#25454;&#38598;&#20307;&#65289;&#21327;&#35843;&#20849;&#20139;&#26368;&#23569;&#25968;&#25454;&#65292;&#20197;&#28385;&#36275;&#22312;&#32447;&#26381;&#21153;&#30340;&#25152;&#38656;&#36136;&#37327;&#65292;&#23558;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#38544;&#31169;&#24674;&#22797;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#21644;&#25193;&#23637;&#22797;&#26434;&#30340;&#38598;&#20307;&#38544;&#31169;&#24674;&#22797;&#23433;&#25490;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#19968;&#20010;&#20005;&#35880;&#30340;&#39640;&#24230;&#36924;&#30495;&#30340;&#23454;&#39564;&#20013;&#27604;&#36739;&#20102;&#24577;&#24230;&#12289;&#20869;&#22312;&#12289;&#22870;&#21169;&#21644;&#21327;&#35843;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#32858;&#31867;&#20998;&#26512;&#26041;&#27861;&#21306;&#20998;&#20102;&#39044;&#27979;&#38544;&#31169;&#21644;&#20116;&#20010;&#20851;&#38190;&#25968;&#25454;&#20849;&#20139;&#34892;&#20026;&#30340;&#26631;&#20934;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25968;&#25454;&#20849;&#20139;&#21327;&#35843;&#23545;&#25152;&#26377;&#20154;&#26469;&#35828;&#37117;&#26159;&#21452;&#36194;&#30340;&#65306;&#38544;&#31169;&#24471;&#21040;&#26174;&#33879;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective privacy loss becomes a colossal problem, an emergency for personal freedoms and democracy. But, are we prepared to handle personal data as scarce resource and collectively share data under the doctrine: as little as possible, as much as necessary? We hypothesize a significant privacy recovery if a population of individuals, the data collective, coordinates to share minimum data for running online services with the required quality. Here we show how to automate and scale-up complex collective arrangements for privacy recovery using decentralized artificial intelligence. For this, we compare for first time attitudinal, intrinsic, rewarded and coordinated data sharing in a rigorous living-lab experiment of high realism involving &gt;27,000 real data disclosures. Using causal inference and cluster analysis, we differentiate criteria predicting privacy and five key data-sharing behaviors. Strikingly, data-sharing coordination proves to be a win-win for all: remarkable privacy recove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedTiny&#65292;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20869;&#23384;&#21644;&#35745;&#31639;&#21463;&#38480;&#30340;&#35774;&#22791;&#29983;&#25104;&#19987;&#38376;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#25209;&#24402;&#19968;&#21270;&#36873;&#25321;&#27169;&#22359;&#26469;&#35299;&#20915;&#21098;&#26525;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.01977</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Distributed Pruning Towards Tiny Neural Networks in Federated Learning. (arXiv:2212.01977v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedTiny&#65292;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20869;&#23384;&#21644;&#35745;&#31639;&#21463;&#38480;&#30340;&#35774;&#22791;&#29983;&#25104;&#19987;&#38376;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#25209;&#24402;&#19968;&#21270;&#36873;&#25321;&#27169;&#22359;&#26469;&#35299;&#20915;&#21098;&#26525;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26159;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#20351;&#24471;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#26469;&#25351;&#23548;&#21098;&#26525;&#31574;&#30053;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21644;&#20445;&#23494;&#25968;&#25454;&#38598;&#25928;&#26524;&#19981;&#22909;&#12290;&#27492;&#22806;&#65292;&#20869;&#23384;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21098;&#26525;&#36807;&#31243;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedTiny&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20869;&#23384;&#21644;&#35745;&#31639;&#21463;&#38480;&#30340;&#35774;&#22791;&#29983;&#25104;&#19987;&#38376;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;FedTiny&#20013;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65292;&#20197;&#36866;&#24212;&#31232;&#30095;&#21644;&#24265;&#20215;&#30340;&#23616;&#37096;&#35745;&#31639;&#37096;&#32626;&#22330;&#26223;&#65292;&#33258;&#36866;&#24212;&#22320;&#25628;&#32034;&#31895;&#21098;&#26525;&#21644;&#32454;&#21098;&#26525;&#30340;&#19987;&#29992;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#25209;&#24402;&#19968;&#21270;&#36873;&#25321;&#27169;&#22359;&#65292;&#26469;&#20943;&#36731;&#21098;&#26525;&#20013;&#30001;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning is an essential technique for reducing the size and complexity of deep neural networks, enabling large-scale models on devices with limited resources. However, existing pruning approaches heavily rely on training data for guiding the pruning strategies, making them ineffective for federated learning over distributed and confidential datasets. Additionally, the memory- and computation-intensive pruning process becomes infeasible for recourse-constrained devices in federated learning. To address these challenges, we propose FedTiny, a distributed pruning framework for federated learning that generates specialized tiny models for memory- and computing-constrained devices. We introduce two key modules in FedTiny to adaptively search coarse- and finer-pruned specialized models to fit deployment scenarios with sparse and cheap local computation. First, an adaptive batch normalization selection module is designed to mitigate biases in pruning caused by the heterogeneity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11030</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21487;&#20197;&#39640;&#24230;&#29305;&#26435;&#22320;&#35775;&#38382;&#21463;&#23475;&#32773;&#30340;&#21442;&#25968;&#12289;&#29615;&#22659;&#25110;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24265;&#20215;&#20132;&#27969;MDP&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#30830;&#23450;&#24615;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#12290;&#23545;&#25163;&#19981;&#33021;&#25513;&#30422;&#22320;&#38754;&#20107;&#23454;&#65292;&#24433;&#21709;&#22522;&#26412;&#29615;&#22659;&#21160;&#24577;&#25110;&#22870;&#21169;&#20449;&#21495;&#65292;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#22686;&#21152;&#38543;&#26426;&#24615;&#65292;&#30475;&#21040;&#21463;&#23475;&#32773;&#30340;&#21160;&#20316;&#25110;&#35775;&#38382;&#20182;&#20204;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23545;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#26174;&#30528;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#12290;&#24433;&#21709;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20026;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Treeformer&#27169;&#22359;&#65292;&#23427;&#20511;&#37492;&#20102;CKY&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#26469;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#65292;&#20174;&#32780;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22359;&#22312;&#32452;&#21512;&#27867;&#21270;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2207.06960</link><description>&lt;p&gt;
&#29992;Treeformers&#29983;&#25104;&#26641;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Treeformer&#27169;&#22359;&#65292;&#23427;&#20511;&#37492;&#20102;CKY&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#26469;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#65292;&#20174;&#32780;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22359;&#22312;&#32452;&#21512;&#27867;&#21270;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#36739;&#23567;&#30340;&#29255;&#27573;&#20013;&#26500;&#24314;&#22797;&#26434;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;&#22914;Transformers&#65289;&#22312;&#20854;&#26550;&#26500;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#21363;&#23427;&#20204;&#23545;&#23618;&#27425;&#32467;&#26500;&#27809;&#26377;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;Transformers&#22312;&#38656;&#35201;&#36825;&#31181;&#32467;&#26500;&#30340;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Treeformer&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21463;&#21040;CKY&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#65292;&#29992;&#20110;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#22312;&#32452;&#21512;&#27867;&#21270;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#12289;&#25277;&#35937;&#25688;&#35201;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is known to exhibit a nested, hierarchical structure, allowing us to form complex sentences out of smaller pieces. However, many state-of-the-art neural networks models such as Transformers have no explicit hierarchical structure in its architecture -- that is, they don't have an inductive bias toward hierarchical structure. Additionally, Transformers are known to perform poorly on compositional generalization tasks which require such structures. In this paper, we introduce Treeformer, a general-purpose encoder module inspired by the CKY algorithm which learns a composition operator and pooling function to construct hierarchical encodings for phrases and sentences. Our extensive experiments demonstrate the benefits of incorporating hierarchical structure into the Transformer and show significant improvements in compositional generalization as well as in downstream tasks such as machine translation, abstractive summarization, and various natural language understanding tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Talmudic Public Announcement Logic&#30340;&#26032;&#39062;&#35299;&#37322;&#24615;&#26041;&#27861;BTPK&#65292;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#20869;&#37096;&#36923;&#36753;&#65292;&#21516;&#26102;&#33021;&#22815;&#25429;&#25417;&#21477;&#23376;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2201.09523</link><description>&lt;p&gt;
&#22522;&#20110;Talmudic Public Announcement Logic&#30340;BTPK&#35299;&#37322;&#24615;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BTPK-based interpretable method for NER tasks based on Talmudic Public Announcement Logic. (arXiv:2201.09523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Talmudic Public Announcement Logic&#30340;&#26032;&#39062;&#35299;&#37322;&#24615;&#26041;&#27861;BTPK&#65292;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#20869;&#37096;&#36923;&#36753;&#65292;&#21516;&#26102;&#33021;&#22815;&#25429;&#25417;&#21477;&#23376;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;NLP&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20449;&#24687;&#25552;&#21462;&#12289;&#21477;&#27861;&#20998;&#26512;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#65289;&#30340;&#37325;&#35201;&#22522;&#30784;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#23545;&#29992;&#25143;&#26469;&#35828;&#26159;&#40657;&#30418;&#25805;&#20316;&#65292;&#29992;&#25143;&#27809;&#26377;&#20381;&#25454;&#26469;&#30830;&#23450;&#21738;&#20010;&#21629;&#21517;&#23454;&#20307;&#26356;&#26377;&#24847;&#20041;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#35782;&#21035;&#36807;&#31243;&#23545;&#35768;&#22810;&#20154;&#26469;&#35828;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;BTPK&#65288;Binary Talmudic Public Announcement Logic&#27169;&#22411;&#65289;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;Talmudic Public Announcement Logic&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#20869;&#37096;&#35782;&#21035;&#36923;&#36753;&#12290;BTPK&#27169;&#22411;&#36824;&#21487;&#20197;&#25429;&#25417;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;BTPK&#30340;&#20844;&#20849;&#20844;&#21578;&#21576;&#29616;&#20102;BRNNs&#30340;&#20869;&#37096;&#20915;&#31574;&#36923;&#36753;&#65292;&#24182;&#20174;&#20013;&#33719;&#24471;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the basic tasks in natural language processing (NLP), named entity recognition (NER) is an important basic tool for downstream tasks of NLP, such as information extraction, syntactic analysis, machine translation and so on. The internal operation logic of current name entity recognition model is black-box to the user, so the user has no basis to determine which name entity makes more sense. Therefore, a user-friendly explainable recognition process would be very useful for many people. In this paper, we propose a novel interpretable method, BTPK (Binary Talmudic Public Announcement Logic model), to help users understand the internal recognition logic of the name entity recognition tasks based on Talmudic Public Announcement Logic. BTPK model can also capture the semantic information in the input sentences, that is, the context dependency of the sentence. We observed the public announcement of BTPK presents the inner decision logic of BRNNs, and the explanations obtained from 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14586</link><description>&lt;p&gt;
&#20351;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14586
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#21644;&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#26080;&#26631;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#24517;&#39035;&#19982;&#26368;&#22823;&#25439;&#22833;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#19981;&#35770;&#26159;&#23545;&#20110;&#22823;&#25439;&#22833;&#36824;&#26159;&#23545;&#20110;&#38750;&#24120;&#23567;&#30340;&#25439;&#22833;&#12290;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#34920;&#26126;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#24182;&#21487;&#33021;&#20855;&#26377;&#24120;&#25968;&#36951;&#25022;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#23613;&#21487;&#33021;&#23569;&#20381;&#36182;&#21442;&#25968;&#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#24212;&#35813;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21644;&#20027;&#35201;&#24037;&#20855;&#26159;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#23427;&#26159;&#24179;&#34913;&#36951;&#25022;&#26435;&#34913;&#30340;&#24605;&#24819;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#36731;&#26494;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#26679;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#65288;&#26080;&#35770;&#26159;&#24120;&#25968;&#12289;$O(\log T)$&#12289;$O(\sqrt{T})$&#31561;&#65289;&#65292;&#24182;&#19988;&#22312;&#21516;&#26679;&#30340;&#35266;&#23519;&#37327;&#19978;&#27604;&#22312;&#20107;&#21518;&#36873;&#25321;&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#24230;&#39640;&#20986;2&#20493;&#12290;&#31532;&#20108;&#20010;&#24037;&#20855;&#26159;&#22312;&#32447;&#26657;&#27491;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;...
&lt;/p&gt;
&lt;p&gt;
We extend and combine several tools of the literature to design fast, adaptive, anytime and scale-free online learning algorithms. Scale-free regret bounds must scale linearly with the maximum loss, both toward large losses and toward very small losses. Adaptive regret bounds demonstrate that an algorithm can take advantage of easy data and potentially have constant regret. We seek to develop fast algorithms that depend on as few parameters as possible, in particular they should be anytime and thus not depend on the time horizon. Our first and main tool, isotuning, is a generalization of the idea of balancing the trade-off of the regret. We develop a set of tools to design and analyze such learning rates easily and show that they adapts automatically to the rate of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a factor 2 of the optimal learning rate in hindsight for the same observed quantities. The second tool is an online correction, which allows us to obtain
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21709;&#24212;&#30340;&#24182;&#34892;&#21270;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;&#32454;&#21270;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#39044;&#27979;CV&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#36873;&#25321;&#36731;&#37327;&#32423;&#24494;&#22411;Web&#26694;&#26550;&#21644;&#20351;&#29992;&#24494;&#26381;&#21153;&#26469;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31649;&#36947;&#65292;&#36798;&#21040;&#22312;&#23569;&#20110;700&#27627;&#31186;&#30340;&#26102;&#38388;&#20869;&#35299;&#26512;&#26222;&#36890;CV&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2112.08933</link><description>&lt;p&gt;
&#21487;&#21709;&#24212;&#30340;&#24182;&#34892;&#21270;&#26550;&#26500;&#29992;&#20110;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Responsive parallelized architecture for deploying deep learning models in production environments. (arXiv:2112.08933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21709;&#24212;&#30340;&#24182;&#34892;&#21270;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;&#32454;&#21270;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#39044;&#27979;CV&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#36873;&#25321;&#36731;&#37327;&#32423;&#24494;&#22411;Web&#26694;&#26550;&#21644;&#20351;&#29992;&#24494;&#26381;&#21153;&#26469;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31649;&#36947;&#65292;&#36798;&#21040;&#22312;&#23569;&#20110;700&#27627;&#31186;&#30340;&#26102;&#38388;&#20869;&#35299;&#26512;&#26222;&#36890;CV&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25307;&#32856;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#27714;&#32844;&#32773;&#30340;&#31616;&#21382;(CV)&#25991;&#26723;&#36731;&#26494;&#31579;&#36873;&#20505;&#36873;&#20154;&#12290;&#26080;&#32467;&#26500;&#30340;&#25991;&#26723;CV&#21253;&#21547;&#20505;&#36873;&#20154;&#30340;&#20316;&#21697;&#38598;&#21644;&#21629;&#21517;&#23454;&#20307;&#21015;&#34920;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#21644;&#25552;&#20986;&#19968;&#20010;&#38754;&#21521;Web&#30340;&#12289;&#39640;&#24230;&#21709;&#24212;&#30340;&#35745;&#31639;&#31649;&#36947;&#65292;&#20351;&#29992;&#23618;&#27425;&#21270;&#32454;&#21270;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#31995;&#32479;&#22320;&#39044;&#27979;CV&#23454;&#20307;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#30456;&#20851;&#23383;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24182;&#34892;&#20013;&#20351;&#29992;&#19968;&#23450;&#25968;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#23454;&#26102;&#39044;&#27979;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#26512;&#23618;&#27425;&#22788;&#29702;&#31639;&#27861;&#36873;&#25321;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24494;&#22411;Web&#26694;&#26550;&#65292;&#24182;&#19987;&#27880;&#20110;&#19968;&#31181;&#26377;&#21161;&#20110;&#22312;&#29983;&#20135;&#23601;&#32490;&#29615;&#22659;&#20013;&#20351;&#29992;&#24494;&#26381;&#21153;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31649;&#36947;&#30340;&#26041;&#27861;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21644;&#25552;&#20986;&#30340;&#26550;&#26500;&#26377;&#21161;&#20110;&#22312;&#23569;&#20110;700&#27627;&#31186;&#30340;&#26102;&#38388;&#20869;&#35299;&#26512;&#26222;&#36890;CV&#12290;
&lt;/p&gt;
&lt;p&gt;
Recruiters can easily shortlist candidates for jobs via viewing their curriculum vitae (CV) document. Unstructured document CV beholds candidate's portfolio and named entities listing details. The main aim of this study is to design and propose a web oriented, highly responsive, computational pipeline that systematically predicts CV entities using hierarchically-refined label attention networks. Deep learning models specialized for named entity recognition were trained on large dataset to predict relevant fields. The article suggests an optimal strategy to use a number of deep learning models in parallel and predict in real time. We demonstrate selection of light weight micro web framework using Analytical Hierarchy Processing algorithm and focus on an approach useful to deploy large deep learning model-based pipelines in production ready environments using microservices. Deployed models and architecture proposed helped in parsing normal CV in less than 700 milliseconds for sequential 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25506;&#35752;&#20102;&#20010;&#20307;&#26234;&#33021;&#26159;&#21542;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#22312;Lotka-Volterra&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20010;&#20307;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026;&#65292;&#26377;&#21033;&#20110;&#19982;&#20854;&#20182;&#31181;&#32676;&#20849;&#23384;&#65292;&#20294;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23558;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2012.12689</link><description>&lt;p&gt;
&#20803;&#32032;&#36234;&#31528;&#65292;&#25972;&#20307;&#36234;&#32874;&#26126;&#12290;&#25110;&#32773;&#65292;&#21487;&#33021;&#24182;&#38750;&#22914;&#27492;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?. (arXiv:2012.12689v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.12689
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20010;&#20307;&#26234;&#33021;&#26159;&#21542;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#22312;Lotka-Volterra&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20010;&#20307;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026;&#65292;&#26377;&#21033;&#20110;&#19982;&#20854;&#20182;&#31181;&#32676;&#20849;&#23384;&#65292;&#20294;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23558;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#20803;&#19982;&#31038;&#20250;&#20013;&#30340;&#20154;&#31867;&#20043;&#38388;&#30340;&#21033;&#32500;&#22374;&#31867;&#27604;&#65292;&#38382;&#33258;&#24049;&#26159;&#21542;&#20010;&#20307;&#26234;&#33021;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36830;&#25509;&#20027;&#20041;&#35748;&#30693;&#31185;&#23398;&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#12289;&#32676;&#20307;&#24515;&#29702;&#23398;&#12289;&#32463;&#27982;&#23398;&#21644;&#29289;&#29702;&#23398;&#30340;&#19981;&#21516;&#27934;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27934;&#35265;&#24212;&#29992;&#20110;Lotka-Volterra&#27169;&#22411;&#20013;&#23548;&#33268;&#25504;&#39135;&#32773;&#21644;&#29454;&#29289;&#35201;&#20040;&#20849;&#23384;&#35201;&#20040;&#20840;&#29699;&#28781;&#32477;&#30340;&#26234;&#33021;&#31867;&#22411;&#21644;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#20960;&#20010;&#20010;&#20307;&#34892;&#20026; - &#23588;&#20854;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026; - &#26377;&#21033;&#20110;&#20849;&#23384;&#65292;&#26368;&#32456;&#22312;&#19968;&#20010;&#24179;&#34913;&#28857;&#21608;&#22260;&#20135;&#29983;&#38663;&#33633;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23601;&#20250;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;&#30001;&#20110;Lotka-Volterra&#27169;&#22411;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a Leviathan analogy between neurons in a brain and human beings in society, asking ourselves whether individual intelligence is necessary for collective intelligence to emerge and, most importantly, what sort of individual intelligence is conducive of greater collective intelligence. We first review disparate insights from connectionist cognitive science, agent-based modeling, group psychology, economics and physics. Subsequently, we apply these insights to the sort and degrees of intelligence that in the Lotka-Volterra model lead to either co-existence or global extinction of predators and preys.  We find several individual behaviors -- particularly of predators -- that are conducive to co-existence, eventually with oscillations around an equilibrium. However, we also find that if both preys and predators are sufficiently intelligent to extrapolate one other's behavior, co-existence comes along with indefinite growth of both populations. Since the Lotka-Volterra model is al
&lt;/p&gt;</description></item></channel></rss>