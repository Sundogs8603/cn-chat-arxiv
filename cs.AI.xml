<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2404.01291</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Visual Generation with Image-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01291
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#32508;&#21512;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VQAScore&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#23545;&#31616;&#21333;&#30340;&#8220;&#36825;&#24133;&#22270;&#34920;&#29616;&#20986;&#20102;'{&#25991;&#26412;}'&#21527;&#65311;&#8221;&#38382;&#39064;&#30340;&#8220;&#26159;&#8221;&#31572;&#26696;&#30340;&#27010;&#29575;&#26469;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#12290;&#23613;&#31649;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#65292;&#20294;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#35745;&#31639;&#30340;VQAScore&#22312;&#35768;&#22810;&#65288;8&#20010;&#65289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.18167</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanisms of non-factual hallucinations in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18167
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26377;&#26102;&#20250;&#20135;&#29983;&#19982;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#38750;&#20107;&#23454;&#24187;&#35273;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#24187;&#35273;&#65292;&#20294;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290; &#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#24187;&#35273;&#30340;&#26426;&#21046;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159; LM &#22312;&#23545;&#20027;&#39064;&#20851;&#31995;&#26597;&#35810;&#20570;&#20986;&#22238;&#31572;&#26102;&#38169;&#35823;&#22320;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#30340;&#38750;&#20107;&#23454;&#24418;&#24335;&#12290;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#23884;&#20837;&#31354;&#38388;&#25237;&#24433;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#36328;&#19981;&#21516;&#35268;&#27169;&#21644;&#35774;&#35745;&#30340; LM &#20013;&#20849;&#20139;&#30340;&#20004;&#20010;&#36896;&#25104;&#24187;&#35273;&#30340;&#19968;&#33324;&#26426;&#21046;&#21407;&#22240;&#65306;1&#65289;&#22312;&#36739;&#20302;&#23618; MLPs &#20013;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#65292;&#20197;&#21450;2&#65289;&#22312;&#36739;&#39640;&#23618;&#27880;&#24847;&#21147;&#22836;&#21644; MLPs &#20013;&#26410;&#33021;&#36873;&#25321;&#27491;&#30830;&#30340;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#20004;&#20010;&#26426;&#21046;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20027;&#23486;&#20851;&#31995;&#12289;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102; LM &#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.17993</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#28982;&#26234;&#33021;&#30456;&#34701;&#21512;&#65306;&#20174;&#32479;&#35745;&#21147;&#23398;&#21040;&#20154;&#24037;&#26234;&#33021;&#20877;&#21040;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17993
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21453;&#24605;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#26410;&#26469;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#28237;&#27969;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#26681;&#26893;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#27169;&#22411;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#23457;&#26597;&#20102;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#21508;&#31181;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#27969;&#20307;&#21147;&#23398;&#30340;&#21516;&#26102;&#21457;&#23637;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.16056</link><description>&lt;p&gt;
Qibo: &#19968;&#31181;&#29992;&#20110;&#20013;&#21307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qibo: A Large Language Model for Traditional Chinese Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#21644;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#19987;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#21307;&#39046;&#22495;&#65292;LLMs&#30340;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#25361;&#25112;&#65292;&#20854;&#21407;&#22240;&#22312;&#20110;&#20013;&#21307;&#29702;&#35770;&#19982;&#29616;&#20195;&#21307;&#23398;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#65292;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#35821;&#26009;&#24211;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#21644;&#25972;&#29702;&#20013;&#21307;&#39046;&#22495;&#30340;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#36171;&#20104;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#20013;&#21307;&#29702;&#35770;&#29305;&#33394;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25104;&#21151;&#22522;&#20110;LLaMA&#24320;&#21457;&#20102;Qibo&#27169;&#22411;&#65292;&#36825;&#26159;&#20013;&#21307;&#39046;&#22495;&#31532;&#19968;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#36807;&#31243;&#65288;&#20174;&#39044;&#35757;&#32451;&#21040;&#30417;&#30563;&#24494;&#35843;&#65289;&#30340;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Qibo&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;&#19987;&#38376;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05680</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#23545;&#22522;&#20110;&#35270;&#35273;&#30340;LLM&#39044;&#27979;&#36827;&#34892;&#20998;&#35299;&#20197;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05680
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#26816;&#26597;&#30340;&#25968;&#37327;&#27599;&#24180;&#37117;&#22312;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#25918;&#23556;&#31185;&#21307;&#29983;&#30130;&#21171;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#20182;&#20204;&#30340;&#36127;&#25285;&#65292;&#20294;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#37319;&#29992;&#21462;&#20915;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20449;&#20219;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#31616;&#21333;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05680v1 Announce Type: new  Abstract: The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. Large Language Models (LLMs) have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of vision-language LLMs in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a GPT-4 model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically eval
&lt;/p&gt;</description></item><item><title>HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05396</link><description>&lt;p&gt;
HistGen&#65306;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05396
&lt;/p&gt;
&lt;p&gt;
HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#30284;&#30151;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20020;&#24202;&#25253;&#21578;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#19968;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25351;&#23548;&#30284;&#30151;&#27835;&#30103;&#21644;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#23558;&#26497;&#22823;&#25552;&#21319;&#20020;&#24202;&#25928;&#29575;&#65292;&#24182;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#22312;&#25253;&#21578;&#25776;&#20889;&#26041;&#38754;&#30340;&#21171;&#21160;&#24378;&#24230;&#21644;&#32791;&#26102;&#36127;&#25285;&#12290;&#20026;&#36861;&#27714;&#36825;&#19968;&#36827;&#27493;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;HistGen&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#22686;&#24378;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;HistGen&#21463;&#35786;&#26029;&#21644;&#25253;&#21578;&#25776;&#20889;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#25253;&#21578;&#65292;&#20174;&#26412;&#22320;&#21644;&#20840;&#23616;&#31890;&#24230;&#25552;&#21319;&#25253;&#21578;&#29983;&#25104;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#20998;&#23618;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#21306;&#22495;&#20013;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.18439</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#65306;LLM&#21033;&#29992;&#26367;&#20195;&#26684;&#24335;&#36827;&#34892;&#22686;&#24378;&#25512;&#29702;&#21644;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18439
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#27807;&#36890;&#30340;&#20027;&#35201;&#26684;&#24335;&#65292;&#22240;&#27492;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;NL&#20043;&#22806;&#65292;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#20102;&#21508;&#31181;&#38750;NL&#26684;&#24335;&#65292;&#22914;&#20195;&#30721;&#21644;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;NL&#20316;&#20026;LLMs&#30340;&#26368;&#20339;&#26684;&#24335;&#65292;&#22312;&#21333;&#19968;LLM&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#30340;&#22320;&#20301;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;NL&#65292;&#36890;&#36807;&#25506;&#32034;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#38750;NL&#26684;&#24335;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#20801;&#35768;LLMs&#22312;&#25512;&#29702;&#25110;&#27807;&#36890;&#20043;&#21069;&#33258;&#20027;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26684;&#24335;&#65292;&#21487;&#23548;&#33268;&#19981;&#21516;LLMs&#25512;&#29702;&#25928;&#29575;&#25552;&#39640;3.3&#33267;5.7&#65285;&#65292;&#24182;&#19988;&#22312;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#26368;&#22810;&#21487;&#20943;&#23569;72.7&#65285;&#30340;&#26631;&#35760;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27807;&#36890;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
&lt;/p&gt;</description></item><item><title>PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.12835</link><description>&lt;p&gt;
PANDA: &#29992;&#20110;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12835
&lt;/p&gt;
&lt;p&gt;
PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#29305;&#23450;&#39046;&#22495;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#32791;&#36153;&#36164;&#28304;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#23553;&#38381;&#28304;&#21830;&#19994;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PANDA&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;LLMs&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PANDA&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;PANDA&#30340;LLM&#29978;&#33267;&#36229;&#36807;&#20102;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11436</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#30340;&#21361;&#38505;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#20559;&#35265;&#34987;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11436
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#21453;&#39304;&#21487;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#21364;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#30683;&#30462;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#20559;&#35265;&#8212;&#8212;&#20542;&#21521;&#20110;&#20559;&#29233;&#33258;&#36523;&#29983;&#25104;&#8212;&#8212;&#24182;&#20351;&#29992;&#20004;&#20010;&#32479;&#35745;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#32763;&#35793;&#12289;&#21463;&#38480;&#25991;&#26412;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#20998;&#26512;&#20102;&#20845;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20559;&#35265;&#22312;&#25152;&#26377;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#22810;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#25552;&#39640;&#20102;&#27169;&#22411;&#36755;&#20986;&#30340;&#27969;&#30021;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#20294;&#23427;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#33258;&#25105;&#20559;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20855;&#26377;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#38469;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09370</link><description>&lt;p&gt;
&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;
&lt;/p&gt;
&lt;p&gt;
Pseudorandom Error-Correcting Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09370
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65288;&#25110;&#31616;&#31216;&#20026;&#20266;&#38543;&#26426;&#30721;&#65289;&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#32416;&#38169;&#30721;&#65306;&#23545;&#20110;&#20219;&#20309;&#35745;&#31639;&#21463;&#38480;&#30340;&#23545;&#25163;&#26469;&#35828;&#65292;&#20219;&#24847;&#22810;&#20010;&#32534;&#30721;&#35789;&#37117;&#26159;&#20266;&#38543;&#26426;&#30340;&#12290;&#36890;&#36807;&#35299;&#30721;&#23494;&#38053;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32416;&#27491;&#26377;&#38169;&#35823;&#30340;&#32534;&#30721;&#35789;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#23545;&#26367;&#25442;&#38169;&#35823;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#20266;&#38543;&#26426;&#30721;&#65292;&#20854;&#20013;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;&#26631;&#20934;&#23494;&#30721;&#23398;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;LPN&#38382;&#39064;&#30340;$2^{O(\sqrt{n})}$&#22256;&#38590;&#31243;&#24230;&#65292;&#25110;&#32773;&#22522;&#20110;LPN&#38382;&#39064;&#21644;&#20302;&#23494;&#24230;&#19979;&#30340;&#25554;&#20837;&#24322;&#25110;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
&lt;/p&gt;</description></item><item><title>"AuditLLM"&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#21333;&#20010;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#20010;&#25506;&#27979;&#26469;&#23545;&#32473;&#23450;&#30340;LLM&#36827;&#34892;&#23457;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#27169;&#22411;&#22312;&#29702;&#35299;&#25110;&#25805;&#20316;&#26041;&#38754;&#30340;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09334</link><description>&lt;p&gt;
AuditLLM:&#19968;&#31181;&#20351;&#29992;&#22810;&#25506;&#27979;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23457;&#35745;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09334
&lt;/p&gt;
&lt;p&gt;
"AuditLLM"&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#21333;&#20010;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#20010;&#25506;&#27979;&#26469;&#23545;&#32473;&#23450;&#30340;LLM&#36827;&#34892;&#23457;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#27169;&#22411;&#22312;&#29702;&#35299;&#25110;&#25805;&#20316;&#26041;&#38754;&#30340;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#34987;&#37319;&#29992;&#26102;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#26159;&#30456;&#23545;&#23433;&#20840;&#12289;&#19968;&#33268;&#21644;&#21487;&#38752;&#30340;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#33021;&#38656;&#35201;&#23545;&#23427;&#20204;&#36827;&#34892;&#25506;&#27979;&#25110;&#23457;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27425;&#36845;&#20195;&#30340;&#21333;&#20010;&#38382;&#39064;&#23545;LLMs&#36827;&#34892;&#25506;&#27979;&#65292;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#30693;&#35782;&#25110;&#21151;&#33021;&#30340;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#33021;&#22815;&#20197;&#31616;&#21333;&#24037;&#20316;&#27969;&#31243;&#21644;&#20302;&#25216;&#26415;&#38376;&#27099;&#36827;&#34892;&#27492;&#31867;&#23457;&#35745;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#20010;&#28436;&#31034;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"AuditLLM"&#30340;&#26032;&#39062;&#24037;&#20855;&#65292;&#23427;&#26088;&#22312;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#12290;AuditLLM&#30340;&#26680;&#24515;&#21151;&#33021;&#22312;&#20110;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#20174;&#21333;&#20010;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#20010;&#25506;&#27979;&#26469;&#23545;&#32473;&#23450;&#30340;LLM&#36827;&#34892;&#23457;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#27169;&#22411;&#22312;&#29702;&#35299;&#25110;&#25805;&#20316;&#26041;&#38754;&#30340;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#12290;&#19968;&#20010;&#30456;&#23545;&#20581;&#22766;&#12289;&#21487;&#38752;&#21644;&#19968;&#33268;&#30340;LLM&#24212;&#35813;&#23545;&#20197;&#19981;&#21516;&#26041;&#24335;&#25110;&#30001;&#19981;&#21516;&#20154;&#25552;&#20986;&#30340;&#38382;&#39064;&#32473;&#20986;&#35821;&#20041;&#30456;&#20284;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09334v1 Announce Type: new Abstract: As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce "AuditLLM," a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08349</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65288;&#20063;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#65289;&#24050;&#25104;&#20026;&#24357;&#21512;&#29992;&#25143;&#33021;&#21147;&#19982;&#22522;&#20110;SQL&#30340;&#25968;&#25454;&#35775;&#38382;&#20043;&#38388;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#35831;&#27714;&#36716;&#21270;&#20026;&#29305;&#23450;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;SQL&#35821;&#21477;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24120;&#24120;&#26159;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19981;&#26029;&#21462;&#24471;&#26032;&#30340;&#39640;&#20998;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#12289;&#29616;&#23454;&#22330;&#26223;&#20013;&#23545;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#26126;&#26174;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#22269;&#38469;&#39033;&#30446;&#20851;&#20110;&#25991;&#26412;&#21040;SQL&#30028;&#38754;&#30340;&#38598;&#20013;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#39318;&#27425;&#28145;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;FootballDB&#30340;&#30495;&#23454;&#37096;&#32626;&#65292;&#35813;&#31995;&#32479;&#22312;FIFA World Cup&#30340;&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06871</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Generative Models for Reranking Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#36890;&#36807;&#24314;&#27169;&#39033;&#30446;&#20043;&#38388;&#30340;&#20869;&#37096;&#30456;&#20851;&#24615;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37325;&#26032;&#25490;&#24207;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#25490;&#21015;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#25506;&#32034;&#26368;&#20339;&#24207;&#21015;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#29983;&#25104;&#22120;-&#35780;&#20272;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#29983;&#25104;&#22120;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#24207;&#21015;&#65292;&#35780;&#20272;&#22120;&#22522;&#20110;&#20272;&#35745;&#30340;&#21015;&#34920;&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#24207;&#21015;&#12290;&#29983;&#25104;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#29983;&#25104;&#22120;&#20989;&#25968;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#33258;&#22238;&#24402;&#31574;&#30053;&#36827;&#34892;&#24207;&#21015;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24037;&#19994;&#31995;&#32479;&#20013;&#37096;&#32626;&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65288;NAR4Rec&#65289;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;m
&lt;/p&gt;
&lt;p&gt;
In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#39068;&#33394;&#31354;&#38388;&#26469;&#35299;&#32806;&#20142;&#24230;&#21644;&#39068;&#33394;&#65292;&#24182;&#35774;&#35745;&#20102;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#20197;&#25913;&#21892;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.05809</link><description>&lt;p&gt;
&#21482;&#38656;&#19968;&#20010;&#39068;&#33394;&#31354;&#38388;&#65306;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#39068;&#33394;&#31354;&#38388;&#26469;&#35299;&#32806;&#20142;&#24230;&#21644;&#39068;&#33394;&#65292;&#24182;&#35774;&#35745;&#20102;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#20197;&#25913;&#21892;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#65288;Low-Light Image Enhancement&#65292;LLIE&#65289;&#20219;&#21153;&#26088;&#22312;&#20174;&#21463;&#25439;&#30340;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#32454;&#33410;&#21644;&#35270;&#35273;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;sRGB&#21644;HSV&#39068;&#33394;&#31354;&#38388;&#19978;&#23398;&#20064;&#20302;/&#27491;&#24120;&#20809;&#22270;&#20687;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22686;&#24378;&#28041;&#21450;&#25918;&#22823;&#22270;&#20687;&#20449;&#21495;&#65292;&#24182;&#19988;&#23558;&#36825;&#20123;&#39068;&#33394;&#31354;&#38388;&#24212;&#29992;&#20110;&#20449;&#22122;&#27604;&#20302;&#30340;&#20302;&#20809;&#22270;&#20687;&#21487;&#33021;&#20250;&#24341;&#20837;&#28789;&#25935;&#24230;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22686;&#24378;&#22270;&#20687;&#20013;&#23384;&#22312;&#39068;&#33394;&#20266;&#24433;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35757;&#32451;&#39068;&#33394;&#31354;&#38388;&#65292;&#31216;&#20026;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#12290;&#23427;&#19981;&#20165;&#23558;&#20142;&#24230;&#21644;&#39068;&#33394;&#20174;RGB&#36890;&#36947;&#20998;&#31163;&#20986;&#26469;&#20197;&#20943;&#36731;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#32780;&#19988;&#30001;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#23427;&#36824;&#36866;&#24212;&#19981;&#21516;&#20809;&#29031;&#33539;&#22260;&#30340;&#20302;&#20809;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#65292;&#21547;&#26377;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04971</link><description>&lt;p&gt;
&#22810;&#21457;&#20449;&#32773;&#35828;&#26381; - &#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-Sender Persuasion -- A Computational Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21040;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#22810;&#20010;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#20351;&#20854;&#37319;&#21462;&#26576;&#20123;&#34892;&#21160;&#12290;&#36825;&#20123;&#35774;&#32622;&#26159;&#35745;&#31639;&#32463;&#27982;&#23398;&#65292;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#26159;&#21457;&#20449;&#32773;&#20449;&#21495;&#31574;&#30053;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;&#22343;&#34913;&#26159;PPAD-Hard&#30340;;&#23454;&#38469;&#19978;&#65292;&#35745;&#31639;&#19968;&#20010;&#21457;&#20449;&#32773;&#30340;&#26368;&#20339;&#21709;&#24212;&#29978;&#33267;&#26159;NP-Hard&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#22266;&#26377;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36716;&#32780;&#23547;&#25214;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35813;&#28216;&#25103;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#25928;&#29992;&#12290;&#32467;&#21512;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#23637;&#31034;&#22343;&#34913;&#21644;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#30340;&#23616;&#37096;&#22343;&#34913;&#12290;&#24191;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#36129;&#29486;&#23545;&#24191;&#27867;&#30340;&#31867;&#21035;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;R&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#27425;&#23581;&#35797;&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#31934;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#31561;&#22810;&#20010;&#36136;&#37327;&#23646;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.03130</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;R&#31243;&#24207;&#20195;&#30721;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
User-Centric Evaluation of ChatGPT Capability of Generating R Program Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;R&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#27425;&#23581;&#35797;&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#31934;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#31561;&#22810;&#20010;&#36136;&#37327;&#23646;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;&#23545;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;R&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;R&#31243;&#24207;&#20195;&#30721;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#21644;&#19981;&#21516;&#31867;&#22411;&#31243;&#24207;&#30340;&#21508;&#31181;&#20351;&#29992;&#24773;&#22659;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#35780;&#20272;&#37319;&#29992;&#22810;&#27425;&#23581;&#35797;&#30340;&#36807;&#31243;&#65292;&#27979;&#35797;&#20154;&#21592;&#36890;&#36807;&#22810;&#27425;&#23581;&#35797;&#26469;&#23436;&#25104;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#30452;&#33267;&#33719;&#24471;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#36798;&#21040;&#26368;&#22823;&#23581;&#35797;&#27425;&#25968;&#21518;&#25918;&#24323;&#12290;&#27599;&#27425;&#23581;&#35797;&#20013;&#65292;&#27979;&#35797;&#20154;&#21592;&#26681;&#25454;&#20808;&#21069;&#30340;&#32467;&#26524;&#21644;&#24453;&#23436;&#25104;&#30340;&#20219;&#21153;&#21046;&#23450;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#32473;ChatGPT&#12290;&#38500;&#20102;&#24179;&#22343;&#23581;&#35797;&#27425;&#25968;&#21644;&#24179;&#22343;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#25351;&#26631;&#22806;&#65292;&#26368;&#32456;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#26681;&#25454;&#31934;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#31561;&#22810;&#20010;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, concise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01737</link><description>&lt;p&gt;
&#20026;&#31038;&#20132;&#24863;&#30693;&#30340;&#35848;&#21028;&#23545;&#35805;&#24320;&#21457;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25198;&#28436;&#27599;&#27425;&#23545;&#35805;&#20013;&#30340;&#20004;&#21517;&#35848;&#21028;&#32773;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#35848;&#21028;&#12290;&#31532;&#19977;&#20010;LLM&#20805;&#24403;&#20462;&#27491;&#20195;&#29702;&#65292;&#37325;&#26032;&#32534;&#20889;&#36829;&#21453;&#35268;&#33539;&#30340;&#35805;&#35821;&#20197;&#25913;&#21892;&#35848;&#21028;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#19981;&#23384;&#22312;&#25163;&#21160;&#26500;&#24314;&#30340;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20135;&#21697;&#38144;&#21806;&#12289;&#25151;&#20215;&#21644;&#34218;&#36164;&#35848;&#21028;&#12290;&#28304;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#20102;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#21442;&#19982;&#32773;&#32676;&#20307;&#26469;&#35828;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#27969;&#31243;&#31354;&#38388;&#30340;&#24418;&#25104;&#20027;&#35201;&#21463;&#21040;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.06231</link><description>&lt;p&gt;
&#25581;&#31034;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;
&lt;/p&gt;
&lt;p&gt;
Uncovering communities of pipelines in the task-fMRI analytical space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#20102;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#21442;&#19982;&#32773;&#32676;&#20307;&#26469;&#35828;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#27969;&#31243;&#31354;&#38388;&#30340;&#24418;&#25104;&#20027;&#35201;&#21463;&#21040;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#36873;&#25321;&#27969;&#31243;&#30340;&#26368;&#20339;&#23454;&#36341;&#26377;&#38480;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#20986;&#20351;&#29992;&#19981;&#21516;&#27969;&#31243;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#39537;&#21160;&#36825;&#20123;&#24046;&#24322;&#30340;&#22240;&#32032;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#32570;&#20047;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25506;&#32034;&#27969;&#31243;&#31354;&#38388;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#32972;&#26223;&#19979;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#65288;&#20363;&#22914;&#36816;&#21160;&#22238;&#24402;&#22120;&#30340;&#25968;&#37327;&#12289;&#36719;&#20214;&#21253;&#31561;&#65289;&#12290;&#36825;&#20123;&#27969;&#31243;&#19982;&#27969;&#31243;&#20043;&#38388;&#30340;&#27169;&#24335;&#22312;&#21442;&#19982;&#32773;&#32676;&#20307;&#20013;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#32676;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21457;&#29616;&#27969;&#31243;&#31354;&#38388;&#20027;&#35201;&#21463;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#30340;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#30340;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytical workflows in functional magnetic resonance imaging are highly flexible with limited best practices as to how to choose a pipeline. While it has been shown that the use of different pipelines might lead to different results, there is still a lack of understanding of the factors that drive these differences and of the stability of these differences across contexts. We use community detection algorithms to explore the pipeline space and assess the stability of pipeline relationships across different contexts. We show that there are subsets of pipelines that give similar results, especially those sharing specific parameters (e.g. number of motion regressors, software packages, etc.). Those pipeline-to-pipeline patterns are stable across groups of participants but not across different tasks. By visualizing the differences between communities, we show that the pipeline space is mainly driven by the size of the activation area in the brain and the scale of statistic values in stati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ERASER&#65292;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#26381;&#21153;&#22120;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;MLaaS&#20013;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#21435;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.16136</link><description>&lt;p&gt;
ERASER: &#36890;&#36807;&#25512;&#29702;&#26381;&#21153;&#22120;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;MLaaS&#20013;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ERASER&#65292;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#26381;&#21153;&#22120;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;MLaaS&#20013;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#21435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26381;&#21153;&#65288;MLaaS&#65289;&#22312;&#25903;&#25345;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26381;&#21153;&#26041;&#38754;&#26377;&#30528;&#22823;&#37327;&#38656;&#27714;&#65292;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#38761;&#21629;&#24615;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;MLaaS&#22522;&#20110;&#20174;&#20247;&#22810;&#20010;&#20307;&#25968;&#25454;&#25152;&#26377;&#32773;&#25910;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#25512;&#29702;&#26381;&#21153;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#38544;&#31169;&#24182;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#27861;&#30340;&#8220;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#8221;&#65292;&#35768;&#22810;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20197;&#22312;&#35831;&#27714;&#21435;&#23398;&#20064;&#26102;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#37117;&#29420;&#31435;&#22788;&#29702;&#21435;&#23398;&#20064;&#35831;&#27714;&#21644;&#25512;&#29702;&#35831;&#27714;&#65292;&#36825;&#19981;&#24184;&#22320;&#24341;&#20837;&#20102;&#25512;&#29702;&#26381;&#21153;&#36807;&#26102;&#24615;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#26426;&#22120;&#21435;&#23398;&#20064;&#20013;&#26497;&#26131;&#36973;&#21463;&#19981;&#24517;&#35201;&#30340;&#26333;&#20809;&#30340;&#38544;&#31169;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the "right to be forgotten (RTBF)" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.15935</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65306;&#29983;&#25104;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33719;&#21462;&#20107;&#20214;&#24207;&#21015;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27169;&#24577;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#38134;&#34892;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#25105;&#20204;&#23545;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#21035;&#24212;&#29992;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#32477;&#23545;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#24403;&#20195;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#20114;&#26021;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#23427;&#20204;&#30340;&#32852;&#21512;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#33267;&#23569;&#19982;&#29616;&#26377;&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26222;&#36866;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14043</link><description>&lt;p&gt;
&#26397;&#30528;&#30446;&#26631;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#24037;&#31243;&#22312;&#20248;&#21270;LLM&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#35774;&#35745;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#36861;&#27714;LLM&#20687;&#20154;&#31867;&#24605;&#32771;&#30340;&#20154;&#31867;&#23398;&#20551;&#35774;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#20844;&#24335;&#25351;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#26041;&#27861;&#20998;&#20026;&#20116;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#24378;&#35843;&#21644;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26059;&#36716;&#26657;&#27491;&#31561;&#22270;&#20687;&#21464;&#24418;&#20219;&#21153;&#12290;&#36890;&#36807;&#32806;&#21512;&#22810;&#20010;&#34180;&#26495;&#26679;&#26465;&#21464;&#25442;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#25554;&#20540;&#35823;&#24046;&#65292;&#31361;&#30772;&#20102;&#25511;&#21046;&#28857;&#25968;&#37327;&#29942;&#39048;&#65292;&#24182;&#19988;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#24102;&#26631;&#27880;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13432</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#29992;&#20110;&#26059;&#36716;&#26657;&#27491;&#21450;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond. (arXiv:2401.13432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26059;&#36716;&#26657;&#27491;&#31561;&#22270;&#20687;&#21464;&#24418;&#20219;&#21153;&#12290;&#36890;&#36807;&#32806;&#21512;&#22810;&#20010;&#34180;&#26495;&#26679;&#26465;&#21464;&#25442;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#25554;&#20540;&#35823;&#24046;&#65292;&#31361;&#30772;&#20102;&#25511;&#21046;&#28857;&#25968;&#37327;&#29942;&#39048;&#65292;&#24182;&#19988;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#24102;&#26631;&#27880;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34180;&#26495;&#26679;&#26465;&#65288;TPS&#65289;&#26159;&#19968;&#31181;&#20801;&#35768;&#20351;&#29992;&#25511;&#21046;&#28857;&#36816;&#21160;&#34920;&#31034;&#24377;&#24615;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#20027;&#35201;&#21464;&#24418;&#26041;&#24335;&#12290;&#38543;&#30528;&#25511;&#21046;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#21464;&#24418;&#36234;&#26469;&#36234;&#28789;&#27963;&#65292;&#20294;&#24120;&#24120;&#20250;&#36935;&#21040;&#30001;&#19981;&#24076;&#26395;&#30340;&#38382;&#39064;&#65288;&#22914;&#20869;&#23481;&#30072;&#21464;&#65289;&#23548;&#33268;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;TPS&#22312;&#22522;&#20110;&#21333;&#24133;&#22270;&#20687;&#30340;&#21464;&#24418;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24212;&#29992;&#65292;&#22914;&#26059;&#36716;&#26657;&#27491;&#12289;&#30697;&#24418;&#21270;&#21644;&#32918;&#20687;&#26657;&#27491;&#12290;&#20026;&#20102;&#31361;&#30772;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#65288;CoupledTPS&#65289;&#65292;&#23427;&#23558;&#26377;&#38480;&#25511;&#21046;&#28857;&#30340;&#22810;&#20010;TPS&#36845;&#20195;&#32806;&#21512;&#25104;&#19968;&#20010;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#21464;&#25442;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#36845;&#20195;&#25628;&#32034;&#26469;&#26681;&#25454;&#24403;&#21069;&#28508;&#22312;&#26465;&#20214;&#39044;&#27979;&#26032;&#30340;&#25511;&#21046;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20197;&#21464;&#24418;&#27969;&#20316;&#20026;&#36830;&#25509;&#19981;&#21516;TPS&#21464;&#25442;&#30340;&#26725;&#26753;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#22810;&#20010;&#21464;&#24418;&#24341;&#36215;&#30340;&#25554;&#20540;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#32321;&#29712;&#30340;&#27880;&#37322;&#25104;&#20026;&#21046;&#32422;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#26377;&#38480;&#24102;&#26631;&#27880;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#33021;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation co
&lt;/p&gt;</description></item><item><title>BEV-CLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;BEV&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#24615;&#25991;&#26412;&#36827;&#34892;&#26816;&#32034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340; semantic feature extraction &#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01065</link><description>&lt;p&gt;
BEV-CLIP&#65306;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;BEV&#26816;&#32034;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01065
&lt;/p&gt;
&lt;p&gt;
BEV-CLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;BEV&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#24615;&#25991;&#26412;&#36827;&#34892;&#26816;&#32034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340; semantic feature extraction &#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20056;&#29992;&#36710;&#36742;&#20855;&#22791;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#33021;&#21147;&#65292;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#22797;&#26434;&#22330;&#26223;&#25968;&#25454;&#30340;&#26816;&#32034;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38271;&#23614;&#24773;&#26223;&#26102;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#20108;&#32500;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#22330;&#26223;&#26816;&#32034;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20840;&#23616;&#29305;&#24449;&#34920;&#24449;&#19981;&#36275;&#21644;&#25991;&#26412;&#26816;&#32034;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEV-CLIP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#25551;&#36848;&#24615;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;&#40479;&#30640;&#22270;&#26816;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#30340;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#23884;&#20837;&#30340;&#35821;&#20041;&#20016;&#23500;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;NuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;87.66%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for the retrieval of complex scene data in autonomous driving is increasing, especially as passenger vehicles have been equipped with the ability to navigate urban settings, with the imperative to address long-tail scenarios. Meanwhile, under the pre-existing two dimensional image retrieval method, some problems may arise with scene retrieval, such as lack of global feature representation and subpar text retrieval ability. To address these issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-Eye View(BEV) retrieval methodology that utilizes descriptive text as an input to retrieve corresponding scenes. This methodology applies the semantic feature extraction abilities of a large language model (LLM) to facilitate zero-shot retrieval of extensive text descriptions, and incorporates semi-structured information from a knowledge graph to improve the semantic richness and variety of the language embedding. Our experiments result in 87.66% accuracy on NuScenes d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11604</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20104;&#20302;&#32423;&#25216;&#33021;&#36873;&#25321;&#26102;&#33021;&#22815;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;LLMs&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#29992;&#20110;&#20302;&#32423;&#36712;&#36857;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#35843;&#26597;&#20102;&#24403;&#32473;&#20104;LLM&#65288;GPT-4&#65289;&#20165;&#33021;&#35775;&#38382;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#26102;&#65292;&#23427;&#33021;&#21542;&#30452;&#25509;&#39044;&#27979;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#29992;&#20110;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#27809;&#26377;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#12289;&#36816;&#21160;&#21407;&#35821;&#25110;&#22806;&#37096;&#36712;&#36857;&#20248;&#21270;&#22120;&#65292;&#23427;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#22914;&#8220;&#25171;&#24320;&#29942;&#30422;&#8221;&#21644;&#8220;&#29992;&#28023;&#32501;&#25830;&#25325;&#30424;&#23376;&#8221;&#65292;&#20197;&#21450;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#25552;&#31034;&#20013;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01424</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#20854;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#39118;&#38505;&#12290;&#23588;&#20854;&#26159;&#38543;&#30528;LMs&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#28508;&#21147;&#22686;&#21152;&#65292;&#20174;&#32780;&#23548;&#33268;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#38543;&#30528;LMs&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#36825;&#20123;&#38544;&#31169;&#39118;&#38505;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#23427;&#20204;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#20102;&#35299;LM&#38544;&#31169;&#25915;&#20987;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#30693;&#35782;&#29366;&#20917;&#65292;&#21253;&#25324;&#38656;&#35201;&#26356;&#22810;&#24037;&#20316;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20221;&#20851;&#20110;LM&#38544;&#31169;&#30340;&#25216;&#26415;&#35843;&#26597;&#12290;&#25105;&#20204;&#65288;i&#65289;&#30830;&#23450;&#20102;&#25915;&#20987;&#22312;LMs&#19978;&#23384;&#22312;&#30340;&#26174;&#33879;&#32500;&#24230;&#30340;&#20998;&#31867;&#27861;&#65292;&#65288;ii&#65289;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#26469;&#31361;&#20986;&#20027;&#35201;&#36235;&#21183;&#65292;&#65288;iii&#65289;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#65292;&#31361;&#20986;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35782;&#21035;&#20851;&#38190;&#24046;&#36317;&#65292;&#23637;&#31034;&#24320;&#25918;&#38382;&#39064;&#21644;&#24314;&#35758;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;</title><link>http://arxiv.org/abs/2309.14564</link><description>&lt;p&gt;
&#29983;&#25104;&#33406;&#33293;&#23572;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#12289;&#20197;&#25991;&#26412;&#20026;&#23548;&#21521;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#12289;&#21487;&#37325;&#22797;&#30340;&#20108;&#32500;&#33402;&#26415;&#20316;&#21697;&#65292;&#22914;&#22320;&#26495;&#12289;&#39532;&#36187;&#20811;&#12289;&#38518;&#29943;&#21644;&#33406;&#33293;&#23572;&#30340;&#20316;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#26080;&#32541;&#32441;&#29702;&#27010;&#24565;&#19981;&#21516;&#65292;&#21363;&#24179;&#38138;&#26080;&#32541;&#30340;&#27491;&#26041;&#24418;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#26159;&#30001;&#37325;&#22797;&#30340;&#30456;&#21516;&#23545;&#35937;&#32452;&#25104;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#20108;&#32500;&#32593;&#26684;&#30340;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#38750;&#27491;&#26041;&#24418;&#29943;&#30742;&#65292;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#32972;&#26223;&#32454;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#36129;&#29486;&#23454;&#29616;&#20102;&#38262;&#23884;&#22270;&#26696;&#30340;&#20960;&#20309;&#20248;&#21270;&#65306;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32473;&#23450;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#21487;&#38138;&#30742;&#24418;&#29366;&#31354;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20462;&#25913;&#20108;&#32500;&#32593;&#26684;&#26144;&#23556;&#25216;&#26415;Orbifold Tutte Embedding&#20013;&#20351;&#29992;&#30340;Laplacian&#31639;&#23376;&#21487;&#20197;&#23454;&#29616;&#25152;&#36873;&#24179;&#38754;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#38138;&#30742;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the standard concept of a seamless texture, i.e., square images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and color of a 2D mesh, in order to generate a non-square tile in the shape and appearance of the desired object, with close to no additional background details. We enable geometric optimization of tilings by our key technical contribution: an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. Namely, we prove that modifying the laplacian used in a 2D mesh-mapping technique Orbifold Tutte Embedding - can achieve all possible tiling configurations for a chosen planar 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#27979;&#26469;&#19982;&#20219;&#21153;&#29615;&#22659;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2308.05295</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#24207;&#21015;&#20915;&#31574;&#65306;&#32508;&#21512;&#12289;&#39564;&#35777;&#12289;&#22522;&#30784;&#21644;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception. (arXiv:2308.05295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#27979;&#26469;&#19982;&#20219;&#21153;&#29615;&#22659;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#20197;&#22810;&#31181;&#24418;&#24335;&#34920;&#36798;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#33021;&#38598;&#25104;&#21040;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#26500;&#24314;&#21644;&#39564;&#35777;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#27979;&#23558;&#36825;&#20123;&#25511;&#21046;&#22120;&#19982;&#20219;&#21153;&#29615;&#22659;&#30456;&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#25551;&#36848;&#26597;&#35810;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#30340;&#36755;&#20986;&#26500;&#24314;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#32534;&#30721;&#27169;&#22411;&#30340;&#20219;&#21153;&#30456;&#20851;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#23427;&#39564;&#35777;&#25511;&#21046;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#26159;&#21542;&#19982;&#20854;&#20182;&#29420;&#31435;&#21487;&#29992;&#30340;&#30693;&#35782;&#19968;&#33268;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#33021;&#21253;&#25324;&#29615;&#22659;&#30340;&#25277;&#35937;&#20449;&#24687;&#25110;&#29992;&#25143;&#25552;&#20379;&#30340;&#35268;&#33539;&#12290;&#22914;&#26524;&#39564;&#35777;&#27493;&#39588;&#21457;&#29616;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#65292;&#31639;&#27861;&#20250;&#33258;&#21160;&#32416;&#27491;&#24182;&#37325;&#26032;&#39564;&#35777;&#65292;&#30452;&#21040;&#25152;&#26377;&#30693;&#35782;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decision-making tasks, and to ground these controllers to task environments through visual observations. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model's output to construct an automaton-based controller that encodes the model's task-relevant knowledge. It then verifies whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or user-provided specifications. If this verification step discovers any inconsistency, the algorithm automaticall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09638</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#38454;&#27573;&#20419;&#36827;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#65292;&#29305;&#21035;&#26159;Adam&#65292;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#23545;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#26356;&#21152;&#40065;&#26834;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#27867;&#21270;&#25928;&#26524;&#26356;&#24046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#24402;&#22240;&#20110;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#65306;&#33258;&#36866;&#24212;&#26041;&#27861;&#20542;&#21521;&#20110;&#22312;&#25439;&#22833;&#20989;&#25968;&#26354;&#38754;&#20013;&#26356;&#23574;&#38160;&#30340;&#30406;&#22320;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#20351;&#24471;&#20248;&#21270;&#22120;&#22914;&#26524;&#30406;&#22320;&#30340;&#21560;&#24341;&#33539;&#22260;&#19981;&#22815;&#23485;&#65292;&#23601;&#20250;&#36229;&#20986;&#20854;&#33539;&#22260;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21307;&#38498;&#32508;&#21512;&#35268;&#21010;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#24378;&#35843;&#20102;&#32508;&#21512;&#35268;&#21010;&#22810;&#20010;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.05258</link><description>&lt;p&gt;
&#21307;&#38498;&#20013;&#30340;&#32508;&#21512;&#35268;&#21010;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Integrated Planning in Hospitals: A Review. (arXiv:2307.05258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21307;&#38498;&#32508;&#21512;&#35268;&#21010;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#24378;&#35843;&#20102;&#32508;&#21512;&#35268;&#21010;&#22810;&#20010;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1950&#24180;&#20197;&#26469;&#65292;&#39640;&#25928;&#35268;&#21010;&#21307;&#38498;&#31232;&#32570;&#36164;&#28304;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20026;&#27492;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#26041;&#27861;&#12290;&#23613;&#31649;&#39640;&#25928;&#35268;&#21010;&#21333;&#19968;&#36164;&#28304;&#22914;&#25163;&#26415;&#23460;&#12289;&#24202;&#20301;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#21307;&#25252;&#20154;&#21592;&#24050;&#32463;&#33021;&#22815;&#24102;&#26469;&#24040;&#22823;&#30340;&#25928;&#30410;&#65292;&#20294;&#32508;&#21512;&#35268;&#21010;&#22810;&#20010;&#36164;&#28304;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22823;&#30340;&#28508;&#21147;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#32508;&#21512;&#35268;&#21010;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;&#21307;&#38498;&#19981;&#21516;&#36164;&#28304;&#32508;&#21512;&#35268;&#21010;&#30340;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#25991;&#29486;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#38754;&#36827;&#34892;&#20998;&#26512;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#12290;&#20960;&#20010;&#20132;&#21449;&#27604;&#36739;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27934;&#23519;&#65292;&#20363;&#22914;&#24314;&#27169;&#21644;&#27714;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient planning of scarce resources in hospitals is a challenging task for which a large variety of Operations Research and Management Science approaches have been developed since the 1950s. While efficient planning of single resources such as operating rooms, beds, or specific types of staff can already lead to enormous efficiency gains, integrated planning of several resources has been shown to hold even greater potential, and a large number of integrated planning approaches have been presented in the literature over the past decades.  This paper provides the first literature review that focuses specifically on the Operations Research and Management Science literature related to integrated planning of different resources in hospitals. We collect the relevant literature and analyze it regarding different aspects such as uncertainty modeling and the use of real-life data. Several cross comparisons reveal interesting insights concerning, e.g., relations between the modeling and solut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25351;&#20986;&#20102;VLMs&#20013;&#30456;&#20284;&#24230;&#20998;&#25968;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16048</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65306;&#31890;&#24230;&#21644;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness. (arXiv:2306.16048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25351;&#20986;&#20102;VLMs&#20013;&#30456;&#20284;&#24230;&#20998;&#25968;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24212;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#20102;VLMs&#22312;&#19981;&#21516;&#31890;&#24230;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#27491;&#35780;&#20272;&#20004;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#24615;&#33021;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#20135;&#29983;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#24182;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#25991;&#26412;&#36755;&#20837;&#22312;&#35270;&#35273;&#36755;&#20837;&#19979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#20998;&#25968;&#21487;&#33021;&#20250;&#20559;&#21521;&#26356;&#20855;&#20449;&#24687;&#30340;&#25551;&#36848;&#65292;&#24182;&#19988;&#30001;&#20110;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;VLMs&#26469;&#35828;&#35782;&#21035;&#30456;&#20284;&#20294;&#38169;&#35823;&#30340;&#25551;&#36848;&#20043;&#38388;&#30340;&#27491;&#30830;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#20351;&#29992;VLMs&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the challenges of applying vision-language models (VLMs) to zero-shot visual recognition tasks in an open-world setting, with a focus on contrastive vision-language models such as CLIP. We first examine the performance of VLMs on concepts of different granularity levels. We propose a way to fairly evaluate the performance discrepancy under two experimental setups and find that VLMs are better at recognizing fine-grained concepts. Furthermore, we find that the similarity scores from VLMs do not strictly reflect the correctness of the textual inputs given visual input. We propose an evaluation protocol to test our hypothesis that the scores can be biased towards more informative descriptions, and the nature of the similarity score between embedding makes it challenging for VLMs to recognize the correctness between similar but wrong descriptions. Our study highlights the challenges of using VLMs in open-world settings and suggests directions for future research to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.08041</link><description>&lt;p&gt;
&#20851;&#20110;&#20266;&#36896;&#32435;&#20160;&#22343;&#34913;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#26356;&#25913;&#25968;&#25454;&#38598;&#20197;&#23433;&#35013;&#65288;&#28508;&#22312;&#34394;&#20551;&#30340;&#65289;&#21807;&#19968;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#32435;&#20160;&#22343;&#34913;&#28857;(Nash equilibrium)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#21363;&#30001;&#20854;Q&#20989;&#25968;&#35268;&#23450;&#30340;&#28216;&#25103;&#30340;&#38598;&#21512;&#65292;&#20854;&#20855;&#26377;&#21807;&#19968;&#30340;&#32852;&#21512;&#31574;&#30053;&#20316;&#20026;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23545;&#20110;&#27745;&#26579;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#25968;&#25454;&#27745;&#26579;&#20351;&#25152;&#26377;&#21512;&#29702;&#30340;&#28216;&#25103;&#37117;&#22312;&#20854;&#20013;&#26102;&#65292;&#25915;&#20987;&#25165;&#25104;&#21151;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23558;&#24120;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#22810;&#38754;&#20307;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#23545;&#20110;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#36870;&#32435;&#20160;&#38598;&#20197;&#21450;&#30001;&#25968;&#25454;&#24341;&#36215;&#30340;&#21512;&#29702;&#28216;&#25103;&#38598;&#37117;&#26159;Q&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#22810;&#38754;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26368;&#20248;&#30340;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#24517;&#35201;&#30340;&#27493;&#39588;&#25581;&#31034;&#20102;&#31163;&#32447;MARL&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#32467;&#26500;&#30340;&#19968;&#20123;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DU-Shapley&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;Shapley&#20540;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38598;&#20215;&#20540;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02071</link><description>&lt;p&gt;
DU-Shapley: &#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#20215;&#20540;&#35780;&#20272;&#30340;Shapley&#20540;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation. (arXiv:2306.02071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DU-Shapley&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;Shapley&#20540;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38598;&#20215;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#38598;&#35780;&#20272;&#65292;&#21363;&#37327;&#21270;&#23558;&#19968;&#20010;&#21333;&#29420;&#30340;&#25968;&#25454;&#38598;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#32858;&#21512;&#30340;&#22686;&#37327;&#25910;&#30410;&#65292;&#20197;&#26576;&#20123;&#30456;&#20851;&#39044;&#23450;&#20041;&#20844;&#29992;&#20107;&#19994;&#20026;&#22522;&#30784;&#12290;&#26368;&#36817;&#65292;Shapley&#20540;&#34987;&#25552;&#20986;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#31181;&#22522;&#26412;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24418;&#24335;&#20844;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#20854;&#35745;&#31639;&#36890;&#24120;&#38656;&#35201;&#25351;&#25968;&#26102;&#38388;&#65292;&#22240;&#27492;&#32771;&#34385;&#22522;&#20110;Monte Carlo&#31215;&#20998;&#30340;&#26631;&#20934;&#36817;&#20284;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#27861;&#20173;&#28982;&#26114;&#36149;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#25454;&#38598;&#35780;&#20272;&#38382;&#39064;&#30340;&#32467;&#26500;&#30693;&#35782;&#65292;&#35774;&#35745;&#20102;&#26356;&#26377;&#25928;&#30340;Shapley&#20540;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Shapley&#20540;&#36817;&#20284;&#65292;&#31216;&#20026;&#31163;&#25955;&#22343;&#21248;Shapley (DU-Shapley)&#65292;&#20854;&#34920;&#36798;&#20026;&#26399;&#26395;&#20540;
&lt;/p&gt;
&lt;p&gt;
Many machine learning problems require performing dataset valuation, i.e. to quantify the incremental gain, to some relevant pre-defined utility, of aggregating an individual dataset to others. As seminal examples, dataset valuation has been leveraged in collaborative and federated learning to create incentives for data sharing across several data owners. The Shapley value has recently been proposed as a principled tool to achieve this goal due to formal axiomatic justification. Since its computation often requires exponential time, standard approximation strategies based on Monte Carlo integration have been considered. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation of the Shapley value, referred to as discrete uniform Shapley (DU-Shapley) which is expressed as an expectation under 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.19860</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#22312;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#19968;&#20123;&#26377;&#25928;&#30340;&#36716;&#31227;&#25216;&#26415;&#65288;&#22914;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#65289;&#31561;&#25163;&#27573;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#36136;&#37327;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#23427;&#20204;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29305;&#24449;&#34920;&#31034;&#21644;&#22823;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#35206;&#30422;&#65292;&#24314;&#31435;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#20998;&#21035;&#26159;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#33539;&#24335;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
&lt;/p&gt;</description></item><item><title>ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14106</link><description>&lt;p&gt;
ChatLog: &#35760;&#24405;&#21644;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14106
&lt;/p&gt;
&lt;p&gt;
ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;ChatGPT&#30340;&#30740;&#31350;&#65292;&#20294;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;ChatGPT&#30340;&#34892;&#20026;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#31895;&#21040;&#32454;&#30340;&#26102;&#38388;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ChatLog&#65292;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#27599;&#26376;&#21644;&#27599;&#22825;&#26356;&#26032;&#65306;ChatLog-Monthly&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#27599;&#20010;&#26376;&#25910;&#38598;&#30340;38,730&#20010;&#38382;&#39064;-&#22238;&#31572;&#23545;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;ChatLog-Daily&#21253;&#25324;ChatGPT&#27599;&#22825;&#23545;1000&#20010;&#30456;&#21516;&#38382;&#39064;&#30340;&#38271;&#31687;&#22238;&#31572;&#12290;&#25105;&#20204;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;ChatGPT&#36827;&#21270;&#27169;&#24335;&#23384;&#22312;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#21462;&#20854;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#20998;&#26512;&#20102;ChatGPT&#38543;&#26102;&#38388;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;RoBERTa&#30340;&#26816;&#27979;&#22120;&#22312;&#26032;&#29256;&#26412;&#30340;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#32487;&#32493;&#32500;&#25252;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.00234</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;LLM&#20165;&#22522;&#20110;&#21152;&#20837;&#23569;&#37327;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#39044;&#27979;&#12290;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;LLM&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#21644;&#24635;&#32467;ICL&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#28548;&#28165;&#20854;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32452;&#32455;&#21644;&#35752;&#35770;&#39640;&#32423;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#31574;&#30053;&#12289;&#28436;&#31034;&#35774;&#35745;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;ICL&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#25913;&#36827;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
&lt;/p&gt;</description></item></channel></rss>