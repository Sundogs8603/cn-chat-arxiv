<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.14496</link><description>&lt;p&gt;
&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#23398;&#20064;&#20551;&#35774;&#23384;&#22312;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#26377;&#20851;&#20110;&#27169;&#22411;&#24212;&#22914;&#20309;&#36816;&#34892;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20174;&#35299;&#37322;&#32422;&#26463;&#20013;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;EPAC&#27169;&#22411;&#65288;&#22312;&#26032;&#25968;&#25454;&#26399;&#26395;&#20013;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#27169;&#22411;&#65289;&#26469;&#22238;&#31572;&#21738;&#20123;&#27169;&#22411;&#20250;&#21463;&#30410;&#20110;&#35299;&#37322;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23398;&#20064;&#29702;&#35770;&#24037;&#20855;&#20998;&#26512;&#20102;&#36825;&#31867;&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#20110;&#30001;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20449;&#24687;&#32473;&#20986;&#30340;&#35268;&#33539;&#35299;&#37322;&#30340;&#38480;&#21046;&#65288;&#20197;&#20854;Rademacher&#22797;&#26434;&#24230;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21464;&#20998;&#36817;&#20284;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
&lt;/p&gt;</description></item><item><title>GANTEE &#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#26032;&#30340;&#32508;&#21512;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#27604;&#20854;&#20182;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14480</link><description>&lt;p&gt;
GANTEE&#65306;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation. (arXiv:2303.14480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14480
&lt;/p&gt;
&lt;p&gt;
GANTEE &#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#26032;&#30340;&#32508;&#21512;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#27604;&#20854;&#20182;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27861;&#34987;&#21046;&#23450;&#20026;&#25903;&#25345;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21521;&#26080;&#29615;&#27010;&#24565;&#22270;&#25110;&#26641;&#12290;&#35768;&#22810;&#26032;&#27010;&#24565;&#38656;&#35201;&#28155;&#21152;&#21040;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20998;&#31867;&#27861;&#25193;&#23637;&#20219;&#21153;&#20165;&#26088;&#22312;&#25214;&#21040;&#26032;&#27010;&#24565;&#22312;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#30340;&#26368;&#20339;&#20301;&#32622;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#24403;&#22823;&#22810;&#25968;&#26032;&#27010;&#24565;&#23454;&#38469;&#19978;&#26159;&#22122;&#22768;&#27010;&#24565;&#26102;&#65292;&#23427;&#20204;&#20250;&#28010;&#36153;&#24456;&#22810;&#26102;&#38388;&#12290;&#23427;&#20204;&#20063;&#22240;&#20165;&#20174;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#25910;&#38598;&#35757;&#32451;&#26679;&#26412;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#25366;&#25496;&#30495;&#23454;&#27010;&#24565;&#20043;&#38388;&#26356;&#22810;&#30340;&#19978;&#19979;&#20301;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25554;&#25300;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#25932;&#23545;&#32593;&#32476;&#65288;GANTEE&#65289;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#32570;&#28857;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#35774;&#35745;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#36776;&#21035;&#27169;&#22411;&#20943;&#36731;&#31532;&#19968;&#20010;&#32570;&#28857;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#32508;&#21512;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20197;&#32531;&#35299;&#31532;&#20108;&#20010;&#32570;&#28857;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;GANTEE&#26694;&#26550;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy is formulated as directed acyclic concepts graphs or trees that support many downstream tasks. Many new coming concepts need to be added to an existing taxonomy. The traditional taxonomy expansion task aims only at finding the best position for new coming concepts in the existing taxonomy. However, they have two drawbacks when being applied to the real-scenarios. The previous methods suffer from low-efficiency since they waste much time when most of the new coming concepts are indeed noisy concepts. They also suffer from low-effectiveness since they collect training samples only from the existing taxonomy, which limits the ability of the model to mine more hypernym-hyponym relationships among real concepts. This paper proposes a pluggable framework called Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE) to alleviate these drawbacks. A generative adversarial network is designed in this framework by discriminative models to alleviate the first drawback an
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#30740;&#31350;&#20171;&#32461;&#20102;&#30740;&#21457;&#20986;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;, &#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#26597;&#25214;&#24182;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#29486;,&#20197;&#21327;&#21161;&#22303;&#33879;&#20154;&#31867;&#23547;&#25214;&#20854;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#36951;&#39608;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14475</link><description>&lt;p&gt;
&#20449;&#24687;&#23398;&#20064;&#12289;&#20013;&#24515;&#24615;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#30456;&#20851;&#25991;&#29486;&#26816;&#27979;&#12289;&#22303;&#33879;&#20154;&#31867;&#36951;&#39608;&#24402;&#36824;
&lt;/p&gt;
&lt;p&gt;
Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains. (arXiv:2303.14475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#30740;&#31350;&#20171;&#32461;&#20102;&#30740;&#21457;&#20986;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;, &#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#26597;&#25214;&#24182;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#29486;,&#20197;&#21327;&#21161;&#22303;&#33879;&#20154;&#31867;&#23547;&#25214;&#20854;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#36951;&#39608;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28595;&#22823;&#21033;&#20122;&#21644;&#20854;&#20182;&#21407;&#20303;&#27665;&#38754;&#20020;&#30340;&#32039;&#36843;&#38382;&#39064;&#20043;&#19968;&#26159;&#23558;&#20182;&#20204;&#31062;&#20808;&#30340;&#23608;&#20307;&#36951;&#39608;&#24402;&#36824;&#21040;&#35199;&#26041;&#31185;&#23398;&#26426;&#26500;&#12290;&#25104;&#21151;&#23558;&#36825;&#20123;&#36951;&#39608;&#36820;&#36824;&#21040;&#20854;&#31038;&#21306;&#20197;&#37325;&#26032;&#23433;&#33900;&#65292;&#20027;&#35201;&#21462;&#20915;&#20110;&#22312;1790&#24180;&#33267;1970&#24180;&#26399;&#38388;&#21457;&#34920;&#30340;&#31185;&#23398;&#21644;&#20854;&#20182;&#25991;&#29486;&#20013;&#25214;&#21040;&#35760;&#24405;&#23427;&#20204;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#30001;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#22312;&#8220;&#30740;&#31350;&#12289;&#21644;&#35299;&#12289;&#26356;&#26032;&#8221;&#32593;&#32476;&#65288;RRR&#65289;&#20013;&#36827;&#34892;&#30340;&#21327;&#20316;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#21644;&#24212;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#26469;&#30830;&#23450;&#36825;&#20123;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#33258;&#21160;&#21270;&#26597;&#25214;&#21644;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#26412;&#30340;&#24037;&#20316;&#12290;&#20998;&#31867;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#31934;&#24230;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the pressing issues facing Australian and other First Nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in Western scientific institutions. The success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. This article reports on collaborative research by data scientists and social science researchers in the Research, Reconcile, Renew Network (RRR) to develop and apply text mining techniques to identify this vital information. We describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevant texts. Classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. rele
&lt;/p&gt;</description></item><item><title>3Mformer&#26159;&#19968;&#31181;&#22810;&#38454;&#22810;&#27169;&#21464;&#24418;&#22120;&#65292;&#36890;&#36807;&#24418;&#25104;&#36229;&#22270;&#25429;&#25417;&#36523;&#20307;&#20851;&#33410;&#32452;&#30340;&#39640;&#38454;&#36816;&#21160;&#27169;&#24335;&#65292;&#20351;&#24471;&#22312;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;</title><link>http://arxiv.org/abs/2303.14474</link><description>&lt;p&gt;
3Mformer: &#22810;&#38454;&#22810;&#27169;&#21464;&#24418;&#22120;&#29992;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition. (arXiv:2303.14474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14474
&lt;/p&gt;
&lt;p&gt;
3Mformer&#26159;&#19968;&#31181;&#22810;&#38454;&#22810;&#27169;&#21464;&#24418;&#22120;&#65292;&#36890;&#36807;&#24418;&#25104;&#36229;&#22270;&#25429;&#25417;&#36523;&#20307;&#20851;&#33410;&#32452;&#30340;&#39640;&#38454;&#36816;&#21160;&#27169;&#24335;&#65292;&#20351;&#24471;&#22312;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#27169;&#22411;&#20351;&#29992;GCN&#36890;&#36807;&#36830;&#25509;&#30340;3D&#20154;&#20307;&#20851;&#33410;&#20195;&#34920;&#20154;&#20307;&#12290;GCNs&#32858;&#21512;&#19968;&#21040;&#23569;&#37327;&#36339;&#36291;&#22270;&#37051;&#22495;&#65292;&#24182;&#24573;&#30053;&#26410;&#36830;&#25509;&#36523;&#20307;&#20851;&#33410;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#24418;&#25104;&#36229;&#22270;&#26469;&#27169;&#25311;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36229;&#36793;&#65288;&#20363;&#22914;&#65292;&#31532;&#19977;&#21644;&#31532;&#22235;&#38454;&#36229;&#36793;&#25429;&#33719;&#19977;&#20010;&#21644;&#22235;&#20010;&#33410;&#28857;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#25429;&#33719;&#36523;&#20307;&#20851;&#33410;&#32452;&#30340;&#39640;&#38454;&#36816;&#21160;&#27169;&#24335;&#12290;&#25105;&#20204;&#23558;&#21160;&#20316;&#24207;&#21015;&#20998;&#25104;&#26102;&#38388;&#22359;&#65292;Higher-order Transformer (HoT)&#22522;&#20110;&#65288;i&#65289;&#36523;&#20307;&#20851;&#33410;&#65292;&#65288;ii&#65289;&#36523;&#20307;&#20851;&#33410;&#30340;&#25104;&#23545;&#38142;&#25509;&#21644;&#65288;iii&#65289;&#39592;&#26550;&#36523;&#20307;&#20851;&#33410;&#30340;&#39640;&#38454;&#36229;&#36793;&#65292;&#20135;&#29983;&#27599;&#20010;&#26102;&#38388;&#22359;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#26032;&#22411;Multi-order Multi-mode Transformer(3Mformer)&#32467;&#21512;&#36825;&#20123;&#36229;&#32423;&#36793;&#30340;HoT&#23884;&#20837;&#65292;&#20854;&#20013;&#20004;&#20010;&#27169;&#22359;&#30340;&#39034;&#24207;&#21487;&#20197;&#20132;&#25442;&#65292;&#20197;&#23454;&#29616;&#22522;&#20110;'channel-temporal block'&#65292;'order-channel-body joint'&#65292;'channel-hyper-edge-order'&#30340;&#32806;&#21512;&#27169;&#24335;&#20196;&#29260;&#19978;&#30340;&#32806;&#21512;&#27169;&#24335;&#27880;&#24847;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;3Mformer&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many skeletal action recognition models use GCNs to represent the human body by 3D body joints connected body parts. GCNs aggregate one- or few-hop graph neighbourhoods, and ignore the dependency between not linked body joints. We propose to form hypergraph to model hyper-edges between graph nodes (e.g., third- and fourth-order hyper-edges capture three and four nodes) which help capture higher-order motion patterns of groups of body joints. We split action sequences into temporal blocks, Higher-order Transformer (HoT) produces embeddings of each temporal block based on (i) the body joints, (ii) pairwise links of body joints and (iii) higher-order hyper-edges of skeleton body joints. We combine such HoT embeddings of hyper-edges of orders 1, ..., r by a novel Multi-order Multi-mode Transformer (3Mformer) with two modules whose order can be exchanged to achieve coupled-mode attention on coupled-mode tokens based on 'channel-temporal block', 'order-channel-body joint', 'channel-hyper-edg
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35752;&#35770;&#26368;&#26032;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#24212;&#29992;&#24191;&#27867;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14467</link><description>&lt;p&gt;
&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#21450;&#20854;&#21464;&#31181;&#30340;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on the Densest Subgraph Problem and its Variants. (arXiv:2303.14467v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35752;&#35770;&#26368;&#26032;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#24212;&#29992;&#24191;&#27867;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#35201;&#27714;&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25214;&#21040;&#19968;&#20010;&#39030;&#28857;&#23376;&#38598;&#65292;&#20854;&#35825;&#23548;&#23376;&#22270;&#30340;&#23494;&#24230;&#24230;&#37327;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31639;&#27861;&#25991;&#29486;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#20116;&#21313;&#24180;&#26469;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#21464;&#31181;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#24314;&#31435;&#22312;&#36825;&#20010;&#22522;&#26412;&#23450;&#20041;&#20043;&#19978;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#38382;&#39064;&#30340;&#30740;&#31350;&#20852;&#36259;&#20877;&#27425;&#22797;&#33487;&#65292;2022&#24180;&#21644;2023&#24180;&#21457;&#34920;&#20102;&#19968;&#20123;&#24320;&#21019;&#24615;&#30340;&#32467;&#26524;&#12290;&#26412;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;&#22522;&#26412;&#32467;&#26524;&#30340;&#28145;&#20837;&#27010;&#36848;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#35768;&#22810;&#21464;&#31181;&#30340;&#35814;&#23613;&#35206;&#30422;&#65292;&#29305;&#21035;&#20851;&#27880;&#26368;&#26032;&#32467;&#26524;&#12290;&#35843;&#26597;&#36824;&#25552;&#20379;&#20102;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20010;&#27704;&#24658;&#30740;&#31350;&#35805;&#39064;&#20013;&#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Densest Subgraph Problem requires to find, in a given graph, a subset of vertices whose induced subgraph maximizes a measure of density. The problem has received a great deal of attention in the algorithmic literature over the last five decades, with many variants proposed and many applications built on top of this basic definition. Recent years have witnessed a revival of research interest on this problem with several interesting contributions, including some groundbreaking results, published in 2022 and 2023. This survey provides a deep overview of the fundamental results and an exhaustive coverage of the many variants proposed in the literature, with a special attention on the most recent results. The survey also presents a comprehensive overview of applications and discusses some interesting open problems for this evergreen research topic.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21482;&#26377;&#31532;&#19968;&#24103;&#21644;&#26368;&#21518;&#19968;&#24103;&#30340;&#24773;&#20917;&#30340;&#22810;&#26679;&#21270;&#21160;&#20316;&#20013;&#38388;&#24103;&#29983;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#26041;&#26696;&#21644;&#20004;&#20010;&#23545;&#25239;&#30340;&#33258;&#22238;&#24402;&#32593;&#32476;&#65292;&#20197;&#21450;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25340;&#25509;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#36816;&#21160;&#36136;&#37327;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14457</link><description>&lt;p&gt;
&#21452;&#23039;&#24577;&#25340;&#21512;&#30340;&#22810;&#26679;&#21270;&#21160;&#20316;&#20013;&#38388;&#24103;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Diverse Motion In-betweening with Dual Posture Stitching. (arXiv:2303.14457v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21482;&#26377;&#31532;&#19968;&#24103;&#21644;&#26368;&#21518;&#19968;&#24103;&#30340;&#24773;&#20917;&#30340;&#22810;&#26679;&#21270;&#21160;&#20316;&#20013;&#38388;&#24103;&#29983;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#26041;&#26696;&#21644;&#20004;&#20010;&#23545;&#25239;&#30340;&#33258;&#22238;&#24402;&#32593;&#32476;&#65292;&#20197;&#21450;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25340;&#25509;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#36816;&#21160;&#36136;&#37327;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#38388;&#24103;&#29983;&#25104;&#26159;&#19968;&#31181;&#26681;&#25454;&#21021;&#22987;&#21644;&#30446;&#26631;&#35282;&#33394;&#29366;&#24577;&#29983;&#25104;&#36716;&#25442;&#30340;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20316;&#21697;&#38656;&#35201;&#22810;&#20010;&#65288;&#36890;&#24120;&gt;10&#65289;&#24103;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#20123;&#24103;&#24182;&#19981;&#24635;&#26159;&#21487;&#35775;&#38382;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#26159;&#19968;&#20010;&#32858;&#28966;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#24403;&#21482;&#26377;&#31532;&#19968;&#24103;&#21644;&#26368;&#21518;&#19968;&#24103;&#26102;&#29983;&#25104;&#36807;&#28193;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21452;&#21521;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#20004;&#20010;&#23545;&#25239;&#30340;&#33258;&#22238;&#24402;&#32593;&#32476;&#20174;&#36215;&#22987;&#21644;&#32456;&#27490;&#24103;&#29983;&#25104;&#21069;&#21521;&#21644;&#21518;&#21521;&#36807;&#28193;&#65292;&#24182;&#23558;&#23427;&#20204;&#22312;&#36807;&#28193;&#30340;&#20013;&#38388;&#20301;&#32622;&#65288;&#21363;&#27809;&#26377;&#20005;&#26684;&#30340;&#23454;&#22320;&#30495;&#30456;&#65289;&#36827;&#34892;&#25340;&#25509;&#12290;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#33258;&#22238;&#24402;&#32593;&#32476;&#26159;&#36890;&#36807;&#23547;&#25214;&#19968;&#23545;&#20248;&#21270;&#30340;&#28508;&#22312;&#20195;&#30721;&#65292;&#20351;&#23427;&#20204;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#26032;&#22411;&#25340;&#25509;&#25439;&#22833;&#26368;&#23567;&#21270;&#26469;&#20248;&#21270;&#30340;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LaFAN1&#21644;Human3.6m&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#36816;&#21160;&#36136;&#37327;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-betweening is a technique for generating transitions given initial and target character states. The majority of existing works require multiple (often $&gt;$10) frames as input, which are not always accessible. Our work deals with a focused yet challenging problem: to generate the transition when given exactly two frames (only the first and last). To cope with this challenging scenario, we implement our bi-directional scheme which generates forward and backward transitions from the start and end frames with two adversarial autoregressive networks, and stitches them in the middle of the transition where there is no strict ground truth. The autoregressive networks based on conditional variational autoencoders (CVAE) are optimized by searching for a pair of optimal latent codes that minimize a novel stitching loss between their outputs. Results show that our method achieves higher motion quality and more diverse results than existing methods on both the LaFAN1 and Human3.6m datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#35299;&#20915;&#26080;&#20840;&#26631;&#31614;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#24635;&#32467;&#20102;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#21644;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.14453</link><description>&lt;p&gt;
&#26080;&#20840;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning without Full Labels: A Survey. (arXiv:2303.14453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#35299;&#20915;&#26080;&#20840;&#26631;&#31614;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#24635;&#32467;&#20102;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#21644;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#24050;&#25104;&#20026;&#23454;&#38469;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#21033;&#29992;&#20998;&#25955;&#19988;&#38544;&#31169;&#30340;&#25968;&#25454;&#26500;&#24314;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#36825;&#35201;&#27714;&#25968;&#25454;&#24517;&#39035;&#26159;&#20840;&#26631;&#31614;&#30340;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#24471;&#65292;&#22240;&#20026;&#21442;&#19982;&#32773;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#25110;&#32773;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#21160;&#26426;&#21644;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26080;&#20840;&#26631;&#31614;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy has become an increasingly important concern in real-world big data applications such as machine learning. To address the problem, federated learning (FL) has been a promising solution to building effective machine learning models from decentralized and private data. Existing federated learning algorithms mainly tackle the supervised learning problem, where data are assumed to be fully labeled. However, in practice, fully labeled data is often hard to obtain, as the participants may not have sufficient domain expertise, or they lack the motivation and tools to label data. Therefore, the problem of federated learning without full labels is important in real-world FL applications. In this paper, we discuss how the problem can be solved with machine learning techniques that leverage unlabeled data. We present a survey of methods that combine FL with semi-supervised learning, self-supervised learning, and transfer learning methods. We also summarize the datasets used to evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#21335;&#38750;&#32422;&#32752;&#20869;&#26031;&#22561;&#24066;&#35774;&#35745;&#19968;&#31181;&#26234;&#33021;&#22403;&#22334;&#31649;&#29702;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#22478;&#24066;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#24223;&#24323;&#29289;&#36164;&#28304;&#19981;&#36275;&#38382;&#39064;&#21644;&#22403;&#22334;&#31649;&#29702;&#19981;&#21892;&#25152;&#23548;&#33268;&#30340;&#29615;&#22659;&#12289;&#20581;&#24247;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14436</link><description>&lt;p&gt;
&#32422;&#32752;&#20869;&#26031;&#22561;&#26234;&#33021;&#22403;&#22334;&#31649;&#29702;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of a Smart Waste Management System for the City of Johannesburg. (arXiv:2303.14436v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#21335;&#38750;&#32422;&#32752;&#20869;&#26031;&#22561;&#24066;&#35774;&#35745;&#19968;&#31181;&#26234;&#33021;&#22403;&#22334;&#31649;&#29702;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#22478;&#24066;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#24223;&#24323;&#29289;&#36164;&#28304;&#19981;&#36275;&#38382;&#39064;&#21644;&#22403;&#22334;&#31649;&#29702;&#19981;&#21892;&#25152;&#23548;&#33268;&#30340;&#29615;&#22659;&#12289;&#20581;&#24247;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#27599;&#20010;&#20154;&#37117;&#20250;&#20135;&#29983;&#22403;&#22334;&#12290;&#21335;&#38750;&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#35768;&#22810;&#22478;&#38215;&#20165;&#26377;&#26377;&#38480;&#30340;&#24223;&#24323;&#29289;&#36164;&#28304;&#12290;&#22478;&#38215;&#20013;&#30340;&#22403;&#22334;&#26469;&#28304;&#21253;&#25324;&#20081;&#25172;&#12289;&#22403;&#22334;&#26742;&#20498;&#32622;&#12289;&#26641;&#26408;&#34987;&#21098;&#26525;&#12289;&#22403;&#22334;&#34987;&#20498;&#22312;&#27827;&#36793;&#20197;&#21450;&#24223;&#24323;&#29289;&#26080;&#27861;&#22238;&#25910;&#21033;&#29992;&#12290;&#36825;&#20123;&#24223;&#24323;&#29289;&#23548;&#33268;&#20102;&#30149;&#33740;&#25968;&#37327;&#22686;&#21152;&#12289;&#31354;&#27668;&#27745;&#26579;&#12289;&#29615;&#22659;&#27745;&#26579;&#20197;&#21450;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#12290;&#26410;&#25910;&#38598;&#30340;&#22403;&#22334;&#34987;&#38543;&#24847;&#20002;&#24323;&#22312;&#34903;&#36947;&#21644;&#25490;&#27700;&#27807;&#20013;&#65292;&#23548;&#33268;&#27946;&#27700;&#12289;&#20256;&#26579;&#30149;&#21644;&#21870;&#40831;&#21160;&#29289;&#30340;&#28363;&#29983;&#21644;&#20256;&#25773;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#32422;&#32752;&#20869;&#26031;&#22561;&#24066;&#35774;&#35745;&#19968;&#31181;&#26234;&#33021;&#22403;&#22334;&#31649;&#29702;&#31995;&#32479;&#65292;&#27492;&#22478;&#24066;&#24050;&#26377;&#22403;&#22334;&#31649;&#29702;&#21592;&#25552;&#20379;&#22403;&#22334;&#36164;&#28304;&#65292;&#22914;&#24223;&#24323;&#29289;&#22403;&#22334;&#26742;&#21644;&#22403;&#22334;&#36710;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every human being in this world produces waste. South Africa is a developing country with many townships that have limited waste resources. Over-increasing population growth overpowers the volume of most municipal authorities to provide even the most essential services. Waste in townships is produced via littering, dumping of bins, cutting of trees, dumping of waste near rivers, and overrunning of waste bins. Waste increases diseases, air pollution, and environmental pollution, and lastly increases gas emissions that contribute to the release of greenhouse gases. The ungathered waste is dumped widely in the streets and drains contributing to flooding, breeding of insects, rodent vectors, and spreading of diseases. Therefore, the aim of this paper is to design a smart waste management system for the city of Johannesburg. The city of Johannesburg contains waste municipality workers and has provided some areas with waste resources such as waste bins and trucks for collecting waste. But th
&lt;/p&gt;</description></item><item><title>Beta-VAE&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#21463;&#28508;&#22312;&#21464;&#37327;&#24635;&#37327;&#24433;&#21709;&#65306;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;PCA&#65292;&#20351;&#29992;&#22823;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;ICA&#12290;</title><link>http://arxiv.org/abs/2303.14430</link><description>&lt;p&gt;
Beta-VAE&#26377;&#20004;&#31181;&#34920;&#29616;&#24418;&#24335;&#65306;PCA&#25110;ICA&#65311;
&lt;/p&gt;
&lt;p&gt;
Beta-VAE has 2 Behaviors: PCA or ICA?. (arXiv:2303.14430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14430
&lt;/p&gt;
&lt;p&gt;
Beta-VAE&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#21463;&#28508;&#22312;&#21464;&#37327;&#24635;&#37327;&#24433;&#21709;&#65306;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;PCA&#65292;&#20351;&#29992;&#22823;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;ICA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Beta-VAE&#26159;&#19968;&#31181;&#38750;&#24120;&#32463;&#20856;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#25193;&#23637;&#29942;&#39048;&#30340;&#20351;&#29992;&#21487;&#20351;&#20449;&#24687;&#36880;&#28176;&#36827;&#20837;&#35299;&#30721;&#22120;&#65292;&#36825;&#26159;&#34920;&#31034;&#35299;&#32544;&#20197;&#21450;&#39640;&#36136;&#37327;&#37325;&#24314;&#30340;&#20851;&#38190;&#12290;&#22312;&#26368;&#36817;&#23545;&#36825;&#31181;&#36855;&#20154;&#32467;&#26500;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#28508;&#22312;&#21464;&#37327;&#30340;&#24635;&#37327;&#21487;&#20197;&#24433;&#21709;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65306;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#28508;&#22312;&#21464;&#37327;&#26102;&#65292;&#32593;&#32476;&#20542;&#21521;&#20110;&#23398;&#20064;&#26368;&#37325;&#35201;&#25110;&#20027;&#35201;&#30340;&#21464;&#37327;&#65292;&#34920;&#29616;&#24471;&#20687;&#19968;&#20010;PCA; &#20351;&#29992;&#38750;&#24120;&#22823;&#37327;&#30340;&#28508;&#22312;&#21464;&#37327;&#26102;&#65292;&#21464;&#37327;&#20542;&#21521;&#20110;&#26356;&#21152;&#35299;&#32544;&#65292;&#34920;&#29616;&#20986;&#31867;&#20284;ICA&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#20026;&#33719;&#21462;&#26368;&#22823;&#20449;&#24687;&#24102;&#23485;&#32780;&#36827;&#34892;&#30340;&#31454;&#20105;&#21487;&#33021;&#23548;&#33268;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beta-VAE is a very classical model for disentangled representation learning, the use of an expanding bottleneck that allow information into the decoder gradually is key to representation disentanglement as well as high-quality reconstruction. During recent experiments on such fascinating structure, we discovered that the total amount of latent variables can affect the representation learnt by the network: with very few latent variables, the network tend to learn the most important or principal variables, acting like a PCA; with very large numbers of latent variables, the variables tend to be more disentangled, and act like an ICA. Our assumption is that the competition between latent variables while trying to gain the most information bandwidth can lead to this phenomenon.
&lt;/p&gt;</description></item><item><title>Sem4SAP&#21033;&#29992;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#36827;&#34892;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;&#21516;&#20041;&#35789;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Sem4SAP&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.14425</link><description>&lt;p&gt;
Sem4SAP: &#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21516;&#20041;&#34920;&#36798;&#24335;&#25366;&#25496;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining. (arXiv:2303.14425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14425
&lt;/p&gt;
&lt;p&gt;
Sem4SAP&#21033;&#29992;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#36827;&#34892;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;&#21516;&#20041;&#35789;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Sem4SAP&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#32780;&#35328;&#65292;&#27169;&#22411;&#29702;&#35299;&#21516;&#20041;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23558;&#20351;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26356;&#20855;&#26377;&#25269;&#24481;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sem4SAP&#30340;&#26694;&#26550;&#65292;&#20174;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#65288;Open-KG&#65289;&#20013;&#25366;&#25496;&#21516;&#20041;&#35789;&#65292;&#24182;&#21033;&#29992;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#31616;&#35201;&#36807;&#28388;Open-KG&#20013;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#39057;&#29575;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#24110;&#21161;&#20302;&#36164;&#28304;&#26080;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#21516;&#20041;&#34920;&#36798;&#24335;&#20043;&#38388;&#30340;&#26680;&#24515;&#35821;&#20041;&#26469;&#25193;&#23637;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#21516;&#20041;&#35789;&#30693;&#35782;&#27880;&#20837;PLMs (Pretrained Language Model)&#20013;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Sem4SAP&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The model's ability to understand synonymous expression is crucial in many kinds of downstream tasks. It will make the model to better understand the similarity between context, and more robust to the synonym substitution attack. However, many Pretrained Language Model (PLM) lack synonym knowledge due to limitation of small-scale synsets and PLM's pretraining objectives. In this paper, we propose a framework called Sem4SAP to mine synsets from Open Knowledge Graph (Open-KG) and using the mined synsets to do synonym-aware pretraining for language models. We propose to coarsly filter the content in Open-KG and use the frequency information to better help the clustering process under low-resource unsupervised conditions. We expand the mined synsets by migrating core semantics between synonymous expressions.We also propose two novel and effective synonym-aware pre-training methods for injecting synonym knowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can dramatically outp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#36873;&#25321;&#30456;&#20851;&#24615;&#19981;&#24378;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;HPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#23558;Stable Diffusion&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20154;&#31867;&#36873;&#25321;&#26041;&#38754;&#20248;&#20110;CLIP&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.14420</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Better Aligning Text-to-Image Models with Human Preference. (arXiv:2303.14420v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14420
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#36873;&#25321;&#30456;&#20851;&#24615;&#19981;&#24378;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;HPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#23558;Stable Diffusion&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20154;&#31867;&#36873;&#25321;&#26041;&#38754;&#20248;&#20110;CLIP&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#34028;&#21187;&#21457;&#23637;&#65292;&#20854;&#20013;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#19981;&#31526;&#65292;&#20363;&#22914;&#32930;&#20307;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#32452;&#21512;&#19981;&#33258;&#28982;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;Stable Foundation Discord&#39057;&#36947;&#30340;&#20154;&#31867;&#36873;&#25321;&#29983;&#25104;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#36873;&#25321;&#30456;&#20851;&#24615;&#19981;&#24378;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#20998;&#31867;&#22120;&#65292;&#24182;&#22522;&#20110;&#35813;&#20998;&#31867;&#22120;&#24471;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20998;&#25968;&#65288;HPS&#65289;&#12290;&#36890;&#36807;HPS&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#23558;Stable Diffusion&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HPS&#22312;&#39044;&#27979;&#20154;&#31867;&#36873;&#25321;&#26041;&#38754;&#20248;&#20110;CLIP&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#26469;&#33258;&#20854;&#20182;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;HPS&#35843;&#25972;Stable Diffusion&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning
&lt;/p&gt;</description></item><item><title>IFSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22270;&#20687;&#20998;&#21106;&#23545;&#26356;&#26032;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#65292;&#23545;&#26410;&#30693;&#31867;&#21035;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.14396</link><description>&lt;p&gt;
IFSeg&#65306;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
IFSeg: Image-free Semantic Segmentation via Vision-Language Model. (arXiv:2303.14396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14396
&lt;/p&gt;
&lt;p&gt;
IFSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22270;&#20687;&#20998;&#21106;&#23545;&#26356;&#26032;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#65292;&#23545;&#26410;&#30693;&#31867;&#21035;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22240;&#20854;&#22312;&#19981;&#21516;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#36328;&#27169;&#24577;&#36716;&#31227;&#65289;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;VL&#39537;&#21160;&#30340;&#20998;&#21106;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#24182;&#19988;&#29616;&#26377;&#26041;&#27861;&#20173;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#22270;&#20687;&#29978;&#33267;&#20998;&#21106;&#27880;&#37322;&#26469;&#36866;&#24212;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#21482;&#26377;&#19968;&#32452;&#30446;&#26631;&#35821;&#20041;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#22270;&#20687;&#21644;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;IFSeg&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;VL&#30340;&#20154;&#24037;&#22270;&#20687;&#20998;&#21106;&#23545;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;VL&#27169;&#22411;&#20197;&#36866;&#24212;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#38543;&#26426;&#35821;&#20041;&#31867;&#21035;&#30340;2D&#22320;&#22270;&#20197;&#21450;&#21478;&#19968;&#20010;&#22320;&#22270;&#30340;&#30456;&#24212;&#21333;&#35789;&#26631;&#35760;&#26469;&#26500;&#36896;&#36825;&#20123;&#20154;&#36896;&#35757;&#32451;&#25968;&#25454;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;VL&#27169;&#22411;&#21487;&#20197;&#23558;&#35821;&#20041;&#30701;&#35821;&#19982;&#20854;&#35270;&#35273;&#34920;&#31034;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#29983;&#25104;&#24102;&#26377;&#22320;&#38754;&#30495;&#23454;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#19981;&#21516;&#32423;&#21035;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) pre-training has recently gained much attention for its transferability and flexibility in novel concepts (e.g., cross-modality transfer) across various visual tasks. However, VL-driven segmentation has been under-explored, and the existing approaches still have the burden of acquiring additional training images or even segmentation annotations to adapt a VL model to downstream segmentation tasks. In this paper, we introduce a novel image-free segmentation task where the goal is to perform semantic segmentation given only a set of the target semantic categories, but without any task-specific images and annotations. To tackle this challenging task, our proposed method, coined IFSeg, generates VL-driven artificial image-segmentation pairs and updates a pre-trained VL model to a segmentation task. We construct this artificial training data by creating a 2D map of random semantic categories and another map of their corresponding word tokens. Given that a pre-trained VL
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21435;&#37325;&#21472;&#32593;&#32476; DoNet&#65292;&#21253;&#21547;&#21452;&#36335;&#24452;&#21306;&#22495;&#20998;&#21106;&#27169;&#22359;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#37325;&#26032;&#32452;&#21512;&#27169;&#22359;&#65292;&#20197;&#21450;&#25513;&#27169;&#24341;&#23548;&#30340;&#21306;&#22495;&#24314;&#35758;&#31574;&#30053;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DoNet&#22312;&#32454;&#32990;&#23398;&#23454;&#20363;&#20998;&#21106;&#39046;&#22495;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14373</link><description>&lt;p&gt;
DoNet&#65306;&#19968;&#31181;&#29992;&#20110;&#32454;&#32990;&#23398;&#23454;&#20363;&#20998;&#21106;&#30340;&#28145;&#24230;&#21435;&#37325;&#21472;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DoNet: Deep De-overlapping Network for Cytology Instance Segmentation. (arXiv:2303.14373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14373
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21435;&#37325;&#21472;&#32593;&#32476; DoNet&#65292;&#21253;&#21547;&#21452;&#36335;&#24452;&#21306;&#22495;&#20998;&#21106;&#27169;&#22359;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#37325;&#26032;&#32452;&#21512;&#27169;&#22359;&#65292;&#20197;&#21450;&#25513;&#27169;&#24341;&#23548;&#30340;&#21306;&#22495;&#24314;&#35758;&#31574;&#30053;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DoNet&#22312;&#32454;&#32990;&#23398;&#23454;&#20363;&#20998;&#21106;&#39046;&#22495;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#32990;&#23398;&#22270;&#20687;&#20013;&#65292;&#32454;&#32990;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#29983;&#29289;&#23398;&#20998;&#26512;&#21644;&#30284;&#30151;&#31579;&#26597;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#37325;&#21472;&#30340;&#21322;&#36879;&#26126;&#32454;&#32990;&#31751;&#23548;&#33268;&#36793;&#30028;&#19981;&#28165;&#26224;&#65292;&#20197;&#21450;&#23558;&#31867;&#20284;&#29289;&#21644;&#30862;&#29255;&#28151;&#28102;&#20026;&#32454;&#32990;&#26680;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#37325;&#21472;&#32593;&#32476;&#65288;DoNet&#65289;&#65292;&#37319;&#29992;&#20998;&#35299;&#21644;&#37325;&#26032;&#32452;&#21512;&#31574;&#30053;&#12290;&#21452;&#36335;&#24452;&#21306;&#22495;&#20998;&#21106;&#27169;&#22359;&#65288;DRM&#65289;&#26126;&#30830;&#23558;&#32454;&#32990;&#32676;&#20998;&#35299;&#20026;&#20132;&#38598;&#21306;&#22495;&#21644;&#34917;&#38598;&#21306;&#22495;&#65292;&#38543;&#21518;&#26159;&#19968;&#20010;&#35821;&#20041;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#37325;&#26032;&#32452;&#21512;&#27169;&#22359;&#65288;CRM&#65289;&#36827;&#34892;&#38598;&#25104;&#12290;&#20026;&#36827;&#19968;&#27493;&#24341;&#20837;&#32990;&#36136;&#20013;&#32454;&#32990;&#26680;&#30340;&#21253;&#21547;&#20851;&#31995;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25513;&#27169;&#24341;&#23548;&#30340;&#21306;&#22495;&#24314;&#35758;&#31574;&#30053;&#65288;MRP&#65289;&#65292;&#23558;&#32454;&#32990;&#27880;&#24847;&#21147;&#22270;&#38598;&#25104;&#21040;&#20869;&#37096;&#32454;&#32990;&#23454;&#20363;&#39044;&#27979;&#20013;&#12290;&#25105;&#20204;&#22312;ISBI2014&#21644;CPS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DoNet&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#32454;&#32990;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell instance segmentation in cytology images has significant importance for biology analysis and cancer screening, while remains challenging due to 1) the extensive overlapping translucent cell clusters that cause the ambiguous boundaries, and 2) the confusion of mimics and debris as nuclei. In this work, we proposed a De-overlapping Network (DoNet) in a decompose-and-recombined strategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposes the cell clusters into intersection and complement regions, followed by a Semantic Consistency-guided Recombination Module (CRM) for integration. To further introduce the containment relationship of the nucleus in the cytoplasm, we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cell attention maps for inner-cell instance prediction. We validate the proposed approach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNet significantly outperforms other state-of-the-art (SOTA) cell instance segme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FlexNeRF&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;&#12290;&#36890;&#36807;&#23545;&#26102;&#38388;&#21644;&#23039;&#24577;&#37197;&#32622;&#30340;&#20248;&#21270;&#20197;&#21450;&#39069;&#22806;&#30340;&#25439;&#22833;&#65292;&#21487;&#22312;&#35266;&#23519;&#35270;&#35282;&#21464;&#24471;&#26356;&#31232;&#30095;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#36825;&#22312;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#34892;&#25429;&#33719;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14368</link><description>&lt;p&gt;
FlexNeRF&#65306;&#20174;&#31232;&#30095;&#35270;&#35282;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views. (arXiv:2303.14368v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlexNeRF&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;&#12290;&#36890;&#36807;&#23545;&#26102;&#38388;&#21644;&#23039;&#24577;&#37197;&#32622;&#30340;&#20248;&#21270;&#20197;&#21450;&#39069;&#22806;&#30340;&#25439;&#22833;&#65292;&#21487;&#22312;&#35266;&#23519;&#35270;&#35282;&#21464;&#24471;&#26356;&#31232;&#30095;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#36825;&#22312;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#34892;&#25429;&#33719;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlexNeRF&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#65292;&#23588;&#20854;&#26159;&#24403;&#20027;&#39064;&#34920;&#29616;&#20986;&#24555;&#36895;/&#22797;&#26434;&#36816;&#21160;&#26102;&#65292;&#38656;&#35201;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#35268;&#33539;&#26102;&#38388;&#21644;&#23039;&#24577;&#37197;&#32622;&#65292;&#20351;&#23039;&#24577;&#30456;&#20851;&#30340;&#36816;&#21160;&#22330;&#21644;&#23039;&#24577;&#26080;&#20851;&#30340;&#26102;&#38388;&#21464;&#24418;&#20114;&#34917;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#26102;&#38388;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#32422;&#26463;&#20197;&#21450;&#23545;&#20013;&#38388;&#34920;&#31034;&#30340;&#39069;&#22806;&#25439;&#22833;&#65288;&#22914;&#20998;&#21106;&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#29978;&#33267;&#22312;&#35266;&#23519;&#21040;&#30340;&#35270;&#35282;&#21464;&#24471;&#26356;&#31232;&#30095;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#23454;&#35777;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#34892;&#25429;&#33719;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#39033;&#30446;&#39029;&#38754;&#32593;&#22336;&#20026;&#65306;https://flex-nerf.github.io/
&lt;/p&gt;
&lt;p&gt;
We present FlexNeRF, a method for photorealistic freeviewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empirically demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is available at: https://flex-nerf.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#26032;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;MOLAO*&#21644;MOLRTDP&#65292;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;SSP&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#38656;&#35201;&#35745;&#31639;&#38750;&#25903;&#37197;&#31574;&#30053;&#30340;&#35206;&#30422;&#38598;&#30340;&#22810;&#30446;&#26631;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;(MOSSP)&#38382;&#39064;&#19978;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#26469;&#25351;&#23548;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2303.14363</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#27010;&#29575;&#35268;&#21010;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heuristic Search for Multi-Objective Probabilistic Planning. (arXiv:2303.14363v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#26032;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;MOLAO*&#21644;MOLRTDP&#65292;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;SSP&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#38656;&#35201;&#35745;&#31639;&#38750;&#25903;&#37197;&#31574;&#30053;&#30340;&#35206;&#30422;&#38598;&#30340;&#22810;&#30446;&#26631;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;(MOSSP)&#38382;&#39064;&#19978;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#26469;&#25351;&#23548;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#24335;&#25628;&#32034;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#32463;&#20856;&#35268;&#21010;&#65292;&#22810;&#30446;&#26631;&#35268;&#21010;&#21644;&#27010;&#29575;&#35268;&#21010;&#65292;&#23427;&#34987;&#24314;&#27169;&#20026;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#65288;SSP&#65289;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#21551;&#21457;&#24335;&#25628;&#32034;&#25193;&#23637;&#21040;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#38382;&#39064;&#31867;&#65292;&#21363;&#22810;&#30446;&#26631;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#65288;MOSSP&#65289;&#65292;&#20854;&#20013;&#38656;&#35201;&#35745;&#31639;&#38750;&#25903;&#37197;&#31574;&#30053;&#30340;&#35206;&#30422;&#38598;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;MOLAO*&#21644;MOLRTDP&#65292;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;SSP&#31639;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#24773;&#20917;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#65292;&#23427;&#20204;&#22312;&#32771;&#34385;&#38382;&#39064;&#30340;&#38543;&#26426;&#21644;&#22810;&#30446;&#26631;&#29305;&#24615;&#26102;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#26469;&#25351;&#23548;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20248;&#28857;&#21644;&#21551;&#21457;&#24335;&#20989;&#25968;&#30340;&#30456;&#23545;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heuristic search is a powerful approach that has successfully been applied to a broad class of planning problems, including classical planning, multi-objective planning, and probabilistic planning modelled as a stochastic shortest path (SSP) problem. Here, we extend the reach of heuristic search to a more expressive class of problems, namely multi-objective stochastic shortest paths (MOSSPs), which require computing a coverage set of non-dominated policies. We design new heuristic search algorithms MOLAO* and MOLRTDP, which extend well-known SSP algorithms to the multi-objective case. We further construct a spectrum of domain-independent heuristic functions differing in their ability to take into account the stochastic and multi-objective features of the problem to guide the search. Our experiments demonstrate the benefits of these algorithms and the relative merits of the heuristics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;(mmLBRA)&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;O-RAN&#20013;&#30340;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#32593;&#32476;&#25317;&#22622;&#21644;&#29992;&#25143;&#25925;&#38556;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14355</link><description>&lt;p&gt;
O-RAN&#20013;&#30340;&#26234;&#33021;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intelligent Load Balancing and Resource Allocation in O-RAN: A Multi-Agent Multi-Armed Bandit Approach. (arXiv:2303.14355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;(mmLBRA)&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;O-RAN&#20013;&#30340;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#32593;&#32476;&#25317;&#22622;&#21644;&#29992;&#25143;&#25925;&#38556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;(O-RAN)&#26550;&#26500;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26550;&#26500;&#30340;&#24320;&#25918;&#25509;&#21475;&#23454;&#29616;&#20102;&#32593;&#32476;&#21151;&#33021;&#34394;&#25311;&#21270;&#65292;&#20351;O-RAN&#25104;&#20026;&#20027;&#35201;&#30340;&#29992;&#25143;&#36890;&#20449;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#39057;&#29575;&#36164;&#28304;&#21644;&#20449;&#24687;&#29190;&#28856;&#20351;&#24471;&#22312;&#27809;&#26377;&#26377;&#25928;&#30340;&#27969;&#37327;&#25511;&#21046;&#25110;&#36164;&#28304;&#20998;&#37197;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20339;&#30340;&#32593;&#32476;&#20307;&#39564;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36827;&#34892;&#22522;&#20110;&#31227;&#21160;&#24615;&#30340;&#36127;&#36733;&#22343;&#34913;&#26469;&#22343;&#21248;&#22320;&#20998;&#24067;&#32593;&#32476;&#36127;&#36733;&#65292;&#36991;&#20813;&#30001;&#21333;&#20010;&#24320;&#25918;&#24335;&#20998;&#24067;&#24335;&#21333;&#20803;(O-DU)&#31649;&#36758;&#30340;&#24320;&#25918;&#24335;&#26080;&#32447;&#21333;&#20803;(O-RU)&#19978;&#36807;&#24230;&#32858;&#38598;&#30340;&#36127;&#36733;&#23548;&#33268;&#32593;&#32476;&#25317;&#22622;&#21644;&#29992;&#25143;&#25925;&#38556;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#26469;&#23454;&#29616;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;(mmLBRA)&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#36127;&#36733;&#22343;&#34913;&#21644;&#25552;&#39640;O-RAN&#32593;&#32476;&#30340;&#26377;&#25928;&#24635;&#21644;&#36895;&#29575;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The open radio access network (O-RAN) architecture offers a cost-effective and scalable solution for internet service providers to optimize their networks using machine learning algorithms. The architecture's open interfaces enable network function virtualization, with the O-RAN serving as the primary communication device for users. However, the limited frequency resources and information explosion make it difficult to achieve an optimal network experience without effective traffic control or resource allocation. To address this, we consider mobility-aware load balancing to evenly distribute loads across the network, preventing network congestion and user outages caused by excessive load concentration on open radio unit (O-RU) governed by a single open distributed unit (O-DU). We have proposed a multi-agent multi-armed bandit for load balancing and resource allocation (mmLBRA) scheme, designed to both achieve load balancing and improve the effective sum-rate performance of the O-RAN ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2303.14338</link><description>&lt;p&gt;
&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#65288;&#25193;&#23637;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract). (arXiv:2303.14338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hilbert &#21644; Ackermann &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#23436;&#22791;&#29702;&#35770;&#19968;&#33268;&#22320;&#25193;&#23637;&#21040;&#23436;&#22791;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#21733;&#24503;&#23572;&#22522;&#26412;&#19978;&#35777;&#26126;&#20102;&#20219;&#20309;&#33021;&#22815;&#23545;&#20854;&#33258;&#36523;&#38472;&#36848;&#21450;&#20854;&#35777;&#26126;&#36827;&#34892;&#32534;&#30721;&#30340;&#29702;&#35770;&#37117;&#21253;&#21547;&#20102;&#30495;&#23454;&#20294;&#19981;&#33021;&#34987;&#35777;&#26126;&#30340;&#38472;&#36848;&#12290;&#21733;&#24503;&#23572;&#30340;&#26500;&#36896;&#24182;&#27809;&#26377;&#22238;&#31572;&#24076;&#23572;&#20271;&#29305;&#30340;&#38382;&#39064;&#65292;&#24076;&#23572;&#20271;&#29305;&#35748;&#20026;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#20844;&#29702;&#26469;&#35777;&#26126;&#36234;&#26469;&#36234;&#22810;&#30340;&#30495;&#23454;&#38472;&#36848;&#65292;&#23601;&#20687;&#31185;&#23398;&#19968;&#26679;&#65292;&#23436;&#22791;&#24615;&#26159;&#28040;&#22833;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24213;&#23618;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#24182;&#25551;&#36848;&#20102;&#23548;&#33268;&#21487;&#27979;&#35797;&#20294;&#19981;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#36712;&#36857;&#65292;&#36825;&#20123;&#23447;&#25945;&#25193;&#23637;&#20102;&#20256;&#32479;&#23447;&#25945;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20202;&#24335;&#21644;&#20449;&#20208;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#37117;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36923;&#36753;&#29702;&#35770;&#30340;&#24819;&#27861;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#24182;&#20837;&#36825;&#20010;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#26469;&#23450;&#20041;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hilbert and Ackermann asked for a method to consistently extend incomplete theories to complete theories. G\"odel essentially proved that any theory capable of encoding its own statements and their proofs contains statements that are true but not provable. Hilbert did not accept that G\"odel's construction answered his question, and in his late writings and lectures, G\"odel agreed that it did not, since theories can be completed incrementally, by adding axioms to prove ever more true statements, as science normally does, with completeness as the vanishing point. This pragmatic view of validity is familiar not only to scientists who conjecture test hypotheses but also to real estate agents and other dealers, who conjure claims, albeit invalid, as necessary to close a deal, confident that they will be able to conjure other claims, albeit invalid, sufficient to make the first claims valid. We study the underlying logical process and describe the trajectories leading to testable but unfal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#21152;&#36895;&#30697;&#38453;&#35206;&#30422;&#31639;&#27861;(GAMCA), &#36890;&#36807;&#20351;&#29992;&#24182;&#34892;GPU&#30697;&#38453;&#25805;&#20316;&#26367;&#25442;CPU&#36339;&#33310;&#38142;&#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#21152;&#36895;&#35299;&#20915;&#22522;&#20110;&#31934;&#30830;&#35206;&#30422;&#30340;&#22810;&#37325;&#22270;&#26696;&#29256;&#24335;&#20998;&#35299;&#20013;&#30340;NP&#38590;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GAMCA&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;5-7&#20493;&#65292;&#22312;24&#26680;&#26426;&#22120;&#19978;&#65292;&#27604;&#24182;&#34892;CPU&#31639;&#27861;&#24555;100&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.14335</link><description>&lt;p&gt;
&#22810;&#37325;&#22270;&#26696;&#29256;&#24335;&#20998;&#35299;&#20013;&#30340;GPU&#21152;&#36895;&#30697;&#38453;&#35206;&#30422;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GPU-accelerated Matrix Cover Algorithm for Multiple Patterning Layout Decomposition. (arXiv:2303.14335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#21152;&#36895;&#30697;&#38453;&#35206;&#30422;&#31639;&#27861;(GAMCA), &#36890;&#36807;&#20351;&#29992;&#24182;&#34892;GPU&#30697;&#38453;&#25805;&#20316;&#26367;&#25442;CPU&#36339;&#33310;&#38142;&#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#21152;&#36895;&#35299;&#20915;&#22522;&#20110;&#31934;&#30830;&#35206;&#30422;&#30340;&#22810;&#37325;&#22270;&#26696;&#29256;&#24335;&#20998;&#35299;&#20013;&#30340;NP&#38590;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GAMCA&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;5-7&#20493;&#65292;&#22312;24&#26680;&#26426;&#22120;&#19978;&#65292;&#27604;&#24182;&#34892;CPU&#31639;&#27861;&#24555;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#22270;&#26696;&#20809;&#21051;&#65288;MPL&#65289;&#34987;&#35748;&#20026;&#26159;&#20811;&#26381;&#24120;&#35268;&#20809;&#23398;&#20809;&#21051;&#20998;&#36776;&#29575;&#38480;&#21046;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#30001;&#20110;&#19979;&#19968;&#20195;&#20809;&#21051;&#25216;&#26415;&#30340;&#24310;&#36831;&#12290; &#38543;&#30528;&#26230;&#20307;&#31649;&#23610;&#23544;&#30340;&#19981;&#26029;&#32553;&#23567;&#65292;&#36866;&#29992;&#20110;&#22810;&#27425;&#20114;&#38145;&#65288;MPLD&#65289;&#25216;&#26415;&#30340;&#24067;&#23616;&#20998;&#35299;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#20197;&#25552;&#39640;&#20808;&#36827;&#33410;&#28857;&#20013;&#30340;&#21487;&#21046;&#36896;&#24615;&#12290;&#24403;&#25513;&#33180;&#25968; $k \geq 3$ &#26102;&#65292;MPLD&#38382;&#39064;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#22240;&#27492;&#23545;&#20110;&#23454;&#38469;&#35774;&#35745;&#21487;&#33021;&#20250;&#20986;&#29616;&#36816;&#34892;&#26102;&#38388;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#24067;&#23616;&#27169;&#24335;&#30340;&#25968;&#37327;&#22312;&#24037;&#19994;&#24067;&#23616;&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36825;&#22952;&#30861;&#20102;MPLD&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29992;&#24182;&#34892;GPU&#30697;&#38453;&#25805;&#20316;&#26367;&#25442;&#20102;CPU&#30340;&#36339;&#33310;&#38142;&#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#21152;&#36895;&#35299;&#20915;&#22522;&#20110;&#31934;&#30830;&#35206;&#30422;&#30340;MPLD&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;GPU&#21152;&#36895;&#30697;&#38453;&#35206;&#30422;&#31639;&#27861;&#65288;GAMCA&#65289;&#26088;&#22312;&#22788;&#29702;&#20855;&#26377;&#25968;&#21315;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;MPLD&#38382;&#39064;&#65292;&#26102;&#38388;&#20165;&#20026;&#20197;&#21069;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;NVIDIA GTX 1080 Ti&#19978;&#65292;GAMCA&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;5-7&#20493;&#65292;&#22312;24&#26680;&#26426;&#22120;&#19978;&#65292;&#27604;&#24182;&#34892;CPU&#31639;&#27861;&#24555;100&#20493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MPLD&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;EDA&#36719;&#20214;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple patterning lithography (MPL) is regarded as one of the most promising ways of overcoming the resolution limitations of conventional optical lithography due to the delay of next-generation lithography technology. As the feature size continues to decrease, layout decomposition for multiple patterning lithography (MPLD) technology is becoming increasingly crucial for improving the manufacturability in advanced nodes. The decomposition process refers to assigning the layout features to different mask layers according to the design rules and density requirements. When the number of masks $k \geq 3$, the MPLD problems are NP-hard and thus may suffer from runtime overhead for practical designs. However, the number of layout patterns is increasing exponentially in industrial layouts, which hinders the runtime performance of MPLD models. In this research, we substitute the CPU's dance link data structure with parallel GPU matrix operations to accelerate the solution for exact cover-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2303.14334</link><description>&lt;p&gt;
&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#38405;&#35835;&#30028;&#38754;&#22686;&#24378;&#23398;&#26415;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#26159;&#23398;&#32773;&#21521;&#20182;&#20154;&#20256;&#36882;&#30693;&#35782;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#35770;&#25991;&#20449;&#24687;&#23494;&#38598;&#65292;&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#37327;&#30340;&#22686;&#38271;&#65292;&#38656;&#35201;&#26032;&#25216;&#26415;&#25903;&#25345;&#38405;&#35835;&#36807;&#31243;&#12290;&#19982;&#36890;&#36807;&#20114;&#32852;&#32593;&#25216;&#26415;&#36716;&#21464;&#30340;&#26597;&#25214;&#35770;&#25991;&#36807;&#31243;&#19981;&#21516;&#65292;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#30340;&#20307;&#39564;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;&#34429;&#28982;PDF&#26684;&#24335;&#22240;&#20854;&#20415;&#25658;&#24615;&#32780;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#26377;&#37325;&#22823;&#32570;&#28857;&#65292;&#21253;&#25324;&#65306;&#38745;&#24577;&#20869;&#23481;&#65292;&#20302;&#35270;&#35273;&#35835;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#24046;&#65292;&#20197;&#21450;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#38405;&#35835;&#22256;&#38590;&#12290;&#26412;&#25991;&#25506;&#35752;&#8220;&#26368;&#36817;&#30340;AI&#21644;HCI&#36827;&#23637;&#33021;&#21542;&#20026;&#36951;&#30041;&#30340;PDF&#25552;&#20379;&#26234;&#33021;&#65292;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#65311;&#8221;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65292;&#36825;&#26159;&#22810;&#20010;&#26426;&#26500;&#30340;&#21327;&#20316;&#21162;&#21147;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;&#30740;&#31350;&#35770;&#25991;&#33258;&#21160;&#21019;&#24314;&#21160;&#24577;&#38405;&#35835;&#30028;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#35757;&#32451;/&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#65292;&#36824;&#21033;&#29992;&#26816;&#32034;&#30340;&#30495;&#23454;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#35753;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#37096;&#32626;&#21518;&#26356;&#26032;&#30456;&#20851;&#25968;&#25454;&#20197;&#25913;&#21892;&#27169;&#22411;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14333</link><description>&lt;p&gt;
&#37319;&#29992;&#26816;&#32034;&#30340;&#35757;&#32451;/&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Train/Test-Time Adaptation with Retrieval. (arXiv:2303.14333v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#35757;&#32451;/&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#65292;&#36824;&#21033;&#29992;&#26816;&#32034;&#30340;&#30495;&#23454;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#35753;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#37096;&#32626;&#21518;&#26356;&#26032;&#30456;&#20851;&#25968;&#25454;&#20197;&#25913;&#21892;&#27169;&#22411;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Train/Test-Time Adaptation with Retrieval (${\rm T^3AR}$)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#26816;&#32034;&#30340;&#22806;&#37096;&#26679;&#26412;&#27744;&#21644;&#26816;&#32034;&#27169;&#22359;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#23545;&#27169;&#22411;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#25512;&#26029;&#20043;&#21069;&#65292;${\rm T^3AR}$&#20351;&#29992;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#26469;&#20351;&#32473;&#23450;&#30340;&#27169;&#22411;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#20854;&#22122;&#22768;&#20998;&#24067;&#21033;&#29992;&#26816;&#32034;&#30340;&#30495;&#23454;&#26679;&#26412;&#26469;&#25552;&#39640;&#30446;&#26631;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#29305;&#24449;&#36866;&#24212;&#24615;&#12290;&#26816;&#32034;&#30495;&#23454;&#22270;&#20687;&#23545;&#20110;${\rm T^3AR}$&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26469;&#24357;&#34917;&#36866;&#24212;&#25968;&#25454;&#30340;&#32570;&#20047;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#20854;&#20182;&#36866;&#24212;&#24615;&#31639;&#27861;&#25152;&#20570;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26816;&#32034;&#27169;&#22359;&#30340;&#23384;&#22312;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35753;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#26377;&#21487;&#33021;&#36890;&#36807;&#21152;&#20837;&#26356;&#22810;&#30456;&#20851;&#25968;&#25454;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#36866;&#24212;&#24615;&#65292;&#25110;&#23436;&#20840;&#21024;&#38500;&#21487;&#33021;&#30001;&#20110;&#37096;&#32626;&#21518;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#32780;&#19981;&#20877;&#21487;&#29992;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Train/Test-Time Adaptation with Retrieval (${\rm T^3AR}$), a method to adapt models both at train and test time by means of a retrieval module and a searchable pool of external samples. Before inference, ${\rm T^3AR}$ adapts a given model to the downstream task using refined pseudo-labels and a self-supervised contrastive objective function whose noise distribution leverages retrieved real samples to improve feature adaptation on the target data manifold. The retrieval of real images is key to ${\rm T^3AR}$ since it does not rely solely on synthetic data augmentations to compensate for the lack of adaptation data, as typically done by other adaptation algorithms. Furthermore, thanks to the retrieval module, our method gives the user or service provider the possibility to improve model adaptation on the downstream task by incorporating further relevant data or to fully remove samples that may no longer be available due to changes in user preference after deployment. First, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#28608;&#21169;&#25514;&#26045;&#12289;&#21487;&#20197;&#22312;&#32447;&#23454;&#26045;&#30340;&#20844;&#24179;&#21512;&#29702;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#25913;&#21892;&#20849;&#20139;&#20056;&#36710;&#31995;&#32479;&#20013;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14332</link><description>&lt;p&gt;
&#21033;&#29992;&#31616;&#21333;&#28608;&#21169;&#25514;&#26045;&#25913;&#21892;&#20849;&#20139;&#20056;&#36710;&#31995;&#32479;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Using Simple Incentives to Improve Two-Sided Fairness in Ridesharing Systems. (arXiv:2303.14332v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#28608;&#21169;&#25514;&#26045;&#12289;&#21487;&#20197;&#22312;&#32447;&#23454;&#26045;&#30340;&#20844;&#24179;&#21512;&#29702;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#25913;&#21892;&#20849;&#20139;&#20056;&#36710;&#31995;&#32479;&#20013;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#20849;&#20139;&#20056;&#36710;&#26381;&#21153;&#30340;&#35746;&#21333;&#35843;&#24230;&#31639;&#27861;&#36890;&#36807;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23545;&#35831;&#27714;&#36827;&#34892;&#20248;&#21270;&#21305;&#37197;&#65292;&#20197;&#26368;&#22823;&#21270;&#26381;&#21153;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20248;&#21270;&#31639;&#27861;&#20165;&#27880;&#37325;&#25928;&#29575;&#65292;&#23481;&#26131;&#23548;&#33268;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;&#20363;&#22914;&#21496;&#26426;&#65289;&#21644;&#26381;&#21153;&#38656;&#27714;&#26041;&#65288;&#20363;&#22914;&#20056;&#23458;&#65289;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#28608;&#21169;&#25514;&#26045;&#30340;&#20844;&#24179;&#21512;&#29702;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32447;&#23454;&#26045;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#32780;&#21487;&#20197;&#25913;&#21892;&#22810;&#31181;&#20844;&#24179;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art order dispatching algorithms for ridesharing batch passenger requests and allocate them to a fleet of vehicles in a centralized manner, optimizing over the estimated values of each passenger-vehicle matching using integer linear programming (ILP). Using good estimates of future values, such ILP-based approaches are able to significantly increase the service rates (percentage of requests served) for a fixed fleet of vehicles. However, such approaches that focus solely on maximizing efficiency can lead to disparities for both drivers (e.g., income inequality) and passengers (e.g., inequality of service for different groups). Existing approaches that consider fairness only do it for naive assignment policies, require extensive training, or look at only single-sided fairness. We propose a simple incentive-based fairness scheme that can be implemented online as a part of this ILP formulation that allows us to improve fairness over a variety of fairness metrics. Deriving fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;Concrete Autoencoder&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;49,075&#20363;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#25968;&#25454;&#24211;&#20013;&#30340;&#26368;&#20339;100&#20010;&#29305;&#24449;&#65292;&#24182;&#35777;&#23454;Concrete Autoencoder&#26041;&#27861;&#20013;&#30340;&#26435;&#37325;&#35843;&#25972;&#33021;&#22815;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14303</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#35782;&#21035;ICD-10&#32534;&#30721;&#65292;&#20197;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65306;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#38431;&#21015;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Feature Selection to Identify Important ICD-10 Codes for Machine Learning: A Case Study on a Coronary Artery Disease Patient Cohort. (arXiv:2303.14303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;Concrete Autoencoder&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;49,075&#20363;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#25968;&#25454;&#24211;&#20013;&#30340;&#26368;&#20339;100&#20010;&#29305;&#24449;&#65292;&#24182;&#35777;&#23454;Concrete Autoencoder&#26041;&#27861;&#20013;&#30340;&#26435;&#37325;&#35843;&#25972;&#33021;&#22815;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#30340;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#20195;&#30721;&#22240;&#20195;&#30721;&#25968;&#37327;&#36807;&#22810;&#32780;&#22312;&#36873;&#25321;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30456;&#20851;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;49,075&#20363;&#21152;&#25343;&#22823;&#38463;&#23572;&#20271;&#22612;&#30465;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#30340;ICD&#32534;&#30721;&#25968;&#25454;&#24211;&#30340;&#20960;&#31181;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#12289;&#22810;&#38598;&#32676;&#25968;&#25454;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#12289;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#12289;&#20027;&#35201;&#29305;&#24449;&#20998;&#26512;&#20197;&#21450;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;ICD&#26641;&#26435;&#37325;&#35843;&#25972;&#30340;Concrete Autoencoders&#26469;&#36873;&#25321;&#36229;&#36807;9,000&#20010;&#20195;&#30721;&#20013;&#30340;100&#20010;&#26368;&#20339;&#29305;&#24449;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36873;&#25321;&#30340;&#29305;&#24449;&#33021;&#21147;&#65292;&#22522;&#20110;&#20854;&#37325;&#24314;&#21021;&#22987;&#29305;&#24449;&#31354;&#38388;&#21644;&#39044;&#27979;&#20986;&#38498;&#21518;90&#22825;&#30340;&#27515;&#20129;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#39033;&#20219;&#21153;&#20013;&#65292;Concrete Autoencoder&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;Concrete Autoencoder&#26041;&#27861;&#20013;&#30340;&#26435;&#37325;&#35843;&#25972;&#32463;&#35777;&#23454;&#25552;&#39640;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of International Classification of Diseases (ICD) codes in healthcare presents a challenge in selecting relevant codes as features for machine learning models due to this system's large number of codes. In this study, we compared several unsupervised feature selection methods for an ICD code database of 49,075 coronary artery disease patients in Alberta, Canada. Specifically, we employed Laplacian Score, Unsupervised Feature Selection for Multi-Cluster Data, Autoencoder Inspired Unsupervised Feature Selection, Principal Feature Analysis, and Concrete Autoencoders with and without ICD tree weight adjustment to select the 100 best features from over 9,000 codes. We assessed the selected features based on their ability to reconstruct the initial feature space and predict 90-day mortality following discharge. Our findings revealed that the Concrete Autoencoder methods outperformed all other methods in both tasks. Furthermore, the weight adjustment in the Concrete Autoencoder method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#26816;&#27979;&#26032;&#22855;&#24615;&#24182;&#24555;&#36895;&#36866;&#24212;&#20854;&#39046;&#22495;&#27169;&#22411;&#21644;&#34892;&#21160;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.14272</link><description>&lt;p&gt;
&#36890;&#36807;&#36866;&#24212;&#35268;&#21010;&#27169;&#22411;&#23398;&#20064;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning to Operate in Open Worlds by Adapting Planning Models. (arXiv:2303.14272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#26816;&#27979;&#26032;&#22855;&#24615;&#24182;&#24555;&#36895;&#36866;&#24212;&#20854;&#39046;&#22495;&#27169;&#22411;&#21644;&#34892;&#21160;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#20195;&#29702;&#22312;&#39046;&#22495;&#27169;&#22411;&#19981;&#33021;&#20934;&#30830;&#20195;&#34920;&#19990;&#30028;&#30340;&#26032;&#24773;&#20917;&#19979;&#26080;&#27861;&#24456;&#22909;&#22320;&#34892;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#36825;&#31181;&#20195;&#29702;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#33021;&#22815;&#26816;&#27979;&#21040;&#26032;&#22855;&#24615;&#24182;&#26377;&#25928;&#22320;&#36866;&#24212;&#20854;&#39046;&#22495;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#34892;&#21160;&#36873;&#25321;&#12290;&#23427;&#21033;&#29992;&#34892;&#21160;&#25191;&#34892;&#30340;&#35266;&#23519;&#21644;&#26681;&#25454;&#29615;&#22659;&#27169;&#22411;&#30340;&#39044;&#26399;&#27979;&#37327;&#23427;&#20204;&#30340;&#20559;&#24046;&#26469;&#25512;&#26029;&#26032;&#22855;&#24615;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#23545;&#27169;&#22411;&#21464;&#21270;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#20462;&#35746;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;CartPole&#19978;&#25253;&#21578;&#20102;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#19988;&#21487;&#35299;&#37322;&#22320;&#22788;&#29702;&#19968;&#31867;&#26032;&#22855;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning agents are ill-equipped to act in novel situations in which their domain model no longer accurately represents the world. We introduce an approach for such agents operating in open worlds that detects the presence of novelties and effectively adapts their domain models and consequent action selection. It uses observations of action execution and measures their divergence from what is expected, according to the environment model, to infer existence of a novelty. Then, it revises the model through a heuristics-guided search over model changes. We report empirical evaluations on the CartPole problem, a standard Reinforcement Learning (RL) benchmark. The results show that our approach can deal with a class of novelties very quickly and in an interpretable fashion.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#39057;&#35889;&#21644;&#26102;&#38388;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#22686;&#24378;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14254</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#21270;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Diverse and Coherent Augmentation for Time-Series Forecasting. (arXiv:2303.14254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#39057;&#35889;&#21644;&#26102;&#38388;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#22686;&#24378;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#24378;&#25216;&#26415;&#32531;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#35774;&#35745;&#65292;&#21363;&#20351;&#22686;&#24378;&#25913;&#21464;&#20102;&#26102;&#38388;&#21160;&#24577;&#65292;&#31867;&#21035;&#26631;&#31614;&#20063;&#21487;&#20197;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#38024;&#23545;&#39044;&#27979;&#35774;&#35745;&#30340;&#22686;&#24378;&#38656;&#35201;&#22810;&#26679;&#24615;&#21644;&#19982;&#21407;&#22987;&#26102;&#38388;&#21160;&#24577;&#30340;&#36830;&#36143;&#24615;&#12290;&#30001;&#20110;&#23454;&#38469;&#29289;&#29702;&#36807;&#31243;&#20135;&#29983;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#26377;&#26102;&#22495;&#21644;&#39057;&#22495;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#39057;&#35889;&#21644;&#26102;&#38388;&#22686;&#24378;&#65288;STAug&#65289;&#26469;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39057;&#22495;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#26469;&#20998;&#35299;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26435;&#37325;&#37325;&#26032;&#32452;&#35013;&#23376;&#20998;&#37327;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#19982;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#20851;&#31995;&#36830;&#36143;&#19968;&#33268;&#65292;&#22240;&#20026;&#23427;&#20204;&#37117;&#21253;&#21547;&#30456;&#21516;&#30340;&#22522;&#30784;&#20998;&#37327;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#31574;&#30053;&#26469;&#32452;&#21512;&#25968;&#25454;&#65292;&#36825;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series data augmentation mitigates the issue of insufficient training data for deep learning models. Yet, existing augmentation methods are mainly designed for classification, where class labels can be preserved even if augmentation alters the temporal dynamics. We note that augmentation designed for forecasting requires diversity as well as coherence with the original temporal dynamics. As time-series data generated by real-life physical processes exhibit characteristics in both the time and frequency domains, we propose to combine Spectral and Time Augmentation (STAug) for generating more diverse and coherent samples. Specifically, in the frequency domain, we use the Empirical Mode Decomposition to decompose a time series and reassemble the subcomponents with random weights. This way, we generate diverse samples while being coherent with the original temporal relationships as they contain the same set of base components. In the time domain, we adapt a mix-up strategy that genera
&lt;/p&gt;</description></item><item><title>IDGI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569; integrated gradients &#35299;&#37322;&#26174;&#33879;&#24615;&#22270;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#22312;&#20247;&#22810;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14242</link><description>&lt;p&gt;
IDGI&#65306;&#19968;&#20010;&#28040;&#38500; integrated gradients &#35299;&#37322;&#22122;&#22768;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients. (arXiv:2303.14242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14242
&lt;/p&gt;
&lt;p&gt;
IDGI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569; integrated gradients &#35299;&#37322;&#26174;&#33879;&#24615;&#22270;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#22312;&#20247;&#22810;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Integrated Gradients&#65288;IG&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#22522;&#20110;IG&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#23558;&#22122;&#22768;&#38598;&#25104;&#21040;&#20854;&#35299;&#37322;&#26174;&#33879;&#24615;&#22270;&#20013;&#65292;&#20174;&#32780;&#38477;&#20302;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#22122;&#22768;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#25214;&#20986;&#20102;&#22122;&#22768;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#35299;&#37322;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#35201;&#26041;&#21521;&#26799;&#24230;&#38598;&#25104;&#65288;IDGI&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#20351;&#29992;Reimann&#31215;&#20998;&#35745;&#31639;&#38598;&#25104;&#26799;&#24230;&#30340;&#22522;&#20110;IG&#30340;&#26041;&#27861;&#20013;&#12290;&#19977;&#31181;&#22522;&#20110;IG&#30340;&#26041;&#27861;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;IDGI&#22312;&#20247;&#22810;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrated Gradients (IG) as well as its variants are well-known techniques for interpreting the decisions of deep neural networks. While IG-based approaches attain state-of-the-art performance, they often integrate noise into their explanation saliency maps, which reduce their interpretability. To minimize the noise, we examine the source of the noise analytically and propose a new approach to reduce the explanation noise based on our analytical findings. We propose the Important Direction Gradient Integration (IDGI) framework, which can be easily incorporated into any IG-based method that uses the Reimann Integration for integrated gradient computation. Extensive experiments with three IG-based methods show that IDGI improves them drastically on numerous interpretability metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20123;&#20195;&#29702;&#26080;&#27861;&#29702;&#35299;&#20182;&#20204;&#22312;&#22242;&#38431;&#34920;&#29616;&#20013;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#23548;&#33268;&#23398;&#20064;&#27425;&#20248;&#31574;&#30053;&#65292;&#34920;&#29616;&#25042;&#24816;&#12290;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#26816;&#27979;&#24809;&#32602;&#25042;&#24816;&#20195;&#29702;&#24182;&#25913;&#21892;&#20854;&#34892;&#20026;&#65292;&#22242;&#38431;&#25972;&#20307;&#24615;&#33021;&#21644;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20307;&#33021;&#21147;&#37117;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.14227</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Causality Detection for Efficient Multi-Agent Reinforcement Learning. (arXiv:2303.14227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20123;&#20195;&#29702;&#26080;&#27861;&#29702;&#35299;&#20182;&#20204;&#22312;&#22242;&#38431;&#34920;&#29616;&#20013;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#23548;&#33268;&#23398;&#20064;&#27425;&#20248;&#31574;&#30053;&#65292;&#34920;&#29616;&#25042;&#24816;&#12290;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#26816;&#27979;&#24809;&#32602;&#25042;&#24816;&#20195;&#29702;&#24182;&#25913;&#21892;&#20854;&#34892;&#20026;&#65292;&#22242;&#38431;&#25972;&#20307;&#24615;&#33021;&#21644;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20307;&#33021;&#21147;&#37117;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20316;&#20026;&#22242;&#38431;&#23398;&#20064;&#20219;&#21153;&#26102;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#19968;&#20123;&#20195;&#29702;&#21487;&#33021;&#26080;&#27861;&#29702;&#35299;&#20182;&#20204;&#22312;&#22242;&#38431;&#34920;&#29616;&#20013;&#30340;&#30495;&#23454;&#24433;&#21709;&#12290;&#36825;&#20123;&#20195;&#29702;&#26368;&#32456;&#20250;&#23398;&#20064;&#27425;&#20248;&#31574;&#30053;&#65292;&#34920;&#29616;&#20986;&#19981;&#33391;&#30340;&#25042;&#24816;&#34892;&#20026;&#12290;&#26412;&#25991;&#36890;&#36807;&#27491;&#24335;&#34920;&#36848;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#22312;MARL&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#24809;&#32602;&#36825;&#20123;&#25042;&#24816;&#20195;&#29702;&#24182;&#25913;&#21892;&#20854;&#34892;&#20026;&#12290;&#36890;&#36807;&#29702;&#35299;&#20182;&#20204;&#30340;&#26412;&#22320;&#35266;&#27979;&#22914;&#20309;&#22240;&#26524;&#30456;&#20851;&#20110;&#22242;&#38431;&#22870;&#21169;&#65292;&#22242;&#38431;&#20013;&#30340;&#27599;&#20010;&#20195;&#29702;&#37117;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#26159;&#21542;&#26377;&#21161;&#20110;&#23548;&#33268;&#22870;&#21169;&#26469;&#35843;&#25972;&#20854;&#20010;&#20154;&#20449;&#29992;&#36129;&#29486;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#22312;MARL&#20013;&#20351;&#29992;&#22240;&#26524;&#20272;&#35745;&#19981;&#20165;&#21487;&#20197;&#25913;&#21892;&#22242;&#38431;&#25972;&#20307;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#25552;&#21319;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20307;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19968;&#32452;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning a task as a team, some agents in Multi-Agent Reinforcement Learning (MARL) may fail to understand their true impact in the performance of the team. Such agents end up learning sub-optimal policies, demonstrating undesired lazy behaviours. To investigate this problem, we start by formalising the use of temporal causality applied to MARL problems. We then show how causality can be used to penalise such lazy agents and improve their behaviours. By understanding how their local observations are causally related to the team reward, each agent in the team can adjust their individual credit based on whether they helped to cause the reward or not. We show empirically that using causality estimations in MARL improves not only the holistic performance of the team, but also the individual capabilities of each agent. We observe that the improvements are consistent in a set of different environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DRL&#22522;&#30784;&#30340;AVs&#20013;&#25506;&#32034;&#26368;&#20339;&#24179;&#28369;&#20998;&#24067;&#20197;&#28040;&#38500;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#26469;&#20013;&#21644;&#25915;&#20987;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.14197</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#31995;&#32479;&#20013;&#21518;&#38376;&#28040;&#38500;&#30340;&#26368;&#20339;&#24179;&#28369;&#20998;&#24067;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems. (arXiv:2303.14197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DRL&#22522;&#30784;&#30340;AVs&#20013;&#25506;&#32034;&#26368;&#20339;&#24179;&#28369;&#20998;&#24067;&#20197;&#28040;&#38500;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#26469;&#20013;&#21644;&#25915;&#20987;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25552;&#39640;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#25928;&#29575;&#65292;&#20294;&#20063;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20132;&#36890;&#25317;&#22581;&#25110;&#30896;&#25758;&#12290;&#21518;&#38376;&#21151;&#33021;&#36890;&#24120;&#26159;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#20445;&#25345;&#23545;&#30495;&#23454;&#36755;&#20837;&#30340;&#39640;&#20934;&#30830;&#24615;&#24182;&#35825;&#23548;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#29305;&#23450;&#36755;&#20837;&#30340;&#26399;&#26395;&#65288;&#24694;&#24847;&#65289;&#36755;&#20986;&#26469;&#23454;&#29616;&#30340;&#12290;&#24403;&#21069;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#19978;&#65292;&#36825;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;DRL&#22522;&#30784;&#30340;AV&#25511;&#21046;&#22120;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#36755;&#20837;&#26159;&#36830;&#32493;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21363;AV&#21450;&#20854;&#21608;&#22260;&#36710;&#36742;&#30340;&#36895;&#24230;&#21644;&#36317;&#31163;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#31934;&#24515;&#35774;&#35745;&#30340;&#22122;&#22768;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#26469;&#20013;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#19968;&#20010;&#26368;&#20339;&#24179;&#28369;&#65288;&#22122;&#22768;&#65289;&#20998;&#24067;&#26469;&#20445;&#25345;&#30495;&#23454;&#36755;&#20837;&#30340;&#27491;&#24120;&#21151;&#33021;&#21516;&#26102;&#20013;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;DRL&#22522;&#30784;&#30340;AVs&#20013;&#26356;&#22909;&#30340;&#21518;&#38376;&#25915;&#20987;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) enhances the efficiency of Autonomous Vehicles (AV), but also makes them susceptible to backdoor attacks that can result in traffic congestion or collisions. Backdoor functionality is typically incorporated by contaminating training datasets with covert malicious data to maintain high precision on genuine inputs while inducing the desired (malicious) outputs for specific inputs chosen by adversaries. Current defenses against backdoors mainly focus on image classification using image-based features, which cannot be readily transferred to the regression task of DRL-based AV controllers since the inputs are continuous sensor data, i.e., the combinations of velocity and distance of AV and its surrounding vehicles. Our proposed method adds well-designed noise to the input to neutralize backdoors. The approach involves learning an optimal smoothing (noise) distribution to preserve the normal functionality of genuine inputs while neutralizing backdoors. By do
&lt;/p&gt;</description></item><item><title>DeepEpiSolver&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20316;&#20026;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20272;&#35745;SIR&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#30456;&#23545;&#20110;Physics Informed Neural Networks (PINNs)&#26041;&#27861;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;SIDR&#36712;&#36857;&#19978;&#65292;&#24182;&#22312; COVID-19&#12289;HIV&#12289;&#22467;&#21338;&#25289;&#21644;&#30142;&#30149;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#39564;&#35777;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14194</link><description>&lt;p&gt;
DeepEpiSolver&#65306;&#25581;&#31034;Covid&#65292;HIV&#65292;Ebola&#21644;&#30142;&#30149;&#20256;&#25773;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
DeepEpiSolver: Unravelling Inverse problems in Covid, HIV, Ebola and Disease Transmission. (arXiv:2303.14194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14194
&lt;/p&gt;
&lt;p&gt;
DeepEpiSolver&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20316;&#20026;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20272;&#35745;SIR&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#30456;&#23545;&#20110;Physics Informed Neural Networks (PINNs)&#26041;&#27861;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;SIDR&#36712;&#36857;&#19978;&#65292;&#24182;&#22312; COVID-19&#12289;HIV&#12289;&#22467;&#21338;&#25289;&#21644;&#30142;&#30149;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#39564;&#35777;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#37117;&#26159;&#29992;SIR&#38548;&#23460;&#27169;&#22411;&#30340;&#21464;&#20307;&#36827;&#34892;&#24314;&#27169;&#30340;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#32452;&#32806;&#21512;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290; SIR&#27169;&#22411;&#30340;&#31995;&#25968;&#30830;&#23450;&#20102;&#30142;&#30149;&#20256;&#25773;&#36712;&#36857;&#65292;&#22522;&#20110;&#27492;&#21487;&#20197;&#37319;&#21462;&#31215;&#26497;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#31995;&#25968;&#20272;&#35745;&#24517;&#39035;&#26082;&#24555;&#21448;&#20934;&#30830;&#12290;Shaier&#31561;&#20154;&#22312;&#35770;&#25991;&#8220;&#30142;&#30149;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#8221;&#20013;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20272;&#35745;&#20102;SIR&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;PINN&#30340;&#35757;&#32451;&#26102;&#38388;&#24456;&#38271;&#65292;&#26576;&#20123;&#30142;&#30149;&#38656;&#35201;&#25509;&#36817;90&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;PINN&#23545;&#20110;&#26032;&#30340;SIDR&#36712;&#36857;&#19981;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#23398;&#20064;&#20854;&#23545;&#24212;&#30340;SIR&#21442;&#25968;&#38656;&#35201;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;PINN&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28040;&#38500;&#36825;&#20004;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;LSODA&#31639;&#27861;&#35299;&#20915;&#21442;&#25968;&#21644;&#20256;&#25773;&#36712;&#36857;&#20043;&#38388;&#30340;&#27491;&#21521;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20316;&#20026;&#21453;&#28436;&#27714;&#35299;&#22120;&#65292;&#26681;&#25454;&#20256;&#25773;&#36712;&#36857;&#20272;&#35745;SIR&#27169;&#22411;&#30340;&#27491;&#30830;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;DeepEpiSolver&#65292;&#27604;PINN&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;SIDR&#36712;&#36857;&#19978;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#30142;&#30149;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Covid&#65292;HIV&#65292;Ebola&#21644;&#30142;&#30149;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of many infectious diseases is modeled using variants of the SIR compartmental model, which is a coupled differential equation. The coefficients of the SIR model determine the spread trajectories of disease, on whose basis proactive measures can be taken. Hence, the coefficient estimates must be both fast and accurate. Shaier et al. in the paper "Disease Informed Neural Networks" used Physics Informed Neural Networks (PINNs) to estimate the parameters of the SIR model. There are two drawbacks to this approach. First, the training time for PINNs is high, with certain diseases taking close to 90 hrs to train. Second, PINNs don't generalize for a new SIDR trajectory, and learning its corresponding SIR parameters requires retraining the PINN from scratch. In this work, we aim to eliminate both of these drawbacks. We generate a dataset between the parameters of ODE and the spread trajectories by solving the forward problem for a large distribution of parameters using the LSODA al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14077</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#20363;&#32423;&#25439;&#22833;&#24179;&#28369;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#23545;&#25239;&#25200;&#21160;&#65306;&#21363;&#20154;&#31867;&#38590;&#20197;&#23519;&#35273;&#30340;&#20154;&#36896;&#22122;&#22768;&#65292;&#21487;&#20197;&#36731;&#26131;&#22320;&#36855;&#24785;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#32780;&#20570;&#20986;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#30446;&#21069;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#26368;&#25104;&#21151;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#20197;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#20174;&#23454;&#20363;&#32423;&#21035;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#26399;&#38388;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#28436;&#21464;&#12290;&#21457;&#29616;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#36890;&#36807;&#29306;&#29298;&#30456;&#24403;&#27604;&#20363;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#24615;&#25439;&#22833;&#30340;&#25972;&#20307;&#38477;&#20302;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#21516;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#20998;&#24067;&#19981;&#22343;&#34913;&#12290;&#36825;&#31181;&#8220;&#19981;&#22343;&#34913;&#33030;&#24369;&#24615;&#8221;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#26041;&#27861;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#19982;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65306;Instance-adaptive Adversarial Training (IAAT)&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#12290;&#26412;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#35782;&#21035;&#20986;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21487;&#33021;&#34987;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#30340;&#21361;&#38505;&#21306;&#22495;&#21644;&#21487;&#36798;&#36335;&#24452;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#25104;&#21151;&#29575;&#25509;&#36817;100&#65285;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#21457;&#29616;AV&#30340;&#28431;&#27934;&#24182;&#23454;&#26045;&#26377;&#25928;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.13992</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#36798;&#24615;&#20998;&#26512;&#28608;&#27963;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#29289;&#29702;&#21518;&#38376;&#35302;&#21457;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis. (arXiv:2303.13992v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#35782;&#21035;&#20986;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21487;&#33021;&#34987;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#30340;&#21361;&#38505;&#21306;&#22495;&#21644;&#21487;&#36798;&#36335;&#24452;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#25104;&#21151;&#29575;&#25509;&#36817;100&#65285;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#21457;&#29616;AV&#30340;&#28431;&#27934;&#24182;&#23454;&#26045;&#26377;&#25928;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21487;&#33021;&#34987;&#38544;&#34255;&#30340;&#21518;&#38376;&#25805;&#32437;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#34987;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#26102;&#25191;&#34892;&#26377;&#23475;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#35302;&#21457;&#22120;&#22914;&#20309;&#22312;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#34987;&#28608;&#27963;&#12290;&#22312;&#21160;&#24577;&#20132;&#36890;&#29615;&#22659;&#20013;&#20102;&#35299;&#36825;&#31181;&#28431;&#27934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#35270;&#20026;&#21487;&#25511;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#35782;&#21035;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#65292;&#21487;&#20197;&#21040;&#36798;&#20107;&#25925;&#35302;&#21457;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#21040;&#36798;&#36825;&#20123;&#26465;&#20214;&#30340;&#24847;&#22270;&#36712;&#36857;&#12290;&#22312;&#20856;&#22411;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25104;&#21151;&#22320;&#25512;&#21160;&#35302;&#21457;&#22120;&#26465;&#20214;&#65292;&#35302;&#21457;&#29575;&#25509;&#36817;100&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35782;&#21035;AV&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#21551;&#29992;&#26377;&#25928;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies reveal that Autonomous Vehicles (AVs) can be manipulated by hidden backdoors, causing them to perform harmful actions when activated by physical triggers. However, it is still unclear how these triggers can be activated while adhering to traffic principles. Understanding this vulnerability in a dynamic traffic environment is crucial. This work addresses this gap by presenting physical trigger activation as a reachability problem of controlled dynamic system. Our technique identifies security-critical areas in traffic systems where trigger conditions for accidents can be reached, and provides intended trajectories for how those conditions can be reached. Testing on typical traffic scenarios showed the system can be successfully driven to trigger conditions with near 100% activation rate. Our method benefits from identifying AV vulnerability and enabling effective safety strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;</title><link>http://arxiv.org/abs/2303.13763</link><description>&lt;p&gt;
&#26080;&#38656;&#36793;&#32536;&#20294;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#24615;&#65306;&#20174;GNN&#21040;MLP&#30340;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#31934;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20219;&#21153;&#20013;&#21387;&#32553;&#25104;&#20302;&#24310;&#36831;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20250;&#23558;&#22270;&#30340;&#36793;&#32536;&#22788;&#29702;&#25104;&#39069;&#22806;&#30340;&#36755;&#20837;&#32473;MLP&#65292;&#20294;&#36825;&#26679;&#30340;&#22270;&#32467;&#26500;&#23545;&#20110;&#21508;&#31181;&#22330;&#26223;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GNN&#25945;&#24072;&#20013;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21407;&#22411;&#22312;&#26080;&#36793;&#32536;&#35774;&#32622;&#20013;&#20174;GNN&#21040;MLP&#36827;&#34892;&#20102;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#23454;&#39564;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PGKD&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13540</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;: &#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20419;&#36827;&#21487;&#25345;&#32493;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision. (arXiv:2303.13540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#23454;&#29616;&#28165;&#27905;&#29983;&#20135;&#21644;&#21487;&#25345;&#32493;&#24615;&#30446;&#30340;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#23588;&#20854;&#26159;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26469;&#35782;&#21035;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#65292;&#24182;&#23558;&#36825;&#20123;&#32467;&#26524;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25104;&#26524;&#39044;&#35745;&#23558;&#20419;&#36827;&#20135;&#21697;&#20351;&#29992;&#30340;&#25913;&#36827;&#21644;&#30740;&#21457;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#20135;&#21697;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;:&#21152;&#24037;&#24037;&#20855;&#21644;&#26059;&#36716;X&#23556;&#32447;&#38451;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage and impact of deep learning for cleaner production and sustainability purposes remain little explored. This work shows how deep learning can be harnessed to increase sustainability in production and product usage. Specifically, we utilize deep learning-based computer vision to determine the wear states of products. The resulting insights serve as a basis for novel product-service systems with improved integration and result orientation. Moreover, these insights are expected to facilitate product usage improvements and R&amp;D innovations. We demonstrate our approach on two products: machining tools and rotating X-ray anodes. From a technical standpoint, we show that it is possible to recognize the wear state of these products using deep-learning-based computer vision. In particular, we detect wear through microscopic images of the two products. We utilize a U-Net for semantic segmentation to detect wear based on pixel granularity. The resulting mean dice coefficients of 0.631 and
&lt;/p&gt;</description></item><item><title>DreamBooth3D&#26159;&#19968;&#31181;&#21487;&#20174;3-6&#24352;&#22270;&#29255;&#20013;&#29983;&#25104;&#20027;&#20307;&#29305;&#23450;3D&#32032;&#26448;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#32032;&#26448;&#12290;</title><link>http://arxiv.org/abs/2303.13508</link><description>&lt;p&gt;
DreamBooth3D&#65306;&#20027;&#20307;&#39537;&#21160;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamBooth3D: Subject-Driven Text-to-3D Generation. (arXiv:2303.13508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13508
&lt;/p&gt;
&lt;p&gt;
DreamBooth3D&#26159;&#19968;&#31181;&#21487;&#20174;3-6&#24352;&#22270;&#29255;&#20013;&#29983;&#25104;&#20027;&#20307;&#29305;&#23450;3D&#32032;&#26448;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#32032;&#26448;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DreamBooth3D&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;3-6&#20010;&#38543;&#24847;&#25293;&#25668;&#30340;&#20027;&#20307;&#22270;&#20687;&#20010;&#24615;&#21270;&#29983;&#25104;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;(DreamBooth)&#19982;&#25991;&#26412;&#21040;3D&#29983;&#25104;(DreamFusion)&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#36825;&#20123;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#20027;&#20307;&#29305;&#23450;&#30340;3D&#32032;&#26448;&#65292;&#22240;&#20026;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20250;&#36807;&#24230;&#25311;&#21512;&#20027;&#20307;&#22270;&#20687;&#30340;&#36755;&#20837;&#35270;&#35282;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#31574;&#30053;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#21033;&#29992;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;3D&#19968;&#33268;&#24615;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#12289;&#20027;&#20307;&#29305;&#23450;&#30340;3D&#32032;&#26448;&#65292;&#20855;&#26377;&#25991;&#26412;&#39537;&#21160;&#30340;&#20462;&#25913;&#65292;&#22914;&#26032;&#39062;&#30340;&#23039;&#21183;&#12289;&#39068;&#33394;&#21644;&#23646;&#24615;&#65292;&#36825;&#20123;&#20462;&#25913;&#22312;&#20027;&#20307;&#30340;&#20219;&#20309;&#36755;&#20837;&#22270;&#20687;&#20013;&#37117;&#27809;&#26377;&#30475;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naively combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13217</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#27491;&#24341;&#23548;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26500;&#24314;&#30340;&#25552;&#31034;&#36827;&#34892;&#30452;&#25509;&#24212;&#29992;&#26469;&#35299;&#20915;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#31034;&#20363;&#65292;&#31034;&#20363;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#30340;&#21464;&#21270;&#23548;&#33268;&#19978;&#19979;&#25991;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#23545;&#20110;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#20559;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#22266;&#23450;&#25552;&#31034;&#30456;&#23545;&#20110;&#26631;&#31614;&#25110;&#32473;&#23450;&#23646;&#24615;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#27979;&#20559;&#24046;&#36739;&#22823;&#30340;&#25552;&#31034;&#24635;&#26159;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#36138;&#23146;&#25628;&#32034;&#26469;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21483;&#20570;"&#20844;&#27491;&#25552;&#31034;"&#65292;&#20854;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20197;&#25351;&#23548;&#25628;&#32034;&#19981;&#23637;&#29616;&#20986;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;FairPrompt&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SNB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#20004;&#20010;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.13126</link><description>&lt;p&gt;
MagicFusion&#65306;&#36890;&#36807;&#34701;&#21512;&#25193;&#25955;&#27169;&#22411;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models. (arXiv:2303.13126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SNB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#20004;&#20010;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;AI&#31038;&#21306;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#24378;&#22823;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Saliency-aware Noise Blending (SNB)&#8221; &#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#34701;&#21512;&#30340;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20998;&#31867;&#22120;&#33258;&#30001;&#25351;&#23548;&#30340;&#21709;&#24212;&#19982;&#29983;&#25104;&#22270;&#20687;&#30340;&#26174;&#30528;&#24615;&#39640;&#24230;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26174;&#30528;&#24615;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#28151;&#21512;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#30340;&#39044;&#27979;&#22122;&#22768;&#26469;&#20449;&#20219;&#20854;&#19987;&#19994;&#39046;&#22495;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;SNB&#26159;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23436;&#25104;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;DDIM&#37319;&#26679;&#36807;&#31243;&#20013;&#33258;&#21160;&#23545;&#40784;&#20004;&#20010;&#22122;&#22768;&#31354;&#38388;&#30340;&#35821;&#20041;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#20363;&#22914;&#25513;&#27169;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;SNB&#30340;&#26174;&#30528;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The advent of open-source AI communities has produced a cornucopia of powerful text-guided diffusion models that are trained on various datasets. While few explorations have been conducted on ensembling such models to combine their strengths. In this work, we propose a simple yet effective method called Saliency-aware Noise Blending (SNB) that can empower the fused text-guided diffusion models to achieve more controllable generation. Specifically, we experimentally find that the responses of classifier-free guidance are highly related to the saliency of generated images. Thus we propose to trust different models in their areas of expertise by blending the predicted noises of two diffusion models in a saliency-aware manner. SNB is training-free and can be completed within a DDIM sampling process. Additionally, it can automatically align the semantics of two noise spaces without requiring additional annotations such as masks. Extensive experiments show the impressive effectiveness of SNB
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.12961</link><description>&lt;p&gt;
&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#22522;&#30784;&#65306;&#38024;&#23545; EMR &#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs. (arXiv:2303.12961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284; ChatGPT &#21644; AlphaFold &#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#26500;&#24314;&#31867;&#20284;&#27169;&#22411;&#20197;&#25913;&#21892; EMR&#65288;&#30005;&#23376;&#30149;&#21382;&#65289;&#20197;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#38498;&#36816;&#33829;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#28818;&#20316;&#25513;&#30422;&#20102;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#20851;&#38190;&#32570;&#22833;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#65288;&#21363;&#20020;&#24202;&#25991;&#26412;&#21644;/&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#35828;&#26126;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#28508;&#22312;&#29992;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26159;&#22312;&#23567;&#22411;&#12289;&#33539;&#22260;&#26377;&#38480;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;MIMIC-III&#65289;&#25110;&#24191;&#27867;&#30340;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914;PubMed&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#25552;&#20379;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#26377;&#29992;&#22788;&#30340;&#26377;&#24847;&#20041;&#35265;&#35299;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#30340;&#25913;&#36827;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19981;&#21516;&#39046;&#22495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#23545;&#20110;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.12810</link><description>&lt;p&gt;
LLM&#26159;&#19975;&#33021;&#30340;&#22823;&#24072;&#21527;&#65311;&#25506;&#32034;LLM&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19981;&#21516;&#39046;&#22495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#23545;&#20110;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#31867;&#20284;&#20110;&#20154;&#31867;&#25512;&#29702;&#30340;&#28508;&#21147;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20105;&#35758;&#26368;&#28608;&#28872;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#24418;&#24335;&#36827;&#34892;&#20307;&#29616;&#65292;&#21253;&#25324;&#31867;&#27604;&#12289;&#31354;&#38388;&#21644;&#36947;&#24503;&#25512;&#29702;&#31561;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;LLM&#33021;&#21542;&#22312;&#25152;&#26377;&#36825;&#20123;&#19981;&#21516;&#39046;&#22495;&#20013;&#21516;&#26679;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#25110;&#20174;&#29616;&#26377;&#31867;&#27604;&#21644;&#31354;&#38388;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#27762;&#21462;&#21551;&#31034;&#65292;&#23545;LLM&#22312;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;LLM&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#36824;&#23545;&#26356;&#24320;&#25918;&#12289;&#33258;&#28982;&#30340;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#19981;&#22815;&#29087;&#32451;&#12290;&#25105;&#35748;&#20026;&#36825;&#20123;&#23454;&#39564;&#23545;&#20110;&#25512;&#21160;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25913;&#36827;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models (LLMs) to reason like humans has been a highly contested topic in Machine Learning communities. However, the reasoning abilities of humans are multifaceted and can be seen in various forms, including analogical, spatial and moral reasoning, among others. This fact raises the question whether LLMs can perform equally well across all these different domains. This research work aims to investigate the performance of LLMs on different reasoning tasks by conducting experiments that directly use or draw inspirations from existing datasets on analogical and spatial reasoning. Additionally, to evaluate the ability of LLMs to reason like human, their performance is evaluted on more open-ended, natural language questions. My findings indicate that LLMs excel at analogical and moral reasoning, yet struggle to perform as proficiently on spatial reasoning tasks. I believe these experiments are crucial for informing the future development of LLMs, particularly 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.12642</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#30340;&#27665;&#20027;&#21270;&#65306;&#22810;&#31181;&#21547;&#20041;&#12289;&#30446;&#26631;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21628;&#21505;&#23454;&#29616;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#36825;&#20010;&#35789;&#35821;&#29992;&#26469;&#25351;&#20195;&#22810;&#31181;&#30446;&#26631;&#65292;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#36890;&#24120;&#35752;&#35770;&#30340;&#22235;&#31181;AI&#27665;&#20027;&#21270;&#31867;&#22411;&#65306;(1) AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;(2) AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;(3) AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;(4) AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23454;&#29616;&#27599;&#31181;&#27665;&#20027;&#21270;&#24418;&#24335;&#30340;&#22810;&#20010;&#30446;&#26631;&#21644;&#26041;&#27861;&#12290;&#20174;&#26412;&#25991;&#20013;&#20027;&#35201;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;AI&#30340;&#27665;&#20027;&#21270;&#26159;&#19968;&#20010;&#22810;&#20803;&#32780;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#30340;&#27010;&#24565;&#65292;&#19981;&#24212;&#28151;&#28102;AI&#21487;&#35775;&#38382;&#24615;&#30340;&#25913;&#21892;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#35201;&#36229;&#36234;&#23545;&#26234;&#33021;&#21270;&#27665;&#20027;&#21270;&#30340;&#27169;&#31946;&#25215;&#35834;&#65292;&#36827;&#20837;&#20855;&#20307;&#25919;&#31574;&#21644;&#26435;&#34913;&#30340;&#29983;&#20135;&#24615;&#35752;&#35770;&#65292;&#25105;&#20204;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#36328;&#36234;&#20851;&#20110;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21033;&#28070;&#30340;&#20915;&#31574;&#20013;&#23548;&#33322;&#26435;&#34913;&#21644;&#39118;&#38505;&#30340;&#20027;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65288;R2R&#65289;&#65292;&#20801;&#35768;&#20174;&#19994;&#32773;&#36880;&#27493;&#35782;&#21035;&#12289;&#20943;&#23569;&#21644;&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#20559;&#24046;&#65292;&#21253;&#25324;&#23547;&#25214;&#24322;&#24120;&#20540;&#12289;&#26816;&#27979;&#36127;&#36131;&#30340;&#25991;&#29289;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.12641</link><description>&lt;p&gt;
&#25581;&#31034;&#19982;&#20462;&#27491;&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#27169;&#22411;&#36845;&#20195;&#20559;&#24046;&#26657;&#27491;&#29983;&#21629;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models. (arXiv:2303.12641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65288;R2R&#65289;&#65292;&#20801;&#35768;&#20174;&#19994;&#32773;&#36880;&#27493;&#35782;&#21035;&#12289;&#20943;&#23569;&#21644;&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#20559;&#24046;&#65292;&#21253;&#25324;&#23547;&#25214;&#24322;&#24120;&#20540;&#12289;&#26816;&#27979;&#36127;&#36131;&#30340;&#25991;&#29289;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#65292;&#22914;&#30382;&#32932;&#30284;&#26816;&#27979;&#31561;&#21307;&#30103;&#24212;&#29992;&#26102;&#65292;&#36825;&#20250;&#24102;&#26469;&#39118;&#38505;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25581;&#31034;&#19982;&#20462;&#27491;&#8221;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#25972;&#20010;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#29983;&#21629;&#21608;&#26399;&#65292;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#36880;&#27493;&#35782;&#21035;&#12289;&#20943;&#23569;&#21644;(&#37325;&#26032;)&#35780;&#20272;&#34394;&#20551;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art machine learning models often learn spurious correlations embedded in the training data. This poses risks when deploying these models for high-stake decision-making, such as in medical applications like skin cancer detection. To tackle this problem, we propose Reveal to Revise (R2R), a framework entailing the entire eXplainable Artificial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction. In the first step (1), R2R reveals model weaknesses by finding outliers in attributions or through inspection of latent concepts learned by the model. Secondly (2), the responsible artifacts are detected and spatially localized in the input data, which is then leveraged to (3) revise the model behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model's performance and remaining sensitivity towards the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;$P^{3}O$&#30340;&#19977;&#38454;&#27573;DRL&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#38382;&#26469;&#36716;&#31227;&#35270;&#35273;&#34920;&#31034;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35270;&#35273;&#20256;&#36755;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12371</link><description>&lt;p&gt;
$P^{3}O$: Prompting&#26041;&#27861;&#20013;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting. (arXiv:2303.12371v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;$P^{3}O$&#30340;&#19977;&#38454;&#27573;DRL&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#38382;&#26469;&#36716;&#31227;&#35270;&#35273;&#34920;&#31034;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35270;&#35273;&#20256;&#36755;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23558;&#23398;&#21040;&#30340;&#31574;&#30053;&#36716;&#31227;&#33267;&#35270;&#35273;&#36755;&#20837;&#19981;&#21516;&#30340;&#26032;&#29615;&#22659;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;($P^{3}O$)&#30340;&#19977;&#38454;&#27573;DRL&#31639;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Prompt&#20351;&#24471;&#35270;&#35273;&#34920;&#31034;&#20174;&#30446;&#26631;&#29615;&#22659;&#20256;&#36882;&#21040;&#28304;&#29615;&#22659;&#12290;$P^{3}O$&#30340;&#36807;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;:&#39044;&#35757;&#32451;&#12289;Prompting&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Prompt-Transformer&#29992;&#20110;&#34920;&#31034;&#36716;&#25442;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#29615;&#22659;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35757;&#32451;&#36807;&#31243;&#65292;&#35757;&#32451;Prompt-Transformer&#65292;&#32780;DRL&#31649;&#36947;&#30340;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;OpenAI CarRacing&#35270;&#39057;&#28216;&#25103;&#19978;&#23454;&#26045;$P^{3}O$&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;$P^{3}O$&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#36716;&#31227;&#26041;&#26696;&#65292;&#32780;&#19988;&#33021;&#35753;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#35270;&#35273;&#36755;&#20837;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important for deep reinforcement learning (DRL) algorithms to transfer their learned policies to new environments that have different visual inputs. In this paper, we introduce Prompt based Proximal Policy Optimization ($P^{3}O$), a three-stage DRL algorithm that transfers visual representations from a target to a source environment by applying prompting. The process of $P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In particular, we specify a prompt-transformer for representation conversion and propose a two-step training process to train the prompt-transformer for the target environment, while the rest of the DRL pipeline remains unchanged. We implement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The experimental results show that $P^{3}O$ outperforms the state-of-the-art visual transferring schemes. In particular, $P^{3}O$ allows the learned policies to perform well in environments with different visual inputs, which is much more e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#33258;&#25105;&#20462;&#27491;&#21644;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#36890;&#29992;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#26657;&#27491;&#32593;&#32476;&#21644;&#20581;&#24247;&#21453;&#39304;&#32593;&#32476;&#26469;&#35299;&#20915;&#32593;&#32476;&#39044;&#27979;&#30340;&#27867;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11180</link><description>&lt;p&gt;
&#21487;&#33258;&#25105;&#20462;&#27491;&#21644;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#36890;&#29992;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation. (arXiv:2303.11180v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#33258;&#25105;&#20462;&#27491;&#21644;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#36890;&#29992;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#26657;&#27491;&#32593;&#32476;&#21644;&#20581;&#24247;&#21453;&#39304;&#32593;&#32476;&#26469;&#35299;&#20915;&#32593;&#32476;&#39044;&#27979;&#30340;&#27867;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#30340;&#32593;&#32476;&#26080;&#27861;&#23545;&#39044;&#27979;&#35823;&#24046;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#19981;&#33021;&#20174;&#27979;&#35797;&#26679;&#26412;&#20013;&#29983;&#25104;&#21453;&#39304;&#20449;&#24687;&#24182;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#21160;&#24577;&#20462;&#27491;&#39044;&#27979;&#35823;&#24046;&#65292;&#23548;&#33268;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#33258;&#25105;&#20462;&#27491;&#21644;&#33258;&#36866;&#24212;&#25512;&#29702;&#65288;SCAI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#32593;&#32476;&#39044;&#27979;&#30340;&#27867;&#21270;&#25361;&#25112;&#65292;&#24182;&#20197;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20026;&#20363;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central challenge in human pose estimation, as well as in many other machine learning and prediction tasks, is the generalization problem. The learned network does not have the capability to characterize the prediction error, generate feedback information from the test sample, and correct the prediction error on the fly for each individual test sample, which results in degraded performance in generalization. In this work, we introduce a self-correctable and adaptable inference (SCAI) method to address the generalization challenge of network prediction and use human pose estimation as an example to demonstrate its effectiveness and performance. We learn a correction network to correct the prediction result conditioned by a fitness feedback error. This feedback error is generated by a learned fitness feedback network which maps the prediction result to the original input domain and compares it against the original input. Interestingly, we find that this self-referential feedback error 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.10880</link><description>&lt;p&gt;
&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#25163;&#37096;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#24863;&#20449;&#24687;&#22312;&#20154;&#31867;&#28789;&#24039;&#24615;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#35270;&#35273;&#20013;&#26080;&#27861;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#20855;&#22791;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25972;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#65288;&#35302;&#25720;&#25110;&#26410;&#35302;&#25720;&#65289;&#20195;&#26367;&#20165;&#20165;&#22312;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31934;&#20934;&#30340;&#35302;&#35273;&#20256;&#24863;&#65292;&#20351;&#31995;&#32479;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#35206;&#30422;&#33539;&#22260;&#24191;&#31561;&#20248;&#28857;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#30340;&#29289;&#20307;&#27169;&#25311;&#20013;&#35757;&#32451;&#20986;&#20102;&#19968;&#31181;&#35302;&#24863;&#26059;&#36716;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25163;&#19978;&#30452;&#25509;&#23454;&#26045;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#26032;&#22411;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
&lt;/p&gt;</description></item><item><title>ROBOSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#38543;&#26426;&#23376;&#38598;&#30340;&#38431;&#21451;&#26469;&#23545;&#27604;&#21327;&#21516;&#24863;&#30693;&#21644;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#65292;&#20197;&#25490;&#38500;&#28508;&#22312;&#25915;&#20987;&#32773;&#65292;&#24182;&#25512;&#23548;&#20986;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#25152;&#38656;&#30340;&#37319;&#26679;&#35797;&#39564;&#20010;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09495</link><description>&lt;p&gt;
Among Us: &#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Among Us: Adversarially Robust Collaborative Perception by Consensus. (arXiv:2303.09495v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09495
&lt;/p&gt;
&lt;p&gt;
ROBOSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#38543;&#26426;&#23376;&#38598;&#30340;&#38431;&#21451;&#26469;&#23545;&#27604;&#21327;&#21516;&#24863;&#30693;&#21644;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#65292;&#20197;&#25490;&#38500;&#28508;&#22312;&#25915;&#20987;&#32773;&#65292;&#24182;&#25512;&#23548;&#20986;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#25152;&#38656;&#30340;&#37319;&#26679;&#35797;&#39564;&#20010;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#33021;&#22815;&#27604;&#21333;&#20010;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#24863;&#30693;&#22330;&#26223;(&#20363;&#22914;&#65292;&#26816;&#27979;&#29289;&#20307;)&#65292;&#20294;&#22312;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26102;&#24456;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#12290;&#36825;&#19968;&#38382;&#39064;&#21487;&#36890;&#36807;&#23545;&#25239;&#24615;&#38450;&#24481;&#26469;&#35299;&#20915;&#65292;&#20294;&#35757;&#32451;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#26426;&#21046;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ROBOSAC&#65292;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26032;&#22411;&#38450;&#24481;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#24212;&#23545;&#26410;&#30693;&#30340;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#21327;&#21516;&#24863;&#30693;&#24212;&#35813;&#27604;&#21333;&#20010;&#24863;&#30693;&#26356;&#33021;&#36798;&#25104;&#19968;&#33268;&#65292;&#32780;&#19981;&#24212;&#30456;&#20114;&#20135;&#29983;&#20998;&#27495;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35828;&#21644;&#39564;&#35777;&#30340;&#26694;&#26550;&#65306;&#21033;&#29992;&#19968;&#32452;&#38543;&#26426;&#36873;&#25321;&#30340;&#38431;&#21451;&#65292;&#23545;&#21327;&#21516;&#24863;&#30693;&#19982;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#12290;&#22312;&#36825;&#26679;&#30340;&#26694;&#26550;&#19979;&#65292;&#26356;&#22810;&#30340;&#38431;&#21451;&#36890;&#24120;&#24847;&#21619;&#30528;&#26356;&#22909;&#30340;&#24863;&#30693;&#34920;&#29616;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#37319;&#26679;&#26102;&#38388;&#26469;&#25490;&#38500;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#38656;&#35201;&#22810;&#23569;&#20010;&#37319;&#26679;&#35797;&#39564;&#25165;&#33021;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#30340;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple robots could perceive a scene (e.g., detect objects) collaboratively better than individuals, although easily suffer from adversarial attacks when using deep learning. This could be addressed by the adversarial defense, but its training requires the often-unknown attacking mechanism. Differently, we propose ROBOSAC, a novel sampling-based defense strategy generalizable to unseen attackers. Our key idea is that collaborative perception should lead to consensus rather than dissensus in results compared to individual perception. This leads to our hypothesize-and-verify framework: perception results with and without collaboration from a random subset of teammates are compared until reaching a consensus. In such a framework, more teammates in the sampled subset often entail better perception performance but require longer sampling time to reject potential attackers. Thus, we derive how many sampling trials are needed to ensure the desired size of an attacker-free subset, or equival
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;</title><link>http://arxiv.org/abs/2303.08233</link><description>&lt;p&gt;
NL4Opt &#27604;&#36187;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26500;&#24314;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08233
&lt;/p&gt;
&lt;p&gt;
NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20248;&#21270;&#65288;NL4Opt&#65289;&#31454;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#20248;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#25551;&#36848;&#25552;&#21462;&#20854;&#21547;&#20041;&#21644;&#34920;&#36848;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31454;&#36187;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20013;&#20171;&#26469;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#25509;&#21475;&#20351;&#29992;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#20197;&#22686;&#21152;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#25361;&#25112;&#24615;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1)&#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#26631;&#35760;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#20307;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;(LP)&#38382;&#39064;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#35813;&#20013;&#38388;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#29992;&#30340;&#26684;&#24335;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LP&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;NL4Opt&#27604;&#36187;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#20102;&#31454;&#36187;&#26465;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e., a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08179</link><description>&lt;p&gt;
MEDBERT.de&#65306;&#19968;&#20010;&#22522;&#20110;&#24503;&#35821;&#30340;&#12289;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20840;&#38754;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#24050;&#32463;&#22312;470&#19975;&#20221;&#24503;&#35821;&#21307;&#23398;&#25991;&#26723;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#28041;&#21450;&#21508;&#31181;&#23398;&#31185;&#21644;&#21307;&#23398;&#25991;&#29486;&#31867;&#22411;&#12290;&#38500;&#20102;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#22806;&#65292;&#26412;&#25991;&#36824;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#21435;&#37325;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#26356;&#26377;&#25928;&#30340;&#20998;&#35789;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;medBERT.de&#36825;&#26679;&#30340;&#39046;&#22495;&#19987;&#29992;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#36739;&#38271;&#30340;&#25991;&#26412;&#65292;&#24182;&#19988;&#25968;&#25454;&#21435;&#37325;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#25928;&#30340;&#20998;&#35789;&#21482;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#20102;&#36739;&#23567;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#25913;&#36827;&#28304;&#20110;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents medBERT.de, a pre-trained German BERT model specifically designed for the German medical domain. The model has been trained on a large corpus of 4.7 Million German medical documents and has been shown to achieve new state-of-the-art performance on eight different medical benchmarks covering a wide range of disciplines and medical document types. In addition to evaluating the overall performance of the model, this paper also conducts a more in-depth analysis of its capabilities. We investigate the impact of data deduplication on the model's performance, as well as the potential benefits of using more efficient tokenization methods. Our results indicate that domain-specific models such as medBERT.de are particularly useful for longer texts, and that deduplication of training data does not necessarily lead to improved performance. Furthermore, we found that efficient tokenization plays only a minor role in improving model performance, and attribute most of the improved
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07522</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#26159;&#19968;&#31181;&#22810;&#24863;&#23448;&#30340;&#20307;&#39564;&#65292;&#20294;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#24863;&#30693;&#26469;&#32472;&#21046;&#21644;&#23548;&#33322;&#20182;&#20204;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;3D&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#12290;&#22312;&#23548;&#33322;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;(&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#25110;&#22320;&#26631;&#30340;&#38899;&#39057;&#29255;&#27573;)&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#28155;&#21152;&#38899;&#39057;&#20449;&#24687;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#28040;&#38500;&#30446;&#26631;&#20301;&#32622;&#30340;&#27495;&#20041;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#22312;&#27169;&#31946;&#22330;&#26223;&#20013;&#25552;&#20379;50%&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06333</link><description>&lt;p&gt;
&#38477;&#33853;&#20254;&#65306;&#35780;&#20272;&#20132;&#20114;&#24335;&#20154;&#26426;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#39134;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#20026;&#20849;&#21516;&#30340;&#20889;&#20316;&#25104;&#26524;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#20110;&#20132;&#20114;&#24335;&#29615;&#22659;&#19979;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;Parachute&#23637;&#31034;&#20102;&#20132;&#20114;&#35780;&#20272;&#30340;&#32508;&#21512;&#35270;&#35282;&#65292;&#20854;&#20013;&#27599;&#20010;&#35780;&#20272;&#26041;&#38754;&#37117;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;Parachute&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2303.06246</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;mHealth&#21644;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#20174;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25110;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#31227;&#21160;&#24863;&#30693;DL&#31995;&#32479;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65292;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;ZoneFL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#12290;ZoneFL&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;&#21306;&#22495;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#12290;&#21463;&#30410;&#20110;FL&#35774;&#35745;&#65292;ZoneFL&#22521;&#35757;&#26399;&#38388;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#26469;&#20248;&#21270;&#21306;&#22495;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65306;&#21306;&#22495;&#21512;&#24182;&#21644;&#20998;&#35010;&#65288;ZMS&#65289;&#21644;Zo
&lt;/p&gt;
&lt;p&gt;
Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06237</link><description>&lt;p&gt;
&#20302;&#24320;&#38144;&#27169;&#22411;&#21098;&#26525;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#34917;&#20805;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#20363;&#65292;&#28041;&#21450;&#22823;&#37327;&#36890;&#20449;&#21644;&#35745;&#31639;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#27169;&#22411;&#21098;&#26525;/&#31232;&#30095;&#21270;&#24320;&#21457;&#20102;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#65292;&#22312;FL&#20551;&#35774;&#19979;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#20197;&#24494;&#35843;&#20462;&#21098;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34917;&#20805;&#31232;&#30095;&#21270;&#65288;CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;CS&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#31232;&#30095;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#25429;&#33719;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26435;&#37325;&#65292;&#32780;&#23458;&#25143;&#31471;&#21017;&#21019;&#24314;&#26412;&#22320;&#31232;&#30095;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02841</link><description>&lt;p&gt;
&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance. (arXiv:2303.02841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#22240;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#21644;&#29305;&#27530;&#35821;&#35328;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#24494;&#35843;&#32463;&#24120;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21487;&#33021;&#20250;&#20559;&#34962;&#22823;&#37327;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;1.&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22810;&#31181;&#31867;&#22411;&#20219;&#21153;&#30340;MAML&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;GLUE&#25968;&#25454;&#38598;&#65292;SNLI&#65292;Sci-Tail&#21644;Financial PhraseBank&#65307;2.&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#30495;&#23454;&#22330;&#26223;&#30340;&#22522;&#20110;&#25512;&#29305;&#25991;&#26412;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#20013;&#20351;&#29992;MAML&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding(NLU) is challenging for finance due to the lack of annotated data and the specialized language in that domain. As a result, researchers have proposed to use pre-trained language model and multi-task learning to learn robust representations. However, aggressive fine-tuning often causes over-fitting and multi-task learning may favor tasks with significantly larger amounts data, etc. To address these problems, in this paper, we investigate model-agnostic meta-learning algorithm(MAML) in low-resource financial NLU tasks. Our contribution includes: 1. we explore the performance of MAML method with multiple types of tasks: GLUE datasets, SNLI, Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML method with multiple single-type tasks: a real scenario stock price prediction problem with twitter text data. Our models achieve the state-of-the-art performance according to the experimental results, which demonstrate that our method can adapt fast a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#21147;&#23398;&#27010;&#24565;&#24211;&#35780;&#20272;&#20102;ChatGPT3.5&#21644;ChatGPT4&#22312;&#22823;&#23398;&#31532;&#19968;&#23398;&#26399;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT3.5&#21487;&#20197;&#22312;&#26576;&#20123;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36807;&#20256;&#32479;&#25945;&#23398;&#30340;&#20013;&#20301;&#25968;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01067</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;FCI&#65306;ChatGPT&#33021;&#21542;&#29702;&#35299;&#21021;&#32423;&#29289;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
AI and the FCI: Can ChatGPT Project an Understanding of Introductory Physics?. (arXiv:2303.01067v2 [physics.ed-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#21147;&#23398;&#27010;&#24565;&#24211;&#35780;&#20272;&#20102;ChatGPT3.5&#21644;ChatGPT4&#22312;&#22823;&#23398;&#31532;&#19968;&#23398;&#26399;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT3.5&#21487;&#20197;&#22312;&#26576;&#20123;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36807;&#20256;&#32479;&#25945;&#23398;&#30340;&#20013;&#20301;&#25968;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#8220;&#32842;&#22825;&#26426;&#22120;&#20154;&#8221;&#65292;&#23427;&#26159;&#24314;&#31435;&#22312;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#25509;&#21475;&#65292;&#35813;&#27169;&#22411;&#26159;&#30001;&#22823;&#37327;&#20154;&#31867;&#25991;&#26412;&#35757;&#32451;&#32780;&#25104;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#38500;&#20102;&#33021;&#20197;&#19968;&#31181;&#21512;&#29702;&#30340;&#26041;&#24335;&#36827;&#34892;&#20132;&#35848;&#65292;&#23427;&#36824;&#22240;&#20854;&#33021;&#22815;&#32988;&#20219;&#24459;&#24072;&#32771;&#35797;&#21644;MBA&#35838;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#25552;&#20379;&#32534;&#20889;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#26377;&#29992;&#24110;&#21161;&#31561;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#26126;&#26174;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26377;&#20851;ChatGPT&#20316;&#20026;&#39640;&#31561;&#25945;&#32946;&#23436;&#25972;&#24615;&#30340;&#23041;&#32961;&#30340;&#35752;&#35770;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#23427;&#20063;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25945;&#23398;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20004;&#20010;&#29256;&#26412;&#30340;ChatGPT&#65288;ChatGPT3.5&#21644;ChatGPT4&#65289;&#22312;&#22823;&#23398;&#31532;&#19968;&#23398;&#26399;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#30340;&#21021;&#27493;&#20998;&#26512;&#65292;&#20351;&#29992;&#20462;&#25913;&#29256;&#30340;&#21147;&#23398;&#27010;&#24565;&#24211;&#65288;FCI&#65289;&#65292;&#35780;&#20272;&#23427;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#20851;&#20110;&#36816;&#21160;&#23398;&#21644;&#29275;&#39039;&#21160;&#21147;&#23398;&#30340;&#27010;&#24565;&#29289;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20174;&#26576;&#20123;&#26041;&#38754;&#26469;&#30475;&#65292;ChatGPT3.5&#21487;&#20197;&#21305;&#37197;&#25110;&#36229;&#36807;&#20256;&#32479;&#25945;&#23398;&#26041;&#27861;&#30340;&#20013;&#20301;&#25968;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a groundbreaking ``chatbot"--an AI interface built on a large language model that was trained on an enormous corpus of human text to emulate human conversation. Beyond its ability to converse in a plausible way, it has attracted attention for its ability to competently answer questions from the bar exam and from MBA coursework, and to provide useful assistance in writing computer code. These apparent abilities have prompted discussion of ChatGPT as both a threat to the integrity of higher education and conversely as a powerful teaching tool. In this work we present a preliminary analysis of how two versions of ChatGPT (ChatGPT3.5 and ChatGPT4) fare in the field of first-semester university physics, using a modified version of the Force Concept Inventory (FCI) to assess whether it can give correct responses to conceptual physics questions about kinematics and Newtonian dynamics. We demonstrate that, by some measures, ChatGPT3.5 can match or exceed the median performance of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23376;&#32593;&#35268;&#33539;&#30340; LTQP &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;&#25968;&#25454;&#21457;&#24067;&#32773;&#33021;&#22815;&#24314;&#35758;&#26597;&#35810;&#26469;&#28304;&#20197;&#21450;&#24341;&#23548;&#25968;&#25454;&#20351;&#29992;&#32773;&#21521;&#30456;&#20851;&#19988;&#21487;&#38752;&#30340;&#25968;&#25454;&#26597;&#35810;&#65292;&#35299;&#20915;&#20102; LTQP &#23384;&#22312;&#30340;&#24615;&#33021;&#21644;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14411</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23376;&#32593;&#35268;&#33539;&#29992;&#20110;&#36941;&#21382; Web
&lt;/p&gt;
&lt;p&gt;
Distributed Subweb Specifications for Traversing the Web. (arXiv:2302.14411v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23376;&#32593;&#35268;&#33539;&#30340; LTQP &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;&#25968;&#25454;&#21457;&#24067;&#32773;&#33021;&#22815;&#24314;&#35758;&#26597;&#35810;&#26469;&#28304;&#20197;&#21450;&#24341;&#23548;&#25968;&#25454;&#20351;&#29992;&#32773;&#21521;&#30456;&#20851;&#19988;&#21487;&#38752;&#30340;&#25968;&#25454;&#26597;&#35810;&#65292;&#35299;&#20915;&#20102; LTQP &#23384;&#22312;&#30340;&#24615;&#33021;&#21644;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38142;&#25509;&#36941;&#21382;&#30340;&#26597;&#35810;&#22788;&#29702;&#65288;LTQP&#65289;&#22312;&#19968;&#32452;&#25991;&#26723;&#19978;&#35780;&#20272; sparql &#26597;&#35810;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#22312;&#29702;&#35770;&#19978;&#26377;&#36259;&#20294;&#19981;&#23454;&#29992;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#30340;&#36229;&#38598;&#20013;&#24515;&#21270;&#36234;&#26469;&#36234;&#21463;&#21040;&#23457;&#26597;&#30340;&#26102;&#20195;&#65292;&#20855;&#26377;&#31616;&#21333;&#22522;&#20110;&#25991;&#26723;&#30340;&#30028;&#38754;&#30340;&#20998;&#25955;&#24335;&#25968;&#25454;&#32593;&#32476;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20351;&#25968;&#25454;&#21457;&#24067;&#32773;&#33021;&#22815;&#25511;&#21046;&#20854;&#25968;&#25454;&#21644;&#35775;&#38382;&#26435;&#12290;&#34429;&#28982; LTQP &#20801;&#35768;&#22312;&#36825;&#26679;&#30340;&#32593;&#32476;&#19978;&#35780;&#20272;&#22797;&#26434;&#30340;&#26597;&#35810;&#65292;&#20294;&#30001;&#20110;&#21253;&#21547;&#25968;&#25454;&#30340;&#25991;&#26723;&#25968;&#37327;&#36807;&#22810;&#65292;&#23427;&#21463;&#21040;&#24615;&#33021;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#20449;&#24687;&#36136;&#37327;&#26041;&#38754;&#30340;&#25285;&#24551;&#65288;&#30001;&#20110;&#25552;&#20379;&#27492;&#31867;&#25991;&#26723;&#30340;&#35768;&#22810;&#26469;&#28304;&#65289;&#12290;&#22312;&#29616;&#26377;&#30340; LTQP &#26041;&#27861;&#20013;&#65292;&#23547;&#25214;&#35201;&#26597;&#35810;&#30340;&#26469;&#28304;&#30340;&#36127;&#25285;&#23436;&#20840;&#30001;&#25968;&#25454;&#20351;&#29992;&#32773;&#25215;&#25285;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25968;&#25454;&#21457;&#24067;&#32773;&#24212;&#35813;&#20063;&#33021;&#22815;&#24314;&#35758;&#24863;&#20852;&#36259;&#30340;&#26469;&#28304;&#65292;&#24341;&#23548;&#25968;&#25454;&#20351;&#29992;&#32773;&#20351;&#29992;&#30456;&#20851;&#19988;&#21487;&#38752;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link Traversal-based Query Processing (ltqp), in which a sparql query is evaluated over a web of documents rather than a single dataset, is often seen as a theoretically interesting yet impractical technique. However, in a time where the hypercentralization of data has increasingly come under scrutiny, a decentralized Web of Data with a simple document-based interface is appealing, as it enables data publishers to control their data and access rights. While ltqp allows evaluating complex queries over such webs, it suffers from performance issues (due to the high number of documents containing data) as well as information quality concerns (due to the many sources providing such documents). In existing ltqp approaches, the burden of finding sources to query is entirely in the hands of the data consumer. In this paper, we argue that to solve these issues, data publishers should also be able to suggest sources of interest and guide the data consumer towards relevant and trustworthy data. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14383</link><description>&lt;p&gt;
&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#32452;&#21512;&#24615;&#19982;&#39044;&#20808;&#23384;&#22312;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#21333;&#35789;&#23884;&#20837;&#30340;&#20195;&#25968;&#36816;&#31639;&#26377;&#20851;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;&#27010;&#24565;&#30340;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20960;&#20309;&#23398;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#29702;&#35299;&#32452;&#21512;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;VLM&#23884;&#20837;&#22312;&#27010;&#29575;&#19978;&#30340;&#36825;&#20123;&#32452;&#21512;&#32467;&#26500;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#30452;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#31867;&#12289;&#21435;&#20559;&#21644;&#26816;&#32034;&#31561;&#19981;&#21516;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Im2Hands&#65292;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#20132;&#20114;&#21452;&#25163;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#39640;&#25163;&#21040;&#25163;&#21644;&#25163;&#21040;&#22270;&#20687;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#20960;&#20309;&#24418;&#24577;&#12290;</title><link>http://arxiv.org/abs/2302.14348</link><description>&lt;p&gt;
Im2Hands: &#23398;&#20064;&#20132;&#20114;&#21452;&#25163;&#24418;&#29366;&#30340;&#20851;&#27880;&#38544;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes. (arXiv:2302.14348v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Im2Hands&#65292;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#20132;&#20114;&#21452;&#25163;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#39640;&#25163;&#21040;&#25163;&#21644;&#25163;&#21040;&#22270;&#20687;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#20960;&#20309;&#24418;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Implicit Two Hands (Im2Hands)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#20004;&#21482;&#20132;&#20114;&#25163;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290; Im2Hands&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20004;&#21482;&#25163;&#37325;&#24314;&#26041;&#27861;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#21442;&#25968;&#25163;&#27169;&#22411;&#21644;/&#25110;&#20302;&#20998;&#36776;&#29575;&#32593;&#26684;&#65292;Im2Hands&#21487;&#20197;&#20135;&#29983;&#39640;&#25163;&#21040;&#25163;&#21644;&#25163;&#21040;&#22270;&#20687;&#19968;&#33268;&#24615;&#30340;&#20004;&#21482;&#25163;&#30340;&#32454;&#31890;&#24230;&#20960;&#20309;&#24418;&#24577;&#12290;&#20026;&#20102;&#22788;&#29702;&#20004;&#21482;&#25163;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21644;&#20132;&#20114;&#19978;&#19979;&#25991;&#65292;Im2Hands&#36890;&#36807;&#20004;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22359;&#26469;&#24314;&#27169;&#20004;&#21482;&#25163;&#30340;&#21344;&#29992;&#20307;&#31215; - &#19968;&#20010;&#36127;&#36131;&#21021;&#22987;&#21344;&#29992;&#20272;&#35745;&#65292;&#19968;&#20010;&#36127;&#36131;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21344;&#29992;&#31934;&#21270;&#12290;Im2Hands&#39318;&#20808;&#20351;&#29992;&#26597;&#35810;-&#22270;&#20687;&#27880;&#24847;&#21147;&#22312;&#20026;&#27599;&#20010;&#25163;&#35774;&#35745;&#30340;&#35268;&#33539;&#31354;&#38388;&#20013;&#23398;&#20064;&#27599;&#21482;&#25163;&#30340;&#31070;&#32463;&#20851;&#33410;&#21344;&#29992;&#65292;&#28982;&#21518;&#20351;&#29992;&#26597;&#35810;-&#38170;&#28857;&#27880;&#24847;&#21147;&#22312;&#23039;&#21183;&#31354;&#38388;&#20013;&#20248;&#21270;&#20004;&#21482;&#25163;&#30340;&#21021;&#22987;&#21344;&#29992;&#20197;&#22686;&#24378;&#20004;&#21482;&#25163;&#24418;&#29366;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#21487;&#36873;&#30340;&#20851;&#38190;&#28857;&#31934;&#21270;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands - conditioned on an RGB image and coarse 3D keypoints - by two novel attention-based modules responsible for (1) initial occupancy estimation and (2) context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement
&lt;/p&gt;</description></item><item><title>VoxFormer&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20174;2D&#22270;&#20687;&#36755;&#20986;&#23436;&#25972;&#30340;&#19977;&#32500;&#20307;&#31215;&#35821;&#20041;&#12290;&#23427;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65292;&#20174;&#21487;&#35265;&#30340;&#20307;&#32032;&#26597;&#35810;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26469;&#20256;&#25773;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#19977;&#32500;&#22330;&#26223;&#34917;&#20840;&#12290;</title><link>http://arxiv.org/abs/2302.12251</link><description>&lt;p&gt;
VoxFormer&#65306; &#22522;&#20110;&#31232;&#30095;&#20307;&#32032;&#21464;&#25442;&#30340;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion. (arXiv:2302.12251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12251
&lt;/p&gt;
&lt;p&gt;
VoxFormer&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20174;2D&#22270;&#20687;&#36755;&#20986;&#23436;&#25972;&#30340;&#19977;&#32500;&#20307;&#31215;&#35821;&#20041;&#12290;&#23427;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65292;&#20174;&#21487;&#35265;&#30340;&#20307;&#32032;&#26597;&#35810;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26469;&#20256;&#25773;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#19977;&#32500;&#22330;&#26223;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24456;&#23481;&#26131;&#24819;&#35937;&#34987;&#36974;&#25377;&#29289;&#20307;&#21644;&#22330;&#26223;&#30340;&#23436;&#25972;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#65292;&#22312;&#35782;&#21035;&#21644;&#29702;&#35299;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20351;AI&#31995;&#32479;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VoxFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20174;2D&#22270;&#20687;&#36755;&#20986;&#23436;&#25972;&#30340;&#19977;&#32500;&#20307;&#31215;&#35821;&#20041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65292;&#20174;&#28145;&#24230;&#20272;&#35745;&#30340;&#31232;&#30095;&#21487;&#35265;&#21644;&#21344;&#29992;&#20307;&#32032;&#26597;&#35810;&#24320;&#22987;&#65292;&#38543;&#21518;&#36827;&#34892;&#29983;&#25104;&#31264;&#23494;3D&#20307;&#32032;&#30340;&#31264;&#23494;&#21270;&#38454;&#27573;&#12290;&#36825;&#20010;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;2D&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#29305;&#24449;&#20165;&#23545;&#24212;&#20110;&#21487;&#35265;&#22330;&#26223;&#32467;&#26500;&#32780;&#19981;&#26159;&#36974;&#25377;&#25110;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#20174;&#21487;&#35265;&#32467;&#26500;&#30340;&#29305;&#24449;&#21270;&#21644;&#39044;&#27979;&#24320;&#22987;&#26356;&#21152;&#21487;&#38752;&#12290;&#19968;&#26086;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#32452;&#31232;&#30095;&#26597;&#35810;&#65292;&#25105;&#20204;&#23601;&#24212;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#35774;&#35745;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#23558;&#20449;&#24687;&#20256;&#25773;&#21040;&#25152;&#26377;&#20307;&#32032;&#12290;&#22312;SemanticKITTI&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show th
&lt;/p&gt;</description></item><item><title>HumanMAC&#26159;&#19968;&#20010;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#38454;&#27573;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.03665</link><description>&lt;p&gt;
HumanMAC&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
HumanMAC: Masked Motion Completion for Human Motion Prediction. (arXiv:2302.03665v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03665
&lt;/p&gt;
&lt;p&gt;
HumanMAC&#26159;&#19968;&#20010;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#38454;&#27573;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20197;&#32534;&#30721;-&#35299;&#30721;&#39118;&#26684;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#22312;&#32463;&#39564;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#25439;&#22833;&#32422;&#26463;&#12289;&#32321;&#29712;&#30340;&#22521;&#35757;&#36807;&#31243;&#20197;&#21450;&#39044;&#27979;&#20013;&#19981;&#21516;&#31867;&#21035;&#36816;&#21160;&#30340;&#31232;&#32570;&#20999;&#25442;&#12290;&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#25513;&#34109;&#23436;&#25104;&#26041;&#24335;&#35299;&#20915;&#20102;&#20197;&#19978;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#29983;&#25104;&#36816;&#21160;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36816;&#21160;&#39044;&#27979;&#24182;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HumanMAC&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction is a classical problem in computer vision and computer graphics, which has a wide range of practical applications. Previous effects achieve great empirical performance based on an encoding-decoding style. The methods of this style work by first encoding previous motions to latent representations and then decoding the latent representations into predicted motions. However, in practice, they are still unsatisfactory due to several issues, including complicated loss constraints, cumbersome training processes, and scarce switch of different categories of motions in prediction. In this paper, to address the above issues, we jump out of the foregoing style and propose a novel framework from a new perspective. Specifically, our framework works in a masked completion fashion. In the training stage, we learn a motion diffusion model that generates motions from random noise. In the inference stage, with a denoising procedure, we make motion prediction conditioning on obse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.13166</link><description>&lt;p&gt;
ESC&#65306;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#23548;&#33322;&#21040;&#29305;&#23450;&#29289;&#20307;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#24182;&#19982;&#29289;&#20307;&#20132;&#20114;&#20197;&#23436;&#25104;&#20219;&#21153;&#30340;&#23454;&#20307;&#20195;&#29702;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#26631;&#35760;&#29289;&#20307;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#36825;&#31181;&#35757;&#32451;&#25928;&#26524;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26032;&#39062;&#29289;&#20307;&#19978;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861;&#8212;&#8212;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#25506;&#32034;&#65288;ESC&#65289;&#65292;&#23427;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#36716;&#31227;&#21040;&#22312;&#35270;&#35273;&#29615;&#22659;&#19978;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#23548;&#33322;&#26102;&#19981;&#38656;&#35201;&#36827;&#34892;&#23548;&#33322;&#25110;&#20854;&#20182;&#35270;&#35273;&#29615;&#22659;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;ESC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#22320;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24120;&#35782;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25151;&#38388;&#21644;&#29289;&#20307;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;ESC&#36890;&#36807;&#23558;&#24120;&#35782;&#30693;&#35782;&#24314;&#27169;&#20026;&#36719;&#36923;&#36753;&#35859;&#35789;&#26469;&#20351;&#20854;&#36716;&#21270;&#20026;&#23548;&#33322;&#21160;&#20316;&#65292;&#20174;&#32780;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#22312;MP3D&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;......
&lt;/p&gt;
&lt;p&gt;
The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12896</link><description>&lt;p&gt;
&#37492;&#23450;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#21644;&#24378;&#38887;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Identifying Adversarially Attackable and Robust Samples. (arXiv:2301.12896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#23558;&#24494;&#23567;&#30340;&#65292;&#38590;&#20197;&#24863;&#30693;&#30340;&#25200;&#21160;&#25554;&#20837;&#36755;&#20837;&#26679;&#26412;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21457;&#29983;&#22823;&#37327;&#19981;&#26399;&#26395;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29983;&#25104;&#21644;&#38450;&#24481;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20174;&#36755;&#20837;&#25968;&#25454;&#35282;&#24230;&#29702;&#35299;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26679;&#26412;&#25915;&#20987;&#24615;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#25915;&#20987;&#24615;&#26679;&#26412;&#65289;&#65292;&#20174;&#32780;&#21453;&#36807;&#26469;&#30830;&#23450;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#24378;&#38887;&#26679;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#26410;&#30693;&#30446;&#26631;&#27169;&#22411;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#20013;&#65292;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#24378;&#38887;&#24615;&#26679;&#26412;&#12290;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22522;&#20110;&#31616;&#21333;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#25514;&#26045;&#30456;&#27604;&#65292;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks insert small, imperceptible perturbations to input samples that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. This work introduces the notion of sample attackability, where we aim to identify samples that are most susceptible to adversarial attacks (attackable samples) and conversely also identify the least susceptible samples (robust samples). We propose a deep-learning-based method to detect the adversarially attackable and robust samples in an unseen dataset for an unseen target model. Experiments on standard image classification datasets enables us to assess the portability of the deep attackability detector across a range of architectures. We find that the deep attackability detector performs better than simple model uncertainty-based measures for i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#20219;&#21153;&#25511;&#21046;&#20013;&#38754;&#20020;&#30340;&#24320;&#25918;&#19990;&#30028;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;Goal-Sensitive Backbone&#40723;&#21169;&#20986;&#29616;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#35270;&#35273;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#27169;&#22359;&#26469;&#20943;&#36731;&#38750;&#38745;&#24577;&#21160;&#24577;&#24341;&#36215;&#30340;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10034</link><description>&lt;p&gt;
&#36890;&#36807;&#30446;&#26631;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#30340;&#24320;&#25918;&#19990;&#30028;&#22810;&#20219;&#21153;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction. (arXiv:2301.10034v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#20219;&#21153;&#25511;&#21046;&#20013;&#38754;&#20020;&#30340;&#24320;&#25918;&#19990;&#30028;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;Goal-Sensitive Backbone&#40723;&#21169;&#20986;&#29616;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#35270;&#35273;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#27169;&#22359;&#26469;&#20943;&#36731;&#38750;&#38745;&#24577;&#21160;&#24577;&#24341;&#36215;&#30340;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;Minecraft&#20013;&#23398;&#20064;&#30446;&#26631;&#26465;&#20214;&#19979;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;Goal-Sensitive Backbone&#65288;GSB&#65289;&#20351;&#25919;&#31574;&#40723;&#21169;&#20986;&#29616;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#35270;&#35273;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#27169;&#22359;&#20943;&#36731;&#22240;&#38750;&#38745;&#24577;&#21160;&#24577;&#24341;&#36215;&#30340;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;20&#20010;Minecraft&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#22522;&#32447;&#65292;&#20854;&#20013;&#35768;&#22810;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#22522;&#32447;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning goal-conditioned policies in Minecraft, a popular, widely accessible yet challenging open-ended environment for developing human-level multi-task agents. We first identify two main challenges of learning such policies: 1) the indistinguishability of tasks from the state distribution, due to the vast scene diversity, and 2) the non-stationary nature of environment dynamics caused by partial observability. To tackle the first challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage the emergence of goal-relevant visual state representations. To tackle the second challenge, the policy is further fueled by an adaptive horizon prediction module that helps alleviate the learning uncertainty brought by the non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method significantly outperforms the best baseline so far; in many of them, we double the performance. Our ablation and exploratory studies then explain how our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PIRLNav&#65292;&#36890;&#36807;&#20154;&#31867;&#28436;&#31034;&#30340;BC&#39044;&#35757;&#32451;&#21644;RL&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;ObjectNav&#30340;65.0&#65285;&#65292;&#27604;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#39640;5.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2301.07302</link><description>&lt;p&gt;
PIRLNav: &#23545;&#20110;ObjectNav&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav. (arXiv:2301.07302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PIRLNav&#65292;&#36890;&#36807;&#20154;&#31867;&#28436;&#31034;&#30340;BC&#39044;&#35757;&#32451;&#21644;RL&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;ObjectNav&#30340;65.0&#65285;&#65292;&#27604;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#39640;5.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ObjectGoal Navigation&#8212;&#8212;&#22312;&#26032;&#29615;&#22659;&#20013;&#35201;&#27714;&#34394;&#25311;&#26426;&#22120;&#20154;&#23548;&#33322;&#21040;&#19968;&#20010;&#29289;&#20307;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#8212;&#8212;1&#65289;&#30001;&#20110;&#35757;&#32451;&#21482;&#27169;&#20223;&#21160;&#20316;&#32780;&#19981;&#26159;&#21518;&#26524;&#65292;&#22240;&#27492;BC&#31574;&#30053;&#23545;&#26032;&#29366;&#24577;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;2&#65289;&#25910;&#38598;&#28436;&#31034;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23481;&#26131;&#25193;&#23637;&#65292;&#20294;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#22870;&#21169;&#26469;&#23454;&#29616;&#29702;&#24819;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;PIRLNav&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#20154;&#31867;&#28436;&#31034;&#36827;&#34892;BC&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;RL&#24494;&#35843;&#12290;&#36825;&#23548;&#33268;&#35813;&#31574;&#30053;&#22312;ObjectNav&#19978;&#23454;&#29616;&#20102;65.0&#65285;&#30340;&#25104;&#21151;&#29575;&#65288;&#32477;&#23545;&#20540;&#27604;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#39640;5.0&#65285;&#65289;&#12290;&#20351;&#29992;&#36825;&#31181;BC $ \rightarrow $ RL&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#20154;&#31867;&#28436;&#31034;&#26159;&#21542;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of $65.0\%$ on ObjectNav ($+5.0\%$ absolute over previous state-of-the-art). Using this BC$\rightarrow$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.12380</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25193;&#23637;&#29289;&#29702;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#25968;&#25454;&#39537;&#21160;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#34987;&#25910;&#38598;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#22312;&#29289;&#29702;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35782;&#21035;&#21644;&#25193;&#23637;&#65292;&#24182;&#19988;&#21463;&#20854;&#26377;&#38480;&#30340;&#34920;&#29616;&#21147;&#24433;&#21709;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24120;&#24120;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476; (NNs) &#30340;&#32463;&#20856;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#32479;&#35745;&#27169;&#24335;&#65292;&#21363;&#20351;&#22312;&#25193;&#23637;&#26041;&#38754;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#28508;&#22312;&#30340;&#29289;&#29702;&#23450;&#24459;&#23436;&#20840;&#26080;&#35270;&#65292;&#22914;&#26524;&#22522;&#20110;&#23427;&#20204;&#20570;&#20915;&#31574;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26368;&#36817;&#24320;&#21457;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476; (PCNNs) &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992; NNs &#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558; PCNN &#25193;&#23637;&#21040;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#19982;&#32463;&#20856;&#28784;&#30418;&#21644;&#40657;&#30418;&#26041;&#27861;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#22810;&#21306;&#22495;&#24314;&#31569;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#21306;&#22495;&#30340;&#28909;&#34892;&#20026;&#30001;&#33021;&#37327;&#24179;&#34913;&#26041;&#31243;&#24335;&#32479;&#27835;&#65292;&#20854;&#21442;&#25968;&#24517;&#39035;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#35782;&#21035;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20214;&#26500;&#25104;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#31995;&#32479;&#65292;PCNNs &#20063;&#21487;&#20197;&#22312;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126; PCNN &#22312;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12053</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#20998;&#26512;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Calibrating Semantic Segmentation Models: Analyses and An Algorithm. (arXiv:2212.12053v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#22270;&#20687;&#20998;&#31867;&#32622;&#20449;&#24230;&#30340;&#27169;&#22411;&#35823;&#26657;&#20934;&#65292;&#20294;&#33267;&#20170;&#20026;&#27490;&#65292;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23481;&#37327;&#12289;&#35009;&#21098;&#22823;&#23567;&#12289;&#22810;&#23610;&#24230;&#27979;&#35797;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#23545;&#26657;&#20934;&#26377;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;&#39044;&#27979;&#27491;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#38169;&#35823;&#39044;&#27979;&#65292;&#23545;&#30001;&#20110;&#36807;&#24230;&#32622;&#20449;&#32780;&#23548;&#33268;&#30340;&#35823;&#26657;&#20934;&#26356;&#20026;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#32479;&#19968;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#29616;&#26377;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36873;&#25321;&#24615;&#32553;&#25918;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a vari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RepMode&#65292;&#19968;&#31181;&#32593;&#32476;&#65292;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#20808;&#39564;&#21160;&#24577;&#32452;&#32455;&#20854;&#21442;&#25968;&#20197;&#22788;&#29702;&#20122;&#32454;&#32990;&#32467;&#26500;&#39044;&#27979;&#31561;&#25351;&#23450;&#30340;&#21333;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.10066</link><description>&lt;p&gt;
RepMode&#65306;&#23398;&#20064;&#37325;&#26032;&#21442;&#25968;&#21270;&#29992;&#20110;&#20122;&#32454;&#32990;&#32467;&#26500;&#39044;&#27979;&#30340;&#19981;&#21516;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
RepMode: Learning to Re-parameterize Diverse Experts for Subcellular Structure Prediction. (arXiv:2212.10066v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RepMode&#65292;&#19968;&#31181;&#32593;&#32476;&#65292;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#20808;&#39564;&#21160;&#24577;&#32452;&#32455;&#20854;&#21442;&#25968;&#20197;&#22788;&#29702;&#20122;&#32454;&#32990;&#32467;&#26500;&#39044;&#27979;&#31561;&#25351;&#23450;&#30340;&#21333;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#30740;&#31350;&#20013;&#65292;&#33639;&#20809;&#26579;&#33394;&#26159;&#25581;&#31034;&#20122;&#32454;&#32990;&#32467;&#26500;&#20301;&#32622;&#21644;&#24418;&#24577;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25216;&#26415;&#36895;&#24230;&#24930;&#12289;&#36153;&#29992;&#39640;&#19988;&#23545;&#32454;&#32990;&#26377;&#23475;&#12290;&#26412;&#25991;&#23558;&#20854;&#24314;&#27169;&#20026;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#8212;&#8212;&#20122;&#32454;&#32990;&#32467;&#26500;&#39044;&#27979;&#65288;SSP&#65289;&#65292;&#26088;&#22312;&#39044;&#27979;&#22810;&#20010;&#20122;&#32454;&#32990;&#32467;&#26500;&#30340;&#19977;&#32500;&#33639;&#20809;&#22270;&#20687;&#65292;&#20174;&#32780;&#20174;3D&#36879;&#23556;&#20809;&#22270;&#20687;&#20013;&#33719;&#21462;&#26356;&#22810;&#20449;&#24687;&#12290;&#30001;&#20110;&#24403;&#21069;&#29983;&#29289;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;&#27599;&#20010;&#22270;&#20687;&#22312;SSP&#20013;&#21482;&#26377;&#37096;&#20998;&#26631;&#27880;&#12290;&#27492;&#22806;&#65292;&#20122;&#32454;&#32990;&#32467;&#26500;&#30340;&#22823;&#23567;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23548;&#33268;&#20102;SSP&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#26032;&#21442;&#25968;&#21270;&#22810;&#20803;&#19987;&#23478;&#28151;&#21512;&#65288;RepMode&#65289;&#65292;&#19968;&#31181;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#20808;&#39564;&#21160;&#24577;&#32452;&#32455;&#20854;&#21442;&#25968;&#20197;&#22788;&#29702;&#25351;&#23450;&#30340;&#21333;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;RepMode&#20013;&#65292;&#22810;&#20803;&#19987;&#23478;&#28151;&#21512;&#65288;MoDE&#65289;&#22359;&#34987;&#35774;&#35745;&#20026;&#23398;&#20064;&#25152;&#26377;&#20219;&#21153;&#30340;&#24191;&#20041;&#21442;&#25968;&#65292;&#21516;&#26102;&#38376;&#25511;&#37325;&#26032;&#21442;&#25968;&#21270;&#65288;GatRep&#65289;&#34987;&#29992;&#26469;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In biological research, fluorescence staining is a key technique to reveal the locations and morphology of subcellular structures. However, it is slow, expensive, and harmful to cells. In this paper, we model it as a deep learning task termed subcellular structure prediction (SSP), aiming to predict the 3D fluorescent images of multiple subcellular structures from a 3D transmitted-light image. Unfortunately, due to the limitations of current biotechnology, each image is partially labeled in SSP. Besides, naturally, subcellular structures vary considerably in size, which causes the multi-scale issue of SSP. To overcome these challenges, we propose Re-parameterizing Mixture-of-Diverse-Experts (RepMode), a network that dynamically organizes its parameters with task-aware priors to handle specified single-label prediction tasks. In RepMode, the Mixture-of-Diverse-Experts (MoDE) block is designed to learn the generalized parameters for all tasks, and gating re-parameterization (GatRep) is p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#25193;&#23637;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#27010;&#24565;&#30340;&#8220;&#29238;&#27597;&#8221;&#65292;&#28982;&#21518;&#30001;&#20154;&#31867;&#19987;&#23478;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#35777;&#39044;&#27979;&#30340;&#29238;&#27597;&#36317;&#31163;&#27010;&#24565;&#30340;&#30495;&#23454;&#29238;&#27597;&#8220;&#36817;&#8221;&#65292;&#33021;&#22815;&#25552;&#39640;&#20154;&#31639;&#21327;&#20316;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26032;&#38395;&#21644;&#23089;&#20048;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.05189</link><description>&lt;p&gt;
&#20154;&#24037;&#21442;&#19982;&#30340;&#30693;&#35782;&#22270;&#35889;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Expanding Knowledge Graphs with Humans in the Loop. (arXiv:2212.05189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#25193;&#23637;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#27010;&#24565;&#30340;&#8220;&#29238;&#27597;&#8221;&#65292;&#28982;&#21518;&#30001;&#20154;&#31867;&#19987;&#23478;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#35777;&#39044;&#27979;&#30340;&#29238;&#27597;&#36317;&#31163;&#27010;&#24565;&#30340;&#30495;&#23454;&#29238;&#27597;&#8220;&#36817;&#8221;&#65292;&#33021;&#22815;&#25552;&#39640;&#20154;&#31639;&#21327;&#20316;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26032;&#38395;&#21644;&#23089;&#20048;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#31574;&#21010;&#30340;&#30693;&#35782;&#22270;&#35889;&#32534;&#30721;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#25512;&#33616;&#12289;&#20998;&#21106;&#12289;&#24191;&#21578;&#23450;&#21521;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#39046;&#22495;&#20013;&#20986;&#29616;&#26032;&#27010;&#24565;&#65292;&#30693;&#35782;&#22270;&#35889;&#24517;&#39035;&#25193;&#23637;&#20197;&#20445;&#25345;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22312;&#35268;&#27169;&#19978;&#25163;&#21160;&#25193;&#23637;&#30693;&#35782;&#22270;&#35889;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#25299;&#23637;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39044;&#27979;&#20102;&#38656;&#35201;&#28155;&#21152;&#21040;&#27492;&#22270;&#35889;&#20013;&#30340;&#26032;&#27010;&#24565;&#30340;&#8220;&#29238;&#27597;&#8221;&#65292;&#20197;&#20379;&#20154;&#31867;&#19987;&#23478;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20934;&#30830;&#21448;&#21487;&#35777;&#26126;&#26159;&#8220;&#20154;&#24615;&#21270;&#8221;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#39044;&#27979;&#19981;&#27491;&#30830;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#39044;&#27979;&#36317;&#31163;&#27010;&#24565;&#30340;&#30495;&#23454;&#29238;&#27597;&#8220;&#36817;&#8221;&#30340;&#29238;&#27597;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21463;&#25511;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#28385;&#36275;&#27492;&#23646;&#24615;&#21487;&#20197;&#22686;&#21152;&#20154;&#31639;&#21327;&#20316;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26032;&#38395;&#21644;&#23089;&#20048;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curated knowledge graphs encode domain expertise and improve the performance of recommendation, segmentation, ad targeting, and other machine learning systems in several domains. As new concepts emerge in a domain, knowledge graphs must be expanded to preserve machine learning performance. Manually expanding knowledge graphs, however, is infeasible at scale. In this work, we propose a method for knowledge graph expansion with humans-in-the-loop. Concretely, given a knowledge graph, our method predicts the "parents" of new concepts to be added to this graph for further verification by human experts. We show that our method is both accurate and provably "human-friendly". Specifically, we prove that our method predicts parents that are "near" concepts' true parents in the knowledge graph, even when the predictions are incorrect. We then show, with a controlled experiment, that satisfying this property increases both the speed and the accuracy of the human-algorithm collaboration. We furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#65292;&#36890;&#36807;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.03640</link><description>&lt;p&gt;
&#32454;&#35843;CLIP&#27169;&#22411;&#26159;&#39640;&#25928;&#30340;&#35270;&#39057;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned CLIP Models are Efficient Video Learners. (arXiv:2212.03640v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#65292;&#36890;&#36807;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline, which effectively transfers image-level CLIP representations to videos by frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings, thus bridging the domain gap from images to videos.
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#65292;CLIP&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30001;&#20110;&#22312;&#31867;&#20284;&#35268;&#27169;&#19978;&#23545;&#35270;&#39057;&#36827;&#34892;&#35757;&#32451;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;CLIP&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#35270;&#39057;&#39046;&#22495;&#12290;&#22312;&#36825;&#20010;&#36861;&#27714;&#20013;&#65292;&#28155;&#21152;&#20102;&#26032;&#30340;&#21442;&#25968;&#27169;&#22359;&#26469;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#21644;&#24103;&#38388;&#20851;&#31995;&#65292;&#36825;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#24403;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35270;&#39057;&#19978;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#20219;&#21153;&#20998;&#24067;&#65292;&#24182;&#19988;&#32570;&#20047;&#27867;&#21270;&#26041;&#38754;&#12290;&#36825;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#36890;&#24120;&#36275;&#20197;&#24357;&#21512;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overfit on the given task distribution and lack in generalization aspect. This begs the following question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings helps in imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#20998;&#35299;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#31616;&#21333;&#30340;&#29305;&#24449;&#35843;&#25972;&#26469;&#30830;&#20445;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#28385;&#36275;&#37325;&#24314;&#21644;&#32534;&#36753;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21463;&#37326;&#22806;&#38754;&#37096;&#35270;&#39057;&#30340;&#35282;&#33853;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02802</link><description>&lt;p&gt;
&#25193;&#25955;&#35270;&#39057;&#33258;&#32534;&#30721;&#22120;&#65306;&#36890;&#36807;&#20998;&#35299;&#35270;&#39057;&#29305;&#24449;&#23454;&#29616;&#19968;&#33268;&#30340;&#20154;&#33080;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding. (arXiv:2212.02802v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#20998;&#35299;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#31616;&#21333;&#30340;&#29305;&#24449;&#35843;&#25972;&#26469;&#30830;&#20445;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#28385;&#36275;&#37325;&#24314;&#21644;&#32534;&#36753;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21463;&#37326;&#22806;&#38754;&#37096;&#35270;&#39057;&#30340;&#35282;&#33853;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#26368;&#36817;&#38754;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#30340;&#24778;&#20154;&#34920;&#29616;&#30340;&#21551;&#21457;&#65292;&#33258;&#28982;&#20250;&#26377;&#20960;&#39033;&#30740;&#31350;&#26469;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290; &#36825;&#37324;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32534;&#36753;&#24103;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#36825;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#20998;&#35299;&#30340;&#29305;&#24449;-&#39318;&#27425;&#20316;&#20026;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#27169;&#22411;-&#26631;&#35782;&#21644;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#35268;&#33539;&#21270;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#26469;&#25552;&#39640;&#35299;&#37322;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.17174</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#35268;&#33539;&#21270;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#20248;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Optimizing Explanations by Network Canonization and Hyperparameter Search. (arXiv:2211.17174v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17174
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#35268;&#33539;&#21270;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#26469;&#25552;&#39640;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27491;&#22312;&#36880;&#28176;&#25104;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21017;&#21644;&#20462;&#25913;&#21518;&#30340;&#21453;&#21521;&#20256;&#25773;XAI&#26041;&#27861;&#24448;&#24448;&#22312;&#24212;&#29992;&#20110;&#29616;&#20195;&#27169;&#22411;&#26550;&#26500;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#21019;&#26032;&#30340;&#23618;&#26500;&#24314;&#22359;&#65292;&#36825;&#26159;&#30001;&#20004;&#20010;&#21407;&#22240;&#36896;&#25104;&#30340;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;XAI&#26041;&#27861;&#30340;&#39640;&#28789;&#27963;&#24615;&#23548;&#33268;&#20102;&#35768;&#22810;&#28508;&#22312;&#30340;&#21442;&#25968;&#21270;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;XAI&#26041;&#27861;&#30772;&#22351;&#20102;&#23454;&#29616;&#19981;&#21464;&#24615;&#20844;&#29702;&#65292;&#22240;&#20026;&#20182;&#20204;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#27169;&#22411;&#32452;&#20214;&#65292;&#20363;&#22914;BatchNorm&#23618;&#12290;&#21518;&#32773;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#35268;&#33539;&#21270;&#26469;&#35299;&#20915;&#65292;&#27169;&#22411;&#35268;&#33539;&#21270;&#26159;&#37325;&#26032;&#32452;&#32455;&#27169;&#22411;&#20197;&#24573;&#30053;&#26377;&#38382;&#39064;&#30340;&#32452;&#20214;&#32780;&#19981;&#25913;&#21464;&#22522;&#26412;&#20989;&#25968;&#30340;&#36807;&#31243;&#12290;&#34429;&#28982;&#23545;&#20110;&#31616;&#21333;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;VGG&#12289;ResNet&#65289;&#65292;&#27169;&#22411;&#35268;&#33539;&#21270;&#24456;&#31616;&#21333;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#39640;&#24230;&#20114;&#32852;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;DenseNet&#65289;&#65292;&#23427;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#21487;&#37327;&#21270;&#35777;&#25454;&#34920;&#26126;&#27169;&#22411;&#35268;&#33539;&#21270;&#23545;XAI&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is slowly becoming a key component for many AI applications. Rule-based and modified backpropagation XAI approaches however often face challenges when being applied to modern model architectures including innovative layer building blocks, which is caused by two reasons. Firstly, the high flexibility of rule-based XAI methods leads to numerous potential parameterizations. Secondly, many XAI methods break the implementation-invariance axiom because they struggle with certain model components, e.g., BatchNorm layers. The latter can be addressed with model canonization, which is the process of re-structuring the model to disregard problematic components without changing the underlying function. While model canonization is straightforward for simple architectures (e.g., VGG, ResNet), it can be challenging for more complex and highly interconnected models (e.g., DenseNet). Moreover, there is only little quantifiable evidence that model canonization is beneficial for XAI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;Shifted Diffusion&#27169;&#22411;&#26356;&#22909;&#22320;&#29983;&#25104;&#26469;&#33258;&#36755;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25903;&#25345;&#21322;&#30417;&#30563;&#21644;&#26080;&#35821;&#35328;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2211.15388</link><description>&lt;p&gt;
Shifted Diffusion&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shifted Diffusion for Text-to-image Generation. (arXiv:2211.15388v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;Shifted Diffusion&#27169;&#22411;&#26356;&#22909;&#22320;&#29983;&#25104;&#26469;&#33258;&#36755;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25903;&#25345;&#21322;&#30417;&#30563;&#21644;&#26080;&#35821;&#35328;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Corgi&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;Corgi&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;Shifted Diffusion&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#26469;&#33258;&#36755;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#23884;&#20837;&#12290;&#19982;DALL-E 2&#20013;&#20351;&#29992;&#30340;&#22522;&#32447;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#21021;&#22987;&#21270;&#20998;&#24067;&#21644;&#25193;&#25955;&#30340;&#26032;&#36807;&#28193;&#27493;&#39588;&#65292;&#22312;&#20854;&#25193;&#25955;&#36807;&#31243;&#20013;&#26080;&#32541;&#22320;&#32534;&#30721;&#20102;&#39044;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#19982;&#24378;&#21170;&#30340;DALL-E 2&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#23884;&#20837;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#24182;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21322;&#30417;&#30563;&#21644;&#26080;&#35821;&#35328;&#35757;&#32451;&#65292;&#21482;&#38656;&#35201;&#37096;&#20998;&#25110;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#19982;&#36755;&#20837;&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Corgi, a novel method for text-to-image generation. Corgi is based on our proposed shifted diffusion model, which achieves better image embedding generation from input text. Unlike the baseline diffusion model used in DALL-E 2, our method seamlessly encodes prior knowledge of the pre-trained CLIP model in its diffusion process by designing a new initialization distribution and a new transition step of the diffusion. Compared to the strong DALL-E 2 baseline, our method performs better in generating image embedding from the text in terms of both efficiency and effectiveness, resulting in better text-to-image generation. Extensive large-scale experiments are conducted and evaluated in terms of both quantitative measures and human evaluation, indicating a stronger generation ability of our method compared to existing ones. Furthermore, our model enables semi-supervised and language-free training for text-to-image generation, where only part or none of the images in the training 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; MUPPET &#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#32452;&#21512;&#26102;&#24207;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#35299;&#20915;&#20102;&#38382;&#39064;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14905</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Few-Shot Temporal Action Detection. (arXiv:2211.14905v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; MUPPET &#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#32452;&#21512;&#26102;&#24207;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#35299;&#20915;&#20102;&#38382;&#39064;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412; (FS) &#21644;&#38646;&#26679;&#26412; (ZS) &#23398;&#20064;&#26159;&#32553;&#25918;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979; (TAD) &#21040;&#26032;&#31867;&#30340;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#12290;&#21069;&#32773;&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36866;&#24212;&#20110;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#30001;&#27599;&#31867;&#20165;&#26377;&#19968;&#20010;&#35270;&#39057;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#36890;&#36807;&#21033;&#29992;&#26032;&#31867;&#30340;&#35821;&#20041;&#25551;&#36848;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412; (MMFS) TAD &#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#20849;&#21516;&#21033;&#29992;&#23569;&#25968;&#25903;&#25345;&#35270;&#39057;&#21644;&#26032;&#31867;&#21517;&#23383;&#26469;&#32467;&#21512; FS-TAD &#21644; ZS-TAD &#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; MUlti-modality PromPt mETa-learning (MUPPET) &#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#37325;&#29992;&#24050;&#32463;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#36866;&#37197;&#22120;&#35013;&#22791;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#35789;&#22120;&#65292;&#23558;&#25903;&#25345;&#35270;&#39057;&#26144;&#23556;&#21040;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#26631;&#35760;&#31354;&#38388;&#20013;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#23545;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#25552;&#35758;&#29255;&#27573;&#36827;&#34892;&#20998;&#32452;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot (FS) and zero-shot (ZS) learning are two different approaches for scaling temporal action detection (TAD) to new classes. The former adapts a pretrained vision model to a new task represented by as few as a single video per class, whilst the latter requires no training examples by exploiting a semantic description of the new class. In this work, we introduce a new multi-modality few-shot (MMFS) TAD problem, which can be considered as a marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new class names jointly. To tackle this problem, we further introduce a novel MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by efficiently bridging pretrained vision and language models whilst maximally reusing already learned capacity. Concretely, we construct multi-modal prompts by mapping support videos into the textual token space of a vision-language model using a meta-learned adapter-equipped visual semantics tokenizer. To tackle large intra-clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21098;&#26525;&#25216;&#26415;&#35774;&#35745;&#21387;&#32553;&#29256;&#30340;&#39640;&#24230;&#21387;&#32553;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#12290;&#36890;&#36807;&#23545;CNN&#21644;Transformers&#36319;&#36394;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#25581;&#31034;&#20986;&#22312;&#35774;&#35745;&#36731;&#37327;&#32423;&#36319;&#36394;&#22120;&#26102;&#30340;&#26368;&#20339;&#26550;&#26500;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#26497;&#31471;&#21098;&#26525;&#29575;&#30340;&#36319;&#36394;&#32467;&#26524;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#32593;&#32476;&#21098;&#26525;&#22312;&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2211.13769</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#21098;&#26525;&#35774;&#35745;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#65306;&#20351;&#29992;CNN&#36824;&#26159;Transformers?
&lt;/p&gt;
&lt;p&gt;
On Designing Light-Weight Object Trackers through Network Pruning: Use CNNs or Transformers?. (arXiv:2211.13769v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21098;&#26525;&#25216;&#26415;&#35774;&#35745;&#21387;&#32553;&#29256;&#30340;&#39640;&#24230;&#21387;&#32553;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#12290;&#36890;&#36807;&#23545;CNN&#21644;Transformers&#36319;&#36394;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#25581;&#31034;&#20986;&#22312;&#35774;&#35745;&#36731;&#37327;&#32423;&#36319;&#36394;&#22120;&#26102;&#30340;&#26368;&#20339;&#26550;&#26500;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#26497;&#31471;&#21098;&#26525;&#29575;&#30340;&#36319;&#36394;&#32467;&#26524;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#32593;&#32476;&#21098;&#26525;&#22312;&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#29289;&#20307;&#36319;&#36394;&#22120;&#38656;&#35201;&#36731;&#37327;&#32423;&#35774;&#35745;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#26500;&#24314;&#20110;CNN&#25110;transformers&#19978;&#30340;&#35745;&#31639;&#23494;&#38598;&#30340;&#20027;&#24178;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24040;&#22823;&#23610;&#23544;&#19981;&#20801;&#35768;&#22312;&#20302;&#21151;&#32791;&#26465;&#20214;&#19979;&#37096;&#32626;&#23427;&#20204;&#65292;&#22240;&#27492;&#35774;&#35745;&#21387;&#32553;&#29256;&#30340;&#22823;&#22411;&#36319;&#36394;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#21098;&#26525;&#35774;&#35745;&#39640;&#24230;&#21387;&#32553;&#30340;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#35774;&#35745;&#36731;&#37327;&#32423;&#36319;&#36394;&#22120;&#30340;&#26550;&#26500;&#36873;&#25321;&#12290;&#36824;&#25552;&#20379;&#20102;&#20351;&#29992;CNNs&#12289;transformers&#20197;&#21450;&#20004;&#32773;&#32452;&#21512;&#30340;&#26368;&#26032;&#36319;&#36394;&#22120;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20197;&#30740;&#31350;&#23427;&#20204;&#22312;&#21508;&#31181;&#21387;&#32553;&#27604;&#19979;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#23637;&#31034;&#20102;&#26497;&#31471;&#21098;&#26525;&#22330;&#26223;&#19979;&#30340;&#32467;&#26524;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#21098;&#26525;&#27604;&#29575;&#20302;&#33267;1%&#65292;&#20197;&#30740;&#31350;&#32593;&#32476;&#21098;&#26525;&#22312;&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object trackers deployed on low-power devices need to be light-weight, however, most of the current state-of-the-art (SOTA) methods rely on using compute-heavy backbones built using CNNs or transformers. Large sizes of such models do not allow their deployment in low-power conditions and designing compressed variants of large tracking models is of great importance. This paper demonstrates how highly compressed light-weight object trackers can be designed using neural architectural pruning of large CNN and transformer based trackers. Further, a comparative study on architectural choices best suited to design light-weight trackers is provided. A comparison between SOTA trackers using CNNs, transformers as well as the combination of the two is presented to study their stability at various compression ratios. Finally results for extreme pruning scenarios going as low as 1% in some cases are shown to study the limits of network pruning in object tracking. This work provides deeper insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CLEVER&#65292;&#19968;&#31181;&#20197;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24120;&#35782;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#21547;&#26377;&#20851;&#23454;&#20307;&#23545;&#30340;&#22270;&#20687;&#21253;&#27719;&#24635;&#20986;&#24120;&#35782;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#23545;&#22270;&#20687;&#23454;&#20363;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12054</link><description>&lt;p&gt;
&#35270;&#35273;&#22522;&#30784;&#19979;&#30340;&#24120;&#35782;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CLEVER&#65292;&#19968;&#31181;&#20197;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24120;&#35782;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#21547;&#26377;&#20851;&#23454;&#20307;&#23545;&#30340;&#22270;&#20687;&#21253;&#27719;&#24635;&#20986;&#24120;&#35782;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#23545;&#22270;&#20687;&#23454;&#20363;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#24120;&#35782;&#30693;&#35782;&#24211;&#20026;&#24191;&#27867;&#30340;AI&#24212;&#29992;&#25552;&#20379;&#21160;&#21147;&#65292;&#20854;&#20013;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#65288;CKE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#25991;&#26412;&#25552;&#21462;CKE&#30693;&#35782;&#26159;&#24050;&#30693;&#30340;&#21463;&#38480;&#20110;&#26412;&#36136;&#30340;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#20559;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35270;&#35273;&#30693;&#35273;&#21253;&#21547;&#26377;&#20851;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20016;&#23500;&#24120;&#35782;&#30693;&#35782;&#65292;&#20363;&#22914;&#65288;&#20154;&#65292;&#21487;&#20197;&#25345;&#26377;&#65292;&#29942;&#23376;&#65289;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20316;&#20026;&#33719;&#21462;&#22522;&#20110;&#24120;&#35782;&#30340;&#30693;&#35782;&#30340;&#26377;&#24076;&#26395;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;CLEVER&#65292;&#23558;CKE&#20316;&#20026;&#19968;&#31181;&#36828;&#36317;&#31163;&#30417;&#30563;&#22810;&#23454;&#20363;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#20102;&#35268;&#23450;&#65292;&#27169;&#22411;&#36890;&#36807;&#20851;&#20110;&#23454;&#20307;&#23545;&#30340;&#22270;&#20687;&#21253;&#27719;&#24635;&#20986;&#24120;&#35782;&#20851;&#31995;&#32780;&#26080;&#38656;&#23545;&#22270;&#20687;&#23454;&#20363;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;CLEVER&#20351;&#29992;&#20102;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#28145;&#20837;&#29702;&#35299;&#22270;&#20687;&#21253;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#65292;&#24182;&#20174;&#20013;&#36873;&#25321;&#20449;&#24687;&#24615;&#23454;&#20363;&#65292;&#20197;&#36890;&#36807;&#26032;&#39062;&#30340;&#20851;&#31995;&#27719;&#24635;&#26041;&#24335;&#27010;&#25324;&#24120;&#35782;&#30693;&#35782;&#23454;&#20307;&#20851;&#31995;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel cont
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#22686;&#25439;&#22833;&#39033;&#26368;&#23567;&#21270;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#38598;&#31934;&#28860;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.11004</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#26469;&#25552;&#39640;&#25968;&#25454;&#38598;&#31934;&#28860;&#25928;&#26524;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation. (arXiv:2211.11004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11004
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#22686;&#25439;&#22833;&#39033;&#26368;&#23567;&#21270;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#38598;&#31934;&#28860;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#22312;&#35745;&#31639;&#12289;&#23384;&#20648;&#12289;&#35757;&#32451;&#21644;&#25628;&#23547;&#33391;&#22909;&#30340;&#31070;&#32463;&#32467;&#26500;&#31561;&#26041;&#38754;&#25104;&#26412;&#30456;&#24403;&#39640;&#26114;&#65292;&#22240;&#27492;&#25968;&#25454;&#38598;&#31934;&#28860;&#36817;&#26399;&#25104;&#20026;&#28966;&#28857;&#12290;&#36825;&#31181;&#33539;&#24335;&#28041;&#21450;&#33719;&#21462;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#24182;&#23558;&#20854;&#25552;&#28860;&#20026;&#24494;&#23567;&#32039;&#20945;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#22788;&#29702;&#21518;&#32773;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#21069;&#32773;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#23398;&#20064;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#21305;&#37197;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#26799;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26799;&#24230;&#21305;&#37197;&#26041;&#27861;&#21463;&#21040;&#25152;&#35859;&#30340;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#27492;&#35823;&#24046;&#26159;&#30001;&#20110;&#31934;&#28860;&#21644;&#21518;&#32493;&#35780;&#20272;&#20043;&#38388;&#19981;&#19968;&#33268;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#40723;&#21169;&#23558;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#20174;&#35757;&#32451;&#38454;&#27573;&#36716;&#31227;&#21040;&#31934;&#28860;&#38454;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25439;&#22833;&#39033;&#65292;&#22312;&#31934;&#28860;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24182;&#26174;&#33879;&#20943;&#23569;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based deep learning has achieved astounding successes due in part to the availability of large-scale real-world data. However, processing such massive amounts of data comes at a considerable cost in terms of computations, storage, training and the search for good neural architectures. Dataset distillation has thus recently come to the fore. This paradigm involves distilling information from large real-world datasets into tiny and compact synthetic datasets such that processing the latter ideally yields similar performances as the former. State-of-the-art methods primarily rely on learning the synthetic dataset by matching the gradients obtained during training between the real and synthetic data. However, these gradient-matching methods suffer from the so-called accumulated trajectory error caused by the discrepancy between the distillation and subsequent evaluation. To mitigate the adverse impact of this accumulated trajectory error, we propose a novel approach that encourages t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#27454;&#26032;&#39062;&#30340;&#34892;&#20026;&#20449;&#24687;&#32858;&#21512;&#32593;&#32476;&#65288;BIAN&#65289;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#20010;&#20154;&#36164;&#26009;&#12289;&#34892;&#20026;&#20197;&#21450;&#31038;&#20132;&#32852;&#31995;&#65292;&#20197;&#26377;&#25928;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#12290;</title><link>http://arxiv.org/abs/2211.06315</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#20026;&#20449;&#24687;&#32858;&#21512;&#32593;&#32476;&#65288;BIAN&#65289;&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#29992;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fraudulent User Detection Via Behavior Information Aggregation Network (BIAN) On Large-Scale Financial Social Network. (arXiv:2211.06315v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#27454;&#26032;&#39062;&#30340;&#34892;&#20026;&#20449;&#24687;&#32858;&#21512;&#32593;&#32476;&#65288;BIAN&#65289;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#20010;&#20154;&#36164;&#26009;&#12289;&#34892;&#20026;&#20197;&#21450;&#31038;&#20132;&#32852;&#31995;&#65292;&#20197;&#26377;&#25928;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#27450;&#35784;&#27599;&#24180;&#36896;&#25104;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#25439;&#22833;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#32771;&#34385;&#31038;&#20132;&#32593;&#32476;&#20013;&#29992;&#25143;&#30340;&#20010;&#20154;&#36164;&#26009;&#21644;&#34892;&#20026;&#20197;&#26816;&#27979;&#27450;&#35784;&#12290;&#31038;&#20132;&#32593;&#32476;&#24418;&#25104;&#20102;&#19968;&#20010;&#22270;&#24418;&#32467;&#26500;&#65292;&#32780;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#26080;&#32541;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#22270;&#24418;&#25968;&#25454;&#12290;&#22312;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#20010;&#20154;&#36164;&#26009;&#21644;&#34892;&#20026;&#65288;&#22914;&#20132;&#26131;&#12289;&#36151;&#27454;&#31561;&#65289;&#20197;&#21450;&#20182;&#20204;&#30340;&#31038;&#20132;&#32852;&#31995;&#65292;&#21487;&#20197;&#35782;&#21035;&#29359;&#32618;&#20998;&#23376;&#30340;&#20316;&#26696;&#26041;&#24335;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;GNN&#26080;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#37051;&#23621;&#65292;&#22240;&#20026;&#37051;&#23621;&#30340;&#36793;&#23646;&#24615;&#65288;&#21363;&#34892;&#20026;&#65289;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34892;&#20026;&#20449;&#24687;&#32858;&#21512;&#32593;&#32476;&#65288;BIAN&#65289;&#65292;&#23558;&#29992;&#25143;&#30340;&#34892;&#20026;&#19982;&#20854;&#20182;&#29992;&#25143;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#19982;&#20854;&#36817;&#20146;Graph Attention Networks&#65288;GAT&#65289;&#21644;Graph Transformer Networks&#65288;GTN&#65289;&#19981;&#21516;&#65292;&#23427;&#22522;&#20110;&#30456;&#37051;&#36793;&#23646;&#24615;&#36827;&#34892;&#37051;&#22495;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial frauds cause billions of losses annually and yet it lacks efficient approaches in detecting frauds considering user profile and their behaviors simultaneously in social network . A social network forms a graph structure whilst Graph neural networks (GNN), a promising research domain in Deep Learning, can seamlessly process non-Euclidean graph data . In financial fraud detection, the modus operandi of criminals can be identified by analyzing user profile and their behaviors such as transaction, loaning etc. as well as their social connectivity. Currently, most GNNs are incapable of selecting important neighbors since the neighbors' edge attributes (i.e., behaviors) are ignored. In this paper, we propose a novel behavior information aggregation network (BIAN) to combine the user behaviors with other user features. Different from its close "relatives" such as Graph Attention Networks (GAT) and Graph Transformer Networks (GTN), it aggregates neighbors based on neighboring edge at
&lt;/p&gt;</description></item><item><title>DynamicISP&#26159;&#19968;&#20010;&#21160;&#24577;&#25511;&#21046;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65292;&#33021;&#22815;&#26681;&#25454;&#21069;&#19968;&#24103;&#30340;&#35782;&#21035;&#32467;&#26524;&#33258;&#21160;&#35843;&#25972;&#27599;&#24103;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21333;&#31867;&#21035;&#21644;&#22810;&#31867;&#21035;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#20302;&#12290;</title><link>http://arxiv.org/abs/2211.01146</link><description>&lt;p&gt;
DynamicISP&#65306;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;&#21160;&#24577;&#25511;&#21046;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition. (arXiv:2211.01146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01146
&lt;/p&gt;
&lt;p&gt;
DynamicISP&#26159;&#19968;&#20010;&#21160;&#24577;&#25511;&#21046;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65292;&#33021;&#22815;&#26681;&#25454;&#21069;&#19968;&#24103;&#30340;&#35782;&#21035;&#32467;&#26524;&#33258;&#21160;&#35843;&#25972;&#27599;&#24103;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21333;&#31867;&#21035;&#21644;&#22810;&#31867;&#21035;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65288;ISPs&#65289;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#21644;&#25429;&#33719;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19987;&#23478;&#20204;&#20250;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#25163;&#21160;&#35843;&#25972;ISPs&#30340;&#35768;&#22810;&#21442;&#25968;&#65292;&#20294;&#36825;&#20123;&#21442;&#25968;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#31215;&#26497;&#30740;&#31350;&#20102;&#20004;&#31181;&#25216;&#26415;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21442;&#25968;&#35843;&#25972;&#25216;&#26415;&#21644;&#22522;&#20110;DNN&#30340;ISP&#25216;&#26415;&#12290;&#21069;&#32773;&#36731;&#37327;&#32423;&#20294;&#32570;&#20047;&#34920;&#29616;&#21147;&#65292;&#21518;&#32773;&#20855;&#26377;&#34920;&#29616;&#21147;&#65292;&#20294;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#22826;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;DynamicISP&#8221;&#65292;&#23427;&#30001;&#22810;&#20010;&#20256;&#32479;&#30340;ISP&#20989;&#25968;&#32452;&#25104;&#65292;&#24182;&#26681;&#25454;&#21069;&#19968;&#24103;&#30340;&#35782;&#21035;&#32467;&#26524;&#21160;&#24577;&#25511;&#21046;&#27599;&#20010;&#24103;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25511;&#21046;&#20102;&#22810;&#20010;ISP&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#22312;&#21333;&#31867;&#21035;&#21644;&#22810;&#31867;&#21035;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Signal Processors (ISPs) play important roles in image recognition tasks as well as in the perceptual quality of captured images. In most cases, experts make a lot of effort to manually tune many parameters of ISPs, but the parameters are sub-optimal. In the literature, two types of techniques have been actively studied: a machine learning-based parameter tuning technique and a DNN-based ISP technique. The former is lightweight but lacks expressive power. The latter has expressive power, but the computational cost is too heavy on edge devices. To solve these problems, we propose "DynamicISP," which consists of multiple classical ISP functions and dynamically controls the parameters of each frame according to the recognition result of the previous frame. We show our method successfully controls the parameters of multiple ISP functions and achieves state-of-the-art accuracy with low computational cost in single and multi-category object detection tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.17426</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#20998;&#26512;&#23454;&#29616;&#19968;&#33268;&#19988;&#30495;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#38656;&#35201;&#19982;&#24403;&#21069;&#26696;&#20363;&#30456;&#20851;&#30340;&#20551;&#35774;&#24773;&#26223;&#19968;&#33268;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#22240;&#32032;&#25913;&#21464;&#65292;&#27169;&#22411;&#20250;&#22914;&#20309;&#21453;&#24212;&#65311;&#23613;&#31649;&#24402;&#22240;&#26041;&#27861;&#30001;&#20248;&#38597;&#30340;&#20844;&#29702;&#31995;&#32479;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20837;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#19968;&#33268;&#12290;&#20026;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#24067;&#23572;&#20989;&#25968;&#30340;&#20613;&#31435;&#21494;&#20998;&#26512;&#26469;&#33719;&#24471;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#21508;&#31181;&#21322;&#24452;&#30340;&#37051;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#33267;50&#20493;&#26356;&#20302;&#30340;&#35299;&#37322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
&lt;/p&gt;</description></item><item><title>LongShortNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#36335;&#24452;&#32593;&#32476;&#30340;&#27969;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#26399;&#26102;&#38388;&#36816;&#21160;&#21644;&#30701;&#26399;&#31354;&#38388;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;Argoverse-HD&#25968;&#25454;&#38598;&#19978;&#65292;LongShortNet&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2210.15518</link><description>&lt;p&gt;
LongShortNet&#65306;&#25506;&#32034;&#26102;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#34701;&#21512;&#22312;&#27969;&#24335;&#24863;&#30693;&#20013;
&lt;/p&gt;
&lt;p&gt;
LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception. (arXiv:2210.15518v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15518
&lt;/p&gt;
&lt;p&gt;
LongShortNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#36335;&#24452;&#32593;&#32476;&#30340;&#27969;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#26399;&#26102;&#38388;&#36816;&#21160;&#21644;&#30701;&#26399;&#31354;&#38388;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;Argoverse-HD&#25968;&#25454;&#38598;&#19978;&#65292;LongShortNet&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#24863;&#30693;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#20180;&#32454;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27969;&#24335;&#24863;&#30693;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#24403;&#21069;&#24103;&#21450;&#20854;&#30456;&#37051;&#30340;&#20004;&#24103;&#26469;&#23398;&#20064;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#22797;&#26434;&#22330;&#26223;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24448;&#24448;&#23548;&#33268;&#26816;&#27979;&#32467;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongShortNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#36335;&#24452;&#32593;&#32476;&#65292;&#23427;&#25429;&#25417;&#38271;&#26399;&#30340;&#26102;&#38388;&#36816;&#21160;&#65292;&#24182;&#23558;&#20854;&#19982;&#30701;&#26399;&#30340;&#31354;&#38388;&#35821;&#20041;&#38598;&#25104;&#21040;&#23454;&#26102;&#24863;&#30693;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;LongShortNet&#20540;&#24471;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#26159;&#31532;&#19968;&#20010;&#23558;&#38271;&#26399;&#26102;&#38388;&#24314;&#27169;&#25193;&#23637;&#21040;&#27969;&#24335;&#24863;&#30693;&#30340;&#24037;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Argoverse-HD&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LongShortNet&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming perception is a fundamental task in autonomous driving that requires a careful balance between the latency and accuracy of the autopilot system. However, current methods for streaming perception are limited as they rely only on the current and adjacent two frames to learn movement patterns, which restricts their ability to model complex scenes, often leading to poor detection results. To address this limitation, we propose LongShortNet, a novel dual-path network that captures long-term temporal motion and integrates it with short-term spatial semantics for real-time perception. Our proposed LongShortNet is notable as it is the first work to extend long-term temporal modeling to streaming perception, enabling spatiotemporal feature fusion. We evaluate LongShortNet on the challenging Argoverse-HD dataset and demonstrate that it outperforms existing state-of-the-art methods with almost no additional computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#36827;&#30340;&#19978;&#19979;&#25991;&#21464;&#25442;&#26426;&#21046;&#30340;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861; ProContEXT&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#36880;&#27493;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#22810;&#23610;&#24230;&#27169;&#26495;&#36827;&#34892;&#20934;&#30830;&#36319;&#36394;&#12290;&#23427;&#25506;&#32034;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#20026;&#22522;&#20110; transformer &#30340;&#36319;&#36394;&#22120;&#30340;&#22810;&#19978;&#19979;&#25991;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2210.15511</link><description>&lt;p&gt;
ProContEXT&#65306;&#22522;&#20110;&#36882;&#36827;&#30340;&#19978;&#19979;&#25991;&#21464;&#25442;&#26426;&#21046;&#30340;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ProContEXT: Exploring Progressive Context Transformer for Tracking. (arXiv:2210.15511v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#36827;&#30340;&#19978;&#19979;&#25991;&#21464;&#25442;&#26426;&#21046;&#30340;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861; ProContEXT&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#36880;&#27493;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#22810;&#23610;&#24230;&#27169;&#26495;&#36827;&#34892;&#20934;&#30830;&#36319;&#36394;&#12290;&#23427;&#25506;&#32034;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#20026;&#22522;&#20110; transformer &#30340;&#36319;&#36394;&#22120;&#30340;&#22810;&#19978;&#19979;&#25991;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35270;&#35273;&#30446;&#26631;&#36319;&#36394;&#20165;&#23558;&#31532;&#19968;&#20010;&#24103;&#20013;&#30340;&#30446;&#26631;&#21306;&#22495;&#20316;&#20026;&#27169;&#26495;&#65292;&#26080;&#27861;&#36866;&#24212;&#24555;&#36895;&#21464;&#21270;&#21644;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#22806;&#35266;&#21464;&#21270;&#65292;&#23548;&#33268;&#36319;&#36394;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#36827;&#24335;&#19978;&#19979;&#25991;&#32534;&#30721;&#21464;&#25442;&#26426;&#21046;&#30340;&#36319;&#36394;&#26041;&#27861; ProContEXT&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#36880;&#27493;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#22810;&#23610;&#24230;&#27169;&#26495;&#36827;&#34892;&#20934;&#30830;&#36319;&#36394;&#12290;&#23427;&#25506;&#32034;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#20026;&#22522;&#20110; transformer &#30340;&#36319;&#36394;&#22120;&#30340;&#22810;&#19978;&#19979;&#25991;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#21478;&#22806;&#65292;ProContEXT &#20462;&#35746;&#20102;&#26631;&#35760;&#20462;&#21098;&#25216;&#26415;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Visual Object Tracking (VOT) only takes the target area in the first frame as a template. This causes tracking to inevitably fail in fast-changing and crowded scenes, as it cannot account for changes in object appearance between frames. To this end, we revamped the tracking framework with Progressive Context Encoding Transformer Tracker (ProContEXT), which coherently exploits spatial and temporal contexts to predict object motion trajectories. Specifically, ProContEXT leverages a context-aware self-attention module to encode the spatial and temporal context, refining and updating the multi-scale static and dynamic templates to progressively perform accurate tracking. It explores the complementary between spatial and temporal context, raising a new pathway to multi-context modeling for transformer-based trackers. In addition, ProContEXT revised the token pruning technique to reduce computational complexity. Extensive experiments on popular benchmark datasets such as GOT-10k and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411; TFAD&#12290;&#22312;&#35774;&#35745;&#30340;&#26102;&#39057;&#26550;&#26500;&#20013;&#65292;&#21516;&#26102;&#21152;&#20837;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#20197;&#25552;&#21319;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#24615;&#33021;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2210.09693</link><description>&lt;p&gt;
TFAD: &#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TFAD: A Decomposition Time Series Anomaly Detection Architecture with Time-Frequency Analysis. (arXiv:2210.09693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411; TFAD&#12290;&#22312;&#35774;&#35745;&#30340;&#26102;&#39057;&#26550;&#26500;&#20013;&#65292;&#21516;&#26102;&#21152;&#20837;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#20197;&#25552;&#21319;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#26377;&#38480;&#30340;&#26631;&#31614;&#25968;&#25454;&#12290;&#34429;&#28982;&#19968;&#20123;&#31639;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#28145;&#24230;&#27169;&#22411;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#22823;&#22810;&#25968;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26102;&#38388;&#22495;&#24314;&#27169;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#39057;&#22495;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411; TFAD&#65292;&#20197;&#21033;&#29992;&#26102;&#38388;&#21644;&#39057;&#22495;&#36827;&#34892;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#35774;&#35745;&#30340;&#26102;&#39057;&#26550;&#26500;&#20013;&#21152;&#20837;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/DAMO-DI-ML/CIKM22-TFAD &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a challenging problem due to the complex temporal dependencies and the limited label data. Although some algorithms including both traditional and deep models have been proposed, most of them mainly focus on time-domain modeling, and do not fully utilize the information in the frequency domain of the time series data. In this paper, we propose a Time-Frequency analysis based time series Anomaly Detection model, or TFAD for short, to exploit both time and frequency domains for performance improvement. Besides, we incorporate time series decomposition and data augmentation mechanisms in the designed time-frequency architecture to further boost the abilities of performance and interpretability. Empirical studies on widely used benchmark datasets show that our approach obtains state-of-the-art performance in univariate and multivariate time series anomaly detection tasks. Code is provided at https://github.com/DAMO-DI-ML/CIKM22-TFAD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#32422;&#26463;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#65292;&#21487;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2209.14627</link><description>&lt;p&gt;
&#12298;&#19968;&#31181;&#38024;&#23545;&#22810;&#26679;&#23545;&#35805;&#29983;&#25104;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#32422;&#26463;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#65292;&#21487;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19982;&#20154;&#31867;&#20114;&#21160;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#36229;&#22823;&#22411;&#23545;&#35805;&#31995;&#32479;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20351;&#29992;&#20013;&#23567;&#22411;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#21152;&#36731;&#20415;&#26131;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#23545;&#35805;&#21709;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#22823;&#23567;&#30828;EM&#65288;EqHard-EM&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#30828;&#26041;&#24335;&#23558;&#26679;&#26412;&#20998;&#37197;&#32473;&#35299;&#30721;&#22120;&#65292;&#24182;&#39069;&#22806;&#26045;&#21152;&#24179;&#34913;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#25152;&#26377;&#35299;&#30721;&#22120;&#37117;&#32463;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;EqHard-EM&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain dialogue systems aim to interact with humans through natural language texts in an open-ended fashion. Despite the recent success of super large dialogue systems such as ChatGPT, using medium-to-small-sized dialogue systems remains the common practice as they are more lightweight and accessible; however, generating diverse dialogue responses is challenging, especially with smaller models. In this work, we propose an Equal-size Hard Expectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model for diverse dialogue generation. Our algorithm assigns a sample to a decoder in a hard manner and additionally imposes an equal-assignment constraint to ensure that all decoders are well-trained. We provide detailed theoretical analysis to justify our approach. Further, experiments on two large-scale open-domain dialogue datasets verify that our EqHard-EM algorithm generates high-quality diverse responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;ViT&#39592;&#24178;&#32593;&#32476;U-ViT&#65292;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22522;&#20110;CNN&#30340;U-Net&#27169;&#22411;&#65292;U-ViT&#20855;&#26377;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;FID&#20998;&#25968;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2209.12152</link><description>&lt;p&gt;
&#20840;&#37096;&#20540;&#24471;&#19968;&#35797;&#65306;&#36866;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;ViT&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
All are Worth Words: A ViT Backbone for Diffusion Models. (arXiv:2209.12152v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;ViT&#39592;&#24178;&#32593;&#32476;U-ViT&#65292;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22522;&#20110;CNN&#30340;U-Net&#27169;&#22411;&#65292;U-ViT&#20855;&#26377;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;FID&#20998;&#25968;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;U-Net&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#36890;&#29992;&#30340;ViT&#39592;&#24178;&#26550;&#26500;&#65288;&#31216;&#20026;U-ViT&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#12290;U-ViT&#30340;&#29305;&#28857;&#26159;&#23558;&#25152;&#26377;&#36755;&#20837;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#26465;&#20214;&#21644;&#22122;&#22768;&#22270;&#20687;&#22359;&#37117;&#35270;&#20026;&#20196;&#29260;&#65292;&#24182;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;&#20043;&#38388;&#20351;&#29992;&#38271;&#36339;&#36291;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#26080;&#26465;&#20214;&#21644;&#31867;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#20197;&#21450;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;U-ViT&#65292;&#22312;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#20110;&#8220;U-Net&#8221;&#30340;CNN&#27169;&#22411;&#20013;&#65292;U-ViT&#20855;&#26377;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;U-ViT&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;ImageNet 256x256&#31867;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;2.29&#30340;&#26368;&#20339;FID&#20998;&#25968;&#65292;&#22312;MS-COCO&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;5.48&#30340;&#26368;&#20339;FID&#20998;&#25968;&#65292;&#30456;&#27604;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#26399;&#38388;&#35775;&#38382;&#22823;&#22411;&#22806;&#37096;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#22411;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20687;U-ViT&#36825;&#26679;&#30340;ViT&#39592;&#24178;&#26550;&#26500;&#21487;&#20197;&#23454;&#29616;&#19982;&#20256;&#32479;&#22522;&#20110;CNN&#30340;U-Net&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21019;&#36896;&#26032;&#30340;FID&#20998;&#25968;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#20132;&#36890;&#30417;&#27979;&#31995;&#32479;&#21644;CCTV&#30456;&#26426;&#30340;&#33258;&#21160;&#21270;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#26816;&#27979;&#34892;&#20154;&#25110;&#36710;&#36742;&#30340;&#23384;&#22312;&#24182;&#35843;&#33410;LED&#36335;&#28783;&#30340;&#20142;&#24230;&#65292;&#20943;&#23569;&#20102;&#33021;&#28304;&#28010;&#36153;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2209.08633</link><description>&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;CCTV&#30456;&#26426;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;CNN&#26234;&#33021;&#36335;&#28783;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation. (arXiv:2209.08633v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#20132;&#36890;&#30417;&#27979;&#31995;&#32479;&#21644;CCTV&#30456;&#26426;&#30340;&#33258;&#21160;&#21270;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#26816;&#27979;&#34892;&#20154;&#25110;&#36710;&#36742;&#30340;&#23384;&#22312;&#24182;&#35843;&#33410;LED&#36335;&#28783;&#30340;&#20142;&#24230;&#65292;&#20943;&#23569;&#20102;&#33021;&#28304;&#28010;&#36153;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34903;&#28783;&#26159;&#26368;&#23481;&#26131;&#34987;&#24573;&#35270;&#30340;&#33021;&#28304;&#28010;&#36153;&#28304;&#20043;&#19968;&#65292;&#20854;&#22312;&#19981;&#38656;&#35201;&#29031;&#26126;&#30340;&#21306;&#22495;&#21457;&#20986;&#36807;&#22810;&#30340;&#20809;&#32447;&#65292;&#36896;&#25104;&#24040;&#22823;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#25805;&#20316;&#26041;&#24335;&#65292;&#34903;&#28783;&#24120;&#24120;&#20250;&#22312;&#30333;&#22825;&#34987;&#25171;&#24320;&#65292;&#22312;&#26202;&#19978;&#34987;&#20851;&#38381;&#65292;&#36825;&#22312;21&#19990;&#32426;&#20173;&#28982;&#24456;&#36951;&#25022;&#12290;&#22240;&#27492;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30001;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#39537;&#21160;&#30340;&#26234;&#33021;&#20132;&#36890;&#30417;&#27979;&#31995;&#32479;&#19982;&#38381;&#36335;&#30005;&#35270;(CCTV)&#30456;&#26426;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;CCTV&#35270;&#39057;&#27969;&#20013;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#26816;&#27979;&#34892;&#20154;&#25110;&#36710;&#36742;&#30340;&#23384;&#22312;&#24182;&#22312;&#20854;&#32570;&#24109;&#26102;&#35843;&#33410;&#34903;&#28783;&#30340;&#20142;&#24230;&#65292;&#20174;&#32780;&#24320;&#21457;&#19968;&#31181;&#26032;&#22411;&#30340;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21306;&#20998;&#30333;&#22825;&#21644;&#40657;&#22812;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#33258;&#21160;&#35843;&#33410;LED&#36335;&#28783;&#30340;&#20142;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most neglected sources of energy loss is streetlights which generate too much light in areas where it is not required. Energy waste has enormous economic and environmental effects. In addition, due to the conventional manual nature of the operation, streetlights are frequently seen being turned ON during the day and OFF in the evening, which is regrettable even in the twenty-first century. These issues require automated streetlight control in order to be resolved. This study aims to develop a novel streetlight controlling method by combining a smart transport monitoring system powered by computer vision technology with a closed circuit television (CCTV) camera that allows the light-emitting diode (LED) streetlight to automatically light up with the appropriate brightness by detecting the presence of pedestrians or vehicles and dimming the streetlight in their absence using semantic image segmentation from the CCTV video streaming. Consequently, our model distinguishes daylig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#36164;&#28304;&#20998;&#37197;&#21644;QoE&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#21512;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#22312;&#25968;&#23398;&#19978;&#27169;&#25311;QoE&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;Meta-Immersion&#26377;&#21161;&#20110;&#22312;&#28385;&#36275;&#23458;&#25143;&#31471;&#29289;&#29702;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2208.05438</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#36164;&#28304;&#20998;&#37197;&#21644;QoE&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Attention-aware Resource Allocation and QoE Analysis for Metaverse xURLLC Services. (arXiv:2208.05438v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#36164;&#28304;&#20998;&#37197;&#21644;QoE&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#21512;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#22312;&#25968;&#23398;&#19978;&#27169;&#25311;QoE&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;Meta-Immersion&#26377;&#21161;&#20110;&#22312;&#28385;&#36275;&#23458;&#25143;&#31471;&#29289;&#29702;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#20195;&#34920;&#20102;&#25105;&#20204;&#23545;&#19979;&#19968;&#20195;&#20114;&#32852;&#32593;&#30340;&#26399;&#26395;&#65292;&#24182;&#24102;&#26469;&#20102;&#26032;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;KPI&#65289; &#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#36229;&#21487;&#38752;&#24615;&#21644;&#20302;&#26102;&#24310;&#36890;&#20449;&#65288;URLLC&#65289;&#21487;&#20197;&#28385;&#36275;&#23458;&#35266;&#30340;KPI&#65292;&#20294;&#24456;&#38590;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#65292;&#36825;&#26159;&#20803;&#23431;&#23449;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;&#30001;&#20110;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#21487;&#20197;&#34987;&#35270;&#20026;&#32508;&#21512;&#30340;KPI&#65292;&#22240;&#27492;URLLC&#34987;&#28436;&#21464;&#20026;&#20855;&#26377;&#20010;&#24615;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#30340;&#19979;&#19968;&#20195;URLLC&#65288;xURLLC&#65289;&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;QoE&#12290;&#20026;&#20102;&#37096;&#32626;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20803;&#23431;&#23449;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;MSP&#65289;&#21644;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21830;&#65288;InP&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#21512;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#26368;&#22823;&#21270;MSP&#30340;&#25928;&#29992;&#65292;&#35813;&#25928;&#29992;&#34987;&#23450;&#20041;&#20026;&#20803;&#23431;&#23449;&#29992;&#25143;QoE&#30340;&#20989;&#25968;&#65292;&#21516;&#26102;&#30830;&#20445;InP&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#22312;&#25968;&#23398;&#19978;&#27169;&#25311;QoE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Meta-Immersion&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaverse encapsulates our expectations of the next-generation Internet, while bringing new key performance indicators (KPIs). Although conventional ultra-reliable and low-latency communications (URLLC) can satisfy objective KPIs, it is difficult to provide a personalized immersive experience that is a distinctive feature of the Metaverse. Since the quality of experience (QoE) can be regarded as a comprehensive KPI, the URLLC is evolved towards the next generation URLLC (xURLLC) with a personalized resource allocation scheme to achieve higher QoE. To deploy Metaverse xURLLC services, we study the interaction between the Metaverse service provider (MSP) and the network infrastructure provider (InP), and provide an optimal contract design framework. Specifically, the utility of the MSP, defined as a function of Metaverse users' QoE, is to be maximized, while ensuring the incentives of the InP. To model the QoE mathematically, we propose a novel metric named Meta-Immersion that incorporat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#34920;&#31034;&#20026;&#36229;&#22270;&#65292;&#26500;&#24314;&#20102;&#22522;&#20110;AI&#30340;&#26377;&#26426;&#21270;&#23398;&#36229;&#22270;&#32593;&#32476;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#30740;&#31350;&#65292;&#20026;&#21453;&#24212;&#20998;&#31867;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2208.01647</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#26377;&#26426;&#21270;&#23398;&#36229;&#22270;&#32593;&#32476;&#65306;&#32593;&#32476;&#32479;&#35745;&#21644;&#21453;&#24212;&#20998;&#31867;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI-driven Hypergraph Network of Organic Chemistry: Network Statistics and Applications in Reaction Classification. (arXiv:2208.01647v2 [q-bio.MN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#34920;&#31034;&#20026;&#36229;&#22270;&#65292;&#26500;&#24314;&#20102;&#22522;&#20110;AI&#30340;&#26377;&#26426;&#21270;&#23398;&#36229;&#22270;&#32593;&#32476;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#30740;&#31350;&#65292;&#20026;&#21453;&#24212;&#20998;&#31867;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39640;&#36890;&#37327;&#31579;&#36873;&#30340;&#36827;&#23637;&#12289;&#26356;&#22797;&#26434;&#21270;&#23398;&#35774;&#35745;&#31354;&#38388;&#30340;&#21487;&#35775;&#38382;&#24615;&#20197;&#21450;&#31934;&#30830;&#30340;&#20998;&#23376;&#24314;&#27169;&#26694;&#26550;&#30340;&#21457;&#23637;&#65292;&#20419;&#36827;&#20102;&#26032;&#21453;&#24212;&#21644;&#20998;&#23376;&#30340;&#24555;&#36895;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#20851;&#27880;&#26368;&#36817;&#36235;&#21183;&#24182;&#23558;&#20854;&#22806;&#25512;&#21040;&#21487;&#33021;&#30340;&#26410;&#26469;&#36712;&#36857;&#30340;&#32508;&#21512;&#24615;&#30740;&#31350;&#26469;&#29702;&#35299;&#19981;&#26029;&#22686;&#38271;&#30340;&#21270;&#23398;&#25991;&#29486;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25253;&#21578;&#20102;&#20960;&#20010;&#22522;&#20110;&#32593;&#32476;&#29702;&#35770;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#20351;&#29992;&#21270;&#23398;&#21453;&#24212;&#30340;&#26377;&#21521;&#22270;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#21270;&#23398;&#21453;&#24212;&#65292;&#20854;&#20013;&#36229;&#36793;&#34920;&#31034;&#21270;&#23398;&#21453;&#24212;&#65292;&#33410;&#28857;&#34920;&#31034;&#21442;&#19982;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#24212;&#25968;&#25454;&#38598;&#26500;&#36896;&#20102;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#24182;&#25253;&#21578;&#20854;&#32479;&#35745;&#20449;&#24687;&#65292;&#20363;&#22914;&#24230;&#20998;&#24067;&#12289;&#24179;&#22343;&#36335;&#24452;&#38271;&#24230;&#12289;&#21516;&#37197;&#24615;&#25110;&#24230;&#30456;&#20851;&#24615;&#12289;PageRank &#20013;&#24515;&#24615;&#21644;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#65288;&#25110;&#31038;&#21306;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid discovery of new reactions and molecules in recent years has been facilitated by the advancements in high throughput screening, accessibility to a much more complex chemical design space, and the development of accurate molecular modeling frameworks. A holistic study of the growing chemistry literature is, therefore, required that focuses on understanding the recent trends and extrapolating them into possible future trajectories. To this end, several network theory-based studies have been reported that use a directed graph representation of chemical reactions. Here, we perform a study based on representing chemical reactions as hypergraphs where the hyperedges represent chemical reactions and nodes represent the participating molecules. We use a standard reactions dataset to construct a hypernetwork and report its statistics such as degree distributions, average path length, assortativity or degree correlations, PageRank centrality, and graph-based clusters (or communities). We a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#30456;&#23545;&#27604;&#23398;&#20064;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#20114;&#30456;&#23398;&#20064;&#21040;&#39069;&#22806;&#30340;&#23545;&#27604;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.11518</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#36890;&#36807;&#20114;&#30456;&#23545;&#27604;&#23398;&#20064;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#30456;&#23545;&#27604;&#23398;&#20064;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#20114;&#30456;&#23398;&#20064;&#21040;&#39069;&#22806;&#30340;&#23545;&#27604;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#25945;&#24072;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#26088;&#22312;&#36890;&#36807;&#21512;&#20316;&#35757;&#32451;&#19968;&#32452;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#24182;&#30456;&#20114;&#33976;&#39311;&#30693;&#35782;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#31867;&#27010;&#29575;&#20316;&#20026;&#26680;&#24515;&#30693;&#35782;&#31867;&#22411;&#65292;&#24573;&#30053;&#20102;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#34920;&#31034;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#30456;&#23545;&#27604;&#23398;&#20064;&#30340;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#32452;&#32593;&#32476;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#20132;&#20114;&#21644;&#23545;&#27604;&#20998;&#24067;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27719;&#24635;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#26368;&#22823;&#21270;&#20004;&#20010;&#32593;&#32476;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#19979;&#38480;&#12290;&#36825;&#20351;&#24471;&#27599;&#20010;&#32593;&#32476;&#37117;&#21487;&#20197;&#20174;&#20854;&#20182;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#39069;&#22806;&#30340;&#23545;&#27604;&#30693;&#35782;&#65292;&#20174;&#32780;&#25913;&#21892;&#29305;&#24449;&#34920;&#31034;&#65292;&#25552;&#39640;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#26368;&#21518;&#19968;&#23618;&#65292;&#25105;&#20204;&#36824;&#23558;MCL&#25193;&#23637;&#21040;&#20013;&#38388;&#23618;&#65292;&#24182;&#36827;&#34892;&#33258;&#36866;&#24212;&#23618;&#21305;&#37197;&#26426;&#21046;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The teacher-free online Knowledge Distillation (KD) aims to train an ensemble of multiple student models collaboratively and distill knowledge from each other. Although existing online KD methods achieve desirable performance, they often focus on class probabilities as the core knowledge type, ignoring the valuable feature representational information. We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks in an online manner. Our MCL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations, thus improving the performance of visual recognition tasks. Beyond the final layer, we extend MCL to intermediate layers and perform an adaptive layer-matching mechanism train
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#22987;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.15387</link><description>&lt;p&gt;
&#20174;&#21738;&#37324;&#24320;&#22987;&#65311;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#21644;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning. (arXiv:2206.15387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#22987;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#25361;&#25112;&#26159;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#25351;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#21487;&#33021;&#36981;&#24490;&#38750;&#24120;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#31995;&#32479;&#24322;&#26500;&#24615;&#26159;&#25351;&#23458;&#25143;&#31471;&#35774;&#22791;&#20855;&#26377;&#19981;&#21516;&#30340;&#31995;&#32479;&#33021;&#21147;&#12290;&#35768;&#22810;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#37117;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#36890;&#24120;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#24320;&#22987;&#32852;&#37030;&#35757;&#32451;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#35757;&#32451;&#20219;&#21153;&#30340;&#20195;&#29702;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#36825;&#20123;&#25968;&#25454;&#22312;&#24320;&#22987;&#32852;&#37030;&#35757;&#32451;&#20043;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20351;&#29992;&#22235;&#20010;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#22987;&#30340;&#24433;&#21709;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#22987;&#21487;&#20197;&#20943;&#23569;&#36798;&#21040;&#30446;&#26631;&#35823;&#24046;&#29575;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20351;&#27169;&#22411;&#35757;&#32451;&#26356;&#20934;&#30830;&#65288;&#39640;&#36798;40&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An oft-cited challenge of federated learning is the presence of heterogeneity. \emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \emph{System heterogeneity} refers to client devices having different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. Using four standard federated learning benchmark datasets, we empirically study the impact of starting from a pre-trained model in federated learning. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\%) than is
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13508</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#20221;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#20986;&#33394;&#24615;&#33021;&#30340;&#26041;&#24335;&#24182;&#19981;&#38656;&#35201;&#22826;&#38271;&#26102;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#20016;&#23500;&#65292;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#21644;&#38656;&#35201;&#20445;&#35777;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#25968;&#25454;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#25110;&#32622;&#25442;&#36824;&#26159;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#29992;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#30456;&#20851;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#19981;&#21516;&#21464;&#20307;&#30340;&#25928;&#29575;&#23558;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#20013;&#24515;&#37096;&#20998;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#23558;&#35780;&#20272;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#27599;&#20010;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30830;&#23450;&#20102;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#21487;&#33021;&#26159;&#32447;&#24615;&#30340;&#12289;&#39281;&#21644;&#32447;&#24615;&#20989;&#25968;&#25110;&#22522;&#20110;Hermite&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#20250;&#24433;&#21709;RFR&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#22914;&#21452;&#23792;&#26354;&#32447;&#21644;&#26368;&#20339;&#27491;&#21017;&#21270;&#21442;&#25968;&#19982;&#22122;&#22768;&#27700;&#24179;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01332</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Optimal Activation Functions for the Random Features Regression Model. (arXiv:2206.01332v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#20102;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#21487;&#33021;&#26159;&#32447;&#24615;&#30340;&#12289;&#39281;&#21644;&#32447;&#24615;&#20989;&#25968;&#25110;&#22522;&#20110;Hermite&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#20250;&#24433;&#21709;RFR&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#22914;&#21452;&#23792;&#26354;&#32447;&#21644;&#26368;&#20339;&#27491;&#21017;&#21270;&#21442;&#25968;&#19982;&#22122;&#22768;&#27700;&#24179;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#24050;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;(RFR)&#30340;&#28176;&#36817;&#22343;&#26041;&#27979;&#35797;&#35823;&#24046;&#21644;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#20989;&#25968;&#31616;&#27905;&#27010;&#24565;&#65292;&#30830;&#23450;&#20102;&#22312;&#38381;&#21512;&#24418;&#24335;&#19979;&#26497;&#23567;&#21270;RFR&#27979;&#35797;&#35823;&#24046;&#21644;&#28789;&#25935;&#24230;&#32452;&#21512;&#30340;&#28608;&#27963;&#20989;&#25968;(AF)&#26063;&#32676;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#26368;&#20339;AF&#21487;&#20197;&#26159;&#32447;&#24615;&#30340;&#12289;&#39281;&#21644;&#32447;&#24615;&#20989;&#25968;&#25110;&#22522;&#20110;Hermite&#22810;&#39033;&#24335;&#34920;&#31034;&#30340;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26368;&#20339;AF&#22914;&#20309;&#24433;&#21709;RFR&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#27604;&#22914;&#21452;&#23792;&#26354;&#32447;&#21644;&#20854;&#26368;&#20339;&#27491;&#21017;&#21270;&#21442;&#25968;&#19982;&#35266;&#23519;&#22122;&#22768;&#27700;&#24179;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The asymptotic mean squared test error and sensitivity of the Random Features Regression model (RFR) have been recently studied. We build on this work and identify in closed-form the family of Activation Functions (AFs) that minimize a combination of the test error and sensitivity of the RFR under different notions of functional parsimony. We find scenarios under which the optimal AFs are linear, saturated linear functions, or expressible in terms of Hermite polynomials. Finally, we show how using optimal AFs impacts well-established properties of the RFR model, such as its double descent curve, and the dependency of its optimal regularization parameter on the observation noise level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35838;&#31243;&#39034;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23545;&#20154;&#31867;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.10250</link><description>&lt;p&gt;
&#39034;&#24207;&#20154;&#31867;&#25945;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explanatory machine learning for sequential human teaching. (arXiv:2205.10250v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35838;&#31243;&#39034;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23545;&#20154;&#31867;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#25216;&#26415;&#22522;&#20110;&#32553;&#30053;&#25512;&#29702;&#21644;&#24402;&#32435;&#25512;&#29702;&#65292;&#20174;&#23569;&#37327;&#30340;&#25968;&#25454;&#20013;&#24471;&#20986;&#36923;&#36753;&#29702;&#35770;&#12290;&#23398;&#24471;&#30340;&#29702;&#35770;&#20197;&#35268;&#21017;&#24418;&#24335;&#34920;&#31034;&#65292;&#26159;&#25152;&#33719;&#30693;&#35782;&#30340;&#22768;&#26126;&#24615;&#25551;&#36848;&#12290;&#22312;&#26089;&#26399;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#39318;&#27425;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36923;&#36753;&#35268;&#21017;&#30340;&#31616;&#21333;&#20998;&#31867;&#20219;&#21153;&#23545;&#20154;&#31867;&#29702;&#35299;&#33021;&#21147;&#26377;&#21487;&#34913;&#37327;&#30340;&#25552;&#39640;&#30340;&#35777;&#25454;&#12290;&#22312;&#21518;&#32493;&#30340;&#30740;&#31350;&#20013;&#65292;&#21457;&#29616;&#21521;&#20154;&#31867;&#23637;&#31034;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322; &#22312;&#28216;&#25103;&#23398;&#20064;&#29615;&#22659;&#20013;&#20135;&#29983;&#20102;&#26377;&#30410;&#21644;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#27010;&#24565;&#21576;&#29616;&#30340;&#39034;&#24207;&#23545;&#20154;&#31867;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;&#26469;&#32487;&#32493;&#25506;&#35752;&#21487;&#29702;&#35299;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35838;&#31243;&#39034;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23545;&#20110;&#39034;&#24207;&#38382;&#39064;&#35299;&#20915;&#30340;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topic of comprehensibility of machine-learned theories has recently drawn increasing attention. Inductive Logic Programming (ILP) uses logic programming to derive logic theories from small data based on abduction and induction techniques. Learned theories are represented in the form of rules as declarative descriptions of obtained knowledge. In earlier work, the authors provided the first evidence of a measurable increase in human comprehension based on machine-learned logic rules for simple classification tasks. In a later study, it was found that the presentation of machine-learned explanations to humans can produce both beneficial and harmful effects in the context of game learning. We continue our investigation of comprehensibility by examining the effects of the ordering of concept presentations on human comprehension. In this work, we examine the explanatory effects of curriculum order and the presence of machine-learned explanations for sequential problem-solving. We show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#29983;&#25104;&#30340;&#34892;&#20154;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#20013;&#36827;&#34892;&#20102;&#32467;&#21512;&#12290;&#36890;&#36807;&#25552;&#21462;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#24182;&#36827;&#34892;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#21644;&#21463;&#38480;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20248;&#21270;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.12900</link><description>&lt;p&gt;
&#8220;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#36741;&#21161;&#30456;&#26426;&#32593;&#32476;&#20013;&#30340;&#20154;&#21592;&#26816;&#32034;&#8221;
&lt;/p&gt;
&lt;p&gt;
Cross-Camera Trajectories Help Person Retrieval in a Camera Network. (arXiv:2204.12900v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#29983;&#25104;&#30340;&#34892;&#20154;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#20013;&#36827;&#34892;&#20102;&#32467;&#21512;&#12290;&#36890;&#36807;&#25552;&#21462;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#24182;&#36827;&#34892;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#21644;&#21463;&#38480;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20248;&#21270;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#20174;&#19968;&#20010;&#38750;&#35206;&#30422;&#30340;&#25668;&#20687;&#22836;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#35270;&#39057;&#20013;&#26816;&#32034;&#20986;&#19968;&#20010;&#26597;&#35810;&#23545;&#35937;&#30340;&#36523;&#20221;&#12290;&#30446;&#21069;&#24050;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#35270;&#35273;&#21305;&#37197;&#25110;&#32773;&#32771;&#34385;&#26102;&#38388;&#32422;&#26463;&#65292;&#20294;&#26159;&#24573;&#30053;&#20102;&#25668;&#20687;&#22836;&#32593;&#26684;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#29983;&#25104;&#30340;&#34892;&#20154;&#26816;&#32034;&#26694;&#26550;&#65292;&#23558;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#33719;&#21462;&#34892;&#20154;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#25668;&#20687;&#22836;&#26102;&#31354;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#34892;&#20154;&#30340;&#34892;&#36208;&#20064;&#24815;&#21644;&#30456;&#37051;&#25668;&#20687;&#22836;&#20043;&#38388;&#30340;&#36335;&#24452;&#24067;&#23616;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#26102;&#31354;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31232;&#30095;&#37319;&#26679;&#30340;&#34892;&#20154;&#25968;&#25454;&#22312;&#25668;&#20687;&#22836;&#32593;&#32476;&#20013;&#23454;&#29616;&#12290;&#22522;&#20110;&#36825;&#20010;&#26102;&#31354;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#25552;&#21462;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#21463;&#38480;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36712;&#36857;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are concerned with retrieving a query person from multiple videos captured by a non-overlapping camera network. Existing methods often rely on purely visual matching or consider temporal constraints but ignore the spatial information of the camera network. To address this issue, we propose a pedestrian retrieval framework based on cross-camera trajectory generation, which integrates both temporal and spatial information. To obtain pedestrian trajectories, we propose a novel cross-camera spatio-temporal model that integrates pedestrians' walking habits and the path layout between cameras to form a joint probability distribution. Such a spatio-temporal model among a camera network can be specified using sparsely sampled pedestrian data. Based on the spatio-temporal model, cross-camera trajectories can be extracted by the conditional random field model and further optimized by restricted non-negative matrix factorization. Finally, a trajectory re-ranking technique is proposed to improv
&lt;/p&gt;</description></item><item><title>CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.10965</link><description>&lt;p&gt;
CLIP-Dissect&#65306;&#28145;&#24230;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#34920;&#31034;&#30340;&#33258;&#21160;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10965
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;CLIP-Dissect&#65292;&#21487;&#20197;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#21333;&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;CLIP-Dissect&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CLIP-Dissect&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#25324;&#20855;&#22791;&#8220;&#22320;&#38754;&#30495;&#30456;&#8221;&#65288;ground-truth&#65289;&#30340;&#26368;&#21518;&#19968;&#23618;&#31070;&#32463;&#20803;&#20197;&#21450;&#20855;&#22791;&#23450;&#24615;&#22909;&#30340;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65306;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#25193;&#23637;&#20197;&#21033;&#29992;&#26410;&#26469;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;CLIP-Dissect&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;4&#20998;&#38047;&#20869;&#26631;&#35760;ResNet-50&#30340;&#20116;&#23618;&#25152;&#26377;&#31070;&#32463;&#20803;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/Trustworthy-ML-Lab/CLIP-dissect &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10779</link><description>&lt;p&gt;
CgAT&#65306;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#21319;Hashing&#26816;&#32034;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval. (arXiv:2204.10779v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;Hashing&#22312;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;min-max&#30340;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65288;CgAT&#65289;&#65292;&#36890;&#36807;&#26368;&#22351;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;Hashing&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#28145;&#24230;Hashing&#26816;&#32034;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iterati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2204.08504</link><description>&lt;p&gt;
CGC: &#23545;&#27604;&#22270;&#32858;&#31867;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22270;&#32858;&#31867;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25506;&#35752;&#22312;&#32593;&#32476;&#25968;&#25454;&#20013;&#21457;&#29616;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20197;&#21450;&#23545;&#23427;&#20204;&#36827;&#34892;&#31038;&#21306;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;CGC&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#20102;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#12290;&#22312;&#21508;&#20010;&#30495;&#23454;&#22330;&#26223;&#21644;&#21512;&#25104;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#23545;CGC&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21160;&#24577;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given entities and their interactions in the web data, which may have occurred at different time, how can we find communities of entities and track their evolution? In this paper, we approach this important task from graph clustering perspective. Recently, state-of-the-art clustering performance in various domains has been achieved by deep clustering methods. Especially, deep graph clustering (DGC) methods have successfully extended deep clustering to graph-structured data by learning node representations and cluster assignments in a joint optimization framework. Despite some differences in modeling choices (e.g., encoder architectures), existing DGC methods are mainly based on autoencoders and use the same clustering objective with relatively minor adaptations. Also, while many real-world graphs are dynamic, previous DGC methods considered only static graphs. In this work, we develop CGC, a novel end-to-end framework for graph clustering, which fundamentally differs from existing meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#23567;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#32467;&#21512;&#20102;&#35270;&#35273;&#26679;&#26412;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.07841</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#23567;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Few-Shot Object Detection with Meta-Learning-Based Cross-Modal Prompting. (arXiv:2204.07841v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#23567;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#32467;&#21512;&#20102;&#35270;&#35273;&#26679;&#26412;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23567;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#20102;&#35270;&#35273;&#26679;&#26412;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#30340;&#20351;&#29992;&#26159;&#20114;&#34917;&#30340;&#12290;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#23567;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#22823;&#22810;&#26159;&#22522;&#20110;&#24494;&#35843;&#30340;&#65292;&#36825;&#23545;&#20110;&#22312;&#32447;&#24212;&#29992;&#26469;&#35828;&#25928;&#29575;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#22914;&#31867;&#21517;&#31561;&#65292;&#20197;&#25552;&#21462;&#31867;&#35821;&#20041;&#23884;&#20837;&#65292;&#23545;&#20110;&#32597;&#35265;&#31867;&#26469;&#35828;&#27604;&#36739;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#24230;&#37327;&#23398;&#20064;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#39640;&#23618;&#27010;&#24565;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#20998;&#21035;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#23567;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20803;&#23398;&#20064;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20998;&#21035;&#23398;&#24471;&#30340;&#23567;&#26679;&#26412;&#35270;&#35273;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#24314;&#31435;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#21644;&#26816;&#27979;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#25552;&#31034;&#26469;&#36741;&#21161;&#26816;&#27979;&#22120;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#24314;&#35758;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-modal few-shot object detection (FSOD) in this paper, using both few-shot visual examples and class semantic information for detection, which are complementary to each other by definition. Most of the previous works on multi-modal FSOD are fine-tuning-based which are inefficient for online applications. Moreover, these methods usually require expertise like class names to extract class semantic embedding, which are hard to get for rare classes. Our approach is motivated by the high-level conceptual similarity of (metric-based) meta-learning and prompt-based learning to learn generalizable few-shot and zero-shot object detection models respectively without fine-tuning. Specifically, we combine the few-shot visual classifier and text classifier learned via meta-learning and prompt-based learning respectively to build the multi-modal classifier and detection models. In addition, to fully exploit the pre-trained language models, we propose meta-learning-based cross-modal pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2203.11242</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;GAN&#32508;&#36848;&#65306;&#26368;&#26032;&#30740;&#31350;&#12289;&#20998;&#26512;&#21644;&#20998;&#31867;&#65288;arXiv&#65306;2203.11242v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#27425;&#38761;&#21629;&#65292;&#20854;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24040;&#22823;&#24433;&#21709;&#12290;GAN&#19981;&#20165;&#22312;&#23450;&#20041;&#20854;&#27169;&#22411;&#26102;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#32780;&#19988;&#29983;&#25104;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65292;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#30452;&#25509;&#24433;&#21709;&#12290;&#30001;&#20110;GAN&#24102;&#26469;&#30340;&#37325;&#22823;&#25913;&#36827;&#21644;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#31038;&#21306;&#19981;&#26029;&#25552;&#20986;&#26032;&#30340;&#30740;&#31350;&#65292;&#20351;&#24471;&#36319;&#19978;&#26102;&#20195;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;GAN&#30340;&#27010;&#36848;&#65292;&#23637;&#31034;&#26368;&#26032;&#30340;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#21464;&#20307;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#23558;&#35780;&#20272;&#19981;&#21516;&#21464;&#20307;&#30340;&#27169;&#22411;&#26550;&#26500;&#25928;&#29575;&#65292;&#23637;&#31034;&#26368;&#20339;&#30340;&#24212;&#29992;&#39046;&#22495;&#65307;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23558;&#20998;&#26512;&#35780;&#20272;GAN&#24615;&#33021;&#30340;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#21644;&#32463;&#24120;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#23558;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;GAN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#30740;&#31350;&#20102;&#30001;&#20855;&#26377;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2203.00246</link><description>&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Framework for Supervised Learning. (arXiv:2203.00246v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#30740;&#31350;&#20102;&#30001;&#20855;&#26377;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#23637;&#31034;&#20986;&#26356;&#21152;&#26032;&#39062;&#21644;&#20248;&#31168;&#30340;&#32463;&#39564;&#32467;&#26524;&#65292;&#20854;&#20013;&#37319;&#29992;&#26356;&#28145;&#21644;&#26356;&#24191;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#36229;&#36807;&#20004;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#65292;&#38500;&#38750;&#35785;&#35832;&#20110;&#35745;&#25968;&#21442;&#25968;&#25110;&#36973;&#36935;&#28145;&#24230;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#21516;&#30340;&#35282;&#24230;&#19979;&#20998;&#26512;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#26159;&#26377;&#25104;&#26524;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#24049;&#30340;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#29992;&#20110;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20123;&#32463;&#20856;&#26696;&#20363;&#65292;&#20363;&#22914;&#26631;&#37327;&#20272;&#35745;&#21644;&#32447;&#24615;&#22238;&#24402;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#30452;&#35273;&#21644;&#20171;&#32461;&#19968;&#33324;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35813;&#26694;&#26550;&#30740;&#31350;&#20102;&#30001;&#20855;&#26377;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#26435;&#37325;&#30340;&#29305;&#23450;&#20808;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year, deep learning demonstrates new and improved empirical results with deeper and wider neural networks. Meanwhile, with existing theoretical frameworks, it is difficult to analyze networks deeper than two layers without resorting to counting parameters or encountering sample complexity bounds that are exponential in depth. Perhaps it may be fruitful to try to analyze modern machine learning under a different lens. In this paper, we propose a novel information-theoretic framework with its own notions of regret and sample complexity for analyzing the data requirements of machine learning. With our framework, we first work through some classical examples such as scalar estimation and linear regression to build intuition and introduce general techniques. Then, we use the framework to study the sample complexity of learning from data generated by deep neural networks with ReLU activation units. For a particular prior distribution on weights, we establish sample complexity bounds tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26080;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#30340;&#30456;&#20851;&#30740;&#31350;&#65292;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#28857;&#20113;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#21644;&#26377;&#29992;&#30340;&#28857;&#20113;&#34920;&#31034;&#65292;&#20811;&#26381;&#20102;&#22823;&#35268;&#27169;&#28857;&#20113;&#26631;&#35760;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2202.13589</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey. (arXiv:2202.13589v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26080;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#30340;&#30456;&#20851;&#30740;&#31350;&#65292;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#28857;&#20113;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#21644;&#26377;&#29992;&#30340;&#28857;&#20113;&#34920;&#31034;&#65292;&#20811;&#26381;&#20102;&#22823;&#35268;&#27169;&#28857;&#20113;&#26631;&#35760;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#36870;&#22659;&#19979;&#30340;&#21331;&#36234;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#28857;&#20113;&#25968;&#25454;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#12290;&#21516;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#30417;&#25511;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#28857;&#20113;&#21644;DNN&#30340;&#34701;&#21512;&#23548;&#33268;&#20102;&#35768;&#22810;&#28145;&#23618;&#28857;&#20113;&#27169;&#22411;&#65292;&#20027;&#35201;&#22312;&#22823;&#35268;&#27169;&#21644;&#31264;&#23494;&#26631;&#35760;&#30340;&#28857;&#20113;&#25968;&#25454;&#30340;&#30417;&#30563;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#26631;&#35760;&#30340;&#38480;&#21046;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#20851;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#28857;&#20113;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#21644;&#26377;&#29992;&#30340;&#28857;&#20113;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#36848;&#20102;&#20351;&#29992;DNN&#36827;&#34892;&#26080;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#21160;&#26426;&#12289;&#19968;&#33324;&#27969;&#31243;&#20197;&#21450;&#26415;&#35821;&#12290;&#28982;&#21518;&#25551;&#36848;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;&#21644;DNN&#26550;&#26500;&#30340;&#30456;&#20851;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations. Meanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and autonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the supervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn general and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the constraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation learning using DNNs. It first describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant background including widely adopted point cloud datasets and DNN architectures is then b
&lt;/p&gt;</description></item><item><title>PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2202.04110</link><description>&lt;p&gt;
PGMax: &#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#21644;JAX&#20013;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#30340;&#22240;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04110
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#36731;&#26494;&#25351;&#23450;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#20316;&#20026;&#22240;&#23376;&#22270;&#65292;&#24182;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65288;LBP&#65289;&#12290;PGMax&#25903;&#25345;&#20855;&#26377;&#21487;&#22788;&#29702;&#22240;&#23376;&#30340;&#19968;&#33324;&#22240;&#23376;&#22270;&#65292;&#24182;&#21033;&#29992;&#29616;&#20195;&#21152;&#36895;&#22120;&#65288;&#22914;GPU&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;PGMax&#36824;&#19982;&#24555;&#36895;&#22686;&#38271;&#30340;JAX&#29983;&#24577;&#31995;&#32479;&#26080;&#32541;&#20132;&#20114;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12289;&#31034;&#20363;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/deepmind/PGMax&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#26694;&#26550; AnomMAN&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#12290;&#23427;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23450;&#20041;&#32593;&#32476;&#20013;&#25152;&#26377;&#35270;&#22270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20351;&#29992;&#27531;&#24046;&#23398;&#20064;&#26469;&#25429;&#25417;&#39640;&#39057;&#20449;&#21495;&#12290;&#23454;&#39564;&#35777;&#26126; AnomMAN &#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.02822</link><description>&lt;p&gt;
AnomMAN: &#26816;&#27979;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
AnomMAN: Detect Anomaly on Multi-view Attributed Networks. (arXiv:2201.02822v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02822
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#26694;&#26550; AnomMAN&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#12290;&#23427;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23450;&#20041;&#32593;&#32476;&#20013;&#25152;&#26377;&#35270;&#22270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20351;&#29992;&#27531;&#24046;&#23398;&#20064;&#26469;&#25429;&#25417;&#39640;&#39057;&#20449;&#21495;&#12290;&#23454;&#39564;&#35777;&#26126; AnomMAN &#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#32447;&#36141;&#29289;&#12289;&#37329;&#34701;&#20132;&#26131;&#12289;&#36890;&#20449;&#32593;&#32476;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#32771;&#34385;&#21333;&#19968;&#31181;&#31867;&#30340;&#20132;&#20114;&#26469;&#26816;&#27979;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#65292;&#22240;&#27492;&#26080;&#27861;&#22788;&#29702;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#21508;&#31181;&#19981;&#21516;&#20132;&#20114;&#12290;&#22312;&#32508;&#21512;&#32771;&#34385;&#25152;&#26377;&#19981;&#21516;&#20132;&#20114;&#24182;&#26816;&#27979;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#23454;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnomMAN &#30340;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#26694;&#26550;&#26469;&#26816;&#27979;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#12290;&#20026;&#20102;&#20849;&#21516;&#32771;&#34385;&#22810;&#35270;&#22270;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#23646;&#24615;&#21644;&#25152;&#26377;&#31181;&#31867;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23450;&#20041;&#32593;&#32476;&#20013;&#25152;&#26377;&#35270;&#22270;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#22270;&#21367;&#31215;&#25805;&#20316;&#30340;&#20302;&#36890;&#29305;&#24615;&#28388;&#38500;&#20102;&#22823;&#22810;&#25968;&#39640;&#39057;&#20449;&#21495;&#65288;&#24322;&#24120;&#20449;&#21495;&#65289;&#65292;&#22240;&#27492;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;AnomMAN &#34987;&#35774;&#35745;&#20026;&#32858;&#21512;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#27531;&#24046;&#23398;&#20064;&#26469;&#25429;&#25417;&#39640;&#39057;&#20449;&#21495;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;AnomMAN &#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on attributed networks is widely used in online shopping, financial transactions, communication networks, and so on. However, most existing works trying to detect anomalies on attributed networks only consider a single kind of interaction, so they cannot deal with various kinds of interactions on multi-view attributed networks. It remains a challenging task to jointly consider all different kinds of interactions and detect anomalous instances on multi-view attributed networks. In this paper, we propose a graph convolution-based framework, named AnomMAN, to detect Anomaly on Multi-view Attributed Networks. To jointly consider attributes and all kinds of interactions on multi-view attributed networks, we use the attention mechanism to define the importance of all views in networks. Since the low-pass characteristic of graph convolution operation filters out most high-frequency signals (aonmaly signals), it cannot be directly applied to anomaly detection tasks. AnomMAN i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.09445</link><description>&lt;p&gt;
FLSys&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#30340;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps. (arXiv:2111.09445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces FLSys, a mobile-cloud federated learning (FL) system that can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;FLSys&#20013;&#65292;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;FL&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#34987;&#19981;&#21516;&#30340;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#21644;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;FLSys&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#23454;&#29616;&#22312;Android&#21644;AWS&#20113;&#20013;&#12290;&#25105;&#20204;&#19982;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#27169;&#22411;&#20849;&#21516;&#35774;&#35745;&#20102;FLSys&#12290;&#22312;4&#20010;&#26376;&#30340;&#26102;&#38388;&#37324;&#65292;&#20174;100&#22810;&#21517;&#22823;&#23398;&#29983;&#20013;&#25910;&#38598;&#20102;HAR&#24863;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;HAR-Wild&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#27169;&#22411;&#65292;&#20855;&#26377;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#20197;&#20943;&#36731;p
&lt;/p&gt;
&lt;p&gt;
This article presents the design, implementation, and evaluation of FLSys, a mobile-cloud federated learning (FL) system, which can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. In FLSys, different DL models with different FL aggregation methods can be trained and accessed concurrently by different apps. Furthermore, FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models. FLSys adopts a modular design and is implemented in Android and AWS cloud. We co-designed FLSys with a human activity recognition (HAR) model. HAR sensing data was collected in the wild from 100+ college students during a 4-month period. We implemented HAR-Wild, a CNN model tailored to mobile devices, with a data augmentation mechanism to mitigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#22270;&#20687;&#36741;&#21161;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#22823;&#37327;&#29616;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2109.14196</link><description>&lt;p&gt;
WEDGE&#65306;&#22522;&#20110;&#32593;&#32476;&#22270;&#20687;&#36741;&#21161;&#30340;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#22270;&#20687;&#36741;&#21161;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#22823;&#37327;&#29616;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#38754;&#20020;&#30528;&#35757;&#32451;&#25968;&#25454;&#26080;&#27861;&#35206;&#30422;&#21508;&#31181;&#21487;&#33021;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;WEb-image&#36741;&#21161;&#30340;Domain GEneralization&#65288;WEDGE&#65289;&#26041;&#26696;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#36827;&#34892;&#26222;&#36866;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#29616;&#23454;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#12289;&#32593;&#31449;&#12289;&#20809;&#29031;&#12289;&#30456;&#26426;&#39118;&#26684;&#31561;&#21508;&#31181;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#22320;&#23558;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#30340;&#39118;&#26684;&#34920;&#31034;&#27880;&#20837;&#28304;&#39046;&#22495;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#32463;&#21382;&#20855;&#26377;&#21487;&#38752;&#26631;&#31614;&#30340;&#19981;&#21516;&#39118;&#26684;&#30340;&#22270;&#20687;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#20266;&#26631;&#31614;&#26469;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#27169;&#22411;&#22312;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect a web-crawled dataset which presents large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects the style representation of the web-crawled data into the source domain on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled dataset with predicted pseudo labels for 
&lt;/p&gt;</description></item><item><title>FGLP&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#30340;&#31995;&#32479;&#65292;&#23558;&#26234;&#33021;&#25163;&#26426;&#19978;&#25910;&#38598;&#30340;GPS&#36712;&#36857;&#25277;&#35937;&#20026;2D&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#28857;&#65292;&#21512;&#24182;&#20102;BiLSTM&#21644;CNN&#20197;&#25429;&#33719;&#26102;&#38388;&#21644;&#31354;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;8&#31859;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#19988;&#25968;&#25454;&#21457;&#36865;&#24320;&#38144;&#38477;&#20302;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2106.08946</link><description>&lt;p&gt;
FGLP&#65306;&#31227;&#21160;&#29992;&#25143;&#32852;&#37030;&#32454;&#31890;&#24230;&#20301;&#32622;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FGLP: A Federated Fine-Grained Location Prediction System for Mobile Users. (arXiv:2106.08946v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08946
&lt;/p&gt;
&lt;p&gt;
FGLP&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#30340;&#31995;&#32479;&#65292;&#23558;&#26234;&#33021;&#25163;&#26426;&#19978;&#25910;&#38598;&#30340;GPS&#36712;&#36857;&#25277;&#35937;&#20026;2D&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#28857;&#65292;&#21512;&#24182;&#20102;BiLSTM&#21644;CNN&#20197;&#25429;&#33719;&#26102;&#38388;&#21644;&#31354;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;8&#31859;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#19988;&#25968;&#25454;&#21457;&#36865;&#24320;&#38144;&#38477;&#20302;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#32454;&#31890;&#24230;&#20301;&#32622;&#39044;&#27979;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24212;&#29992;&#31243;&#24207;/&#31995;&#32479;&#24615;&#33021;&#12290; &#24212;&#29992;&#22330;&#26223;&#21253;&#25324;&#26681;&#25454;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;5G&#32593;&#32476;&#36136;&#37327;&#33258;&#36866;&#24212;&#35270;&#39057;&#36136;&#37327;&#65292;&#20197;&#21450;&#22522;&#20110;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#21152;&#36895;&#20869;&#23481;&#28210;&#26579;&#30340;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#12290; &#36825;&#20123;&#29992;&#20363;&#35201;&#27714;&#39044;&#27979;&#35823;&#24046;&#19982;GPS&#35823;&#24046;&#30456;&#21516;&#65292;&#24182;&#19988;&#30446;&#21069;&#22312;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#24037;&#20316;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#31934;&#24230;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25163;&#26426;&#19978;&#25910;&#38598;&#30340;GPS&#36712;&#36857;&#30340;&#31227;&#21160;&#29992;&#25143;&#30340;&#32454;&#31890;&#24230;&#20301;&#32622;&#39044;&#27979;&#31995;&#32479;&#65288;FGLP&#65289;&#12290; FGLP&#26377;&#20004;&#20010;&#32452;&#20214;&#65306;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290; &#35813;&#26694;&#26550;&#22312;&#29992;&#25143;&#30340;&#25163;&#26426;&#19978;&#20197;&#21450;&#22312;&#21327;&#35843;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#29992;&#25143;&#30340;&#26381;&#21153;&#22120;&#19978;&#36816;&#34892;&#23398;&#20064;&#12290; FGLP&#23558;&#29992;&#25143;&#20301;&#32622;&#25968;&#25454;&#34920;&#31034;&#20026;&#25277;&#35937;2D&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#28857;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#36328;&#19981;&#21516;&#30340;&#29289;&#29702;&#31354;&#38388;&#36827;&#34892;&#23398;&#20064;&#12290; &#35813;&#27169;&#22411;&#21512;&#24182;&#20102;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20197;&#25429;&#33719;GPS&#36712;&#36857;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#27169;&#24335;&#12290; &#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;FGLP&#23454;&#29616;&#20102;8&#31859;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#36825;&#19982;GPS&#35823;&#24046;&#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25968;&#25454;&#21457;&#36865;&#24320;&#38144;&#38477;&#20302;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#30456;&#23545;&#20110;&#38598;&#20013;&#24335;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained location prediction on smart phones can be used to improve app/system performance. Application scenarios include video quality adaptation as a function of the 5G network quality at predicted user locations, and augmented reality apps that speed up content rendering based on predicted user locations. Such use cases require prediction error in the same range as the GPS error, and no existing works on location prediction can achieve this level of accuracy. We present a system for fine-grained location prediction (FGLP) of mobile users, based on GPS traces collected on the phones. FGLP has two components: a federated learning framework and a prediction model. The framework runs on the phones of the users and also on a server that coordinates learning from all users in the system. FGLP represents the user location data as relative points in an abstract 2D space, which enables learning across different physical spaces. The model merges Bidirectional Long Short-Term Memory (BiLST
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#65292;&#21517;&#20026;AiR&#65292;&#23427;&#21033;&#29992;Transformer&#26694;&#26550;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#24418;&#22330;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#25110;&#22320;&#38754;&#30495;&#23454;&#24418;&#21464;&#22330;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2105.02282</link><description>&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;AiR&#65289;&#65306;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention for Image Registration (AiR): an unsupervised Transformer approach. (arXiv:2105.02282v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#65292;&#21517;&#20026;AiR&#65292;&#23427;&#21033;&#29992;Transformer&#26694;&#26550;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#24418;&#22330;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#25110;&#22320;&#38754;&#30495;&#23454;&#24418;&#21464;&#22330;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#26159;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#23427;&#32463;&#24120;&#36935;&#21040;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#38750;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#20381;&#36182;&#20110;&#20248;&#21270;&#22266;&#23450;&#22270;&#20687;&#21644;&#31227;&#21160;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#25351;&#26631;&#65292;&#36825;&#21487;&#33021;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#21464;&#24471;&#26114;&#36149;&#12290;&#24403;&#22270;&#20687;&#36739;&#22823;&#25110;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#24418;&#21464;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#20250;&#21152;&#21095;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26041;&#27861;&#65292;&#34987;&#25506;&#32034;&#20316;&#20026;&#38750;&#23398;&#20064;&#26041;&#27861;&#24369;&#28857;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#22270;&#20687;&#37197;&#20934;&#20013;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#38382;&#39064;&#20013;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;Transformer&#26694;&#26550;&#31216;&#20026;AiR&#65292;&#21487;&#20197;&#22312;GPGPU&#35774;&#22791;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#22270;&#20687;&#37197;&#20934;&#38382;&#39064;&#35270;&#20026;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;Transformer&#23398;&#20064;&#21464;&#24418;&#22330;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#37197;&#20934;&#26041;&#27861;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#25110;&#22320;&#38754;&#30495;&#23454;&#24418;&#21464;&#22330;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration is a crucial task in signal processing, but it often encounters issues with stability and efficiency. Non-learning registration approaches rely on optimizing similarity metrics between fixed and moving images, which can be expensive in terms of time and space complexity. This problem can be exacerbated when the images are large or there are significant deformations between them. Recently, deep learning, specifically convolutional neural network (CNN)-based methods, have been explored as an effective solution to the weaknesses of non-learning approaches. To further advance learning approaches in image registration, we introduce an attention mechanism in the deformable image registration problem. Our proposed approach is based on a Transformer framework called AiR, which can be efficiently trained on GPGPU devices. We treat the image registration problem as a language translation task and use the Transformer to learn the deformation field. The method learns an unsuperv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#36816;&#34892;&#26102;&#38388;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;SEMO&#26080;&#27861;&#25214;&#21040;&#25152;&#26377;Pareto&#21069;&#27839;&#12290;&#20294;&#20840;&#23616;SEMO&#35777;&#26126;&#20102;&#22312;&#26399;&#26395;&#36845;&#20195;&#27425;&#25968;&#19978;&#38480;&#20869;&#25214;&#21040;&#20102;&#25152;&#26377;Pareto&#21069;&#27839;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#22312;&#22810;&#23792;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2012.07231</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#22312;&#22810;&#23792;&#30446;&#26631;&#19978;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analyses of Multiobjective Evolutionary Algorithms on Multimodal Objectives. (arXiv:2012.07231v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.07231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#36816;&#34892;&#26102;&#38388;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;SEMO&#26080;&#27861;&#25214;&#21040;&#25152;&#26377;Pareto&#21069;&#27839;&#12290;&#20294;&#20840;&#23616;SEMO&#35777;&#26126;&#20102;&#22312;&#26399;&#26395;&#36845;&#20195;&#27425;&#25968;&#19978;&#38480;&#20869;&#25214;&#21040;&#20102;&#25152;&#26377;Pareto&#21069;&#27839;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#22312;&#22810;&#23792;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#22312;&#23454;&#36341;&#20013;&#30340;&#25104;&#21151;&#36828;&#36828;&#36229;&#36807;&#20102;&#29702;&#35770;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20027;&#35201;&#32771;&#34385;&#30001;&#21333;&#23792;&#30446;&#26631;&#32452;&#25104;&#30340;&#31616;&#21333;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36827;&#21270;&#31639;&#27861;&#22914;&#20309;&#35299;&#20915;&#22810;&#23792;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OJZJ&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19982;&#32463;&#20856;&#36339;&#36291;&#20989;&#25968;&#22522;&#20934;&#21516;&#26500;&#30340;&#30446;&#26631;&#32452;&#25104;&#30340;&#21452;&#30446;&#26631;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;SEMO&#65292;&#26080;&#35770;&#36816;&#34892;&#26102;&#38388;&#22914;&#20309;&#65292;&#27010;&#29575;&#37117;&#19981;&#21487;&#33021;&#35745;&#31639;&#20986;&#23436;&#25972;&#30340; Pareto &#21069;&#27839;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#25152;&#26377;&#38382;&#39064;&#22823;&#23567;n&#21644;&#25152;&#26377;&#36339;&#36291;&#22823;&#23567;k&#8712;[ 4 . . n 2 &#8722;1 ]&#65292;&#20840;&#23616;SEMO&#65288;GSEMO&#65289;&#22312; &#920; ((n&#8722;2k)n^k&#65289;&#27425;&#36845;&#20195;&#20013;&#35206;&#30422; Pareto &#21069;&#27839;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical understanding of MOEAs is lagging far behind their success in practice. In particular, previous theory work considers mostly easy problems that are composed of unimodal objectives.  As a first step towards a deeper understanding of how evolutionary algorithms solve multimodal multiobjective problems, we propose the OJZJ problem, a bi-objective problem composed of two objectives isomorphic to the classic jump function benchmark. We prove that SEMO with probability one does not compute the full Pareto front, regardless of the runtime. In contrast, for all problem sizes $n$ and all jump sizes ${k \in [4..\frac n2 - 1]}$, the global SEMO (GSEMO) covers the Pareto front in an expected number of $\Theta((n-2k)n^{k})$ iterations. For $k = o(n)$, we also show the tighter bound $\frac 32 e n^{k+1} \pm o(n^{k+1})$, which might be the first runtime bound for an MOEA that is tight apart from lower-order terms. We also combine the GSEMO with two approaches that showed advantages in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#31639;&#27861;&#22312;&#36817;&#20284;&#22810;&#20154;&#21338;&#24328;Nash&#22343;&#34913;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;Fictitious Play&#27604;Counterfactual Regret Minimization&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2001.11165</link><description>&lt;p&gt;
Fictitious Play&#20248;&#20110;Counterfactual Regret Minimization
&lt;/p&gt;
&lt;p&gt;
Fictitious Play Outperforms Counterfactual Regret Minimization. (arXiv:2001.11165v7 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.11165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#31639;&#27861;&#22312;&#36817;&#20284;&#22810;&#20154;&#21338;&#24328;Nash&#22343;&#34913;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;Fictitious Play&#27604;Counterfactual Regret Minimization&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#24191;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#8212;&#8212;Fictitious Play&#21644;Counterfactual Regret Minimization&#22312;&#36817;&#20284;&#22810;&#20154;&#21338;&#24328;Nash&#22343;&#34913;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;Counterfactual Regret Minimization&#22312;&#22810;&#20154;&#25169;&#20811;&#20013;&#21462;&#24471;&#20102;&#36739;&#22823;&#25104;&#21151;&#24182;&#34987;&#35748;&#20026;&#26159;&#26356;&#20248;&#31168;&#30340;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;Fictitious Play&#22312;&#21508;&#31181;&#31867;&#21035;&#21644;&#35268;&#27169;&#30340;&#28216;&#25103;&#20013;&#37117;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;Nash&#22343;&#34913;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compare the performance of two popular algorithms, fictitious play and counterfactual regret minimization, in approximating Nash equilibrium in multiplayer games. Despite recent success of counterfactual regret minimization in multiplayer poker and conjectures of its superiority, we show that fictitious play leads to improved Nash equilibrium approximation over a variety of game classes and sizes.
&lt;/p&gt;</description></item></channel></rss>