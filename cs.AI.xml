<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;IPF&#65292;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#65292;&#33021;&#22815;&#39044;&#27979;&#19981;&#21516;&#31038;&#20250;&#25110;&#25919;&#27835;&#21033;&#30410;&#30456;&#20851;&#21442;&#25968;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#21457;&#23637;&#65292;&#24050;&#22312;&#38899;&#20048;&#20048;&#22120;&#27169;&#25311;&#12289;&#33041;&#21160;&#21147;&#23398;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00977</link><description>&lt;p&gt;
&#38024;&#23545;&#22478;&#24066;&#35268;&#21010;&#21644;&#20844;&#20247;&#21442;&#19982;&#30340;&#38750;&#32447;&#24615;&#33033;&#20914;&#27169;&#24335;&#21046;&#23450;&#21160;&#24577;&#31038;&#20250;&#25919;&#27835;&#39044;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;IPF&#65292;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#65292;&#33021;&#22815;&#39044;&#27979;&#19981;&#21516;&#31038;&#20250;&#25110;&#25919;&#27835;&#21033;&#30410;&#30456;&#20851;&#21442;&#25968;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#21457;&#23637;&#65292;&#24050;&#22312;&#38899;&#20048;&#20048;&#22120;&#27169;&#25311;&#12289;&#33041;&#21160;&#21147;&#23398;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#21363;&#33033;&#20914;&#27169;&#24335;&#21046;&#23450;&#65288;IPF&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#31038;&#20250;&#25110;&#25919;&#27835;&#21033;&#30410;&#30456;&#20851;&#21442;&#25968;&#65288;&#22914;&#20581;&#24247;&#12289;&#33402;&#26415;&#33258;&#30001;&#25110;&#36130;&#21153;&#21457;&#23637;&#65289;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#21457;&#23637;&#12290;IPF&#24050;&#32463;&#22312;&#38899;&#20048;&#20048;&#22120;&#27169;&#25311;&#12289;&#33041;&#21160;&#21147;&#23398;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#12290;&#31038;&#20250;&#21644;&#25919;&#27835;IPF&#30001;&#19977;&#20010;&#22522;&#26412;&#26041;&#31243;&#32452;&#25104;&#65292;&#29992;&#20110;&#31995;&#32479;&#29366;&#24577;&#21457;&#23637;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#33258;&#36866;&#24212;&#12289;&#20004;&#31181;&#33258;&#36866;&#24212;&#20114;&#21160;&#21644;&#36866;&#21512;&#20110;&#19981;&#21516;&#35268;&#21010;&#24773;&#22659;&#30340;&#22806;&#37096;&#20914;&#20987;&#39033;&#12290;&#36890;&#36807;&#35843;&#25972;&#19968;&#32452;&#31995;&#32479;&#21442;&#25968;&#65292;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#21644;&#21457;&#23637;&#30340;&#20856;&#22411;&#24773;&#26223;&#36827;&#34892;&#24314;&#27169;&#65292;&#21253;&#25324;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#22806;&#37096;&#36755;&#20837;&#30340;&#21453;&#24212;&#12289;&#33258;&#36866;&#24212;&#36890;&#36807;&#25552;&#39640;&#31995;&#32479;&#31283;&#23450;&#24615;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#22240;&#35843;&#35299;&#20114;&#21160;&#33258;&#36866;&#24212;&#32780;&#25910;&#25947;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00977v1 Announce Type: cross  Abstract: A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process. The IPF has already shown high predictive precision at low computational cost in musical instrument simulations, brain dynamics, and human-human interactions. The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations. Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters. These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as
&lt;/p&gt;</description></item><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.18252</link><description>&lt;p&gt;
&#36229;&#36234;&#23884;&#20837;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35270;&#35273;&#34920;&#26684;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30707;&#65292;&#20174;&#20855;&#26377;&#20154;&#31867;&#27880;&#37322;&#26631;&#31614;&#30340;&#30417;&#30563;&#23398;&#20064;&#21457;&#23637;&#21040;&#23545;&#40784;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#22914;CLIP&#23884;&#20837;&#65289;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Table&#65292;&#36825;&#26159;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#25552;&#20379;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#30340;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#65292;&#21253;&#25324;&#22330;&#26223;&#25551;&#36848;&#21644;&#28085;&#30422;&#31867;&#21035;&#12289;&#23646;&#24615;&#21644;&#23454;&#20363;&#32423;&#21035;&#30693;&#35782;&#30340;&#22810;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20174;GPT4V&#30340;&#23567;&#35268;&#27169;&#27880;&#37322;&#20013;&#29983;&#25104;&#35270;&#35273;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#23427;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#34920;&#26684;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.11879</link><description>&lt;p&gt;
&#21333;&#27169;&#24577;&#22810;&#20219;&#21153;&#34701;&#21512;&#29992;&#20110;&#24773;&#24863;&#27169;&#20223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31532;&#20845;&#23626;&#25143;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#30740;&#35752;&#20250;&#21644;&#31454;&#36187;&#20013;&#36827;&#34892;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#65288;EMI&#65289;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Wav2Vec 2.0&#26694;&#26550;&#65292;&#22312;&#19968;&#20010;&#20840;&#38754;&#30340;&#25773;&#23458;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#21462;&#28085;&#30422;&#35821;&#35328;&#21644;&#35821;&#22806;&#20803;&#32032;&#30340;&#24191;&#27867;&#38899;&#39057;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#34701;&#21512;&#25216;&#26415;&#22686;&#24378;&#20102;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#25216;&#26415;&#23558;&#20010;&#20307;&#29305;&#24449;&#19982;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21040;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;Wav2Vec 2.0&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;valence-arousal-dominance&#65288;VAD&#65289;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#34701;&#21512;&#37319;&#29992;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#23545;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#26102;&#38388;&#20998;&#26512;&#12290;&#20165;&#21033;&#29992;&#25152;&#25552;&#20379;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>MindEye2&#20351;&#29992;&#20849;&#20139;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;1&#23567;&#26102;&#30340;fMRI&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;fMRI&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.11207</link><description>&lt;p&gt;
MindEye2&#65306;&#20849;&#20139;&#20027;&#39064;&#27169;&#22411;&#20351;&#24471;&#21482;&#38656;1&#23567;&#26102;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;fMRI&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11207
&lt;/p&gt;
&lt;p&gt;
MindEye2&#20351;&#29992;&#20849;&#20139;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;1&#23567;&#26102;&#30340;fMRI&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;fMRI&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24863;&#30693;&#30340;&#37325;&#24314;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#38024;&#23545;&#27599;&#20010;&#21463;&#35797;&#32773;&#29420;&#31435;&#35757;&#32451;&#30340;&#65292;&#27599;&#20010;&#21463;&#35797;&#32773;&#38656;&#35201;&#25968;&#21313;&#23567;&#26102;&#26114;&#36149;&#30340;fMRI&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;1&#23567;&#26102;fMRI&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#39640;&#36136;&#37327;&#37325;&#24314;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;7&#21517;&#21463;&#35797;&#32773;&#20013;&#39044;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#26032;&#21463;&#35797;&#32773;&#30340;&#37096;&#20998;&#25968;&#25454;&#19978;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#21151;&#33021;&#23545;&#40784;&#31243;&#24207;&#23558;&#25152;&#26377;&#33041;&#25968;&#25454;&#32447;&#24615;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#20027;&#39064;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#36890;&#36807;&#20849;&#20139;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#21040;CLIP&#22270;&#20687;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;XL&#26469;&#25509;&#21463;CLIP&#28508;&#22312;&#20316;&#20026;&#36755;&#20837;&#32780;&#19981;&#26159;&#25991;&#26412;&#65292;&#23558;CLIP&#31354;&#38388;&#26144;&#23556;&#21040;&#20687;&#32032;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#36328;&#21463;&#35797;&#32773;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#20063;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#37325;&#24314;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11207v1 Announce Type: cross  Abstract: Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruct
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#29992;&#20110;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25972;&#21512;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25968;&#25454;&#38598;&#21512;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#35782;&#21035;&#28508;&#22312;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.10380</link><description>&lt;p&gt;
BirdSet&#65306;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20998;&#31867;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10380
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#29992;&#20110;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25972;&#21512;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25968;&#25454;&#38598;&#21512;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#35782;&#21035;&#28508;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#35786;&#26029;&#29615;&#22659;&#20581;&#24247;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#32473;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32508;&#21512;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#20840;&#38754;&#20998;&#31867;&#40479;&#31867;&#40483;&#21483;&#22768;&#12290;BirdSet&#23558;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25972;&#21512;&#21040;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21512;&#20013;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#35782;&#21035;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#28508;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10380v1 Announce Type: cross  Abstract: Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to diagnose environmental health and biodiversity. However, inconsistencies in research pose notable challenges hindering progress in this domain. Reliable DL models need to analyze bird calls flexibly across various species and environments to fully harness the potential of bioacoustics in a cost-effective passive acoustic monitoring scenario. Data fragmentation and opacity across studies complicate a comprehensive evaluation of general model performance. To overcome these challenges, we present the BirdSet benchmark, a unified framework consolidating research efforts with a holistic approach for classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes open-source bird recordings into a curated dataset collection. This unified approach provides an in-depth understanding of model performance and identifies potential shortcomings across diffe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08335</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Sparsity Principle for Partially Observable Causal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#24863;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#22240;&#26524;&#21464;&#37327;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#27425;&#27979;&#37327;&#20165;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#29366;&#24577;&#23376;&#38598;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#19981;&#37197;&#23545;&#35266;&#23519;&#23398;&#20064;&#65292;&#20854;&#20013;&#23384;&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#35813;&#35774;&#32622;&#24314;&#31435;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65306;&#19968;&#20010;&#26159;&#20851;&#20110;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#23545;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20570;&#21442;&#25968;&#20551;&#35774;&#65292;&#21478;&#19968;&#20010;&#26159;&#23545;&#20855;&#26377;&#39640;&#26031;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08335v1 Announce Type: cross  Abstract: Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variab
&lt;/p&gt;</description></item><item><title>GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;</title><link>https://arxiv.org/abs/2403.08293</link><description>&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65306;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08293
&lt;/p&gt;
&lt;p&gt;
GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#36880;&#27493;&#29983;&#25104;&#24102;&#26377;&#20854;&#21477;&#27861;&#26641;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65288;GPST&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;SLM&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#12290;GPST&#35268;&#36991;&#20102;&#20043;&#21069;SLM&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#19968;&#20010;&#36890;&#24120;&#30340;SLM&#21463;&#21333;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#32452;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#23548;&#21477;&#27861;&#35299;&#26512;&#26641;&#24182;&#35745;&#31639;&#25104;&#20998;&#34920;&#31034;&#65292;&#21463;&#21452;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#27169;&#22411;&#30340;&#32852;&#21512;&#24182;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#30828;EM&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#23545;GPST&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;90&#20159;&#20010;token&#65292;&#24182;&#23637;&#31034;&#20102;GPST&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#28085;&#30422;&#20102;&#19982;GPT-2&#30456;&#24403;&#35268;&#27169;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07183</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#30417;&#27979;AI&#20462;&#25913;&#30340;&#20869;&#23481;&#65306;AI&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;ChatGPT&#24433;&#21709;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#25991;&#26412;&#21487;&#33021;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22823;&#24133;&#20462;&#25913;&#25110;&#29983;&#25104;&#30340;&#37096;&#20998;&#27604;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#22411;&#21033;&#29992;&#19987;&#23478;&#25776;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#20934;&#30830;&#39640;&#25928;&#22320;&#26816;&#26597;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#30495;&#23454;&#19990;&#30028;LLM&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#20250;&#35758;&#19978;&#31185;&#23398;&#21516;&#34892;&#35780;&#23457;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29983;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#65292;&#21253;&#25324;ICLR 2024&#12289;NeurIPS 2023&#12289;CoRL 2023&#21644;EMNLP 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20250;&#35758;&#25552;&#20132;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#30001;LLMs&#22823;&#24133;&#20462;&#25913;&#30340;&#65292;&#21363;&#36229;&#20986;&#25340;&#20889;&#26816;&#26597;&#25110;&#23567;&#24133;&#26356;&#26032;&#30340;&#33539;&#22260;&#12290;&#29983;&#25104;&#25991;&#26412;&#20986;&#29616;&#30340;&#24773;&#20917;&#20026;&#29992;&#25143;&#34892;&#20026;&#25552;&#20379;&#20102;&#35265;&#35299;&#65306;&#22312;&#25253;&#21578;&#20449;&#24515;&#36739;&#20302;&#12289;&#22312;&#25130;&#27490;&#26085;&#26399;&#21069;&#25552;&#20132;&#30340;&#35780;&#35770;&#20197;&#21450;&#20174;&#35780;&#35770;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
&lt;/p&gt;</description></item><item><title>DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04233</link><description>&lt;p&gt;
DEEP-ICL: &#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04233
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#39537;&#21160;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#33539;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25361;&#25112;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEP-ICL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ICL&#12290; DEEP-ICL&#20174;&#32473;&#23450;&#30340;&#31034;&#33539;&#20013;&#26126;&#30830;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;ICL&#30340;&#25913;&#36827;&#24182;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#22522;&#26412;&#19978;&#28304;&#33258;&#20110;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#21644;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;DEEP-ICL&#32467;&#21512;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;3B&#27169;&#22411;&#65288;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#20219;&#21153;&#23450;&#20041;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20219;&#21153;&#31034;&#33539;&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;LLaMA2-13B&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#39044;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02253</link><description>&lt;p&gt;
KnowPhish&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20197;&#22686;&#24378;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24050;&#32473;&#20010;&#20154;&#21644;&#20225;&#19994;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#39640;&#25928;&#30340;&#33258;&#21160;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#65288;RBPDs&#65289;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#27604;&#36739;&#30446;&#26631;&#32593;&#39029;&#19978;&#30340;&#26631;&#24535;&#19982;&#24050;&#30693;&#26631;&#24535;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;RBPDs&#30340;&#20027;&#35201;&#23616;&#38480;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#25163;&#21160;&#26500;&#24314;&#30340;&#21697;&#29260;&#30693;&#35782;&#24211;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#21697;&#29260;&#65292;&#23548;&#33268;&#30001;&#20110;&#30693;&#35782;&#24211;&#20013;&#21697;&#29260;&#35206;&#30422;&#19981;&#36275;&#32780;&#20986;&#29616;&#20551;&#38452;&#24615;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#37319;&#29992;&#35813;&#27969;&#27700;&#32447;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21253;&#21547;20k&#20010;&#21697;&#29260;&#21644;&#27599;&#20010;&#21697;&#29260;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;KnowPhish&#21487;&#20197;&#29992;&#26469;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#25552;&#21319;&#29616;&#26377;RBPDs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
&lt;/p&gt;</description></item><item><title>DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00896</link><description>&lt;p&gt;
DiaHalu&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00896
&lt;/p&gt;
&lt;p&gt;
DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26368;&#36817;&#20960;&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#24187;&#35273;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26377;&#35768;&#22810;&#22522;&#20934;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#22522;&#20934;&#19981;&#26159;&#30001;LLMs&#33258;&#28982;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#26377;&#24847;&#24341;&#21457;&#30340;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#20165;&#20851;&#27880;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#32780;&#24573;&#35270;&#20102;&#24544;&#23454;&#24230;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;LLMs&#26102;&#20195;&#23545;&#35805;&#27169;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#20165;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#30340;&#24187;&#35273;&#19978;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; DiaHalu&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#20027;&#39064;&#38598;&#25104;&#21040;&#31995;&#32479;&#25552;&#31034;&#20013;&#65292;&#20419;&#36827;&#20004;&#20010;ChatGPT3.5&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#20462;&#25913;&#19981;&#31526;&#21512;&#20154;&#31867;&#35821;&#35328;&#32422;&#23450;&#30340;&#20869;&#23481;&#65292;&#28982;&#21518;&#35753;LLMs&#37325;&#26032;&#29983;&#25104;&#65292;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#31867;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00896v1 Announce Type: cross  Abstract: Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-
&lt;/p&gt;</description></item><item><title>InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.00822</link><description>&lt;p&gt;
InteraRec&#65306;&#20351;&#29992;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
InteraRec: Interactive Recommendations Using Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00822
&lt;/p&gt;
&lt;p&gt;
InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weblog&#30001;&#35760;&#24405;&#20219;&#20309;&#32593;&#31449;&#19978;&#29992;&#25143;&#27963;&#21160;&#30340;&#35760;&#24405;&#32452;&#25104;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#20559;&#22909;&#12289;&#34892;&#20026;&#21644;&#20852;&#36259;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#35768;&#22810;&#25512;&#33616;&#31639;&#27861;&#21033;&#29992;&#36890;&#36807;&#36825;&#20123;Weblog&#25366;&#25496;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#21644;&#28151;&#21512;&#26041;&#27861;&#31561;&#31574;&#30053;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;InteraRec&#30340;&#22797;&#26434;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21518;&#32773;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00822v1 Announce Type: cross  Abstract: Weblogs, comprised of records detailing user activities on any website, offer valuable insights into user preferences, behavior, and interests. Numerous recommendation algorithms, employing strategies such as collaborative filtering, content-based filtering, and hybrid methods, leverage the data mined through these weblogs to provide personalized recommendations to users. Despite the abundance of information available in these weblogs, identifying and extracting pertinent information and key features necessitates extensive engineering endeavors. The intricate nature of the data also poses a challenge for interpretation, especially for non-experts. In this study, we introduce a sophisticated and interactive recommendation framework denoted as InteraRec, which diverges from conventional approaches that exclusively depend on weblogs for recommendation generation. This framework captures high-frequency screenshots of web pages as users nav
&lt;/p&gt;</description></item><item><title>ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00510</link><description>&lt;p&gt;
ROME: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35760;&#24518;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00510
&lt;/p&gt;
&lt;p&gt;
ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#29992;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#22797;&#21046;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#38271;&#24230;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#35760;&#24518;&#21270;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#35268;&#27169;&#24040;&#22823;&#19988;&#20854;&#39044;&#22788;&#29702;&#32791;&#26102;&#12290;&#20026;&#20102;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25506;&#32034;&#35760;&#24518;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#39318;&#20808;&#23558;&#36873;&#23450;&#30340;&#26679;&#26412;&#20998;&#20026;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#32452;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35265;&#35299;&#27604;&#36739;&#36825;&#20004;&#32452;&#20013;&#30340;&#28436;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21253;&#25324;&#35789;&#38271;&#12289;&#35789;&#24615;&#12289;&#35789;&#39057;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#22312;&#20869;&#30340;&#22240;&#32032;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19348</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22478;&#24066;&#35745;&#31639;&#20013;&#30340;&#36328;&#22495;&#25968;&#25454;&#34701;&#21512;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19348
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#30340;&#19981;&#26029;&#34028;&#21187;&#21457;&#23637;&#65292;&#22478;&#24066;&#35745;&#31639;&#20316;&#20026;&#19968;&#38376;&#20851;&#38190;&#23398;&#31185;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#65288;&#22914;&#22320;&#29702;&#12289;&#20132;&#36890;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#29615;&#22659;&#25968;&#25454;&#65289;&#21644;&#27169;&#24577;&#65288;&#22914;&#26102;&#31354;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65289;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#21147;&#37327;&#65292;&#25104;&#20026;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#19968;&#31181;&#21033;&#29992;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20221;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19987;&#38376;&#20026;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;&#25968;&#25454;&#35270;&#35282;&#65292;&#20197;&#29702;&#35299;&#27599;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#35770;&#20998;&#31867;&#20026;&#22235;&#22823;&#20027;&#35201;&#31867;&#21035;&#65306;&#22522;&#20110;&#29305;&#24449;&#12289;&#22522;&#20110;&#23545;&#40784;&#12289;&#22522;&#20110;&#23545;&#27604;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#34701;&#21512;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22810;&#27169;&#24577;&#22478;&#24066;&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19348v1 Announce Type: cross  Abstract: As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applicatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17916</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#25239;LLM&#30340;&#25968;&#23398;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-Resistant Math Word Problem Generation via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25913;&#21464;&#20102;&#25945;&#32946;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#35780;&#20272;&#65292;&#36825;&#20123;&#31034;&#20363;&#20445;&#30041;&#20102;&#21407;&#22987;&#38382;&#39064;&#30340;&#32467;&#26500;&#21644;&#38590;&#24230;&#65292;&#20294;LLMs&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25968;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#35789;&#38382;&#39064;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#31034;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#32534;&#36753;&#38382;&#39064;&#20013;&#30340;&#25968;&#23383;&#20540;&#65292;&#23548;&#33268;LLMs&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;</title><link>https://arxiv.org/abs/2402.17065</link><description>&lt;p&gt;
&#39535;&#26381;&#31867;&#21035;&#26465;&#20214;GAN&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#65306;&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17065
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#20351;&#29992;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20174;&#38271;&#23614;&#35757;&#32451;&#20998;&#24067;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#20173;&#28982;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#22810;&#31867;&#21035;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;GANs&#20542;&#21521;&#20110;&#20559;&#29233;&#26679;&#26412;&#26356;&#22810;&#30340;&#31867;&#21035;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#29983;&#25104;&#20302;&#36136;&#37327;&#19988;&#26679;&#26412;&#19981;&#22815;&#22810;&#26679;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#36827;&#20351;&#29992;&#38271;&#23614;&#25968;&#25454;&#35757;&#32451;&#31867;&#21035;&#26465;&#20214;GANs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65292;&#20801;&#35768;&#23614;&#37096;&#31867;&#21035;&#20174;&#35757;&#32451;&#25968;&#25454;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20511;&#37492;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#31867;&#21035;&#26465;&#20214;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#22120;&#30340;&#36739;&#20302;&#20998;&#36776;&#29575;&#23618;&#23436;&#20840;&#26080;&#26465;&#20214;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23558;&#31867;&#21035;&#26465;&#20214;&#29983;&#25104;&#20445;&#30041;&#32473;&#36739;&#39640;&#20998;&#36776;&#29575;&#23618;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve
&lt;/p&gt;</description></item><item><title>GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16174</link><description>&lt;p&gt;
GenNBV: &#36890;&#29992;&#30340;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16174
&lt;/p&gt;
&lt;p&gt;
GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#25216;&#26415;&#36827;&#27493;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#22330;&#26223;&#30340;&#30495;&#23454;&#25968;&#23383;&#21270;, &#20294;&#26159;&#22270;&#20687;&#25429;&#33719;&#36807;&#31243;&#20173;&#28982;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#31574;&#30053;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NBV&#31574;&#30053;&#20005;&#37325;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#26631;&#20934;&#12289;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25110;&#32773;&#26159;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#20248;&#21270;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#32422;&#26463;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#25968;&#25454;&#38598;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenNBV&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#36890;&#29992;&#30340;NBV&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#23558;&#20856;&#22411;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#12290;&#23427;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#26080;&#20154;&#26426;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#26410;&#35265;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#65292;&#21253;&#25324;&#20960;&#20309;&#12289;&#35821;&#20041;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16174v1 Announce Type: cross  Abstract: While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action repres
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.15729</link><description>&lt;p&gt;
&#20154;&#31867;&#26159;&#22914;&#20309;&#32534;&#20889;&#20195;&#30721;&#30340;&#65311;&#22823;&#22411;&#27169;&#22411;&#20063;&#20197;&#21516;&#26679;&#30340;&#26041;&#24335;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
How Do Humans Write Code? Large Models Do It the Same Way Too
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15729
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#12290;&#19982;&#20256;&#32479;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30456;&#27604;&#65292;&#31243;&#24207;&#21270;&#24605;&#32500;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#25191;&#34892;&#36825;&#20123;&#20195;&#30721;&#65292;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#32780;&#19981;&#26159;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;LLMs&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#29983;&#25104;&#27604;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26356;&#22810;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-Think Language&#65288;HTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#30001;&#27169;&#22411;&#29983;&#25104;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35299;&#20915;&#38382;&#39064;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#20195;&#30721;&#65292;&#21453;&#26144;&#20986;&#20154;&#20204;&#22312;&#23558;&#36923;&#36753;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#24605;&#32771;&#21518;&#20877;&#23558;&#20854;&#20889;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15729v1 Announce Type: new  Abstract: Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the P
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.14683</link><description>&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Visual Hallucinations of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14683
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24187;&#35273;&#65288;VH&#65289;&#24847;&#21619;&#30528;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#23545;&#22270;&#20687;&#24819;&#35937;&#20986;&#38169;&#35823;&#30340;&#32454;&#33410;&#12290;&#29616;&#26377;&#30740;&#31350;&#21457;&#29616;VH&#23454;&#20363;&#20165;&#23384;&#22312;&#20110;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;MLLM&#22312;VH&#19979;&#30340;&#24615;&#33021;&#29702;&#35299;&#23384;&#22312;&#20559;&#24046;&#65292;&#21407;&#22240;&#22312;&#20110;&#36825;&#31867;VH&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VHTest&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;VH&#23454;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VHTest&#22312;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;COCO&#65289;&#20013;&#25214;&#21040;&#19968;&#20123;&#21021;&#22987;&#30340;VH&#23454;&#20363;&#65292;&#20026;&#27599;&#20010;VH&#27169;&#24335;&#29983;&#25104;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;DALL-E-3&#65289;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;VH&#22270;&#20687;&#12290;&#25105;&#20204;&#21033;&#29992;VHTest&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;VH&#27169;&#24335;&#20013;1,200&#20010;VH&#23454;&#20363;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#65288;&#20363;&#22914;GPT-4V&#12289;LLaVA-1.5&#21644;MiniGPT-v2&#65289;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#22823;&#37096;&#20998;&#23454;&#20363;&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#36890;&#36807;AI&#27169;&#25311;&#31995;&#32479;&#20998;&#26512;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#24378;&#35843;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14090</link><description>&lt;p&gt;
&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Social Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#36890;&#36807;AI&#27169;&#25311;&#31995;&#32479;&#20998;&#26512;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#24378;&#35843;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#21046;&#23450;&#30340;&#25216;&#26415;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;&#12289;&#32463;&#27982;&#19982;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#31038;&#21306;&#30456;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#21253;&#25324;&#23545;&#25919;&#31574;&#30446;&#26631;&#30340;&#25237;&#31080;&#65292;&#24182;&#20026;&#36890;&#36807;AI&#27169;&#25311;&#23545;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#24320;&#25918;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24076;&#26395;&#23454;&#29616;&#21508;&#31181;&#31038;&#20250;&#31119;&#21033;&#30446;&#26631;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#36947;&#24503;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14090v1 Announce Type: new  Abstract: Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.13731</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36798;&#33452;&#22855;&#23494;&#30721;&#65306;&#35299;&#35835;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#30340;&#26426;&#21046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#26435;&#37325;&#20013;&#65292;&#26576;&#20123;&#23384;&#20648;&#21333;&#20803;&#34920;&#29616;&#20986;&#36864;&#21270;&#24615;&#65292;&#31216;&#20026;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;Degenerate Knowledge Neurons, DKNs&#65289;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#32467;&#26500;&#21644;&#21151;&#33021;&#26041;&#38754;&#30340;DKNs&#20840;&#38754;&#23450;&#20041;&#65292;&#24320;&#21019;&#20102;&#23545;PLMs&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#21333;&#20803;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#24418;&#25104;&#20219;&#24847;&#25968;&#37327;&#21644;&#32467;&#26500;&#30340;DKNs&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12289;&#21487;&#36827;&#21270;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#23545;PLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;34&#20010;&#23454;&#39564;&#65292;&#36328;&#36234;2&#20010;PLMs&#12289;4&#20010;&#25968;&#25454;&#38598;&#21644;6&#20010;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13731v1 Announce Type: cross  Abstract: This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13671</link><description>&lt;p&gt;
KInIT&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#65306;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24494;&#35843;LLMs
&lt;/p&gt;
&lt;p&gt;
KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#20132;&#21449;&#20256;&#25773; &#25688;&#35201;&#65306;SemEval-2024&#20219;&#21153;8&#20391;&#37325;&#20110;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#23545;&#20110;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#22312;&#28389;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20854;&#20013;&#26368;&#26032;&#30340;LLMs&#38750;&#24120;&#25797;&#38271;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#20197;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#21033;&#29992;&#35821;&#35328;&#35782;&#21035;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#27599;&#31181;&#35821;&#35328;&#30340;&#20998;&#31867;&#38408;&#20540;&#26657;&#20934;&#65292;&#23558;&#24494;&#35843;&#30340;&#27169;&#22411;&#39044;&#27979;&#19982;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#29420;&#29305;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#26816;&#27979;&#24615;&#33021;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#25490;&#21517;&#31532;&#22235;&#65292;&#20165;&#33853;&#21518;&#20110;&#33719;&#32988;&#32773;&#19981;&#21040;1&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 Announce Type: cross  Abstract: SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.
&lt;/p&gt;</description></item><item><title>VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.13217</link><description>&lt;p&gt;
VideoPrism: &#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#22522;&#30784;&#35270;&#35273;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
VideoPrism: A Foundational Visual Encoder for Video Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13217
&lt;/p&gt;
&lt;p&gt;
VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VideoPrism&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#21333;&#20010;&#20923;&#32467;&#27169;&#22411;&#22788;&#29702;&#22810;&#26679;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;3600&#19975;&#39640;&#36136;&#37327;&#35270;&#39057;&#26631;&#39064;&#23545;&#21644;58.2&#20159;&#20010;&#24102;&#26377;&#22024;&#26434;&#24179;&#34892;&#25991;&#26412;&#65288;&#22914;ASR&#36716;&#24405;&#65289;&#30340;&#35270;&#39057;&#21098;&#36753;&#30340;&#24322;&#26500;&#35821;&#26009;&#24211;&#19978;&#23545;VideoPrism&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#26041;&#27861;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#19968;&#20010;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#25913;&#36827;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#65292;&#20351;VideoPrism&#33021;&#22815;&#20027;&#35201;&#19987;&#27880;&#20110;&#35270;&#39057;&#27169;&#24577;&#21516;&#26102;&#21033;&#29992;&#19982;&#35270;&#39057;&#30456;&#20851;&#32852;&#30340;&#23453;&#36149;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#32452;&#19978;&#36827;&#34892;&#20102;&#23545;VideoPrism&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#20174;&#32593;&#32476;&#35270;&#39057;&#38382;&#31572;&#21040;&#31185;&#23398;CV&#65292; &#22312;33&#20010;&#35270;&#39057;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13217v1 Announce Type: cross  Abstract: We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13035</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#26597;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#26657;&#27491;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13035
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#19981;&#26029;&#21162;&#21147;&#36890;&#36807;&#33258;&#25105;&#26657;&#27491;&#26469;&#23436;&#21892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#22806;&#37096;&#20934;&#30830;&#30693;&#35782;&#30340;&#33258;&#25105;&#26657;&#27491;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#38480;&#21046;&#21644;&#26377;&#25928;&#24615;&#30340;&#30097;&#38382;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLM&#30340;&#33258;&#26816;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#65292;&#31216;&#20026;&#8220;Step CoT Check&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#23558;&#21407;&#22987;CoT&#25968;&#25454;&#21644;&#26816;&#26597;&#26657;&#27491;&#25968;&#25454;&#25972;&#21512;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20854;&#33258;&#26816;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#24182;&#28040;&#38500;&#20102;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11684</link><description>&lt;p&gt;
&#21033;&#29992;GPT4V&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;ALLaVA
&lt;/p&gt;
&lt;p&gt;
ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11684
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#21457;&#23637;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20294;&#37096;&#32626;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24357;&#21512;&#20256;&#32479;&#23610;&#24230;LVLMs&#21644;&#36164;&#28304;&#21451;&#22909;&#22411;Lite&#29256;&#26412;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;GPT-4V&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#12289;&#22797;&#26434;&#25512;&#29702;&#25351;&#20196;&#21644;&#22270;&#29255;&#35814;&#32454;&#31572;&#26696;&#30340;&#33021;&#21147;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#32467;&#26524;&#27169;&#22411;ALLaVA&#22312;12&#39033;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#35774;&#35745;&#26356;&#39640;&#25928;&#30340;LVLMs&#20013;&#37319;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;\url{https://allava.freedomai.cn}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11684v1 Announce Type: cross  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11541</link><description>&lt;p&gt;
&#36870;&#21521;&#35748;&#30693;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#25797;&#38271;&#29702;&#35299;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#24182;&#20943;&#23569;&#23427;&#20204;&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#22914;&#20309;&#20351;LLMs&#33021;&#22815;&#21363;&#26102;&#25972;&#21512;KGs&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#65288;CQA&#65289;&#20316;&#20026;&#19968;&#39033;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#29702;&#35299;KG&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65288;&#20174;&#19977;&#20803;&#32452;&#21040;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;KG&#30693;&#35782;&#30340;&#26368;&#20339;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10979</link><description>&lt;p&gt;
SportsMetrics:&#23558;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#34701;&#21512;&#20197;&#29702;&#35299;LLM&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25972;&#21512;&#25991;&#26412;&#25991;&#26723;&#21644;&#25968;&#25454;&#24211;&#35760;&#24405;&#31561;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#20808;&#36827;&#20998;&#26512;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;LLMs&#38656;&#35201;&#22788;&#29702;&#21644;&#20132;&#21449;&#24341;&#29992;&#23454;&#20307;&#21644;&#25968;&#23383;&#65292;&#22788;&#29702;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#21457;&#23637;&#35268;&#21010;&#33021;&#21147;&#65292;&#27604;&#22914;&#26500;&#24314;&#29992;&#20110;&#31649;&#29702;&#22797;&#26434;&#25968;&#25454;&#26597;&#35810;&#30340;&#24037;&#20316;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#30340;&#22235;&#39033;&#26032;&#39062;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21521;LLMs&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#22330;&#27604;&#36187;&#25551;&#36848;&#65292;&#28982;&#21518;&#22312;&#38754;&#23545;&#35832;&#22914;&#26032;&#27604;&#36187;&#35268;&#21017;&#12289;&#26356;&#38271;&#25345;&#32493;&#26102;&#38388;&#12289;&#25925;&#20107;&#28151;&#20081;&#20197;&#21450;&#20998;&#26512;&#27604;&#36187;&#25688;&#35201;&#20013;&#30340;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#31561;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;&#25105;&#20204;&#23545;NBA&#21644;NFL&#27604;&#36187;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 Announce Type: cross  Abstract: Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10712</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#25512;&#29702;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#20998;&#35789;&#22120;&#12289;&#35789;&#27719;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#19968;&#20123;LLMs&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26102;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#25928;&#29575;&#20250;&#19979;&#38477;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#21644;&#25104;&#26412;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#31181;&#29983;&#25104;LLMs&#65288;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65289;&#22312;&#22235;&#31181;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#19988;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.10496</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Comparing Hallucination Detection Metrics for Multilingual Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#38024;&#23545;&#33521;&#25991;&#25991;&#26412;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#23545;&#36825;&#20123;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#34920;&#29616;&#22914;&#20309;&#30340;&#35748;&#35782;&#19978;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26816;&#27979;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#35832;&#22914;ROUGE&#21644;&#21629;&#21517;&#23454;&#20307;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#65292;&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#20256;&#35760;&#25688;&#35201;&#20013;&#26816;&#27979;&#24187;&#35273;&#65307;&#25105;&#20204;&#36824;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#34913;&#37327;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#34429;&#28982;&#35789;&#27719;&#25351;&#26631;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22522;&#20110;NLI&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#22312;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#12290;&#30456;&#21453;&#65292;NLI-based&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22810;&#35821;&#35328;&#24187;&#35273;&#26816;&#27979;&#20013;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10496v1 Announce Type: cross  Abstract: While many automatic hallucination detection techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these hallucination detection metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like ROUGE and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at detecting hallucinations in biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, NLI-based metrics perform well in high-resource languages at the sentence level. In contrast, NLI-based metrics often fail to detect atomic fact hallucinations. Our findings highlight existing gaps in multilingual hallucinati
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10340</link><description>&lt;p&gt;
&#35770;&#37096;&#32626;LLMs/VLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#31361;&#26174;&#39118;&#38505;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10340
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25152;&#28041;&#21450;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#25805;&#20316;&#65292;&#23548;&#33322;&#31561;&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#20250;&#24341;&#20837;&#26174;&#30528;&#30340;&#28431;&#27934;&#65292;&#21363;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs/VLMs&#19982;&#26426;&#22120;&#20154;&#30028;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#25805;&#32437;&#25110;&#35823;&#23548;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#24694;&#24847;&#25915;&#20987;&#31034;&#20363;&#65292;&#24182;&#23545;&#38598;&#25104;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#30693;&#21517;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;&#21253;&#25324;KnowNo VIMA&#21644;Instruct2Act&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09614</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#29575;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Reasoning in Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#28041;&#21450;&#27010;&#29575;&#20540;&#26126;&#30830;&#37327;&#21270;&#30340;&#25991;&#26412;&#25512;&#29702;&#38382;&#39064;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#27010;&#29575;&#25512;&#29702;&#23545;&#20110;&#20174;&#26085;&#24120;&#23545;&#35805;&#21040;&#21307;&#23398;&#20915;&#31574;&#31561;&#21508;&#31181;&#24773;&#22659;&#37117;&#24456;&#37325;&#35201;&#12290;&#23613;&#31649;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#22312;&#27010;&#29575;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;LLMs&#27010;&#29575;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#35814;&#32454;&#35828;&#26126;LLMs&#22312;&#28041;&#21450;&#27010;&#29575;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#29305;&#23450;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;&#38382;&#39064;&#26144;&#23556;&#21040;&#19981;&#21516;&#24418;&#24335;&#34920;&#31034;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09614v1 Announce Type: cross  Abstract: This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.05929</link><description>&lt;p&gt;
&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Interactive Agent Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05929
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#27491;&#22312;&#20174;&#21019;&#24314;&#38745;&#24577;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#36716;&#21464;&#20026;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21160;&#24577;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340; AI &#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#32479;&#19968;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#35270;&#35273;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340; AI &#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29420;&#31435;&#39046;&#22495; - &#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#39046;&#22495;&#37117;&#23637;&#31034;&#20102;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24191;&#27867;&#24615;&#65292;&#21033;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#22914;&#26426;&#22120;&#20154;&#24207;&#21015;&#12289;&#28216;&#25103;&#25968;&#25454;&#12289;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effectiv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05201</link><description>&lt;p&gt;
&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Sampling Temperature on Problem Solving in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26631;&#20934;LLM&#22522;&#20934;&#20013;&#38543;&#26426;&#25277;&#21462;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQA&#65289;&#32771;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;LLM&#20197;&#21450;&#20116;&#31181;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#26469;&#35299;&#20915;MCQA&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#37319;&#26679;&#28201;&#24230;&#20174;0.0&#22686;&#21152;&#21040;1.0&#12290;&#23613;&#31649;&#26377;&#20851;&#30340;&#25253;&#36947;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#22312;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#21464;&#21270;&#27809;&#26377;&#32479;&#35745;&#23398;&#19978;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20284;&#20046;&#19981;&#21463;LLM&#12289;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#25110;&#38382;&#39064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#34917;&#20805;&#36164;&#26009;&#37117;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/matthewrenze/jhu-llm-temperature&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03741</link><description>&lt;p&gt;
SUB-PLAY&#65306;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#26080;&#20154;&#26426;&#30340;&#32676;&#20307;&#25511;&#21046;&#12289;&#26426;&#26800;&#33218;&#30340;&#21327;&#20316;&#25805;&#32437;&#20197;&#21450;&#22810;&#30446;&#26631;&#21253;&#22260;&#31561;&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;MARL&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#21644;&#28145;&#20837;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36805;&#36895;&#21033;&#29992;&#21463;&#23475;&#32773;&#30340;&#28431;&#27934;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#65292;&#23548;&#33268;&#21463;&#23475;&#32773;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#23558;&#36229;&#20154;&#32423;&#21035;&#30340;&#22260;&#26827;AI&#30340;&#33719;&#32988;&#29575;&#38477;&#20302;&#21040;&#32422;20%&#12290;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20004;&#20154;&#31454;&#20105;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20855;&#26377;&#23436;&#25972;&#30340;&#20840;&#23616;&#29366;&#24577;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01740</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#36127;&#33655;&#19979;&#30340;&#34917;&#20607;&#24615;&#20559;&#35265;&#65306;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36873;&#25321;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;gpt-3.5-turbo&#21644;claude-instant-1.2&#22312;&#35299;&#37322;&#21644;&#25191;&#34892;&#35821;&#20041;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#24433;&#21709;&#26368;&#22823;&#30340;&#26159;&#20174;&#21015;&#34920;&#20013;&#36827;&#34892;&#23545;&#35937;&#36873;&#25321;&#65292;&#36825;&#26159;&#25968;&#23383;&#23548;&#33322;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#26816;&#26597;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20195;&#34920;&#24615;&#21015;&#34920;&#36873;&#25321;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#25511;&#21046;&#23454;&#39564;&#65292;&#25105;&#20204;&#25805;&#32437;&#20102;&#28201;&#24230;&#12289;&#21015;&#34920;&#38271;&#24230;&#12289;&#23545;&#35937;&#36523;&#20221;&#12289;&#23545;&#35937;&#31867;&#22411;&#12289;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23396;&#31435;&#21644;&#27979;&#37327;&#36825;&#20123;&#20559;&#35265;&#23545;&#36873;&#25321;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#35265;&#32467;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#65292;&#32780;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#23384;&#22312;&#36739;&#24378;&#30340;&#21021;&#29616;&#25928;&#24212;&#65292;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#20250;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.18070</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30456;&#21516;&#30340;&#35748;&#30693;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#27169;&#22411;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20102;&#35299;LLMs&#33021;&#22815;&#27169;&#25311;&#21738;&#20123;&#35748;&#30693;&#29305;&#24615;&#20197;&#21450;&#21738;&#20123;&#19981;&#33021;&#27169;&#25311;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20799;&#31461;&#24050;&#30693;&#35748;&#30693;&#20559;&#35265;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35843;&#26597;&#23398;&#20064;&#31185;&#23398;&#25991;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26126;&#30830;&#30340;&#27493;&#39588;&#65306;&#25991;&#26412;&#29702;&#35299;&#12289;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#21644;&#35299;&#20915;&#26041;&#26696;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#20102;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22914;&#20309;&#24544;&#23454;&#22320;&#27169;&#25311;&#36825;&#20010;&#36807;&#31243;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20026;&#27599;&#20010;&#27979;&#35797;&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#21333;&#35789;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#20004;&#20010;&#35299;&#20915;&#36807;&#31243;&#30340;&#27493;&#39588;&#20013;&#65292;&#19981;&#35770;&#26159;&#21542;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#65292;&#37117;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25351;&#20196;&#34701;&#21512;&#65288;IF&#65289;&#65292;&#36890;&#36807;&#28151;&#21512;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#25913;&#36827;&#20102;&#29992;&#20110;&#20195;&#30721;LLM&#30340;&#35757;&#32451;&#25552;&#31034;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25351;&#20196;&#34701;&#21512;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20195;&#30721;LLM&#22312;&#22810;&#20010;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.15692</link><description>&lt;p&gt;
&#25351;&#20196;&#34701;&#21512;&#65306;&#36890;&#36807;&#28151;&#21512;&#21270;&#25512;&#36827;&#25552;&#31034;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instruction Fusion: Advancing Prompt Evolution through Hybridization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25351;&#20196;&#34701;&#21512;&#65288;IF&#65289;&#65292;&#36890;&#36807;&#28151;&#21512;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#25913;&#36827;&#20102;&#29992;&#20110;&#20195;&#30721;LLM&#30340;&#35757;&#32451;&#25552;&#31034;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25351;&#20196;&#34701;&#21512;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20195;&#30721;LLM&#22312;&#22810;&#20010;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#22495;&#32534;&#30721;&#26597;&#35810;&#65292;&#32454;&#35843;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;Evol-Instruct&#22312;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#25552;&#31034;&#28436;&#21270;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#25351;&#20196;&#34701;&#21512;&#65288;IF&#65289;&#12290;IF&#36890;&#36807;&#28151;&#21512;&#21270;&#36807;&#31243;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29992;&#20110;&#20195;&#30721;LLM&#30340;&#35757;&#32451;&#25552;&#31034;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;LLM&#22312;&#20154;&#24037;&#35780;&#20272;&#12289;&#20154;&#24037;&#35780;&#20272;+&#12289;MBPP&#12289;MBPP+&#21644;MultiPL-E&#31561;&#20116;&#20010;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#65292;&#20984;&#26174;&#20102;&#25351;&#20196;&#34701;&#21512;&#22312;&#25512;&#36827;LLM&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.
&lt;/p&gt;</description></item><item><title>EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;</title><link>https://arxiv.org/abs/2312.04916</link><description>&lt;p&gt;
EE-LLM: &#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#30340;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04916
&lt;/p&gt;
&lt;p&gt;
EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EE-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21021;&#27493;&#35777;&#26126;&#20102;&#26089;&#36864;&#20986;&#22312;&#21152;&#36895;LLM&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;EE-LLM&#36890;&#36807;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#24182;&#34892;&#24615;&#26469;&#25512;&#21160;&#26089;&#36864;&#20986;LLM&#30340;&#35268;&#27169;&#21270;&#12290;&#22522;&#20110;Megatron-LM&#26500;&#24314;&#30340;EE-LLM&#23454;&#29616;&#20102;&#21508;&#31181;&#31639;&#27861;&#21019;&#26032;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#26089;&#36864;&#20986;&#65292;&#21253;&#25324;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#27700;&#32447;&#24182;&#34892;&#24615;&#20419;&#36827;&#26089;&#36864;&#20986;&#35757;&#32451;&#30446;&#26631;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21033;&#29992;&#21407;&#22987;&#27969;&#27700;&#32447;&#35843;&#24230;&#20013;&#30340;&#31354;&#38386;&#36164;&#28304;&#36827;&#34892;&#19982;&#26089;&#36864;&#20986;&#23618;&#30456;&#20851;&#30340;&#35745;&#31639;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#20004;&#31181;&#19982;KV&#32531;&#23384;&#20860;&#23481;&#30340;&#26089;&#36864;&#20986;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#24573;&#30053;&#30340;&#35745;&#31639;&#24320;&#38144;&#30456;&#27604;&#65292;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;VLM&#21644;&#26174;&#33879;&#24615;&#20002;&#24323;&#26469;&#35299;&#20915;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.17095</link><description>&lt;p&gt;
&#20174;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#30340;&#32039;&#24613;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Emergent Open-Vocabulary Semantic Segmentation from Off-the-shelf Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;VLM&#21644;&#26174;&#33879;&#24615;&#20002;&#24323;&#26469;&#35299;&#20915;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#65292;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#23398;&#20064;&#38544;&#24335;&#23558;&#22270;&#20687;&#21306;&#22495;&#19982;&#35789;&#27719;&#20851;&#32852;&#36215;&#26469;&#65292;&#36825;&#23545;&#20110;&#35832;&#22914;&#35270;&#35273;&#38382;&#31572;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20851;&#32852;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS)&#12290;PnP-OVSS&#21033;&#29992;&#20855;&#26377;&#30452;&#25509;&#25991;&#26412;&#21040;&#22270;&#20687;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#25439;&#22833;&#30340;VLM&#12290;&#20026;&#20102;&#22312;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26174;&#33879;&#24615;&#20002;&#24323;&#65288;Salience Dropout&#65289;&#65307;&#36890;&#36807;&#36845;&#20195;&#20002;&#24323;&#27169;&#22411;&#26368;&#20851;&#27880;&#30340;&#34917;&#19969;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#25972;&#20010;&#20998;&#21106;&#25513;&#27169;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17095v2 Announce Type: replace-cross  Abstract: From image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which prove effective for tasks like visual question answering. However, leveraging the learned association for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss. To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. \shortname{} does not require any neural network training and performs hyperparameter tuning without the need for any segmentation annotations, even f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#25429;&#25417;&#21644;&#25512;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16421</link><description>&lt;p&gt;
&#20004;&#31181;&#30707;&#22836;&#20987;&#25171;&#19968;&#21482;&#40479;&#65306;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#20197;&#26356;&#22909;&#22320;&#25512;&#27979;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation. (arXiv:2401.16421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#25429;&#25417;&#21644;&#25512;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#24207;&#21015;&#30340;&#20869;&#22312;&#20998;&#21106;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#20301;&#32622;&#65292;&#25105;&#20204;&#30340;BiPE&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20998;&#27573;&#20869;&#32534;&#30721;&#29992;&#20110;&#35782;&#21035;&#27573;&#20869;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#24110;&#21161;&#27169;&#22411;&#25429;&#25417;&#20854;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20998;&#27573;&#38388;&#32534;&#30721;&#21017;&#29992;&#20110;&#25351;&#23450;&#27573;&#32034;&#24341;&#65292;&#24314;&#27169;&#27573;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#25552;&#39640;&#25512;&#27979;&#33021;&#21147;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#20301;&#32622;&#20449;&#24687;&#30340;&#35299;&#32806;&#20351;&#23398;&#20064;&#26356;&#21152;&#26377;&#25928;&#12290;&#32463;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08743</link><description>&lt;p&gt;
MMToM-QA: &#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08743
&lt;/p&gt;
&lt;p&gt;
MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#20204;&#30340;&#24515;&#26234;&#26159;&#24320;&#21457;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#31038;&#20132;&#26234;&#33021;&#30340;&#26426;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20284;&#20046;&#23637;&#29616;&#20986;&#26576;&#20123;&#24515;&#26234;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#26234;&#29702;&#35770;&#22522;&#20934;&#20351;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;-&#25110;&#32773;&#35270;&#39057;&#25110;&#32773;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24515;&#26234;&#29702;&#35770;&#19981;&#20165;&#20165;&#26159;&#35270;&#39057;&#25110;&#25991;&#26412;&#29702;&#35299;&#12290;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;&#20174;&#20219;&#20309;&#21487;&#29992;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#34920;&#31034;&#65288;&#20363;&#22914;&#30446;&#26631;&#65292;&#20449;&#24565;&#65292;&#35745;&#21010;&#65289;&#28789;&#27963;&#22320;&#25512;&#29702;&#21478;&#19968;&#20010;&#20154;&#30340;&#24515;&#26234;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#21253;&#25324;&#35270;&#35273;&#32447;&#32034;&#65292;&#35821;&#35328;&#21465;&#20107;&#25110;&#20004;&#32773;&#20860;&#26377;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#65288;MMToM-QA&#65289;&#22522;&#20934;&#12290;MMToM-QA&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#20851;&#20110;&#19968;&#20010;&#20154;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#30340;&#19981;&#21516;&#31181;&#31867;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;BIP-ALM&#65288;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.06757</link><description>&lt;p&gt;
&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#20102;&#35299;&#34892;&#20154;&#26159;&#21542;&#23558;&#27178;&#31359;&#22312;&#33258;&#20027;&#36710;&#36742;&#21069;&#26041;&#23545;&#20110;&#25191;&#34892;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#25805;&#25511;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#24207;&#21015;&#22270;&#20687;&#20013;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#39044;&#27979;&#27492;&#31867;&#24847;&#22270;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23548;&#33268;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#21270;&#27178;&#31359;&#21644;&#38750;&#27178;&#31359;&#65288;C/NC&#65289;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;ARCANE&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#22320;&#29983;&#25104;&#21253;&#21547;C/NC&#35270;&#39057;&#21098;&#36753;&#26679;&#26412;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;ARCANE&#29983;&#25104;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PedSynth&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;PedSynth&#22914;&#20309;&#34917;&#20805;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#22914;JAAD&#21644;PIE&#65292;&#20174;&#32780;&#20026;C/NC&#39044;&#27979;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;C/NC&#39044;&#27979;&#27169;&#22411;&#30340;&#36710;&#36733;&#37096;&#32626;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PedGNN&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#23427;&#36895;&#24230;&#24555;&#19988;&#20869;&#23384;&#21344;&#29992;&#38750;&#24120;&#20302;&#12290;PedGNN&#22522;&#20110;GNN-G&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06699</link><description>&lt;p&gt;
&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;&#30340;&#38381;&#21512;&#35299;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#21644;&#38142;&#24335;&#35268;&#21017;&#26799;&#24230;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#25552;&#20379;&#20102;&#38381;&#21512;&#24418;&#24335;&#30340;&#26435;&#37325;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#26159;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65292;&#26032;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#19968;&#32452;&#26435;&#37325;&#65292;&#22312;&#21333;&#27425;&#36845;&#20195;&#20013;&#20197;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#24335;&#20248;&#21270;&#26435;&#37325;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#19981;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#20998;&#31867;&#38382;&#39064;&#65289;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#20960;&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#26368;&#32456;&#35299;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#21183;&#26159;&#36825;&#20123;&#35745;&#31639;&#65288;&#23545;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#25152;&#26377;&#31070;&#32463;&#20803;&#65289;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06431</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21270;&#21040;&#22686;&#24378;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20316;&#25991;&#35780;&#20998;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#25509;&#25910;&#21363;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#38750;&#24120;&#37325;&#35201;&#65292;&#24403;&#20154;&#31867;&#25945;&#24072;&#26080;&#27861;&#25552;&#20379;&#26102;&#65292;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#26159;&#19968;&#31181;&#37325;&#35201;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#65292;&#20316;&#20026;AES&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#31361;&#20986;&#20102;LLM AES&#31995;&#32479;&#30340;&#26174;&#30528;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#36229;&#36234;&#20102;&#20256;&#32479;&#35780;&#20998;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;LLM&#36741;&#21161;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#65292;&#28041;&#21450;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#35780;&#20998;&#21592;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21457;&#29616;&#26159;&#65292;LLM&#19981;&#20165;&#33021;&#33258;&#21160;&#21270;&#35780;&#20998;&#36807;&#31243;&#65292;&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;&#24403;&#21021;&#23398;&#32773;&#35780;&#20998;&#21592;&#33719;&#24471;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#27700;&#24179;&#30456;&#24403;&#65292;&#21516;&#26102;&#19987;&#23478;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02576</link><description>&lt;p&gt;
t-DGR: &#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02576
&lt;/p&gt;
&lt;p&gt;
t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#24050;&#32463;&#25104;&#20026;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20174;&#20197;&#21069;&#36935;&#21040;&#30340;&#20219;&#21153;&#29983;&#25104;&#36712;&#36857;&#26469;&#22686;&#21152;&#24403;&#21069;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#30340;&#36712;&#36857;&#20013;&#20250;&#20986;&#29616;&#32047;&#31215;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#19988;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#36712;&#36857;&#26102;&#38388;&#27493;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#22343;&#25104;&#21151;&#29575;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/WilliamYue37/t-DGR&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this paper, we propose a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.16127</link><description>&lt;p&gt;
LLM-SAP: &#22823;&#35821;&#35328;&#27169;&#22411;&#24773;&#26223;&#24863;&#30693;&#30340;&#22522;&#20110;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-SAP: Large Language Model Situational Awareness Based Planning. (arXiv:2312.16127v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35780;&#20272;&#22522;&#20110;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#65288;i&#65289;&#29992;&#20110;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#26032;&#22411;&#22522;&#20934;&#21644;&#25351;&#26631;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#36827;&#23637;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#28436;&#31034;&#20197;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23558;&#20854;&#32435;&#20837;&#21040;&#19968;&#20010;&#22788;&#22659;&#26234;&#33021;&#20307;&#21644;&#33258;&#21160;&#21270;&#35268;&#21010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22266;&#26377;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;-&#23613;&#31649;&#22312;&#27169;&#25311;&#39046;&#22495;&#30340;&#36827;&#27493;&#20013;&#65292;&#22914;&#20309;&#22312;&#27809;&#26377;&#29615;&#22659;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#19990;&#30028;&#29366;&#24577;&#26144;&#23556;&#21040;&#34892;&#21160;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36229;&#20986;&#33539;&#22260;&#65292;&#23545;&#20110;&#39564;&#35777;&#26041;&#27861;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#34920;&#26126;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#23545;&#25193;&#23637;&#35268;&#21010;&#35821;&#26009;&#24211;&#36827;&#34892;&#24494;&#35843;&#21644;&#38024;&#23545;&#24555;&#36895;&#28508;&#22312;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#36890;&#36807;&#20005;&#23494;&#30340;&#27604;&#36739;&#26469;&#30830;&#23454;&#22320;&#23637;&#31034;&#24403;&#21069;&#26041;&#27861;&#30340;&#25215;&#35834;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25512;&#21160;&#20102;&#23545;&#21487;&#38752;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08648</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#37319;&#29992;&#20102;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#20363;&#20135;&#29983;&#30340;&#35823;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#21477;&#27861;&#29305;&#24449;&#19978;&#65292;&#24573;&#35270;&#20102;&#27010;&#24565;&#32423;&#21035;&#30340;&#30740;&#31350;&#65292;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#21644;&#38590;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#27010;&#24565;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20026;&#25991;&#26412;&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25552;&#31034;&#20013;&#36935;&#21040;&#27010;&#24565;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35823;&#30456;&#20851;&#24615;&#26102;&#65292;&#20250;&#37319;&#21462;&#39044;&#27979;&#30340;&#25463;&#24452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#65292;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
&lt;/p&gt;</description></item><item><title>NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01530</link><description>&lt;p&gt;
NOD-TAMP:&#22810;&#27493;&#39588;&#25805;&#32437;&#35268;&#21010;&#20013;&#30340;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01530
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#23621;&#21644;&#24037;&#21378;&#29615;&#22659;&#20013;&#24320;&#21457;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#26102;&#31243;&#20219;&#21153;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#32437;&#20197;&#21450;&#38656;&#35201;&#22312;&#21508;&#31181;&#29289;&#20307;&#24418;&#29366;&#21644;&#22330;&#26223;&#24067;&#23616;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#30340;&#20551;&#35774;&#65292;&#22914;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26032;&#39062;&#32972;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#65288;NODs&#65289;&#22312;&#29289;&#20307;&#21644;&#22330;&#26223;&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;NOD-TAMP&#20174;&#23569;&#25968;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#30701;&#30340;&#25805;&#32437;&#36712;&#36857;&#65292;&#20351;&#29992;NOD&#29305;&#24449;&#26469;&#35843;&#25972;&#36825;&#20123;&#36712;&#36857;&#65292;&#24182;&#32452;&#21512;&#23427;&#20204;&#26469;&#35299;&#20915;&#24191;&#27867;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;NOD-TAMP&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25805;&#32437;&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12477</link><description>&lt;p&gt;
&#23545;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of In-Context Learning for Speech Language Model. (arXiv:2310.12477v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#26469;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;LM&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26799;&#24230;&#19979;&#38477;&#25110;&#35201;&#27714;&#26174;&#24335;&#20462;&#25913;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36825;&#20351;&#24471;LM&#33021;&#20197;&#40657;&#30418;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#35843;&#25972;&#12290;&#23613;&#31649;ICL&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#65292;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;ICL&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;LM&#30340;ICL&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#35821;&#38899;LM&#27809;&#26377;ICL&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#28909;&#36523;&#35757;&#32451;&#65292;&#35821;&#38899;LM&#22240;&#27492;&#21487;&#20197;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#25191;&#34892;ICL&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#35821;&#38899;LM&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an important role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to learn and adapt in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study proposes the first exploration of ICL with a speech LM without text supervision. We first show that the current speech LM does not have the ICL capability. With the proposed warmup training, the speech LM can, therefore, perform ICL on unseen tasks. In this work, we verify the feasibility of ICL for speech LM on speech classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.08795</link><description>&lt;p&gt;
&#36890;&#36807;&#36861;&#36394;&#20559;&#35265;&#24433;&#21709;&#26469;&#20943;&#36731;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#27169;&#22411;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#32780;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#36755;&#20986;&#30340;&#31572;&#26696;&#21487;&#33021;&#30452;&#25509;&#34987;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#12290;&#24050;&#32463;&#26377;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;QA&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20294;&#26159;&#23545;&#20110;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#20173;&#22788;&#20110;&#25506;&#32034;&#38454;&#27573;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22522;&#20110;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#20174;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20363;&#23376;&#20013;&#23398;&#21040;&#20102;&#19996;&#35199;&#65292;&#23427;&#21487;&#33021;&#26356;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#26469;&#34913;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22914;&#26524;&#21463;&#21040;&#24433;&#21709;&#30340;&#23454;&#20363;&#26356;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26597;&#35810;&#23454;&#20363;&#26159;&#26377;&#20559;&#35265;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#20559;&#35265;&#31243;&#24230;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#38500;&#20102;&#21407;&#26469;&#30340;QA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20840;&#38754;&#32780;&#25935;&#24863;&#30340;&#26041;&#24335;&#37327;&#21270;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#36731;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03965</link><description>&lt;p&gt;
&#24605;&#32500;&#20256;&#25773;&#65306;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#26041;&#27861;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#27861;&#37325;&#29992;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#32047;&#31215;&#20102;&#38169;&#35823;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#20174;&#38646;&#24320;&#22987;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#20256;&#25773;&#8221;&#65288;TP&#65289;&#65292;&#23427;&#25506;&#32034;&#31867;&#20284;&#38382;&#39064;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#27604;&#38382;&#39064;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#65292;&#20855;&#26377;&#21487;&#37325;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23558;&#35299;&#20915;&#20808;&#21069;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#20256;&#25773;&#20197;&#28608;&#21457;&#26032;&#30340;&#38382;&#39064;&#35299;&#20915;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;TP&#39318;&#20808;&#25552;&#31034;LLMs&#25552;&#20986;&#24182;&#35299;&#20915;&#19968;&#32452;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;TP&#37325;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#32467;&#26524;&#30452;&#25509;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32773;&#25512;&#23548;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.15242</link><description>&lt;p&gt;
PlotMap&#65306;&#29992;&#20110;&#26500;&#24314;&#28216;&#25103;&#19990;&#30028;&#30340;&#33258;&#21160;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PlotMap: Automated Layout Design for Building Game Worlds. (arXiv:2309.15242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#26159;&#24320;&#21457;&#28216;&#25103;&#30340;&#21465;&#20107;&#21644;&#29289;&#29702;&#19990;&#30028;&#30340;&#36807;&#31243;&#65292;&#22312;&#28216;&#25103;&#20307;&#39564;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22791;&#21463;&#22909;&#35780;&#30340;&#29420;&#31435;&#28216;&#25103;&#21644;AAA&#32423;&#35270;&#39057;&#28216;&#25103;&#34987;&#36190;&#36175;&#20854;&#20248;&#31168;&#30340;&#19990;&#30028;&#26500;&#24314;&#65292;&#20854;&#20013;&#28216;&#25103;&#22320;&#22270;&#19982;&#21465;&#20107;&#32039;&#23494;&#34701;&#21512;&#24182;&#25552;&#21319;&#20102;&#28216;&#25103;&#20307;&#39564;&#65292;&#21560;&#24341;&#20102;&#29609;&#23478;&#24182;&#30041;&#19979;&#20102;&#28145;&#21051;&#30340;&#21360;&#35937;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28385;&#36275;&#21508;&#31181;&#32771;&#34385;&#22240;&#32032;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#28216;&#25103;&#26426;&#21046;&#25110;&#22320;&#22270;&#22320;&#24418;&#30340;&#32771;&#34385;&#65292;&#32780;&#24573;&#35270;&#20102;&#25903;&#25345;&#25925;&#20107;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#35774;&#35745;&#20986;&#36866;&#24212;&#29305;&#23450;&#25925;&#20107;&#30340;&#28216;&#25103;&#19990;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#27969;&#31243;&#20013;&#24341;&#20837;&#19968;&#20010;&#19982;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#26080;&#20851;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#39069;&#22806;&#23618;&#38754;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
World-building, the process of developing both the narrative and physical world of a game, plays a vital role in the game's experience. Critically acclaimed independent and AAA video games are praised for strong world building, with game maps that masterfully intertwine with and elevate the narrative, captivating players and leaving a lasting impression. However, designing game maps that support a desired narrative is challenging, as it requires satisfying complex constraints from various considerations. Most existing map generation methods focus on considerations about gameplay mechanics or map topography, while the need to support the story is typically neglected. As a result, extensive manual adjustment is still required to design a game world that facilitates particular stories. In this work, we approach this problem by introducing an extra layer of plot facility layout design that is independent of the underlying map generation method in a world-building pipeline. Concretely, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>IDVT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38754;&#21521;&#31038;&#20132;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20852;&#36259;&#24863;&#30693;&#21435;&#22122;&#21644;&#35270;&#22270;&#24341;&#23548;&#35843;&#33410;&#26469;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15926</link><description>&lt;p&gt;
IDVT: &#38754;&#21521;&#31038;&#20132;&#25512;&#33616;&#30340;&#20852;&#36259;&#24863;&#30693;&#21435;&#22122;&#21644;&#35270;&#22270;&#24341;&#23548;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
IDVT: Interest-aware Denoising and View-guided Tuning for Social Recommendation. (arXiv:2308.15926v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15926
&lt;/p&gt;
&lt;p&gt;
IDVT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38754;&#21521;&#31038;&#20132;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20852;&#36259;&#24863;&#30693;&#21435;&#22122;&#21644;&#35270;&#22270;&#24341;&#23548;&#35843;&#33410;&#26469;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26102;&#20195;&#65292;&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#39640;&#25928;&#36807;&#28388;&#20449;&#24687;&#21644;&#35782;&#21035;&#29992;&#25143;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#36890;&#36807;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36741;&#21161;&#20449;&#24687;&#20016;&#23500;&#20102;&#36825;&#20123;&#31995;&#32479;&#12290;&#31038;&#20132;&#36830;&#25509;&#30340;&#29992;&#25143;&#34987;&#35748;&#20026;&#20855;&#26377;&#30456;&#20284;&#30340;&#20559;&#22909;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26576;&#20123;&#31038;&#20132;&#36830;&#25509;&#23454;&#38469;&#19978;&#20250;&#25439;&#23475;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#20998;&#26512;&#34920;&#26126;&#65292;&#31038;&#20132;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#22122;&#22768;&#65292;&#35768;&#22810;&#31038;&#20132;&#36830;&#25509;&#30340;&#29992;&#25143;&#24182;&#19981;&#20849;&#20139;&#30456;&#21516;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38754;&#21521;&#31038;&#20132;&#25512;&#33616;&#30340;&#20852;&#36259;&#24863;&#30693;&#21435;&#22122;&#21644;&#35270;&#22270;&#24341;&#23548;&#35843;&#33410;&#65288;IDVT&#65289;&#26041;&#27861;&#12290;&#31532;&#19968;&#20010;&#21435;&#22122;&#37096;&#20998;&#26377;&#25928;&#22320;&#21435;&#38500;&#20102;&#31038;&#20132;&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21435;&#22122;&#36807;&#31243;&#32771;&#34385;&#20102;&#31038;&#20132;&#32593;&#32476;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the information age, recommendation systems are vital for efficiently filtering information and identifying user preferences. Online social platforms have enriched these systems by providing valuable auxiliary information. Socially connected users are assumed to share similar preferences, enhancing recommendation accuracy and addressing cold start issues. However, empirical findings challenge the assumption, revealing that certain social connections can actually harm system performance. Our statistical analysis indicates a significant amount of noise in the social network, where many socially connected users do not share common interests. To address this issue, we propose an innovative \underline{I}nterest-aware \underline{D}enoising and \underline{V}iew-guided \underline{T}uning (IDVT) method for the social recommendation. The first ID part effectively denoises social connections. Specifically, the denoising process considers both social network structure and user interaction inter
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Fast-MpoxNet&#65292;&#29992;&#20110;&#26089;&#26399;&#29492;&#30168;&#35786;&#26029;&#12290;&#23427;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#37327;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13492</link><description>&lt;p&gt;
&#36229;&#24555;&#36229;&#36731;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26234;&#33021;&#30417;&#27979;&#31995;&#32479;&#29992;&#20110;&#22312;&#20219;&#20309;&#26102;&#38388;&#21644;&#22320;&#28857;&#35786;&#26029;&#26089;&#26399;&#29492;&#30168;
&lt;/p&gt;
&lt;p&gt;
Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere. (arXiv:2308.13492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13492
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Fast-MpoxNet&#65292;&#29992;&#20110;&#26089;&#26399;&#29492;&#30168;&#35786;&#26029;&#12290;&#23427;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#37327;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#26356;&#39640;&#25928;&#30340;&#29492;&#30168;&#35786;&#26029;&#24037;&#20855;&#65292;&#20854;&#20256;&#25773;&#20173;&#28982;&#26410;&#21463;&#25511;&#21046;&#65292;&#32473;&#20840;&#29699;&#20581;&#24247;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#30456;&#20851;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29492;&#30168;&#35786;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#20294;&#26159;&#23545;&#20110;&#26089;&#26399;&#29492;&#30168;&#30340;&#25512;&#29702;&#36895;&#24230;&#12289;&#21442;&#25968;&#22823;&#23567;&#21644;&#35786;&#26029;&#24615;&#33021;&#30340;&#24573;&#35270;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#32593;&#32476;&#65292;&#21517;&#20026;Fast-MpoxNet&#12290;Fast-MpoxNet&#21482;&#26377;0.27M&#20010;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;CPU&#19978;&#20197;&#27599;&#31186;68&#24103;&#30340;&#36895;&#24230;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#23567;&#27169;&#22411;&#23481;&#37327;&#24102;&#26469;&#30340;&#35786;&#26029;&#24615;&#33021;&#38480;&#21046;&#65292;&#23427;&#38598;&#25104;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#21644;&#22810;&#20010;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#24494;&#23567;&#30340;&#22270;&#20687;&#21464;&#21270;&#21644;&#20248;&#21270;&#26435;&#37325;&#12290;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;Fast-MpoxNet&#23454;&#29616;&#20102;94.26%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of more efficient diagnostic tools for monkeypox, its spread remains unchecked, presenting a formidable challenge to global health. While the high efficacy of deep learning models for monkeypox diagnosis has been demonstrated in related studies, the overlook of inference speed, the parameter size and diagnosis performance for early-stage monkeypox renders the models inapplicable in real-world settings. To address these challenges, we proposed an ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possesses only 0.27M parameters and can process input images at 68 frames per second (FPS) on the CPU. To counteract the diagnostic performance limitation brought about by the small model capacity, it integrates the attention-based feature fusion module and the multiple auxiliary losses enhancement strategy for better detecting subtle image changes and optimizing weights. Using transfer learning and five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05213</link><description>&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#37117;&#21253;&#21547;&#38656;&#35201;&#22312;&#35299;&#20915;&#20043;&#21069;&#36827;&#34892;&#39044;&#27979;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#20026;&#20102;&#35757;&#32451;&#28041;&#21450;&#30340;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#12290;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;&#24335;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#20219;&#21153;&#25439;&#22833;&#26469;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;DFL&#26041;&#27861;&#21463;&#21040;&#23427;&#20204;&#23545;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#38382;&#39064;&#26159;&#32447;&#24615;&#30340;&#65289;&#20197;&#21450;&#21482;&#33021;&#39044;&#27979;&#20986;&#29616;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#30340;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30456;&#21453;&#22320;&#39044;&#27979;&#21442;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#65288;SFGE&#65289;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#25193;&#22823;DFL&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiment
&lt;/p&gt;</description></item><item><title>HEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#20999;&#21475;&#30109;&#20462;&#22797;&#30340;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#32771;&#34385;&#33145;&#22721;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#21644;&#35780;&#20272;&#30109;&#30340;&#22823;&#23567;&#12289;&#20307;&#31215;&#21644;&#33145;&#22721;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;31&#21517;&#24739;&#32773;&#30340;&#39044;&#25163;&#26415;&#35780;&#20272;&#20013;&#65292;HEDI&#26174;&#31034;&#20986;&#26126;&#26174;&#25913;&#21892;&#30340;&#25104;&#21151;&#29575;&#65292;&#25152;&#26377;&#24739;&#32773;&#22312;&#38543;&#35775;&#19977;&#24180;&#21518;&#20173;&#28982;&#27809;&#26377;&#30140;&#30171;&#21644;&#30109;&#20877;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.01502</link><description>&lt;p&gt;
HEDI: &#31532;&#19968;&#27425;&#20020;&#24202;&#24212;&#29992;&#30340;&#20999;&#21475;&#30109;&#20462;&#22797;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair. (arXiv:2307.01502v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01502
&lt;/p&gt;
&lt;p&gt;
HEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#20999;&#21475;&#30109;&#20462;&#22797;&#30340;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#32771;&#34385;&#33145;&#22721;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#21644;&#35780;&#20272;&#30109;&#30340;&#22823;&#23567;&#12289;&#20307;&#31215;&#21644;&#33145;&#22721;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;31&#21517;&#24739;&#32773;&#30340;&#39044;&#25163;&#26415;&#35780;&#20272;&#20013;&#65292;HEDI&#26174;&#31034;&#20986;&#26126;&#26174;&#25913;&#21892;&#30340;&#25104;&#21151;&#29575;&#65292;&#25152;&#26377;&#24739;&#32773;&#22312;&#38543;&#35775;&#19977;&#24180;&#21518;&#20173;&#28982;&#27809;&#26377;&#30140;&#30171;&#21644;&#30109;&#20877;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33145;&#22721;&#32570;&#38519;&#36890;&#24120;&#23548;&#33268;&#30140;&#30171;&#12289;&#19981;&#36866;&#20197;&#21450;&#20999;&#21475;&#30109;&#20877;&#21457;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#36896;&#25104;&#37325;&#22823;&#21457;&#30149;&#29575;&#21644;&#22810;&#27425;&#25163;&#26415;&#20462;&#22797;&#12290;&#23545;&#20110;&#22823;&#22411;&#30109;&#65292;&#32593;&#26684;&#20462;&#22797;&#36890;&#24120;&#22522;&#20110;&#32570;&#38519;&#21306;&#22495;&#19982;&#22266;&#23450;&#37325;&#21472;&#65292;&#32780;&#19981;&#32771;&#34385;&#29983;&#29289;&#21147;&#23398;&#26041;&#38754;&#30340;&#22240;&#32032;&#65292;&#22914;&#32908;&#32905;&#28608;&#27963;&#12289;&#33145;&#33108;&#20869;&#21387;&#21147;&#12289;&#32452;&#32455;&#24377;&#24615;&#21644;&#33145;&#22721;&#25193;&#24352;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#31283;&#23450;&#33145;&#22721;&#30340;&#20999;&#21475;&#30109;&#20462;&#22797;&#30340;&#29983;&#29289;&#21147;&#23398;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HEDI&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Valsalva&#21160;&#20316;&#30340;&#21160;&#24577;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25216;&#26415;&#26469;&#33258;&#21160;&#26816;&#27979;&#21644;&#35780;&#20272;&#30109;&#22823;&#23567;&#12289;&#20307;&#31215;&#21644;&#33145;&#22721;&#19981;&#31283;&#23450;&#24615;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;31&#21517;&#24739;&#32773;&#39044;&#25163;&#26415;&#35780;&#20272;&#20013;&#39318;&#27425;&#20020;&#24202;&#24212;&#29992;&#20102;HEDI&#65292;&#19982;&#25253;&#36947;&#30340;&#25104;&#21151;&#29575;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#25913;&#21892;&#65292;&#25152;&#26377;&#24739;&#32773;&#22312;&#38543;&#35775;&#19977;&#24180;&#21518;&#20173;&#28982;&#27809;&#26377;&#30140;&#30171;&#21644;&#30109;&#20877;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abdominal wall defects often lead to pain, discomfort, and recurrence of incisional hernias, resulting in significant morbidity and repeated surgical repairs worldwide. Mesh repair for large hernias is usually based on the defect area with a fixed overlap, without considering biomechanical aspects such as muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal wall distention. To address this issue, we present a biomechanical approach to incisional hernia repair that takes into account the unstable abdominal wall. Additionally, we introduce HEDI, a tool that uses dynamic computed tomography with Valsalva maneuver to automatically detect and assess hernia size, volume, and abdominal wall instability. Our first clinical application of HEDI in the preoperative evaluation of 31 patients shows significantly improved success rates compared to reported rates, with all patients remaining pain-free and showing no hernia recurrence after three years of follow-up.
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#26500;&#65292;&#23558;&#21453;&#21521;&#20256;&#25773;&#21644;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#23398;&#20064;&#21644;&#24341;&#23548;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02415</link><description>&lt;p&gt;
&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#23558;&#21453;&#21521;&#20256;&#25773;&#19982;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Top-Down Network Combines Back-Propagation with Attention. (arXiv:2306.02415v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#26500;&#65292;&#23558;&#21453;&#21521;&#20256;&#25773;&#21644;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#23398;&#20064;&#21644;&#24341;&#23548;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22312;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#30382;&#23618;&#22788;&#29702;&#23558;&#33258;&#19979;&#32780;&#19978;&#30340;&#22788;&#29702;&#19982;&#24191;&#27867;&#30340;&#33258;&#39030;&#21521;&#19979;&#22788;&#29702;&#30456;&#32467;&#21512;&#12290;&#33258;&#39030;&#21521;&#19979;&#22788;&#29702;&#30340;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#21644;&#24341;&#23548;&#27880;&#24847;&#21147;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#21046;&#23454;&#29616;&#36825;&#20004;&#20010;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#24341;&#23548;&#36890;&#24120;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#30340;&#32467;&#26500;&#26469;&#23454;&#29616;&#65292;&#32780;&#23398;&#20064;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#21453;&#21521;&#20256;&#25773;&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#19978;&#36848;&#20004;&#20010;&#30475;&#20284;&#26080;&#20851;&#30340;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#20154;&#33041;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#31216;&#33258;&#19979;&#32780;&#19978;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#23558;&#20256;&#32479;&#30340;&#33258;&#19979;&#32780;&#19978;&#32593;&#32476;&#19982;&#23545;&#31216;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#27599;&#20010;&#32593;&#32476;&#21487;&#20197;&#20114;&#30456;&#24490;&#29615;&#22320;&#24341;&#23548;&#21644;&#24433;&#21709;&#23545;&#26041;&#12290;&#20363;&#22914;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#21516;&#19968;&#20010;&#33258;&#39030;&#21521;&#19979;&#32593;&#32476;&#34987;&#29992;&#20110;&#23398;&#20064;&#21644;&#36890;&#36807;&#20256;&#36882;&#21453;&#39304;&#20449;&#21495;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cortical processing, in vision and other domains, combines bottom-up (BU) with extensive top-down (TD) processing. Two primary goals attributed to TD processing are learning and directing attention. These two roles are accomplished in current network models through distinct mechanisms. Attention guidance is often implemented by extending the model's architecture, while learning is typically accomplished by an external learning algorithm such as back-propagation. In the current work, we present an integration of the two functions above, which appear unrelated, using a single unified mechanism inspired by the human brain. We propose a novel symmetric bottom-up top-down network structure that can integrate conventional bottom-up networks with a symmetric top-down counterpart, allowing each network to recurrently guide and influence the other. For example, during multi-task learning, the same top-down network is being used for both learning, via propagating feedback signals, and at the sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.03518</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#20351;&#29992;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32593;&#32476;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;LLMs&#19978;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#36825;&#19982;&#19981;&#24688;&#24403;&#30340;&#23376;&#31354;&#38388;&#36873;&#25321;&#26377;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26469;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#12290;&#22522;&#20110;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#20197;&#30830;&#23450;&#23376;&#31354;&#38388;&#21487;&#20197;&#35782;&#21035;&#31867;&#20284;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#35777;&#22312;&#30456;&#20284;&#20219;&#21153;&#19978;&#36827;&#34892;&#23376;&#31354;&#38388;&#20248;&#21270;&#25214;&#21040;&#19968;&#20010;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BSL&#26694;&#26550;&#26080;&#35770;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02012</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35780;&#36848;&#65306;SHAP &#21644; LIME
&lt;/p&gt;
&lt;p&gt;
Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#20986;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#36716;&#21270;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#20256;&#36798;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36879;&#26126;&#65292;&#24182;&#22686;&#21152;&#26368;&#32456;&#29992;&#25143;&#23545;&#20854;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290; SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;Local Interpretable Model Agnostic Explanation&#65288;LIME&#65289;&#26159;&#20004;&#31181;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#36755;&#20986;&#30340;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; H2TNE &#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#19982;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.06970</link><description>&lt;p&gt;
H2TNE&#65306;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
H2TNE: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces. (arXiv:2304.06970v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; H2TNE &#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#19982;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;temporal HIN&#65289;&#23884;&#20837;&#65292;&#26088;&#22312;&#23558;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#34920;&#31034;&#20026;&#20302;&#32500;&#31354;&#38388;&#65292;&#24182;&#21516;&#26102;&#20445;&#30041;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#35768;&#22810;&#20851;&#20110;&#26102;&#38388;HIN&#23884;&#20837;&#30340;&#21162;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#20123;&#21487;&#35266;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#32593;&#32476;&#37117;&#26174;&#31034;&#20986;&#20998;&#23618;&#23646;&#24615;&#21644;&#24130;&#24459;&#20998;&#24067;&#65292;&#24182;&#19981;&#26159;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#31561;&#36317;&#30340;&#12290;&#26368;&#36817;&#65292;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23545;&#20855;&#26377;&#20998;&#23618;&#21644;&#24130;&#24459;&#32467;&#26500;&#30340;&#25968;&#25454;&#26159;&#26377;&#25928;&#30340;&#12290;&#21463;&#36825;&#20010;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#26354;&#24322;&#26500;&#26102;&#38388;&#32593;&#32476;&#23884;&#20837;&#65288;H2TNE&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#24577;HIN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#26469;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#28982;&#21518;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Temporal heterogeneous information network (temporal HIN) embedding, aiming to represent various types of nodes of different timestamps into low dimensional spaces while preserving structural and semantic information, is of vital importance in diverse real-life tasks. Researchers have made great efforts on temporal HIN embedding in Euclidean spaces and got some considerable achievements. However, there is always a fundamental conflict that many real-world networks show hierarchical property and power-law distribution, and are not isometric of Euclidean spaces. Recently, representation learning in hyperbolic spaces has been proved to be valid for data with hierarchical and power-law structure. Inspired by this character, we propose a hyperbolic heterogeneous temporal network embedding (H2TNE) model for temporal HINs. Specifically, we leverage a temporally and heterogeneously double-constrained random walk strategy to capture the structural and semantic information, and then calculate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#20351;&#29992;ChatGPT&#30340;&#21033;&#24330;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13856</link><description>&lt;p&gt;
ChatGPT&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#24212;&#29992;&#65306;&#25327;&#25937;&#32773;&#36824;&#26159;&#27585;&#28781;&#32773;?
&lt;/p&gt;
&lt;p&gt;
Unleasing ChatGPT on the Metaverse: Savior or Destroyer?. (arXiv:2303.13856v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#20351;&#29992;ChatGPT&#30340;&#21033;&#24330;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#8220;&#20803;&#23431;&#23449;&#8221;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#27785;&#28024;&#20307;&#39564;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#32780;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#20854;&#20013;&#65292;&#19968;&#20010;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#26159;ChatGPT&#65292;&#36825;&#26159;OpenAI&#35757;&#32451;&#30340;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#35814;&#32454;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#24341;&#20837;ChatGPT&#30340;&#21033;&#24330;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#23089;&#20048;&#12289;&#20010;&#24615;&#21270;&#21644;&#25903;&#25345;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#36825;&#20123;&#26426;&#36935;&#21644;&#38556;&#30861;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;ChatGPT&#23545;&#20803;&#23431;&#23449;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#21019;&#24314;&#26356;&#21152;&#27785;&#28024;&#21644;&#26377;&#36259;&#30340;&#34394;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incorporation of artificial intelligence (AI) technology, and in particular natural language processing (NLP), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. One such artificial intelligence tool that is gaining traction in the metaverse is ChatGPT, a large language model trained by OpenAI. The article delves into the pros and cons of utilizing ChatGPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of ChatGPT on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.
&lt;/p&gt;</description></item></channel></rss>