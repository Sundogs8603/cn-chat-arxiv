<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.01043</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models for Autonomous Driving. (arXiv:2311.01043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20316;&#20026;&#25913;&#21464;&#20132;&#36890;&#21644;&#22478;&#24066;&#27969;&#21160;&#24615;&#30340;&#20652;&#21270;&#21058;&#65292;&#27491;&#36235;&#21521;&#20110;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36716;&#21521;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21463;&#21040;&#32423;&#32852;&#27169;&#22359;&#20013;&#30340;&#32047;&#31215;&#35823;&#24046;&#21644;&#19981;&#28789;&#27963;&#30340;&#39044;&#35774;&#35268;&#21017;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#36807;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26377;&#28508;&#21147;&#36991;&#20813;&#38169;&#35823;&#32047;&#31215;&#65292;&#23613;&#31649;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#20915;&#31574;&#30340;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#21464;&#24471;&#22797;&#26434;&#12290;&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#29702;&#35299;&#32972;&#26223;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#31561;&#33021;&#21147;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#36171;&#20104;&#33258;&#21160;&#39550;&#39542;&#20197;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;LLM&#19982;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#32467;&#21512;&#65292;&#21487;&#33021;&#25171;&#24320;&#23545;&#24320;&#25918;&#19990;&#30028;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#38376;&#65292;&#36825;&#26159;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#25918;&#22312;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#20301;&#32622;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#22522;&#20934;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.16379</link><description>&lt;p&gt;
&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating General-Purpose AI with Psychometrics. (arXiv:2310.16379v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#25918;&#22312;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#20301;&#32622;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#22522;&#20934;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#20174;&#29305;&#23450;&#20219;&#21153;&#21521;&#36890;&#29992;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#36235;&#21521;&#20110;&#20154;&#31867;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#38543;&#30528;AI&#31995;&#32479;&#24320;&#22987;&#22312;&#31038;&#20250;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#30830;&#20445;&#23545;&#20854;&#36827;&#34892;&#20805;&#20998;&#35780;&#20272;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;AI&#22522;&#20934;&#36890;&#24120;&#22312;&#29305;&#23450;&#20219;&#21153;&#38598;&#21512;&#19978;&#35780;&#20272;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35780;&#20272;&#36890;&#29992;AI&#31995;&#32479;&#26469;&#35828;&#65292;&#36825;&#26377;&#19968;&#20123;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#24456;&#38590;&#39044;&#27979;AI&#31995;&#32479;&#26159;&#21542;&#33021;&#23436;&#25104;&#19968;&#39033;&#23427;&#20174;&#26410;&#35265;&#36807;&#25110;&#20043;&#21069;&#19981;&#23384;&#22312;&#30340;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#22522;&#20934;&#24120;&#24120;&#20851;&#27880;&#25972;&#20307;&#24615;&#33021;&#25351;&#26631;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#23545;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#23545;&#29616;&#26377;&#22522;&#20934;&#30340;&#21487;&#38752;&#24615;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#25285;&#24551;&#65292;&#24182;&#23545;&#27491;&#22312;&#36827;&#34892;&#30340;&#27979;&#37327;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24314;&#35758;&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#65292;&#21363;&#24515;&#29702;&#27979;&#37327;&#30340;&#31185;&#23398;&#65292;&#25918;&#22312;&#35780;&#20272;&#36890;&#29992;AI&#30340;&#26680;&#24515;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has witnessed an evolution from task-specific to general-purpose systems that trend toward human versatility. As AI systems begin to play pivotal roles in society, it is important to ensure that they are adequately evaluated. Current AI benchmarks typically assess performance on collections of specific tasks. This has drawbacks when used for assessing general-purpose AI systems. First, it is difficult to predict whether AI systems could complete a new task it has never seen or that did not previously exist. Second, these benchmarks often focus on overall performance metrics, potentially overlooking the finer details crucial for making informed decisions. Lastly, there are growing concerns about the reliability of existing benchmarks and questions about what is being measured. To solve these challenges, this paper suggests that psychometrics, the science of psychological measurement, should be placed at the core of evaluating general-purpose AI. Psychometric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;RUAD&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04693</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#24102;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#30340;&#25552;&#21319;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization. (arXiv:2310.04693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;RUAD&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#22312;&#22312;&#32447;&#33829;&#38144;&#20013;&#23637;&#31034;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#20013;&#23481;&#26131;&#21463;&#21040;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#19978;&#36848;&#29616;&#35937;&#32473;&#20986;&#20102;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#22312;&#32447;&#33829;&#38144;&#20013;&#23384;&#22312;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#19968;&#20123;&#20851;&#38190;&#29305;&#24449;&#30340;&#25200;&#21160;&#20250;&#20005;&#37325;&#24433;&#21709;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#23548;&#33268;&#30456;&#21453;&#30340;&#36235;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;&#65288;RUAD&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;RUAD&#36890;&#36807;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20943;&#36731;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#65292;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#32852;&#21512;&#22810;&#26631;&#31614;&#24314;&#27169;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#65292;&#20197;&#20174;&#36755;&#20837;&#29305;&#24449;&#20013;&#35782;&#21035;&#19968;&#20010;&#20851;&#38190;&#23376;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#36719;&#25554;&#20540;&#25805;&#20316;&#30340;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling has shown very promising results in online marketing. However, most existing works are prone to the robustness challenge in some practical applications. In this paper, we first present a possible explanation for the above phenomenon. We verify that there is a feature sensitivity problem in online marketing using different real-world datasets, where the perturbation of some key features will seriously affect the performance of the uplift model and even cause the opposite trend. To solve the above problem, we propose a novel robustness-enhanced uplift modeling framework with adversarial feature desensitization (RUAD). Specifically, our RUAD can more effectively alleviate the feature sensitivity of the uplift model through two customized modules, including a feature selection module with joint multi-label modeling to identify a key subset from the input features and an adversarial feature desensitization module using adversarial training and soft interpolation operations t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15427</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#31070;&#32463;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21644;&#23450;&#21046;&#27169;&#22411;&#26550;&#26500;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#26159;&#23558;&#27492;&#24212;&#29992;&#20110;LLMs&#23384;&#22312;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#24182;&#36991;&#20813;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#33258;&#23450;&#20041;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#30340;LLMs&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#12290;GNP&#21253;&#25324;&#21508;&#31181;&#35774;&#35745;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#12289;&#36328;&#27169;&#24577;&#27719;&#32858;&#27169;&#22359;&#12289;&#22495;&#25237;&#24433;&#22120;&#21644;&#33258;&#30417;&#30563;&#38142;&#25509;&#39044;&#27979;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;GNP&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.14840</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#20943;&#36731;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating the Security Risks of Generative AI. (arXiv:2308.14840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14840
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#19968;&#39033;&#37325;&#22823;&#25216;&#26415;&#21457;&#26126;&#37117;&#20250;&#24102;&#26469;&#21452;&#37325;&#29992;&#36884;&#30340;&#22256;&#22659; - &#26032;&#25216;&#26415;&#26082;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#21892;&#33391;&#65292;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#34892;&#20026;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20195;&#30721;&#34917;&#20840;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#65289;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21516;&#26679;&#21487;&#20197;&#21033;&#29992;GenAI&#29983;&#25104;&#26032;&#30340;&#25915;&#20987;&#65292;&#24182;&#22686;&#21152;&#29616;&#26377;&#25915;&#20987;&#30340;&#36895;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#22312;Google&#20030;&#21150;&#30340;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#21457;&#29616;&#65288;&#30001;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#23041;&#26031;&#24247;&#26143;&#22823;&#23398;&#40614;&#36842;&#36874;&#20998;&#26657;&#20849;&#21516;&#32452;&#32455;&#65289;&#12290;&#26412;&#25991;&#24182;&#19981;&#24847;&#21619;&#30528;&#20840;&#38754;&#65292;&#32780;&#26159;&#35797;&#22270;&#32508;&#21512;&#19968;&#20123;&#26377;&#36259;&#30340;&#30740;&#35752;&#20250;&#21457;&#29616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#26082;&#20026;&#36825;&#20010;&#37325;&#35201;&#20027;&#39064;&#30340;&#35752;&#35770;&#25552;&#20379;&#19968;&#20010;&#36215;&#28857;&#65292;&#20063;&#24341;&#36215;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.  This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11234</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#20248;&#21270;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#35201;&#27714;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#35745;&#31639;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#25152;&#26377;&#26234;&#33021;&#20307;&#37117;&#22312;&#20849;&#20139;&#22320;&#22270;&#19978;&#31227;&#21160;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#24403;&#21069;&#30340;&#31639;&#27861;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35268;&#21010;&#33258;&#30001;&#27969;&#21160;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36825;&#20250;&#23548;&#33268;&#25317;&#22581;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MAPF&#26041;&#27861;&#65292;&#36890;&#36807;&#36319;&#38543;&#36991;&#20813;&#25317;&#22581;&#30340;&#36335;&#24452;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#21040;&#36798;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#36825;&#20010;&#24819;&#27861;&#65306;&#19968;&#27425;&#24615;MAPF&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#26377;&#19968;&#20010;&#30446;&#30340;&#22320;&#65292;&#20197;&#21450;&#32456;&#36523;MAPF&#65292;&#26234;&#33021;&#20307;&#19981;&#26029;&#34987;&#20998;&#37197;&#26032;&#20219;&#21153;&#12290;&#23545;&#20110;&#19968;&#27425;&#24615;MAPF&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#32456;&#36523;MAPF&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#24635;&#20307;&#21534;&#21520;&#37327;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36317;&#31163;&#24230;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#20174;&#20505;&#36873;&#26550;&#26500;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;/&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25552;&#20379;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.03580</link><description>&lt;p&gt;
&#25581;&#31034;&#28508;&#22312;&#27169;&#24335;&#65306;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#12289;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization. (arXiv:2308.03580v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36317;&#31163;&#24230;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#20174;&#20505;&#36873;&#26550;&#26500;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;/&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25552;&#20379;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#25165;&#33021;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#29992;&#39069;&#22806;&#21644;&#22810;&#26679;&#21270;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12289;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22270;&#20687;-&#22270;&#20687;&#12289;&#25968;&#25454;&#38598;-&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;-&#25968;&#25454;&#38598;&#36317;&#31163;&#65292;&#20197;&#27934;&#23519;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#32467;&#21512;&#27169;&#22411;&#24615;&#33021;&#21487;&#20197;&#24110;&#21161;&#20174;&#20505;&#36873;&#26550;&#26500;&#20013;&#36873;&#25321;&#19968;&#20010;&#21512;&#36866;&#30340;&#27169;&#22411;/&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#38656;&#23558;&#23569;&#37327;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#65288;&#22914;1&#12289;3&#25110;7&#20010;&#65289;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#21363;&#21487;&#25913;&#21892;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20943;&#23569;&#35757;&#32451;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised deep learning models require significant amount of labelled data to achieve an acceptable performance on a specific task. However, when tested on unseen data, the models may not perform well. Therefore, the models need to be trained with additional and varying labelled data to improve the generalization. In this work, our goal is to understand the models, their performance and generalization. We establish image-image, dataset-dataset, and image-dataset distances to gain insights into the model's behavior. Our proposed distance metric when combined with model performance can help in selecting an appropriate model/architecture from a pool of candidate architectures. We have shown that the generalization of these models can be improved by only adding a small number of unseen images (say 1, 3 or 7) into the training set. Our proposed approach reduces training and annotation costs while providing an estimate of model performance on unseen data in dynamic environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#36827;&#34892;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#25506;&#27979;&#20219;&#21153;&#26469;&#30740;&#31350;VLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#23613;&#31649;VLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#21644;&#20301;&#32622;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.06166</link><description>&lt;p&gt;
Vision-Language Models&#33021;&#25104;&#20026;&#33391;&#22909;&#29468;&#27979;&#22120;&#21527;&#65311;&#25506;&#32034;VLMs&#29992;&#20110;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#36827;&#34892;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#25506;&#27979;&#20219;&#21153;&#26469;&#30740;&#31350;VLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#23613;&#31649;VLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#21644;&#20301;&#32622;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;Vision-Language Models&#65288;VLMs&#65289;&#33021;&#20687;&#20154;&#19968;&#26679;&#20855;&#22791;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#65292;&#20154;&#31867;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#30693;&#35782;&#25512;&#26029;&#20986;&#19968;&#24352;&#22270;&#29255;&#30340;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#12290;&#36825;&#35753;&#25105;&#20204;&#24819;&#30693;&#36947;&#65292;&#22522;&#20110;&#35270;&#35273;&#32447;&#32034;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#36164;&#28304;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;Vision-Language Models&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#22312;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#25506;&#27979;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#37492;&#21035;&#24615;&#21644;&#29983;&#25104;&#24615;&#30340;VLMs&#65292;&#20197;&#21457;&#29616;VLMs&#33021;&#21542;&#35782;&#21035;&#20986;&#19982;&#26102;&#38388;&#21644;&#20301;&#32622;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#26041;&#20415;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WikiTiLo&#65292;&#19968;&#20010;&#21253;&#21547;&#20016;&#23500;&#31038;&#20250;&#25991;&#21270;&#32447;&#32034;&#30340;&#31934;&#24515;&#31574;&#21010;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;VLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#30041;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#23436;&#21892;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;...
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location. To address this question, we propose a two-stage \recognition\space and \reasoning\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning. We will release o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item></channel></rss>