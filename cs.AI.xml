<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#36890;&#36807;&#32852;&#24819;&#26469;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#33655;&#20848;&#35821;&#27597;&#35821;&#21644;&#22810;&#35821;&#35328;LLMs&#30340;&#34920;&#29616;&#19982;&#20799;&#31461;&#30456;&#24403;&#65292;&#20294;&#24403;&#25511;&#21046;&#32852;&#24819;&#36807;&#31243;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;1-2&#24180;&#12290;</title><link>http://arxiv.org/abs/2310.20384</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do large language models solve verbal analogies like children do?. (arXiv:2310.20384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#36890;&#36807;&#32852;&#24819;&#26469;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#33655;&#20848;&#35821;&#27597;&#35821;&#21644;&#22810;&#35821;&#35328;LLMs&#30340;&#34920;&#29616;&#19982;&#20799;&#31461;&#30456;&#24403;&#65292;&#20294;&#24403;&#25511;&#21046;&#32852;&#24819;&#36807;&#31243;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;1-2&#24180;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#24605;&#32500;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#12290;&#25104;&#24180;&#20154;&#36890;&#36807;&#26144;&#23556;&#20851;&#31995;&#24182;&#22238;&#31572;&#38382;&#39064;&#65292;&#22914;&#8220;&#39532;&#23646;&#20110;&#39532;&#21417;&#65292;&#40481;&#23646;&#20110;...&#65311;&#8221;&#32780;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#23401;&#23376;&#20204;&#32463;&#24120;&#20351;&#29992;&#32852;&#24819;&#20316;&#31572;&#65292;&#20363;&#22914;&#22238;&#31572;&#8220;&#34507;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#36890;&#36807;&#32852;&#24819;&#26469;&#35299;&#20915;A:B::C:?&#24418;&#24335;&#30340;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#22312;&#32447;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#20013;&#25552;&#21462;&#30340;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65292;&#20854;&#20013;&#26469;&#33258;&#33655;&#20848;&#30340;14,002&#21517;7-12&#23681;&#20799;&#31461;&#35299;&#20915;&#20102;622&#20010;&#33655;&#20848;&#35821;&#30340;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#12290;&#20845;&#20010;&#27979;&#35797;&#30340;&#33655;&#20848;&#35821;&#27597;&#35821;&#21644;&#22810;&#35821;&#35328;LLMs&#30340;&#34920;&#29616;&#19982;&#20799;&#31461;&#22823;&#33268;&#30456;&#24403;&#65292;MGPT&#34920;&#29616;&#26368;&#24046;&#65292;&#25509;&#36817;7&#23681;&#27700;&#24179;&#65292;XLM-V&#21644;GPT-3&#34920;&#29616;&#26368;&#20339;&#65292;&#30053;&#39640;&#20110;11&#23681;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#25511;&#21046;&#32852;&#24819;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21457;&#29983;&#21464;&#21270;&#65292;&#27599;&#20010;&#27169;&#22411;&#30340;&#34920;&#29616;&#27700;&#24179;&#19979;&#38477;1-2&#24180;&#12290;&#36827;&#19968;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#32852;&#24819;&#36807;&#31243;&#30340;&#25511;&#21046;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.20381</link><description>&lt;p&gt;
GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#19981;&#21516;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#39318;&#20010;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;&#22522;&#20934;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#32473;&#20986;&#32467;&#26500;&#33391;&#22909;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4V&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#25581;&#31034;&#20102;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;(&#22914;CIDEr)&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#21307;&#23398;VQA&#39046;&#22495;&#65292;GPT-4V&#22312;&#21306;&#20998;&#38382;&#39064;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#65292;&#20294;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#19981;&#21450;&#29616;&#26377;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#24120;&#35268;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#20998;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#21628;&#21505;&#24320;&#21457;&#26356;&#22909;&#30340;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20303;&#23429;&#30005;&#21147;&#36127;&#33655;&#36718;&#24275;&#36827;&#34892;&#32858;&#31867;&#65292;&#22686;&#24378;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#12290;&#36890;&#36807;&#20351;&#29992;&#20262;&#25958;&#23478;&#24237;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22235;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#27010;&#29575;&#20998;&#31867;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20367</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#23545;&#20303;&#23429;&#30005;&#21147;&#36127;&#33655;&#36718;&#24275;&#36827;&#34892;&#32858;&#31867;&#20197;&#22686;&#24378;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs. (arXiv:2310.20367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20303;&#23429;&#30005;&#21147;&#36127;&#33655;&#36718;&#24275;&#36827;&#34892;&#32858;&#31867;&#65292;&#22686;&#24378;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#12290;&#36890;&#36807;&#20351;&#29992;&#20262;&#25958;&#23478;&#24237;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22235;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#27010;&#29575;&#20998;&#31867;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#34893;&#29983;&#20986;&#30340;&#36127;&#33655;&#26354;&#32447;&#32463;&#24120;&#34987;&#29992;&#20110;&#20998;&#26512;&#26085;&#24120;&#33021;&#28304;&#28040;&#32791;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#31561;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#24037;&#20316;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#35782;&#21035;&#20855;&#26377;&#31867;&#20284;&#28040;&#32791;&#34892;&#20026;&#30340;&#26368;&#21512;&#36866;&#30340;&#28040;&#36153;&#32773;&#32676;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#21033;&#29992;&#20262;&#25958;&#36817;5000&#25143;&#23478;&#24237;&#30340;&#25968;&#25454;&#23454;&#29616;&#26368;&#20339;&#36127;&#33655;&#36718;&#24275;&#12290;&#20855;&#20307;&#24212;&#29992;&#20102;&#22235;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21253;&#25324;K-means&#12289;K-medoids&#12289;&#23618;&#27425;&#20957;&#32858;&#32858;&#31867;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#31354;&#38388;&#32858;&#31867;&#12290;&#36824;&#21033;&#29992;&#32463;&#39564;&#20998;&#26512;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#27010;&#29575;&#20998;&#31867;&#38382;&#39064;&#65292;&#20998;&#31867;&#22120;&#27169;&#25311;&#32858;&#31867;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpre
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.20360</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#65306;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#24102;&#26377;&#25209;&#24402;&#19968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#22522;&#26412;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#12289;&#21152;&#36895;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20960;&#20010;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#31215;&#20998;&#65289;&#12289;&#20248;&#21270;&#29702;&#35770;&#65288;&#21253;&#25324;Kurdyka-Lojasiewicz&#19981;&#31561;&#24335;&#65289;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#20070;&#30340;&#26368;&#21518;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#19968;&#20123;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Galerkin&#26041;&#27861;&#12290;&#24076;&#26395;&#26412;&#20070;&#33021;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#30830;&#30340;&#29289;&#20307;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;&#26597;&#35810;&#20013;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.20357</link><description>&lt;p&gt;
&#25552;&#21319;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model. (arXiv:2310.20357v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#30830;&#30340;&#29289;&#20307;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;&#26597;&#35810;&#20013;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#25351;&#25193;&#23637;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25509;&#25910;&#21644;&#25512;&#26029;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#31354;&#38388;&#24847;&#35782;&#26159;MLLM&#30340;&#20851;&#38190;&#33021;&#21147;&#20043;&#19968;&#65292;&#21253;&#25324;&#20102;&#29702;&#35299;&#29289;&#20307;&#20043;&#38388;&#20197;&#21450;&#29289;&#20307;&#19982;&#22330;&#26223;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#22810;&#31181;&#25216;&#33021;&#12290;&#33258;&#21160;&#39550;&#39542;&#12289;&#26234;&#33021;&#21307;&#30103;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#34394;&#25311;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#31561;&#34892;&#19994;&#23545;MLLM&#30340;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#26377;&#24456;&#22823;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MLLM&#30340;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#19982;&#20154;&#31867;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#31934;&#30830;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#24341;&#23548;MLLM&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#29992;&#25143;&#26597;&#35810;&#21709;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38024;&#23545;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#31639;&#27861;&#33719;&#21462;&#20960;&#20309;&#31354;&#38388;&#20449;&#24687;&#21644;&#22330;&#26223;&#22270;&#26469;&#33719;&#21462;&#30456;&#20851;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#19979;&#32930;18&#20010;&#32908;&#32905;&#30340;&#26041;&#27861;&#65292;&#20197;&#36741;&#21161;&#24418;&#24577;&#27979;&#37327;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28151;&#21512;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#32908;&#32905;&#32452;&#32455;&#26080;&#27861;&#20998;&#36776;&#21644;&#36718;&#24275;&#38590;&#20197;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20355</link><description>&lt;p&gt;
&#32908;&#32905;&#20307;&#31215;&#23450;&#37327;&#21270;&#65306;&#22522;&#20110;&#35299;&#21078;&#20808;&#39564;&#25351;&#23548;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Muscle volume quantification: guiding transformers with anatomical priors. (arXiv:2310.20355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#19979;&#32930;18&#20010;&#32908;&#32905;&#30340;&#26041;&#27861;&#65292;&#20197;&#36741;&#21161;&#24418;&#24577;&#27979;&#37327;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28151;&#21512;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#32908;&#32905;&#32452;&#32455;&#26080;&#27861;&#20998;&#36776;&#21644;&#36718;&#24275;&#38590;&#20197;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#32905;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#23450;&#37327;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#19981;&#20165;&#29992;&#20110;&#36816;&#21160;&#39046;&#22495;&#65292;&#36824;&#29992;&#20110;&#36864;&#34892;&#24615;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#38543;&#35775;&#12290;&#38500;&#20102;&#20307;&#31215;&#22806;&#65292;&#36890;&#36807;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#20998;&#21106;&#24863;&#20852;&#36259;&#30340;&#32908;&#32905;&#65292;&#36824;&#21487;&#20197;&#25552;&#21462;&#20854;&#20182;&#24418;&#29366;&#26631;&#24535;&#29289;&#12290;&#23613;&#31649;&#38750;&#24120;&#32791;&#26102;&#65292;&#20294;&#22312;&#36825;&#31867;&#27979;&#37327;&#20013;&#65292;&#25163;&#24037;&#20998;&#21106;&#20173;&#28982;&#26159;&#37329;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#21106;&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#19979;&#32930;&#30340;18&#20010;&#32908;&#32905;&#65292;&#20197;&#36741;&#21161;&#24418;&#24577;&#27979;&#37327;&#20998;&#26512;&#12290;&#30001;&#20110;&#22312;MR&#22270;&#20687;&#20013;&#35266;&#23519;&#26102;&#65292;&#19981;&#21516;&#32908;&#32905;&#30340;&#32452;&#32455;&#26080;&#27861;&#20998;&#36776;&#12290;&#22240;&#27492;&#65292;&#32908;&#32905;&#20998;&#21106;&#31639;&#27861;&#19981;&#33021;&#20381;&#36182;&#22806;&#35266;&#65292;&#21482;&#33021;&#20381;&#36182;&#36718;&#24275;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#36718;&#24275;&#24456;&#38590;&#26816;&#27979;&#65292;&#24182;&#19988;&#20854;&#21402;&#24230;&#22312;&#21463;&#35797;&#32773;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#22522;&#20110;&#28151;&#21512;&#26550;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#31181;&#28151;&#21512;&#26550;&#26500;&#30340;&#34892;&#20026;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20350</link><description>&lt;p&gt;
&#23558;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#32467;&#21512;&#65292;&#23454;&#29616;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36741;&#21161;&#26426;&#22120;&#20154;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#26159;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26222;&#36866;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#35266;&#27979;&#33021;&#21147;&#26377;&#38480;&#21644;&#21033;&#29992;&#22810;&#25351;&#25163;&#36827;&#34892;&#28789;&#27963;&#25235;&#21462;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24555;&#36895;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#30001;&#22522;&#20110;&#21333;&#20010;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#29289;&#20307;&#24418;&#29366;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#24418;&#29366;&#23436;&#25104;&#32593;&#32476;&#22522;&#20110;VQDIF&#65292;&#22312;&#20219;&#24847;&#26597;&#35810;&#28857;&#19978;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#20540;&#12290;&#20316;&#20026;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#25163;&#23039;&#21183;&#65292;&#28982;&#21518;&#22238;&#24402;&#27599;&#20010;&#23039;&#21183;&#30340;&#25163;&#25351;&#20851;&#33410;&#37197;&#32622;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#36275;&#22815;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22256;&#38590;&#24773;&#20917;&#30340;&#29305;&#27530;&#20851;&#27880;&#12290;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.20327</link><description>&lt;p&gt;
&#20174;&#32858;&#31867;&#35270;&#35282;&#25913;&#36827;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#39046;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23436;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#36935;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#26368;&#23567;&#21270;&#39044;&#27979;&#30340;&#29109;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;EBTTA&#30340;&#26032;&#35270;&#35282;&#21644;&#35299;&#37322;&#12290;&#36825;&#26159;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65306;1&#65289;&#22312;&#20998;&#37197;&#27493;&#39588;&#20013;&#65292;EBTTA&#27169;&#22411;&#30340;&#21069;&#21521;&#36807;&#31243;&#26159;&#20026;&#36825;&#20123;&#27979;&#35797;&#26679;&#26412;&#20998;&#37197;&#26631;&#31614;&#65307;2&#65289;&#22312;&#26356;&#26032;&#27493;&#39588;&#20013;&#65292;&#21453;&#21521;&#36807;&#31243;&#26159;&#36890;&#36807;&#24050;&#20998;&#37197;&#30340;&#26679;&#26412;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;EBTTA&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;EBTTA&#26041;&#27861;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#27604;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA 
&lt;/p&gt;</description></item><item><title>SemanticBoost&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25552;&#21319;&#36816;&#21160;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20174;&#22797;&#26434;&#35821;&#20041;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.20323</link><description>&lt;p&gt;
SemanticBoost&#65306;&#36890;&#36807;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#25552;&#21319;&#36816;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SemanticBoost: Elevating Motion Generation with Augmented Textual Cues. (arXiv:2310.20323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20323
&lt;/p&gt;
&lt;p&gt;
SemanticBoost&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25552;&#21319;&#36816;&#21160;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20174;&#22797;&#26434;&#35821;&#20041;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#25216;&#26415;&#22312;&#20174;&#22797;&#26434;&#30340;&#35821;&#20041;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#35821;&#20041;&#27880;&#37322;&#21644;&#24369;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SemanticBoost&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#21644;&#19978;&#19979;&#25991;&#35843;&#33410;&#36816;&#21160;&#21435;&#22122;&#22120;&#65288;CAMD&#65289;&#12290;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#20174;&#36816;&#21160;&#25968;&#25454;&#20013;&#25552;&#21462;&#34917;&#20805;&#35821;&#20041;&#65292;&#20016;&#23500;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30830;&#20445;&#25991;&#26412;&#21644;&#36816;&#21160;&#25968;&#25454;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#40784;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;CAMD&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#23558;&#29983;&#25104;&#30340;&#36816;&#21160;&#19982;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#35821;&#20041;&#19968;&#33268;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#25104;&#20934;&#30830;&#30340;&#23450;&#21521;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;11&#31181;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#39640;&#32423;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;GPT&#31995;&#21015;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20799;&#31461;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;ToM&#20219;&#21153;&#65292;&#32780;&#35843;&#25972;&#25351;&#20196;&#21017;&#36890;&#36807;&#22870;&#21169;&#21512;&#20316;&#24615;&#27807;&#36890;&#26377;&#21161;&#20110;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20320</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#28789;&#29702;&#35770;&#65306;11&#31181;&#26368;&#26032;&#27169;&#22411;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#39640;&#32423;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests. (arXiv:2310.20320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;11&#31181;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#39640;&#32423;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;GPT&#31995;&#21015;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20799;&#31461;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;ToM&#20219;&#21153;&#65292;&#32780;&#35843;&#25972;&#25351;&#20196;&#21017;&#36890;&#36807;&#22870;&#21169;&#21512;&#20316;&#24615;&#27807;&#36890;&#26377;&#21161;&#20110;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#32473;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22810;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#29702;&#35299;&#24847;&#22270;&#21644;&#20449;&#24565;&#30340;&#29702;&#35770;&#24515;&#28789;&#65288;ToM&#65289;&#33021;&#21147;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#65292;&#20026;&#36825;&#22330;&#26032;&#20852;&#36777;&#35770;&#22686;&#21152;&#19968;&#20123;&#35777;&#25454;&#65306;&#65288;i&#65289;&#27979;&#35797;11&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#25351;&#20196;&#30340;LLMs&#30340;ToM&#30456;&#20851;&#33021;&#21147;&#65292;&#36229;&#36234;&#20027;&#23548;&#30340;&#34394;&#20551;&#20449;&#24565;&#33539;&#24335;&#65292;&#21253;&#25324;&#38750;&#25991;&#23383;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#36882;&#24402;&#30340;&#24847;&#22270;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#26032;&#32534;&#20889;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#29256;&#26412;&#26469;&#35780;&#20272;LLMs&#30340;&#31283;&#20581;&#24615;&#65307;&#65288;iii&#65289;&#25552;&#31034;&#24182;&#35745;&#20998;&#24320;&#25918;&#38382;&#39064;&#21644;&#23553;&#38381;&#38382;&#39064;&#65307;&#65288;iv&#65289;&#23558;LLM&#30340;&#34920;&#29616;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT&#31995;&#21015;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#22312;&#20854;&#20182;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#36890;&#24120;&#20063;&#36229;&#36807;&#20102;&#20799;&#31461;&#30340;&#34920;&#29616;&#12290;&#22522;&#30784;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;ToM&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35821;&#35328;&#21644;ToM&#30340;&#30456;&#20114;&#20851;&#32852;&#24615;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#35843;&#25972;&#25351;&#20196;&#20250;&#22686;&#21152;LLM&#30340;&#24615;&#33021;&#65306;&#22870;&#21169;&#21512;&#20316;&#24615;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;Transformer&#36827;&#34892;&#38646;&#26679;&#26412;&#22240;&#26524;&#21457;&#29616;&#12290;&#36890;&#36807;&#35745;&#31639;&#26368;&#28145;&#27880;&#24847;&#23618;&#20013;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#20026;Transformer&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#22240;&#26524;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.20307</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;Transformer&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Causal Interpretation of Self-Attention in Pre-Trained Transformers. (arXiv:2310.20307v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;Transformer&#36827;&#34892;&#38646;&#26679;&#26412;&#22240;&#26524;&#21457;&#29616;&#12290;&#36890;&#36807;&#35745;&#31639;&#26368;&#28145;&#27880;&#24847;&#23618;&#20013;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#20026;Transformer&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#22240;&#26524;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#27880;&#24847;&#21147;&#35299;&#37322;&#20026;&#19968;&#31181;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#31526;&#21495;&#24207;&#21015;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26426;&#21046;&#12290;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35299;&#37322;&#20173;&#28982;&#26377;&#25928;&#12290;&#26681;&#25454;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#26368;&#28145;&#27880;&#24847;&#23618;&#20013;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#26469;&#20272;&#35745;&#36755;&#20837;&#31526;&#21495;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#20110;&#32422;&#26463;&#30340;&#31639;&#27861;&#23398;&#20064;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;Transformer&#21487;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#20004;&#20010;&#20219;&#21153;&#20013;Transformer&#30340;&#32467;&#26524;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#26469;&#31034;&#33539;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks:
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38598;&#25104;AI&#22522;&#30784;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;&#38761;&#21629;&#24615;&#30340;&#20840;&#29699;&#39135;&#21697;&#23433;&#20840;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#12289;&#25913;&#21892;&#36164;&#28304;&#20998;&#37197;&#21644;&#25903;&#25345;&#26126;&#26234;&#20915;&#31574;&#65292;&#22686;&#24378;&#20102;&#39135;&#21697;&#23433;&#20840;&#20513;&#35758;&#65292;&#23454;&#29616;&#20102;&#23545;&#20840;&#29699;&#39135;&#21697;&#23433;&#20840;&#38480;&#21046;&#30340;&#21464;&#38761;&#24615;&#31361;&#30772;&#12290;</title><link>http://arxiv.org/abs/2310.20301</link><description>&lt;p&gt;
&#38761;&#21629;&#24615;&#30340;&#20840;&#29699;&#39135;&#21697;&#23433;&#20840;&#65306;&#36890;&#36807;&#38598;&#25104;AI&#22522;&#30784;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#22686;&#24378;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions. (arXiv:2310.20301v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20301
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;AI&#22522;&#30784;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;&#38761;&#21629;&#24615;&#30340;&#20840;&#29699;&#39135;&#21697;&#23433;&#20840;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#12289;&#25913;&#21892;&#36164;&#28304;&#20998;&#37197;&#21644;&#25903;&#25345;&#26126;&#26234;&#20915;&#31574;&#65292;&#22686;&#24378;&#20102;&#39135;&#21697;&#23433;&#20840;&#20513;&#35758;&#65292;&#23454;&#29616;&#20102;&#23545;&#20840;&#29699;&#39135;&#21697;&#23433;&#20840;&#38480;&#21046;&#30340;&#21464;&#38761;&#24615;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#23433;&#20840;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#30340;&#20851;&#27880;&#28857;&#65292;&#38656;&#35201;&#31934;&#30830;&#22810;&#26679;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#20854;&#22810;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;AI&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#39135;&#21697;&#23433;&#20840;&#24212;&#29992;&#30340;&#38598;&#25104;&#65292;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#20197;&#20811;&#26381;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#20316;&#29289;&#31867;&#22411;&#26144;&#23556;&#12289;&#20892;&#30000;&#26144;&#23556;&#12289;&#30000;&#22359;&#21010;&#20998;&#21644;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20809;&#35889;&#22270;&#20687;&#12289;&#27668;&#35937;&#25968;&#25454;&#12289;&#22303;&#22756;&#23646;&#24615;&#12289;&#21382;&#21490;&#35760;&#24405;&#21644;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#65292;AI&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;AI&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#12289;&#25913;&#21892;&#36164;&#28304;&#20998;&#37197;&#21644;&#25903;&#25345;&#26126;&#26234;&#20915;&#31574;&#65292;&#22686;&#24378;&#20102;&#39135;&#21697;&#23433;&#20840;&#20513;&#35758;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#20915;&#20840;&#29699;&#39135;&#21697;&#23433;&#20840;&#38480;&#21046;&#26041;&#38754;&#36215;&#21040;&#20102;&#21464;&#38761;&#24615;&#30340;&#20316;&#29992;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#21487;&#25345;&#32493;&#21644;&#23433;&#20840;&#30340;&#26041;&#21521;&#36808;&#36827;&#20102;&#19968;&#22823;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food security, a global concern, necessitates precise and diverse data-driven solutions to address its multifaceted challenges. This paper explores the integration of AI foundation models across various food security applications, leveraging distinct data types, to overcome the limitations of current deep and machine learning methods. Specifically, we investigate their utilization in crop type mapping, cropland mapping, field delineation and crop yield prediction. By capitalizing on multispectral imagery, meteorological data, soil properties, historical records, and high-resolution satellite imagery, AI foundation models offer a versatile approach. The study demonstrates that AI foundation models enhance food security initiatives by providing accurate predictions, improving resource allocation, and supporting informed decision-making. These models serve as a transformative force in addressing global food security limitations, marking a significant leap toward a sustainable and secure f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#32622;&#30340;&#28145;&#24230;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#35299;&#20915;&#22797;&#20301;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20287</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#32622;&#28145;&#24230;&#38598;&#21512;&#20195;&#29702;&#23454;&#29616;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23433;&#20840;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. (arXiv:2310.20287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#32622;&#30340;&#28145;&#24230;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#35299;&#20915;&#22797;&#20301;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;&#20248;&#20808;&#32423;&#20559;&#35265;&#8221;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#36825;&#20123;&#20989;&#25968;&#36924;&#36817;&#22120;&#20542;&#21521;&#20110;&#20248;&#20808;&#32771;&#34385;&#26089;&#26399;&#30340;&#32463;&#39564;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20248;&#20808;&#32423;&#20559;&#35265;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#20445;&#30041;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26041;&#24335;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#20998;&#25110;&#20840;&#37096;&#36827;&#34892;&#21608;&#26399;&#24615;&#37325;&#32622;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#37325;&#32622;&#26041;&#27861;&#21518;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#23849;&#28291;&#65292;&#36825;&#20174;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37325;&#32622;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#38598;&#21512;&#23398;&#20064;&#26469;&#35299;&#20915;&#26222;&#36890;&#37325;&#32622;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#21253;&#25324;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#39564;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numer
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20280</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#22312;BizITOps&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19994;&#21153;&#36807;&#31243;&#30340;&#25928;&#29575;&#20381;&#36182;&#20110;&#19994;&#21153;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;Biz-KPIs&#65289;&#65292;&#32780;IT&#25925;&#38556;&#21487;&#33021;&#23545;&#20854;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;BizITOps&#25968;&#25454;&#23558;Biz-KPIs&#21644;IT&#20107;&#20214;&#36890;&#36947;&#34701;&#21512;&#25104;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25552;&#21069;&#39044;&#27979;Biz-KPIs&#21487;&#20197;&#36890;&#36807;&#20027;&#21160;&#30340;&#32416;&#27491;&#25514;&#26045;&#25552;&#39640;&#25928;&#29575;&#21644;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;BizITOps&#25968;&#25454;&#36890;&#24120;&#23637;&#31034;&#20986;Biz-KPIs&#21644;IT&#20107;&#20214;&#20043;&#38388;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#32806;&#12290;&#24403;&#20351;&#29992;&#29616;&#26377;&#30340;&#22810;&#21464;&#37327;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;AutoMixer&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#12290;AutoMixer&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#36890;&#36947;&#21387;&#32553;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#36827;&#30340;TSMixer&#27169;&#22411;&#38598;&#25104;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36825;&#31181;&#34701;&#21512;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;TSM
&lt;/p&gt;
&lt;p&gt;
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#21040;&#31867;&#21035;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#26356;&#31934;&#32454;&#30340;&#31867;&#21035;&#32423;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26032;&#27010;&#24565;&#30340;&#23398;&#20064;&#21644;&#26087;&#30693;&#35782;&#30340;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2310.20268</link><description>&lt;p&gt;
&#26500;&#24314;&#26679;&#26412;&#21040;&#31867;&#21035;&#22270;&#36827;&#34892;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning. (arXiv:2310.20268v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#21040;&#31867;&#21035;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#26356;&#31934;&#32454;&#30340;&#31867;&#21035;&#32423;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26032;&#27010;&#24565;&#30340;&#23398;&#20064;&#21644;&#26087;&#30693;&#35782;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#20013;&#19981;&#26029;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#21516;&#26102;&#19981;&#20250;&#36951;&#24536;&#26087;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#22312;&#20110;&#26032;&#31867;&#21035;&#30340;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#36825;&#19981;&#20165;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36824;&#20250;&#21152;&#21095;&#33261;&#21517;&#26157;&#33879;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#27491;&#22914;&#26089;&#26399;&#30740;&#31350;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#26500;&#24314;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#20174;&#23569;&#26679;&#26412;&#20013;&#23398;&#20064;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#24605;&#24819;&#25512;&#24191;&#21040;&#22686;&#37327;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26679;&#26412;&#21040;&#31867;&#21035;&#65288;S2C&#65289;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26679;&#26412;&#32423;&#22270;&#32593;&#32476;&#65288;SGN&#65289;&#65292;&#19987;&#27880;&#20110;&#20998;&#26512;&#21333;&#20010;&#20250;&#35805;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#32593;&#32476;&#26377;&#21161;&#20110;&#32858;&#21512;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#26368;&#32456;&#25552;&#21462;&#20986;&#26356;&#31934;&#32454;&#30340;&#31867;&#21035;&#32423;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31867;&#21035;&#32423;&#22270;&#32593;&#32476;&#65288;CGN&#65289;&#65292;&#29992;&#20110;&#24314;&#31435;&#26032;&#31867;&#21035;&#21644;&#26087;&#31867;&#21035;&#30340;&#31867;&#21035;&#32423;&#29305;&#24449;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot class-incremental learning (FSCIL) aims to build machine learning model that can continually learn new concepts from a few data samples, without forgetting knowledge of old classes.  The challenges of FSCIL lies in the limited data of new classes, which not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. As proved in early studies, building sample relationships is beneficial for learning from few-shot samples. In this paper, we promote the idea to the incremental scenario, and propose a Sample-to-Class (S2C) graph learning method for FSCIL.  Specifically, we propose a Sample-level Graph Network (SGN) that focuses on analyzing sample relationships within a single session. This network helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features.  Then, we present a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36229;&#36234;&#24179;&#22343;&#22238;&#25253;&#30340;&#38382;&#39064;&#65292;&#24635;&#32467;&#20102;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#21644;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#26032;&#35268;&#21010;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35770;&#21457;&#23637;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.20266</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36229;&#36234;&#24179;&#22343;&#22238;&#25253;
&lt;/p&gt;
&lt;p&gt;
Beyond Average Return in Markov Decision Processes. (arXiv:2310.20266v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20266
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36229;&#36234;&#24179;&#22343;&#22238;&#25253;&#30340;&#38382;&#39064;&#65292;&#24635;&#32467;&#20102;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#21644;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#26032;&#35268;&#21010;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35770;&#21457;&#23637;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21738;&#20123;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#34987;&#20934;&#30830;&#22320;&#35745;&#31639;&#21644;&#20248;&#21270;&#65311;&#22312;&#26377;&#38480;&#26102;&#38388;&#27573;&#12289;&#26080;&#25240;&#25187;&#35774;&#32622;&#20013;&#65292;&#21160;&#24577;&#35268;&#21010;&#21482;&#33021;&#39640;&#25928;&#22788;&#29702;&#26576;&#20123;&#32479;&#35745;&#31867;&#21035;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#31867;&#21035;&#22312;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#35299;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#65288;DistRL&#65289;&#30340;&#26356;&#19968;&#33324;&#26694;&#26550;&#20013;&#65292;&#21482;&#26377;&#24191;&#20041;&#24179;&#22343;&#20540;&#21487;&#20197;&#34987;&#20934;&#30830;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;DistRL&#20801;&#35768;&#36817;&#20284;&#35780;&#20272;&#20854;&#20182;&#20989;&#25968;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#32467;&#26524;&#20272;&#35745;&#22120;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#27492;&#26041;&#27861;&#30340;&#28508;&#21147;&#21450;&#20854;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36890;&#36807;&#30740;&#31350;&#22238;&#25253;&#30340;&#25972;&#20307;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#39118;&#38505;&#24847;&#35782;&#30340;&#31574;&#30053;&#65292;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35770;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#25289;&#26364;&#20809;&#35889;&#27861;&#23545;&#27927;&#28068;&#21058;&#36827;&#34892;&#36870;&#21521;&#24037;&#31243;&#65292;&#20197;&#24555;&#36895;&#35780;&#20272;&#20854;&#28508;&#22312;&#27602;&#24615;&#65292;&#24182;&#20026;&#36136;&#37327;&#25511;&#21046;&#21644;&#30417;&#31649;&#25552;&#20379;&#31934;&#30830;&#30340;&#37492;&#23450;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.20254</link><description>&lt;p&gt;
&#36870;&#21521;&#24037;&#31243;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#24212;&#29992;&#20110;&#29992;&#25289;&#26364;&#20809;&#35889;&#27861;&#26816;&#27979;&#27927;&#28068;&#21058;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy. (arXiv:2310.20254v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#25289;&#26364;&#20809;&#35889;&#27861;&#23545;&#27927;&#28068;&#21058;&#36827;&#34892;&#36870;&#21521;&#24037;&#31243;&#65292;&#20197;&#24555;&#36895;&#35780;&#20272;&#20854;&#28508;&#22312;&#27602;&#24615;&#65292;&#24182;&#20026;&#36136;&#37327;&#25511;&#21046;&#21644;&#30417;&#31649;&#25552;&#20379;&#31934;&#30830;&#30340;&#37492;&#23450;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#20854;&#24615;&#36136;&#22914;&#20309;&#65292;&#23545;&#22797;&#26434;&#28151;&#21512;&#29289;&#36827;&#34892;&#36870;&#21521;&#24037;&#31243;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#33021;&#22815;&#24555;&#36895;&#35780;&#20272;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#26032;&#21830;&#19994;&#20135;&#21697;&#30340;&#28508;&#22312;&#27602;&#24615;&#23545;&#20110;&#20998;&#26512;&#23398;&#26469;&#35828;&#26159;&#19968;&#39033;&#30495;&#27491;&#30340;&#25361;&#25112;&#12290;&#25968;&#23383;&#24037;&#20855;&#30340;&#21457;&#23637;&#65288;&#25968;&#25454;&#24211;&#12289;&#21270;&#23398;&#35745;&#37327;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#31561;&#65289;&#21644;&#20998;&#26512;&#25216;&#26415;&#65288;&#25289;&#26364;&#20809;&#35889;&#27861;&#12289;&#36817;&#32418;&#22806;&#20809;&#35889;&#27861;&#12289;&#36136;&#35889;&#31561;&#65289;&#23558;&#20801;&#35768;&#37492;&#23450;&#28508;&#22312;&#30340;&#26377;&#27602;&#20998;&#23376;&#12290;&#26412;&#25991;&#20197;&#27927;&#28068;&#21058;&#20135;&#21697;&#20026;&#20363;&#65292;&#20854;&#32452;&#25104;&#23545;&#20154;&#31867;&#25110;&#29615;&#22659;&#26469;&#35828;&#21487;&#33021;&#20855;&#26377;&#21361;&#38505;&#24615;&#65292;&#20026;&#20102;&#36136;&#37327;&#25511;&#21046;&#21644;&#30417;&#31649;&#30446;&#30340;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#30830;&#30340;&#37492;&#23450;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#25968;&#23383;&#24037;&#20855;&#65288;&#20809;&#35889;&#25968;&#25454;&#24211;&#12289;&#28151;&#21512;&#25968;&#25454;&#24211;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#21270;&#23398;&#35745;&#37327;&#23398;/&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;...&#65289;&#12289;&#19981;&#21516;&#30340;&#26679;&#21697;&#21046;&#22791;&#26041;&#27861;&#65288;&#21407;&#22987;&#26679;&#21697;&#25110;&#20960;&#31181;&#27987;&#32553;/&#31232;&#37322;&#26679;&#21697;&#65289;&#21644;&#25289;&#26364;&#20809;&#35889;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reverse engineering of a complex mixture, regardless of its nature, has become significant today. Being able to quickly assess the potential toxicity of new commercial products in relation to the environment presents a genuine analytical challenge. The development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the identification of potential toxic molecules. In this article, we use the example of detergent products, whose composition can prove dangerous to humans or the environment, necessitating precise identification and quantification for quality control and regulation purposes. The combination of various digital tools (spectral database, mixture database, experimental design, Chemometrics / Machine Learning algorithm{\ldots}) together with different sample preparation methods (raw sample, or several concentrated / diluted samples) Raman spectroscopy, has 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#21270;&#33410;&#28857;&#25277;&#26679;&#30340;&#23618;&#27425;&#21270;Transformer&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Transformer&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21516;&#26102;&#22810;&#26679;&#21270;&#37319;&#26679;&#33410;&#28857;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#33410;&#28857;&#20002;&#24323;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.20250</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26679;&#21270;&#33410;&#28857;&#25277;&#26679;&#30340;&#23618;&#27425;&#21270;Transformer&#27744;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning. (arXiv:2310.20250v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#21270;&#33410;&#28857;&#25277;&#26679;&#30340;&#23618;&#27425;&#21270;Transformer&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Transformer&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21516;&#26102;&#22810;&#26679;&#21270;&#37319;&#26679;&#33410;&#28857;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#33410;&#28857;&#20002;&#24323;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27744;&#21270;&#26041;&#27861;&#24191;&#27867;&#29992;&#20110;&#38477;&#37319;&#26679;&#22270;&#24418;&#65292;&#22312;&#22270;&#32423;&#20219;&#21153;&#22914;&#22270;&#20998;&#31867;&#21644;&#22270;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#31216;&#20026;&#33410;&#28857;&#20002;&#24323;&#27744;&#21270;&#65292;&#26088;&#22312;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#35780;&#20998;&#20989;&#25968;&#20002;&#24323;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#26174;&#33879;&#24615;&#24471;&#20998;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33410;&#28857;&#20002;&#24323;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#23545;&#20110;&#27599;&#20010;&#27744;&#21270;&#33410;&#28857;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20027;&#35201;&#20197;GNN&#20026;&#39592;&#26550;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65307;&#65288;2&#65289;&#20165;&#27719;&#38598;&#24471;&#20998;&#26368;&#39640;&#30340;&#33410;&#28857;&#24448;&#24448;&#20250;&#20445;&#30041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20002;&#24323;&#20302;&#24471;&#20998;&#33410;&#28857;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTPool&#30340;&#22270;&#21464;&#25442;&#27744;&#21270;&#26041;&#27861;&#65292;&#23558;Transformer&#24341;&#20837;&#21040;&#33410;&#28857;&#20002;&#24323;&#27744;&#21270;&#20013;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#25104;&#23545;&#20132;&#20114;&#65292;&#24182;&#21516;&#26102;&#22810;&#26679;&#21270;&#37319;&#26679;&#33410;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#35780;&#20998;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;&#33410;&#28857;&#30340;&#20002;&#24323;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both globa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;VividTalker&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#22836;&#37096;&#23039;&#24577;&#21644;&#33258;&#28982;&#38754;&#37096;&#32454;&#33410;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26126;&#30830;&#23558;&#38754;&#37096;&#21160;&#30011;&#20998;&#20026;&#22836;&#37096;&#23039;&#24577;&#21644;&#22068;&#37096;&#36816;&#21160;&#26469;&#35299;&#20915;&#30446;&#21069;&#29616;&#26377;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20240</link><description>&lt;p&gt;
&#20026;&#38754;&#37096;&#27880;&#20837;&#29983;&#21629;&#65306;&#20855;&#26377;&#33258;&#28982;&#22836;&#37096;&#23039;&#24577;&#21644;&#35814;&#32454;&#24418;&#29366;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape. (arXiv:2310.20240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;VividTalker&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#22836;&#37096;&#23039;&#24577;&#21644;&#33258;&#28982;&#38754;&#37096;&#32454;&#33410;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26126;&#30830;&#23558;&#38754;&#37096;&#21160;&#30011;&#20998;&#20026;&#22836;&#37096;&#23039;&#24577;&#21644;&#22068;&#37096;&#36816;&#21160;&#26469;&#35299;&#20915;&#30446;&#21069;&#29616;&#26377;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#36924;&#30495;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#38656;&#35201;&#38899;&#39057;&#36755;&#20837;&#21644;&#38754;&#37096;&#34920;&#24773;&#20043;&#38388;&#30340;&#33258;&#28982;&#21644;&#31934;&#30830;&#30340;&#21516;&#27493;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20173;&#26080;&#27861;&#28210;&#26579;&#20855;&#26377;&#28789;&#27963;&#22836;&#37096;&#23039;&#24577;&#21644;&#33258;&#28982;&#38754;&#37096;&#32454;&#33410;&#65288;&#22914;&#30385;&#32441;&#65289;&#30340;&#24418;&#29366;&#12290;&#36825;&#31181;&#38480;&#21046;&#20027;&#35201;&#30001;&#20004;&#20010;&#26041;&#38754;&#23548;&#33268;&#65306;1&#65289;&#25910;&#38598;&#20855;&#26377;&#35814;&#32454;3D&#38754;&#37096;&#24418;&#29366;&#30340;&#35757;&#32451;&#38598;&#38750;&#24120;&#26114;&#36149;&#12290;&#35814;&#32454;&#24418;&#29366;&#27880;&#37322;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#20855;&#26377;&#34920;&#24773;&#20016;&#23500;&#30340;&#38754;&#37096;&#21160;&#30011;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;2&#65289;&#19982;&#22068;&#37096;&#36816;&#21160;&#30456;&#27604;&#65292;&#22836;&#37096;&#23039;&#24577;&#19982;&#35821;&#38899;&#20869;&#23481;&#30340;&#30456;&#20851;&#24615;&#36739;&#23567;&#12290;&#22240;&#27492;&#65292;&#21516;&#26102;&#23545;&#22068;&#37096;&#36816;&#21160;&#21644;&#22836;&#37096;&#23039;&#24577;&#36827;&#34892;&#24314;&#27169;&#23548;&#33268;&#20102;&#38754;&#37096;&#36816;&#21160;&#30340;&#21487;&#25511;&#24615;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VividTalker&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#22836;&#37096;&#23039;&#24577;&#21644;&#33258;&#28982;&#38754;&#37096;&#32454;&#33410;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#26041;&#27861;&#22686;&#24378;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#21608;&#22260;&#29615;&#22659;&#30340;&#35814;&#32454;&#20840;&#38754;&#25551;&#36848;&#24182;&#25552;&#20379;&#28508;&#22312;&#39118;&#38505;&#30340;&#35686;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.20225</link><description>&lt;p&gt;
VisPercep:&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#26041;&#27861;&#22686;&#24378;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision. (arXiv:2310.20225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#26041;&#27861;&#22686;&#24378;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#21608;&#22260;&#29615;&#22659;&#30340;&#35814;&#32454;&#20840;&#38754;&#25551;&#36848;&#24182;&#25552;&#20379;&#28508;&#22312;&#39118;&#38505;&#30340;&#35686;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#22312;&#23545;&#38476;&#29983;&#29615;&#22659;&#19979;&#30340;&#22330;&#26223;&#35782;&#21035;&#21644;&#31934;&#30830;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#38754;&#38754;&#20020;&#24456;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35270;&#21147;&#20007;&#22833;&#65292;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#24456;&#38590;&#33258;&#24049;&#35775;&#38382;&#21644;&#35782;&#21035;&#28508;&#22312;&#30340;&#32458;&#20498;&#21361;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#65292;&#25552;&#20379;&#21608;&#22260;&#29615;&#22659;&#30340;&#35814;&#32454;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#28508;&#22312;&#39118;&#38505;&#30340;&#35686;&#21578;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#22270;&#20687;&#26631;&#35760;&#27169;&#22411;&#65288;&#21363;Recognize Anything (RAM)&#65289;&#35782;&#21035;&#25429;&#33719;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#24120;&#35265;&#29289;&#20307;&#12290;&#28982;&#21518;&#23558;&#35782;&#21035;&#32467;&#26524;&#21644;&#29992;&#25143;&#26597;&#35810;&#25972;&#21512;&#25104;&#19968;&#20010;&#29305;&#23450;&#20110;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#32676;&#30340;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#36827;&#34892;&#23450;&#21046;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#21644;&#36755;&#20837;&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#19968;&#20010;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;InstructBLIP&#65289;&#29983;&#25104;&#35814;&#32454;&#20840;&#38754;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive desc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#21644;&#31354;&#38388;&#35821;&#20041;&#22270;&#23545;&#22522;&#20110;&#36712;&#36857;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#33021;&#22312;&#19968;&#27493;&#20013;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#37327;&#65292;&#24182;&#20445;&#30041;&#20102;&#22810;&#32500;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.20224</link><description>&lt;p&gt;
&#36873;&#25321;&#19968;&#20010;&#34920;&#65306;&#22522;&#20110;&#22270;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#20056;&#23458;&#36712;&#36857;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering. (arXiv:2310.20224v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#21644;&#31354;&#38388;&#35821;&#20041;&#22270;&#23545;&#22522;&#20110;&#36712;&#36857;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#33021;&#22312;&#19968;&#27493;&#20013;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#37327;&#65292;&#24182;&#20445;&#30041;&#20102;&#22810;&#32500;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36712;&#36857;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#23545;&#20110;&#20132;&#36890;&#36816;&#33829;&#21830;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#20056;&#23458;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#21253;&#25324;&#27599;&#20010;&#20056;&#23458;&#20869;&#37096;&#30340;&#22810;&#27425;&#20986;&#34892;&#20197;&#21450;&#27599;&#27425;&#20986;&#34892;&#30340;&#22810;&#32500;&#20449;&#24687;&#65292;&#26080;&#27861;&#36731;&#26494;&#22320;&#32858;&#31867;&#20056;&#23458;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20934;&#30830;&#25351;&#23450;&#32858;&#31867;&#25968;&#37327;&#30340;&#36215;&#22987;&#20540;&#12290;&#26368;&#21518;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#32771;&#34385;&#31354;&#38388;&#35821;&#20041;&#22270;&#65292;&#22914;&#22320;&#29702;&#37051;&#36817;&#24615;&#21644;&#20301;&#32622;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20445;&#30041;&#22810;&#32500;&#20986;&#34892;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#33021;&#20197;&#32479;&#19968;&#30340;&#19968;&#27493;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#65292;&#20855;&#26377;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#37327;&#30340;&#33021;&#21147;&#12290;&#31354;&#38388;&#22270;&#34987;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#20197;&#36830;&#25509;&#35821;&#20041;&#37051;&#23621;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#24352;&#37327;&#29256;&#26412;&#30340;Coll...
&lt;/p&gt;
&lt;p&gt;
Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Coll
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Transformer&#30340;&#38271;&#26399;&#31995;&#21015;&#39044;&#27979;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;Transformer&#26550;&#26500;&#21450;&#20854;&#25913;&#36827;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12289;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.20218</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38271;&#26399;&#31995;&#21015;&#39044;&#27979;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20218
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38271;&#26399;&#31995;&#21015;&#39044;&#27979;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;Transformer&#26550;&#26500;&#21450;&#20854;&#25913;&#36827;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12289;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;Transformer&#26550;&#26500;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37319;&#29992;&#12290;Transformer&#34987;&#35777;&#26126;&#26159;&#25552;&#21462;&#38271;&#24207;&#21015;&#20869;&#37096;&#20803;&#32032;&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#26368;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21508;&#31181;&#21464;&#20307;&#20351;&#24471;Transformer&#26550;&#26500;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;Transformer&#26550;&#26500;&#21450;&#20854;&#21518;&#32493;&#25913;&#36827;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32972;&#26223;&#19979;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20216</link><description>&lt;p&gt;
GPT-4 &#26159;&#21542;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20216
&lt;/p&gt;
&lt;p&gt;
GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102; GPT-4&#12290;&#22312;&#34920;&#29616;&#26368;&#22909;&#30340; GPT-4 &#25552;&#31034;&#20013;&#65292;&#22312; 41% &#30340;&#28216;&#25103;&#20013;&#36890;&#36807;&#20102;&#27979;&#35797;&#65292;&#36229;&#36807;&#20102; ELIZA&#65288;27%&#65289;&#21644; GPT-3.5&#65288;14%&#65289;&#35774;&#23450;&#30340;&#22522;&#20934;&#65292;&#20294;&#36824;&#19981;&#22914;&#20154;&#31867;&#21442;&#19982;&#32773;&#65288;63%&#65289;&#30340;&#26426;&#20250;&#21644;&#22522;&#20934;&#12290;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#35821;&#35328;&#39118;&#26684;&#65288;35%&#65289;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#65288;27%&#65289;&#65292;&#25903;&#25345;&#26234;&#33021;&#19981;&#36275;&#20197;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#30340;&#35266;&#28857;&#12290;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;&#25945;&#32946;&#27700;&#24179;&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29087;&#24713;&#24230;&#65292;&#24182;&#19981;&#33021;&#39044;&#27979;&#34987;&#35782;&#21035;&#29575;&#65292;&#36825;&#34920;&#26126;&#21363;&#20351;&#26159;&#28145;&#20837;&#20102;&#35299;&#31995;&#32479;&#24182;&#39057;&#32321;&#19982;&#20854;&#20132;&#20114;&#30340;&#20154;&#65292;&#20063;&#20250;&#23481;&#26131;&#34987;&#27450;&#39575;&#12290;&#23613;&#31649;&#22270;&#28789;&#27979;&#35797;&#20316;&#20026;&#26234;&#33021;&#30340;&#27979;&#35797;&#20855;&#26377;&#24050;&#30693;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#22312;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#20855;&#26377;&#20882;&#20805;&#20154;&#31867;&#33021;&#21147;&#30340; AI &#27169;&#22411;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24191;&#27867;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31574;&#30053;&#21644;&#26631;&#20934;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20999;&#25442;&#21327;&#35758;(DHO)&#65292;&#36890;&#36807;&#39044;&#27979;&#33021;&#21147;&#36339;&#36807;&#27979;&#37327;&#25253;&#21578;&#38454;&#27573;&#65292;&#31616;&#21270;&#20999;&#25442;&#36807;&#31243;&#24182;&#28040;&#38500;&#35775;&#38382;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#32593;&#32476;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#36234;&#65292;&#23637;&#31034;&#20102;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.20215</link><description>&lt;p&gt;
LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#20999;&#25442;&#21327;&#35758;&#23398;&#20064;&#65306;&#35775;&#38382;&#24310;&#36831;&#21644;&#30896;&#25758;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization. (arXiv:2310.20215v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20999;&#25442;&#21327;&#35758;(DHO)&#65292;&#36890;&#36807;&#39044;&#27979;&#33021;&#21147;&#36339;&#36807;&#27979;&#37327;&#25253;&#21578;&#38454;&#27573;&#65292;&#31616;&#21270;&#20999;&#25442;&#36807;&#31243;&#24182;&#28040;&#38500;&#35775;&#38382;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#32593;&#32476;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#36234;&#65292;&#23637;&#31034;&#20102;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (DRL) &#30340;&#20999;&#25442; (HO) &#21327;&#35758;&#65292;&#31216;&#20026;DHO&#65292;&#19987;&#38376;&#38024;&#23545;&#20302;&#36712;&#36947;&#21355;&#26143;&#32593;&#32476;&#30340;&#38271;&#20256;&#25773;&#24310;&#36831;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;DHO&#22312;&#39044;&#23450;&#30340;LEO&#21355;&#26143;&#36712;&#36857;&#27169;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;HO&#36807;&#31243;&#20013;&#21033;&#29992;&#20854;&#39044;&#27979;&#33021;&#21147;&#36339;&#36807;&#27979;&#37327;&#25253;&#21578;(MR)&#38454;&#27573;&#65292;&#31616;&#21270;&#20102;&#36807;&#31243;&#24182;&#28040;&#38500;&#20102;MR&#38454;&#27573;&#20135;&#29983;&#30340;&#20256;&#25773;&#24310;&#36831;&#65292;&#21516;&#26102;&#20173;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;HO&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;DHO&#22312;&#19981;&#21516;&#32593;&#32476;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;HO&#21327;&#35758;&#65292;&#21253;&#25324;&#35775;&#38382;&#24310;&#36831;&#12289;&#30896;&#25758;&#29575;&#21644;&#20999;&#25442;&#25104;&#21151;&#29575;&#65292;&#23637;&#31034;&#20102;DHO&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#35775;&#38382;&#24310;&#36831;&#21644;&#30896;&#25758;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;&#21508;&#31181;DRL&#31639;&#27861;&#23545;DHO&#36827;&#34892;&#35757;&#32451;&#30340;&#24615;&#33021;&#21644;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#35843;&#30740;&#32467;&#26524;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#35299;&#20915;&#27169;&#31946;&#35774;&#32622;&#12289;&#36807;&#26102;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.20199</link><description>&lt;p&gt;
&#36861;&#23547;&#22833;&#33853;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
In Search of Lost Online Test-time Adaptation: A Survey. (arXiv:2310.20199v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#35843;&#30740;&#32467;&#26524;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#35299;&#20915;&#27169;&#31946;&#35774;&#32622;&#12289;&#36807;&#26102;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#35843;&#30740;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#32508;&#21512;&#27010;&#20917;&#65292;&#35813;&#33539;&#24335;&#19987;&#27880;&#20110;&#22312;&#25209;&#37327;&#21040;&#36798;&#26102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35843;&#25972;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#23613;&#31649;&#26368;&#36817;OTTA&#26041;&#27861;&#30340;&#22686;&#21152;&#65292;&#20294;&#35813;&#39046;&#22495;&#23384;&#22312;&#27169;&#31946;&#30340;&#35774;&#32622;&#12289;&#36807;&#26102;&#30340;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#30495;&#27491;&#30340;&#25361;&#25112;&#21464;&#24471;&#38590;&#20197;&#22797;&#29616;&#12290;&#20026;&#20102;&#28165;&#26224;&#21644;&#20005;&#26684;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#23558;OTTA&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#24182;&#20351;&#29992;&#21151;&#33021;&#24378;&#22823;&#30340;Vision Transformer&#65288;ViT&#65289;&#39592;&#24178;&#26550;&#26500;&#23545;&#23427;&#20204;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21457;&#29616;&#30495;&#27491;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#30340;&#21463;&#25439;&#25968;&#25454;&#38598;&#65292;&#22914;CIFAR-10/100-C&#21644;ImageNet-C&#65292;&#36824;&#21253;&#25324;&#20307;&#29616;&#22312;CIFAR-10.1&#21644;CIFAR-10-Warehouse&#20013;&#30340;&#29616;&#23454;&#19990;&#30028;&#36716;&#21464;&#65292;&#28085;&#30422;&#20102;&#25628;&#32034;&#24341;&#25806;&#30340;&#21464;&#21270;&#21644;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#34913;&#37327;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20195</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#19979;&#29983;&#25104;&#24310;&#32493;
&lt;/p&gt;
&lt;p&gt;
Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24815;&#29992;&#25110;&#23383;&#38754;&#22810;&#35789;&#34920;&#36798;&#26159;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#21253;&#21547;&#24815;&#29992;&#65288;&#25110;&#23383;&#38754;&#65289;&#34920;&#36798;&#30340;&#21465;&#36848;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24310;&#32493;&#30340;&#20219;&#21153;&#21487;&#20197;&#35753;&#25105;&#20204;&#27979;&#35797;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29702;&#35299;&#38750;&#32452;&#21512;&#24615;&#27604;&#21947;&#25991;&#26412;&#30340;&#32420;&#32454;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#65289;&#30340;&#25968;&#25454;&#38598;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#19979;&#65288;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65289;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#23383;&#38754;&#19978;&#19979;&#25991;&#30340;&#24310;&#32493;&#26102;&#30053;&#20248;&#20110;&#24815;&#29992;&#19978;&#19979;&#25991;&#65292;&#20294;&#24046;&#36317;&#24456;&#23567;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;&#27169;&#22411;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#21516;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#29983;&#25104;&#27169;&#22411;&#22312;&#25191;&#34892;&#27492;&#20219;&#21153;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20187</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#38477;&#27700;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#38450;&#21361;&#38505;&#22825;&#27668;&#20107;&#20214;&#65292;&#30830;&#20445;&#20805;&#36275;&#30340;&#23616;&#22320;&#38477;&#27700;&#39044;&#25253;&#25552;&#21069;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20840;&#29699;&#21464;&#26262;&#24341;&#36215;&#30340;&#27668;&#20505;&#21464;&#21270;&#22686;&#21152;&#20102;&#20934;&#30830;&#39044;&#27979;&#20005;&#37325;&#38477;&#27700;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#12290;&#38477;&#27700;&#21518;&#22788;&#29702;&#21253;&#25324;&#65288;i&#65289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#22312;&#22823;&#27668;&#29289;&#29702;&#39046;&#22495;&#30340;&#36974;&#34109;&#21464;&#37327;&#37325;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#23398;&#20064;&#21040;&#38477;&#27700;&#20998;&#21106;&#20219;&#21153;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21306;&#22495;NWP&#20013;&#30340;&#38477;&#27700;&#26657;&#27491;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20178</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#21457;&#29616;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25216;&#33021;&#20559;&#31163;&#20854;&#21021;&#22987;&#36712;&#36857;&#20250;&#21463;&#21040;&#37325;&#22823;&#24809;&#32602;&#12290;&#20026;&#20102;&#22686;&#24378;&#25506;&#32034;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#22870;&#21169;&#26469;&#26368;&#22823;&#21270;&#29366;&#24577;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25110;&#29109;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#30340;&#25928;&#26524;&#38543;&#30528;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#32780;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;DISCO-DANCE&#65292;&#23427;&#36873;&#25321;&#20855;&#26377;&#36798;&#21040;&#26410;&#25506;&#32034;&#29366;&#24577;&#28508;&#21147;&#26368;&#39640;&#30340;&#24341;&#23548;&#25216;&#33021;&#65292;&#24341;&#23548;&#20854;&#20182;&#25216;&#33021;&#36981;&#24490;&#24341;&#23548;&#25216;&#33021;&#65292;&#28982;&#21518;&#20998;&#25955;&#24341;&#23548;&#25216;&#33021;&#20197;&#26368;&#22823;&#21270;&#22312;&#26410;&#25506;&#32034;&#29366;&#24577;&#20013;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#23548;&#33322;&#22522;&#20934;&#21644;&#19968;&#20010;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#65292;DISCO-DANCE&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#22522;&#32447;&#12290;DISCO-DANCE&#30340;&#23450;&#24615;&#21487;&#35270;&#21270;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#24207;&#21015;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26174;&#24335;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39123;&#39118;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.20174</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GraphTransformers for Geospatial Forecasting. (arXiv:2310.20174v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#24207;&#21015;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26174;&#24335;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39123;&#39118;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#36716;&#25442;&#22120;&#36827;&#34892;&#22320;&#29702;&#31354;&#38388;&#24207;&#21015;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#35266;&#23519;&#22810;&#20010;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#24207;&#21015;&#20043;&#38388;&#20250;&#33258;&#21160;&#24418;&#25104;&#19968;&#20010;&#22270;&#32467;&#26500;&#65292;&#32780;&#36825;&#31181;&#32467;&#26500;&#22312;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26174;&#24335;&#21033;&#29992;&#36825;&#20010;&#22270;&#32467;&#26500;&#65292;&#22320;&#29702;&#31354;&#38388;&#36712;&#36857;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#22270;&#36716;&#25442;&#22120;&#26041;&#27861;&#22312;HURDAT&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;&#23545;&#39123;&#39118;&#36712;&#36857;&#36827;&#34892;6&#23567;&#26102;&#22522;&#30784;&#19978;&#30340;&#39044;&#27979;&#19978;&#65292;&#30456;&#27604;&#22522;&#20110;Transformer&#30340;&#22522;&#20934;&#27169;&#22411;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#40065;&#26834;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#19968;&#20010;&#32763;&#35793;&#26041;&#21521;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.20162</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#40065;&#26834;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?. (arXiv:2310.20162v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#40065;&#26834;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#19968;&#20010;&#32763;&#35793;&#26041;&#21521;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#27169;&#22411;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#20445;&#25345;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20855;&#26377;&#21333;&#19968;&#32763;&#35793;&#26041;&#21521;&#30340;&#21452;&#35821;&#26426;&#22120;&#32763;&#35793;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#24615;&#36716;&#31227;&#20998;&#26512;&#21327;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#23383;&#31526;&#12289;&#35789;&#21644;&#22810;&#32423;&#22122;&#22768;&#26469;&#25915;&#20987;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29305;&#23450;&#32763;&#35793;&#26041;&#21521;&#65292;&#35780;&#20272;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20174;&#19968;&#20010;&#32763;&#35793;&#26041;&#21521;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#30830;&#23454;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#24341;&#23548;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#35270;&#35273;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20159</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#38382;&#31572;: &#20351;&#29992;&#30693;&#35782;&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#20320;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts. (arXiv:2310.20159v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#24341;&#23548;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#35270;&#35273;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#22238;&#31572;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#20551;&#35774;&#20102;&#23545;&#22270;&#20687;&#21644;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#20197;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;VQA&#36817;&#24180;&#26469;&#22240;&#20854;&#22312;&#26426;&#22120;&#20154;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;VQA&#65292;&#21363;&#22238;&#31572;&#38382;&#39064;&#38656;&#35201;&#24120;&#35782;&#30693;&#35782;&#12289;&#19990;&#30028;&#30693;&#35782;&#20197;&#21450;&#20851;&#20110;&#22270;&#20687;&#20013;&#19981;&#23384;&#22312;&#30340;&#24605;&#24819;&#21644;&#27010;&#24565;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#24341;&#23548;&#65288;LG&#65289;&#24418;&#24335;&#30340;&#21512;&#29702;&#24615;&#12289;&#22270;&#20687;&#26631;&#39064;&#12289;&#22330;&#26223;&#22270;&#31561;&#26469;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;A-OKVQA&#12289;Science-QA&#12289;VSR&#21644;IconQA&#25968;&#25454;&#38598;&#30340;&#22810;&#36873;&#39064;&#38382;&#31572;&#20219;&#21153;&#19978;&#20351;&#29992;CLIP&#21644;BLIP&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#26377;&#25928;&#30340;&#35270;&#35273;&#38382;&#31572;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35821;&#35328;&#24341;&#23548;&#25913;&#36827;&#20102;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves
&lt;/p&gt;</description></item><item><title>MLatom 3&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#22686;&#24378;&#35745;&#31639;&#21270;&#23398;&#27169;&#25311;&#21644;&#21019;&#24314;&#22797;&#26434;&#24037;&#20316;&#27969;&#12290;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#22810;&#31181;&#27169;&#25311;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;ML&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#35745;&#31639;&#12289;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#21644;&#27169;&#25311;&#20809;&#35889;&#31561;&#35745;&#31639;&#21270;&#23398;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20155</link><description>&lt;p&gt;
MLatom 3: &#29992;&#20110;&#22686;&#24378;&#35745;&#31639;&#21270;&#23398;&#27169;&#25311;&#21644;&#24037;&#20316;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows. (arXiv:2310.20155v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20155
&lt;/p&gt;
&lt;p&gt;
MLatom 3&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#22686;&#24378;&#35745;&#31639;&#21270;&#23398;&#27169;&#25311;&#21644;&#21019;&#24314;&#22797;&#26434;&#24037;&#20316;&#27969;&#12290;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#22810;&#31181;&#27169;&#25311;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;ML&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#35745;&#31639;&#12289;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#21644;&#27169;&#25311;&#20809;&#35889;&#31561;&#35745;&#31639;&#21270;&#23398;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#21270;&#23398;&#20013;&#36234;&#26469;&#36234;&#25104;&#20026;&#24120;&#29992;&#24037;&#20855;&#12290;MLatom 3 &#26159;&#19968;&#20010;&#31243;&#24207;&#21253;&#65292;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#22686;&#24378;&#20856;&#22411;&#30340;&#35745;&#31639;&#21270;&#23398;&#27169;&#25311;&#65292;&#24182;&#21019;&#24314;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#12290;&#36825;&#20010;&#24320;&#28304;&#21253;&#25552;&#20379;&#20102;&#22810;&#31181;&#36873;&#25321;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21629;&#20196;&#34892;&#36873;&#39033;&#12289;&#36755;&#20837;&#25991;&#20214;&#25110;&#20351;&#29992;MLatom&#20316;&#20026;Python&#21253;&#30340;&#33050;&#26412;&#22312;&#20182;&#20204;&#30340;&#35745;&#31639;&#26426;&#19978;&#25110;&#36890;&#36807;XACS&#20113;&#35745;&#31639;&#24179;&#21488;&#36816;&#34892;&#27169;&#25311;&#12290;&#35745;&#31639;&#21270;&#23398;&#23478;&#21487;&#20197;&#20351;&#29992;ML&#12289;&#37327;&#23376;&#21147;&#23398;&#21644;&#32452;&#21512;&#27169;&#22411;&#35745;&#31639;&#33021;&#37327;&#21644;&#28909;&#21270;&#23398;&#24615;&#36136;&#65292;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65292;&#36816;&#34892;&#20998;&#23376;&#21644;&#37327;&#23376;&#21160;&#21147;&#23398;&#65292;&#24182;&#27169;&#25311;(&#36716;&#21160;)&#25391;&#21160;&#65292;&#21333;&#20809;&#23376;UV/&#21487;&#35265;&#21560;&#25910;&#21644;&#21452;&#20809;&#23376;&#21560;&#25910;&#35889;&#12290;&#29992;&#25143;&#21487;&#20197;&#20174;&#21253;&#21547;&#39044;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#24191;&#27867;&#26041;&#27861;&#24211;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#24320;&#21457;&#23567;&#22411;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24179;&#34913;&#20302;&#20445;&#30495;&#24230;&#33258;&#21160;&#27880;&#37322;&#21644;&#39640;&#20445;&#30495;&#24230;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27880;&#37322;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.20153</link><description>&lt;p&gt;
&#19982;&#31232;&#30095;&#20154;&#31867;&#30417;&#30563;&#30456;&#36866;&#24212;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#24320;&#21457;&#23567;&#22411;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24179;&#34913;&#20302;&#20445;&#30495;&#24230;&#33258;&#21160;&#27880;&#37322;&#21644;&#39640;&#20445;&#30495;&#24230;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27880;&#37322;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#24040;&#22823;&#35268;&#27169;&#12289;&#26131;&#21463;&#38169;&#35823;&#20449;&#24687;&#24433;&#21709;&#20197;&#21450;&#39640;&#26114;&#30340;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#65292;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#65288;IMFL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#24320;&#21457;&#23567;&#22411;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#35843;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#24179;&#34913;&#20302;&#20445;&#30495;&#24230;&#33258;&#21160;LLM&#27880;&#37322;&#21644;&#39640;&#20445;&#30495;&#24230;&#20154;&#31867;&#27880;&#37322;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#33719;&#21462;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27880;&#37322;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26597;&#35810;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#21019;&#26032;&#35774;&#35745;&#65306;1&#65289;&#20174;&#20154;&#31867;&#27880;&#37322;&#26679;&#26412;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#20363;&#23376;&#26469;&#25913;&#36827;LLM&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36951;&#24536;&#26694;&#26550;&#26469;&#22788;&#29702;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#20445;&#25252;&#36829;&#35268;&#12290;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#36951;&#24536;&#23618;&#21040;transformers&#20013;&#65292;&#24182;&#20351;&#29992;&#26377;&#36873;&#25321;&#30340;&#24072;&#29983;&#30446;&#26631;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21024;&#38500;&#25968;&#25454;&#21518;&#26377;&#25928;&#22320;&#26356;&#26032;LLMs&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20150</link><description>&lt;p&gt;
&#24536;&#35760;&#20320;&#24819;&#24536;&#35760;&#30340;&#65306;LLMs&#30340;&#39640;&#25928;&#36951;&#24536;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36951;&#24536;&#26694;&#26550;&#26469;&#22788;&#29702;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#20445;&#25252;&#36829;&#35268;&#12290;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#36951;&#24536;&#23618;&#21040;transformers&#20013;&#65292;&#24182;&#20351;&#29992;&#26377;&#36873;&#25321;&#30340;&#24072;&#29983;&#30446;&#26631;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21024;&#38500;&#25968;&#25454;&#21518;&#26377;&#25928;&#22320;&#26356;&#26032;LLMs&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#35760;&#24518;&#21508;&#31181;&#25991;&#26412;&#25968;&#25454;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#38754;&#20020;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#20445;&#25252;&#35268;&#23450;&#30340;&#36829;&#35268;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#25439;&#23475;&#39044;&#27979;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36731;&#26494;&#22320;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#21024;&#38500;&#19982;&#20010;&#20154;&#29992;&#25143;&#30456;&#20851;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#26377;&#36873;&#25321;&#30340;&#24072;&#29983;&#30446;&#26631;&#30340;&#36731;&#37327;&#32423;&#36951;&#24536;&#23618;&#21040;transformers&#20013;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#21024;&#38500;&#21518;&#26377;&#25928;&#22320;&#26356;&#26032;LLMs&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34701;&#21512;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#36951;&#24536;&#23618;&#65292;&#20197;&#22788;&#29702;&#19968;&#31995;&#21015;&#30340;&#36951;&#24536;&#25805;&#20316;&#12290;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#20114;&#21160;&#24847;&#22270;&#32534;&#30721;&#20026;&#31038;&#20132;&#24515;&#29702;&#21442;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20132;&#36890;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20148</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#20114;&#24863;&#30693;&#34892;&#20026;&#39044;&#27979;&#21644;&#31038;&#20132;-&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network. (arXiv:2310.20148v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#20114;&#21160;&#24847;&#22270;&#32534;&#30721;&#20026;&#31038;&#20132;&#24515;&#29702;&#21442;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20132;&#36890;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#22312;&#20132;&#36890;&#20013;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#20114;&#21160;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21608;&#22260;&#20132;&#36890;&#30340;&#24847;&#22270;&#20197;&#20419;&#36827;&#20219;&#21153;&#30340;&#23436;&#25104;&#65292;&#23558;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37197;&#22791;&#20154;&#24037;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#20114;&#21160;&#24847;&#22270;&#32534;&#30721;&#20026;&#28508;&#22312;&#30340;&#31038;&#20132;&#24515;&#29702;&#21442;&#25968;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#28378;&#21160;&#22320;&#24179;&#38754;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#32771;&#34385;&#20102;&#20132;&#20114;&#39550;&#39542;&#21592;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#32447;&#37096;&#32626;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#22312;&#32447;&#20272;&#35745;&#21442;&#25968;&#20808;&#39564;&#26469;&#27169;&#20223;&#34892;&#20026;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#34892;&#20026;&#27169;&#22411;&#22312;&#23454;&#38469;&#36712;&#36857;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajector
&lt;/p&gt;</description></item><item><title>EELBERT&#26159;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#22238;&#24402;&#21644;&#26174;&#33879;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#23567;&#12290;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#22312;GLUE&#24471;&#20998;&#19978;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#30456;&#24046;4%&#65292;&#24182;&#19988;&#20307;&#31215;&#21482;&#26377;&#20854;15&#20493;&#20043;&#19968;&#65288;1.2MB&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.20144</link><description>&lt;p&gt;
EELBERT:&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20144
&lt;/p&gt;
&lt;p&gt;
EELBERT&#26159;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#22238;&#24402;&#21644;&#26174;&#33879;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#23567;&#12290;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#22312;GLUE&#24471;&#20998;&#19978;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#30456;&#24046;4%&#65292;&#24182;&#19988;&#20307;&#31215;&#21482;&#26377;&#20854;15&#20493;&#20043;&#19968;&#65288;1.2MB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EELBERT&#65292;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#65289;&#30340;&#26041;&#27861;&#65292;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#26368;&#23567;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#26367;&#25442;&#20026;&#21160;&#24577;&#30340;&#65292;&#21363;&#21363;&#26102;&#35745;&#31639;&#30340;&#23884;&#20837;&#23454;&#29616;&#26469;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#36755;&#20837;&#23884;&#20837;&#23618;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;BERT&#21464;&#20307;&#65292;&#29992;&#23884;&#20837;&#35745;&#31639;&#20989;&#25968;&#26367;&#25442;&#35813;&#23618;&#26377;&#21161;&#20110;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;BERT&#21464;&#20307;&#65288;EELBERT&#65289;&#19982;&#20256;&#32479;BERT&#27169;&#22411;&#30456;&#27604;&#20165;&#20855;&#26377;&#26368;&#23567;&#30340;&#22238;&#24402;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20986;&#25105;&#20204;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#65292;&#20854;GLUE&#24471;&#20998;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#39640;4&#65285;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;15&#20493;&#65288;1.2MB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20141</link><description>&lt;p&gt;
&#23545;&#27604;&#24046;&#24322;&#24615;&#39044;&#27979;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#25512;&#29702;&#26410;&#26469;&#26159;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#26680;&#24515;&#12290;&#20363;&#22914;&#65292;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23398;&#20064;&#34920;&#31034;&#20197;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#35775;&#38382;&#30340;&#29366;&#24577;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#23545;&#27604;&#24615;&#39044;&#27979;&#32534;&#30721;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#23398;&#20064;&#32534;&#30721;&#38271;&#26399;&#20381;&#36182;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#20943;&#23569;&#23398;&#20064;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#23548;&#20986;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#25552;&#39640;2&#20493;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#38543;&#26426;&#29615;&#22659;&#12290;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32422;&#20026;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20998;&#31867;&#32534;&#31243;&#35838;&#31243;&#20013;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20998;&#31867;&#26469;&#25552;&#39640;&#25945;&#32946;&#31995;&#32479;&#30340;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20105</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#20998;&#31867;&#32534;&#31243;&#35838;&#31243;&#20013;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;
&lt;/p&gt;
&lt;p&gt;
Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models. (arXiv:2310.20105v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20998;&#31867;&#32534;&#31243;&#35838;&#31243;&#20013;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20998;&#31867;&#26469;&#25552;&#39640;&#25945;&#32946;&#31995;&#32479;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20998;&#31867;&#19982;&#25152;&#23547;&#27714;&#30340;&#24110;&#21161;&#31867;&#22411;&#30456;&#20851;&#30340;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#21487;&#20197;&#23454;&#29616;&#38024;&#23545;&#24615;&#30340;&#26377;&#25928;&#21709;&#24212;&#12290;&#33258;&#21160;&#20998;&#31867;&#36825;&#31867;&#35831;&#27714;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#32463;&#27982;&#23454;&#24800;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#36827;&#34892;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#65292;GPT-3.5&#21644;GPT-4&#22312;&#22823;&#22810;&#25968;&#31867;&#21035;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#24615;&#65292;&#32780;GPT-4&#22312;&#19982;&#35843;&#35797;&#30456;&#20851;&#30340;&#23376;&#31867;&#21035;&#30340;&#20998;&#31867;&#19978;&#32988;&#36807;GPT-3.5&#12290;&#23545;GPT-3.5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20197;&#33267;&#20110;&#23427;&#22312;&#21508;&#31867;&#21035;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#19978;&#25509;&#36817;&#20110;&#20004;&#20301;&#20154;&#31867;&#35780;&#20998;&#32773;&#20043;&#38388;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;LLMs&#36890;&#36807;&#33258;&#21160;&#20998;&#31867;&#23398;&#29983;&#38656;&#27714;&#26469;&#22686;&#24378;&#25945;&#32946;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#25220;&#34989;&#21644;AI&#21161;&#25163;&#28389;&#29992;&#38382;&#39064;&#65292;&#35745;&#21010;&#24320;&#21457;&#33258;&#21160;&#21270;&#24037;&#20855;&#24110;&#21161;&#25945;&#24072;&#35782;&#21035;&#36825;&#20123;&#19981;&#24403;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#21442;&#19982;&#19981;&#24403;&#34892;&#20026;&#30340;&#23398;&#29983;&#22312;&#27979;&#35797;&#25104;&#32489;&#19978;&#19982;&#29420;&#31435;&#23436;&#25104;&#26102;&#38388;&#30701;&#30340;&#23398;&#29983;&#30456;&#24403;&#12290;&#25220;&#34989;&#21644;&#20351;&#29992;AI&#21161;&#25163;&#30340;&#25552;&#20132;&#22312;&#32454;&#33410;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#21518;&#32773;&#26356;&#22797;&#26434;&#19988;&#21487;&#35835;&#24615;&#36739;&#24046;&#12290;&#23398;&#29983;&#35748;&#20026;&#20351;&#29992;AI&#21161;&#25163;&#21487;&#33021;&#26377;&#30410;&#65292;&#20294;&#35201;&#24471;&#21040;&#36866;&#24403;&#25215;&#35748;&#12290;</title><link>http://arxiv.org/abs/2310.20104</link><description>&lt;p&gt;
&#32593;&#39029;&#32534;&#31243;&#20013;&#30340;&#25220;&#34989;&#21644;AI&#21161;&#25163;&#28389;&#29992;&#65306;&#19981;&#20844;&#24179;&#30340;&#21033;&#30410;&#21644;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;
Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics. (arXiv:2310.20104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#25220;&#34989;&#21644;AI&#21161;&#25163;&#28389;&#29992;&#38382;&#39064;&#65292;&#35745;&#21010;&#24320;&#21457;&#33258;&#21160;&#21270;&#24037;&#20855;&#24110;&#21161;&#25945;&#24072;&#35782;&#21035;&#36825;&#20123;&#19981;&#24403;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#21442;&#19982;&#19981;&#24403;&#34892;&#20026;&#30340;&#23398;&#29983;&#22312;&#27979;&#35797;&#25104;&#32489;&#19978;&#19982;&#29420;&#31435;&#23436;&#25104;&#26102;&#38388;&#30701;&#30340;&#23398;&#29983;&#30456;&#24403;&#12290;&#25220;&#34989;&#21644;&#20351;&#29992;AI&#21161;&#25163;&#30340;&#25552;&#20132;&#22312;&#32454;&#33410;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#21518;&#32773;&#26356;&#22797;&#26434;&#19988;&#21487;&#35835;&#24615;&#36739;&#24046;&#12290;&#23398;&#29983;&#35748;&#20026;&#20351;&#29992;AI&#21161;&#25163;&#21487;&#33021;&#26377;&#30410;&#65292;&#20294;&#35201;&#24471;&#21040;&#36866;&#24403;&#25215;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#65292;&#25220;&#34989;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21161;&#25163;&#30340;&#28389;&#29992;&#26159;&#26032;&#20852;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32593;&#39029;&#32534;&#31243;&#26041;&#38754;&#30340;&#30456;&#20851;&#30740;&#31350;&#24182;&#19981;&#22810;&#12290;&#25105;&#20204;&#35745;&#21010;&#24320;&#21457;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#24110;&#21161;&#25945;&#24072;&#35782;&#21035;&#36825;&#20004;&#31181;&#19981;&#24403;&#34892;&#20026;&#12290;&#20026;&#20102;&#20805;&#20998;&#20102;&#35299;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25511;&#21046;&#23454;&#39564;&#65292;&#35266;&#23519;&#20102;&#19981;&#20844;&#24179;&#21033;&#30410;&#21644;&#29305;&#28857;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#23398;&#29983;&#29420;&#31435;&#23436;&#25104;&#32593;&#39029;&#32534;&#31243;&#20219;&#21153;&#12289;&#25220;&#34989;&#25552;&#20132;&#21644;&#20351;&#29992;AI&#21161;&#25163;&#65288;ChatGPT&#65289;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21442;&#19982;&#36825;&#31867;&#19981;&#24403;&#34892;&#20026;&#30340;&#23398;&#29983;&#22312;&#27979;&#35797;&#25104;&#32489;&#19978;&#19982;&#29420;&#31435;&#23436;&#25104;&#26102;&#38388;&#36739;&#30701;&#30340;&#23398;&#29983;&#30456;&#24403;&#12290;&#25220;&#34989;&#30340;&#25552;&#20132;&#19982;&#29420;&#31435;&#25552;&#20132;&#31867;&#20284;&#65292;&#21482;&#26159;&#22312;&#39068;&#33394;&#21644;&#26631;&#35782;&#31526;&#21517;&#31216;&#31561;&#32454;&#33410;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#20351;&#29992;AI&#21161;&#25163;&#30340;&#25552;&#20132;&#26356;&#21152;&#22797;&#26434;&#65292;&#38477;&#20302;&#20102;&#21487;&#35835;&#24615;&#12290;&#23398;&#29983;&#35748;&#20026;&#20351;&#29992;AI&#21161;&#25163;&#21487;&#33021;&#26377;&#29992;&#65292;&#21069;&#25552;&#26159;&#24471;&#21040;&#36866;&#24403;&#30340;&#20351;&#29992;&#25215;&#35748;&#65292;&#23613;&#31649;&#20182;&#20204;&#23545;&#21487;&#35835;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In programming education, plagiarism and misuse of artificial intelligence (AI) assistance are emerging issues. However, not many relevant studies are focused on web programming. We plan to develop automated tools to help instructors identify both misconducts. To fully understand the issues, we conducted a controlled experiment to observe the unfair benefits and the characteristics. We compared student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT). Our study shows that students who are involved in such misconducts get comparable test marks with less completion time. Plagiarized submissions are similar to the independent ones except in trivial aspects such as color and identifier names. AI-assisted submissions are more complex, making them less readable. Students believe AI assistance could be useful given proper acknowledgment of the use, although they are not convinced with readability and c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25910;&#20837;&#26368;&#20248;&#25968;&#25454;&#24066;&#22330;&#35774;&#35745;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25193;&#23637;&#21069;&#27839;&#30740;&#31350;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.20096</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#24066;&#22330;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Data Market Design through Deep Learning. (arXiv:2310.20096v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25910;&#20837;&#26368;&#20248;&#25968;&#25454;&#24066;&#22330;&#35774;&#35745;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25193;&#23637;&#21069;&#27839;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\textit{&#25968;&#25454;&#24066;&#22330;&#35774;&#35745;}$&#38382;&#39064;&#26159;&#32463;&#27982;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#20449;&#21495;&#26041;&#26696;&#65288;&#32479;&#35745;&#23454;&#39564;&#65289;&#65292;&#20197;&#26368;&#22823;&#21270;&#20449;&#24687;&#21334;&#26041;&#30340;&#39044;&#26399;&#25910;&#20837;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#39564;&#25581;&#31034;&#20102;&#21334;&#26041;&#25152;&#30693;&#36947;&#30340;&#19968;&#20123;&#20449;&#24687;&#65292;&#24182;&#38468;&#24102;&#19968;&#20010;&#30456;&#24212;&#30340;&#20215;&#26684;[Bergemann et al., 2018]&#12290;&#27599;&#20010;&#20080;&#26041;&#22312;&#19990;&#30028;&#29615;&#22659;&#20013;&#37117;&#26377;&#33258;&#24049;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#20182;&#20204;&#23545;&#19982;&#29305;&#23450;&#23454;&#39564;&#30456;&#20851;&#32852;&#30340;&#20449;&#24687;&#30340;&#20027;&#35266;&#39044;&#26399;&#20540;&#26469;&#33258;&#20110;&#36825;&#20010;&#20915;&#31574;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#20808;&#39564;&#21644;&#19981;&#21516;&#32467;&#26524;&#30340;&#20215;&#20540;&#12290;&#22312;&#20855;&#26377;&#22810;&#20010;&#20080;&#26041;&#30340;&#29615;&#22659;&#20013;&#65292;&#20080;&#26041;&#23545;&#23454;&#39564;&#30340;&#39044;&#26399;&#20540;&#20063;&#21487;&#33021;&#21462;&#20915;&#20110;&#21334;&#32473;&#20854;&#20182;&#20154;&#30340;&#20449;&#24687;[Bonatti et al., 2022]&#12290;&#25105;&#20204;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#22312;&#25910;&#20837;&#26368;&#20248;&#25968;&#25454;&#24066;&#22330;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25193;&#23637;&#21487;&#20197;&#34987;&#29702;&#35299;&#21644;&#23454;&#29616;&#30340;&#36793;&#30028;&#12290;&#30456;&#23545;&#20110;&#20043;&#21069;&#20851;&#20110;&#25293;&#21334;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;[D\"utting et al., 2023]&#65292;&#25105;&#20204;&#24517;&#39035;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#35299;&#20915;&#25968;&#25454;&#24066;&#22330;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $\textit{data market design}$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\"utting et al., 2023], we must l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#30693;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#35748;&#20026;&#29992;&#20110;&#35780;&#20272;&#21477;&#27861;&#33021;&#21147;&#30340;&#22522;&#20934;&#19981;&#22815;&#20005;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20005;&#36873;&#25968;&#25454;&#38598;&#26469;&#25506;&#32034;&#35821;&#27861;&#32467;&#26500;&#22522;&#30784;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.20093</link><description>&lt;p&gt;
&#35780;&#20272;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#30693;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#35748;&#20026;&#29992;&#20110;&#35780;&#20272;&#21477;&#27861;&#33021;&#21147;&#30340;&#22522;&#20934;&#19981;&#22815;&#20005;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20005;&#36873;&#25968;&#25454;&#38598;&#26469;&#25506;&#32034;&#35821;&#27861;&#32467;&#26500;&#22522;&#30784;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#35757;&#32451;&#26041;&#24335;&#19982;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#35768;&#22810;&#25216;&#26415;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#20026;&#20854;&#20316;&#20026;&#35821;&#35328;&#31185;&#23398;&#29702;&#35770;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#35780;&#20272;LM&#30340;&#21477;&#27861;&#33021;&#21147;&#30340;&#19968;&#20123;&#20027;&#27969;&#22522;&#20934;&#21487;&#33021;&#19981;&#22815;&#20005;&#26684;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#27169;&#26495;&#30340;&#22522;&#20934;&#32570;&#20047;&#35821;&#35328;&#29702;&#35770;&#21644;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;&#24403;&#20351;&#29992;&#23567;&#35268;&#27169;&#25968;&#25454;&#26469;&#27169;&#25311;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#26102;&#65292;&#31616;&#21333;&#30340;&#22522;&#20934;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#21305;&#37197;LM&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#24050;&#32463;&#36807;&#22823;&#37327;&#27597;&#35821;&#32773;&#35780;&#20272;&#36807;&#26799;&#24230;&#21487;&#25509;&#21463;&#24615;&#24182;&#35774;&#35745;&#29992;&#20110;&#25506;&#32034;&#35821;&#27861;&#32467;&#26500;&#22522;&#30784;&#30340;&#20005;&#36873;&#25968;&#25454;&#38598;&#12290;&#22312;&#20854;&#20013;&#19968;&#20010;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;LI-Adger&#19978;&#65292;LM&#35780;&#20272;&#21477;&#23376;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with hu
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#26159;NLP&#31995;&#32479;&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#20851;&#38190;&#65292;&#26412;&#25991;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24635;&#32467;&#21644;&#26816;&#32034;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#24635;&#32467;&#22686;&#24378;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20081</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24635;&#32467;&#21644;&#26816;&#32034;&#25972;&#21512;&#65292;&#22686;&#24378;&#20010;&#24615;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models. (arXiv:2310.20081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20081
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26159;NLP&#31995;&#32479;&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#20851;&#38190;&#65292;&#26412;&#25991;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24635;&#32467;&#21644;&#26816;&#32034;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#24635;&#32467;&#22686;&#24378;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31995;&#32479;&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#26356;&#22909;&#22320;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#19968;&#20010;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#23558;&#36807;&#21435;&#30340;&#29992;&#25143;&#25968;&#25454;&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#36755;&#20837;&#36807;&#38271;&#65292;&#36229;&#20986;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#24182;&#19988;&#24341;&#36215;&#24310;&#36831;&#21644;&#25104;&#26412;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#25552;&#21462;&#30456;&#20851;&#30340;&#29992;&#25143;&#25968;&#25454;&#65288;&#21363;&#36873;&#25321;&#24615;&#26816;&#32034;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#21463;&#38480;&#20110;&#28508;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#12289;&#32570;&#20047;&#26356;&#28145;&#20837;&#30340;&#29992;&#25143;&#29702;&#35299;&#21644;&#20919;&#21551;&#21160;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24635;&#32467;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#26816;&#32034;&#22686;&#24378;&#20010;&#24615;&#21270;&#19982;&#20219;&#21153;&#24863;&#30693;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-awar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FOCAL&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#20174;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#29305;&#24449;&#12290;&#23427;&#36890;&#36807;&#20351;&#27599;&#20010;&#27169;&#24577;&#37117;&#32534;&#30721;&#21040;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#31361;&#20986;&#20849;&#20139;&#29305;&#24449;&#21644;&#19987;&#23646;&#29305;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#24863;&#30693;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#21644;&#19987;&#23646;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.20071</link><description>&lt;p&gt;
FOCAL: &#22312;&#22240;&#23376;&#21270;&#27491;&#20132;&#28508;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space. (arXiv:2310.20071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FOCAL&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#20174;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#29305;&#24449;&#12290;&#23427;&#36890;&#36807;&#20351;&#27599;&#20010;&#27169;&#24577;&#37117;&#32534;&#30721;&#21040;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#31361;&#20986;&#20849;&#20139;&#29305;&#24449;&#21644;&#19987;&#23646;&#29305;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#24863;&#30693;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#21644;&#19987;&#23646;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FOCAL&#30340;&#26032;&#22411;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#20174;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#24863;&#30693;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#23545;&#27604;&#26694;&#26550;&#20027;&#35201;&#20381;&#36182;&#20110;&#24863;&#30693;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#23545;&#29702;&#35299;&#24213;&#23618;&#24863;&#30693;&#29289;&#29702;&#23398;&#33267;&#20851;&#37325;&#35201;&#30340;&#19987;&#23646;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#27604;&#26694;&#26550;&#27809;&#26377;&#36866;&#24403;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#23616;&#37096;&#24615;&#12290;FOCAL&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20855;&#20307;&#36129;&#29486;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#22810;&#27169;&#26102;&#38388;&#24207;&#21015;&#65292;&#23558;&#27599;&#20010;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#35813;&#28508;&#31354;&#38388;&#30001;&#20849;&#20139;&#29305;&#24449;&#21644;&#24444;&#27492;&#27491;&#20132;&#30340;&#19987;&#23646;&#29305;&#24449;&#32452;&#25104;&#12290;&#20849;&#20139;&#31354;&#38388;&#36890;&#36807;&#27169;&#24577;&#21305;&#37197;&#30446;&#26631;&#24378;&#35843;&#36328;&#24863;&#30693;&#27169;&#24577;&#30340;&#29305;&#24449;&#27169;&#24335;&#19968;&#33268;&#24615;&#12290;&#30456;&#21453;&#65292;&#19987;&#23646;&#31354;&#38388;&#36890;&#36807;&#19968;&#20010;&#30446;&#26631;&#25552;&#21462;&#27169;&#24577;&#29420;&#21344;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vignat&#30340;&#28431;&#27934;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#30340;&#22270;&#32423;&#35821;&#20041;&#34920;&#31034;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;C&#24211;&#30340;&#21487;&#38752;&#25968;&#25454;&#38598;&#19978;&#65292;Vignat&#33021;&#22815;&#36798;&#21040;57.38%&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#28431;&#27934;&#27169;&#24335;&#30340;&#23453;&#36149;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.20067</link><description>&lt;p&gt;
Vignat: &#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#20195;&#30721;&#35821;&#20041;&#30340;&#28431;&#27934;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vignat: Vulnerability identification by learning code semantics via graph attention networks. (arXiv:2310.20067v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vignat&#30340;&#28431;&#27934;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#30340;&#22270;&#32423;&#35821;&#20041;&#34920;&#31034;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;C&#24211;&#30340;&#21487;&#38752;&#25968;&#25454;&#38598;&#19978;&#65292;Vignat&#33021;&#22815;&#36798;&#21040;57.38%&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#28431;&#27934;&#27169;&#24335;&#30340;&#23453;&#36149;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28431;&#27934;&#35782;&#21035;&#23545;&#20110;&#20445;&#25252;&#36719;&#20214;&#31995;&#32479;&#20813;&#21463;&#32593;&#32476;&#25915;&#20987;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#39033;&#30446;&#25317;&#26377;&#36229;&#36807;&#25968;&#30334;&#19975;&#34892;&#20195;&#30721;&#65292;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#20351;&#24471;&#20256;&#32479;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#26041;&#27861;&#38590;&#20197;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#28431;&#27934;&#30340;&#35821;&#20041;&#32467;&#26500;&#24046;&#24322;&#24456;&#22823;&#65292;&#24182;&#19988;&#21487;&#33021;&#21516;&#26102;&#21457;&#29983;&#65292;&#20351;&#24471;&#36890;&#29992;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vignat&#30340;&#26032;&#22411;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28431;&#27934;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#30340;&#22270;&#32423;&#35821;&#20041;&#34920;&#31034;&#26469;&#35782;&#21035;&#28431;&#27934;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#34920;&#31034;&#20026;&#32454;&#31890;&#24230;&#30340;&#20195;&#30721;&#23646;&#24615;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Vignat&#22312;&#26469;&#33258;&#27969;&#34892;&#30340;C&#24211;&#30340;&#21487;&#38752;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;57.38%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#20026;&#28431;&#27934;&#27169;&#24335;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability identification is crucial to protect software systems from attacks for cyber-security. However, huge projects have more than millions of lines of code, and the complex dependencies make it hard to carry out traditional static and dynamic methods. Furthermore, the semantic structure of various types of vulnerabilities differs greatly and may occur simultaneously, making general rule-based methods difficult to extend. In this paper, we propose \textit{Vignat}, a novel attention-based framework for identifying vulnerabilities by learning graph-level semantic representations of code. We represent codes with code property graphs (CPGs) in fine grain and use graph attention networks (GATs) for vulnerability detection. The results show that Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from popular C libraries. Furthermore, the interpretability of our GATs provides valuable insights into vulnerability patterns.
&lt;/p&gt;</description></item><item><title>&#20215;&#20540;&#23545;&#40784;&#23545;&#20110;&#26500;&#24314;&#23433;&#20840;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31687;&#35770;&#25991;&#20998;&#26512;&#20102;&#27010;&#24565;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#24573;&#35270;&#27010;&#24565;&#23545;&#40784;&#21487;&#33021;&#23548;&#33268;&#20215;&#20540;&#38169;&#20301;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#32852;&#21512;&#25512;&#29702;&#19968;&#20010;&#20154;&#30340;&#27010;&#24565;&#21644;&#20215;&#20540;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#36825;&#31867;&#22833;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#22312;&#26377;&#24847;&#34892;&#21160;&#26102;&#20250;&#32771;&#34385;&#20195;&#29702;&#20154;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2310.20059</link><description>&lt;p&gt;
&#27010;&#24565;&#23545;&#40784;&#20316;&#20026;&#20215;&#20540;&#23545;&#40784;&#30340;&#20808;&#20915;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Concept Alignment as a Prerequisite for Value Alignment. (arXiv:2310.20059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20059
&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#23545;&#40784;&#23545;&#20110;&#26500;&#24314;&#23433;&#20840;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31687;&#35770;&#25991;&#20998;&#26512;&#20102;&#27010;&#24565;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#24573;&#35270;&#27010;&#24565;&#23545;&#40784;&#21487;&#33021;&#23548;&#33268;&#20215;&#20540;&#38169;&#20301;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#32852;&#21512;&#25512;&#29702;&#19968;&#20010;&#20154;&#30340;&#27010;&#24565;&#21644;&#20215;&#20540;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#36825;&#31867;&#22833;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#22312;&#26377;&#24847;&#34892;&#21160;&#26102;&#20250;&#32771;&#34385;&#20195;&#29702;&#20154;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#23545;&#40784;&#23545;&#20110;&#26500;&#24314;&#21487;&#20197;&#23433;&#20840;&#21487;&#38752;&#22320;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20154;&#25152;&#37325;&#35270;&#30340;&#20107;&#29289; - &#29978;&#33267;&#26159;&#20182;&#33021;&#22815;&#37325;&#35270;&#30340;&#20107;&#29289; - &#21462;&#20915;&#20110;&#20182;&#20204;&#24403;&#21069;&#29992;&#20110;&#29702;&#35299;&#21644;&#35780;&#20272;&#19990;&#30028;&#21457;&#29983;&#30340;&#20107;&#24773;&#30340;&#27010;&#24565;&#12290;&#20215;&#20540;&#20381;&#36182;&#20110;&#27010;&#24565;&#65292;&#36825;&#24847;&#21619;&#30528;&#27010;&#24565;&#23545;&#40784;&#26159;&#23454;&#29616;&#20215;&#20540;&#23545;&#40784;&#30340;&#20808;&#20915;&#26465;&#20214; - &#20195;&#29702;&#38656;&#35201;&#23558;&#20854;&#23545;&#24773;&#22659;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#30340;&#24773;&#22659;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#25104;&#21151;&#22320;&#23545;&#40784;&#20854;&#20215;&#20540;&#35266;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#19979;&#23545;&#27010;&#24565;&#23545;&#40784;&#38382;&#39064;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24573;&#35270;&#27010;&#24565;&#23545;&#40784;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#24615;&#30340;&#20215;&#20540;&#38169;&#20301;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#32852;&#21512;&#25512;&#29702;&#19968;&#20010;&#20154;&#30340;&#27010;&#24565;&#21644;&#20215;&#20540;&#26469;&#26368;&#23567;&#21270;&#27492;&#31867;&#22833;&#35823;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26174;&#31034;&#20154;&#31867;&#22312;&#26377;&#24847;&#34892;&#21160;&#26102;&#20250;&#32771;&#34385;&#20195;&#29702;&#20154;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#65292;&#19982;&#25105;&#20204;&#30340;&#32852;&#21512;&#25512;&#29702;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint re
&lt;/p&gt;</description></item><item><title>&#26377;&#32422;&#26463;&#30340;&#23618;&#27425;&#33945;&#29305;&#21345;&#27931;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#65288;COBeTS&#65289;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#20998;&#35299;&#21644;&#32422;&#26463;&#36873;&#39033;&#25511;&#21046;&#22120;&#65292;&#23558;&#22312;&#32447;&#22522;&#20110;&#25628;&#32034;&#30340;CPOMDP&#35268;&#21010;&#25193;&#23637;&#21040;&#22823;&#22411;&#26426;&#22120;&#20154;&#38382;&#39064;&#65292;&#24182;&#33021;&#21516;&#26102;&#28385;&#36275;&#32422;&#26463;&#21644;&#22870;&#21169;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.20054</link><description>&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#23618;&#27425;&#33945;&#29305;&#21345;&#27931;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Constrained Hierarchical Monte Carlo Belief-State Planning. (arXiv:2310.20054v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20054
&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#23618;&#27425;&#33945;&#29305;&#21345;&#27931;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#65288;COBeTS&#65289;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#20998;&#35299;&#21644;&#32422;&#26463;&#36873;&#39033;&#25511;&#21046;&#22120;&#65292;&#23558;&#22312;&#32447;&#22522;&#20110;&#25628;&#32034;&#30340;CPOMDP&#35268;&#21010;&#25193;&#23637;&#21040;&#22823;&#22411;&#26426;&#22120;&#20154;&#38382;&#39064;&#65292;&#24182;&#33021;&#21516;&#26102;&#28385;&#36275;&#32422;&#26463;&#21644;&#22870;&#21169;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CPOMDPs&#65289;&#20013;&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#28385;&#36275;&#30828;&#24615;&#25104;&#26412;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22870;&#21169;&#30446;&#26631;&#65292;&#25512;&#24191;&#20102;&#29366;&#24577;&#21644;&#36807;&#28193;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23433;&#20840;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#25110;&#36830;&#32493;&#30340;&#38382;&#39064;&#22495;&#20013;&#36827;&#34892;&#22312;&#32447;CPOMDP&#35268;&#21010;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#22823;&#22411;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#23618;&#21160;&#20316;&#21407;&#35821;&#65288;&#36873;&#39033;&#65289;&#20026;&#20302;&#23618;&#25511;&#21046;&#25552;&#20379;&#24037;&#20855;&#65292;&#20998;&#23618;&#20998;&#35299;&#21487;&#20197;&#31616;&#21270;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#32422;&#26463;&#30340;&#36873;&#39033;&#20449;&#24565;&#26641;&#25628;&#32034;&#65288;COBeTS&#65289;&#26469;&#21033;&#29992;&#36825;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#23558;&#22312;&#32447;&#22522;&#20110;&#25628;&#32034;&#30340;CPOMDP&#35268;&#21010;&#25193;&#23637;&#21040;&#22823;&#22411;&#26426;&#22120;&#20154;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#21407;&#22987;&#36873;&#39033;&#25511;&#21046;&#22120;&#34987;&#23450;&#20041;&#20026;&#28385;&#36275;&#25351;&#23450;&#30340;&#32422;&#26463;&#39044;&#31639;&#65292;&#37027;&#20040;COBeTS&#23558;&#38543;&#26102;&#28385;&#36275;&#32422;&#26463;&#12290;&#21542;&#21017;&#65292;COBeTS&#23558;&#24341;&#23548;&#25628;&#32034;&#26397;&#30528;&#23433;&#20840;&#30340;&#36873;&#39033;&#21407;&#35821;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20998;&#23618;&#30417;&#25511;&#26469;&#23454;&#29616;&#36816;&#34892;&#26102;&#23433;&#20840;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23433;&#20840;&#20851;&#38190;&#30340;&#32422;&#26463;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;COBeTS&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrain
&lt;/p&gt;</description></item><item><title>SurpriseNet&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#21644;&#36328;&#20219;&#21153;&#30693;&#35782;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#26041;&#27861;&#21644;&#21463;&#24322;&#24120;&#26816;&#27979;&#21551;&#21457;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.20052</link><description>&lt;p&gt;
&#30475;&#25105;&#65292;&#19981;&#26159;&#37325;&#25773;&#65281;SurpriseNet&#65306;&#21463;&#24322;&#24120;&#26816;&#27979;&#21551;&#21457;&#30340;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning. (arXiv:2310.20052v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20052
&lt;/p&gt;
&lt;p&gt;
SurpriseNet&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#21644;&#36328;&#20219;&#21153;&#30693;&#35782;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#26041;&#27861;&#21644;&#21463;&#24322;&#24120;&#26816;&#27979;&#21551;&#21457;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#33268;&#21147;&#20110;&#21019;&#24314;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#22686;&#37327;&#35757;&#32451;&#20013;&#31215;&#32047;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#36830;&#32493;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#21363;&#26032;&#30693;&#35782;&#35206;&#30422;&#25110;&#24178;&#25200;&#36807;&#21435;&#30340;&#30693;&#35782;&#65292;&#23548;&#33268;&#36951;&#24536;&#12290;&#19982;&#20043;&#30456;&#20851;&#30340;&#38382;&#39064;&#26159;&#23398;&#20064;&#8220;&#36328;&#20219;&#21153;&#30693;&#35782;&#8221;&#65292;&#27169;&#22411;&#26080;&#27861;&#33719;&#21462;&#21644;&#20445;&#30041;&#26377;&#21161;&#20110;&#21306;&#20998;&#36328;&#20219;&#21153;&#36793;&#30028;&#19978;&#30340;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#8220;&#37325;&#25773;&#8221;&#65292;&#21363;&#21033;&#29992;&#26377;&#38480;&#30340;&#36807;&#21435;&#23454;&#20363;&#32531;&#20914;&#21306;&#26469;&#23398;&#20064;&#36328;&#20219;&#21153;&#30693;&#35782;&#24182;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#20542;&#21521;&#20110;&#36807;&#24230;&#25311;&#21512;&#26377;&#38480;&#30340;&#37325;&#25773;&#32531;&#20914;&#21306;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;SurpriseNet&#36890;&#36807;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#26041;&#27861;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#24182;&#20351;&#29992;&#21463;&#24322;&#24120;&#26816;&#27979;&#21551;&#21457;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#36328;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning "cross-task knowledge," where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is "replay," where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20049</link><description>&lt;p&gt;
SURF: GNN&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#23545;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#20174;&#31616;&#21333;&#38400;&#38376;&#21040;&#22797;&#26434;&#28065;&#36718;&#26426;&#26800;&#30340;&#33539;&#22260;&#12290;&#20934;&#30830;&#27714;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#26041;&#31243;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#29702;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#27867;&#21270;&#32780;&#38750;&#25554;&#20540;&#12290;&#27867;&#21270;&#26159;&#36890;&#29992;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12289;&#20998;&#36776;&#29575;&#25110;&#28909;&#21147;&#23398;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SURF&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#27979;&#35797;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;SURF&#21253;&#25324;&#21508;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20033</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#24635;&#32467;&#20013;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;LLaMA&#31995;&#21015;&#22312;&#25429;&#25417;&#21644;&#27987;&#32553;&#20851;&#38190;&#19978;&#19979;&#25991;&#20449;&#24687;&#21450;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31038;&#21306;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#34394;&#26500;&#38382;&#39064;&#30340;&#25285;&#24551;&#20173;&#22312;&#19981;&#26029;&#19978;&#21319;&#12290;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#34394;&#26500;&#30340;&#25688;&#35201;&#65292;&#36825;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#65289;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#38169;&#35823;&#30340;&#35786;&#26029;&#12290;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25215;&#35834;&#65292;&#20294;&#36825;&#31181;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#65292;&#32780;&#22312;&#20020;&#24202;&#39046;&#22495;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31649;&#36947;&#65292;&#20351;&#29992;ChatGPT&#20195;&#26367;&#20154;&#31867;&#19987;&#23478;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
&lt;/p&gt;</description></item><item><title>GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20025</link><description>&lt;p&gt;
GOPlan:&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20025
&lt;/p&gt;
&lt;p&gt;
GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#20026;&#20174;&#22810;&#26679;&#21270;&#21644;&#22810;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#23548;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#23545;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;Goal-conditioned Offline Planning&#65288;GOPlan&#65289;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#22810;&#27169;&#24577;&#21160;&#20316;&#20998;&#24067;&#30340;&#20808;&#39564;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21033;&#29992;&#35268;&#21010;&#30340;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#20026;&#24494;&#35843;&#31574;&#30053;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#39564;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#20998;&#31163;&#30340;&#24102;&#20248;&#21183;&#26435;&#37325;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21160;&#20316;&#30340;&#32570;&#28857;&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#65292;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23481;&#38169;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#30340;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#25925;&#38556;&#21644;&#21518;&#25925;&#38556;&#30340;&#20004;&#20010;&#19981;&#21516;&#39044;&#27979;&#36335;&#24452;&#39044;&#27979;&#20856;&#22411;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.20024</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#30340;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23481;&#38169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach. (arXiv:2310.20024v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23481;&#38169;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#30340;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#25925;&#38556;&#21644;&#21518;&#25925;&#38556;&#30340;&#20004;&#20010;&#19981;&#21516;&#39044;&#27979;&#36335;&#24452;&#39044;&#27979;&#20856;&#22411;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#25299;&#25169;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#20013;&#30340;&#25925;&#38556;&#21487;&#33021;&#20250;&#20005;&#37325;&#25200;&#20081;&#20854;&#25299;&#25169;&#32467;&#26500;&#65292;&#23548;&#33268;&#20854;&#23376;&#38598;&#20043;&#38388;&#22833;&#21435;&#36830;&#25509;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#33258;&#32452;&#32455;&#26426;&#22120;&#20154;&#32593;&#32476;&#65292;&#36827;&#34892;&#26368;&#20339;&#25299;&#25169;&#21512;&#25104;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#19988;&#32791;&#26102;&#30340;&#65292;&#38590;&#20197;&#23454;&#26102;&#23436;&#25104;&#12290;&#21482;&#26377;&#24403;&#20219;&#20309;&#25925;&#38556;&#21457;&#29983;&#21518;&#25299;&#25169;&#24674;&#22797;&#30340;&#27010;&#29575;&#36229;&#36807;&#19981;&#21487;&#24674;&#22797;&#30340;&#27010;&#29575;&#26102;&#65292;&#25165;&#24212;&#35813;&#25191;&#34892;&#25299;&#25169;&#37325;&#26032;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#27492;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#36125;&#21494;&#26031;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#25925;&#38556;&#21069;&#21644;&#25925;&#38556;&#21518;&#39044;&#27979;&#36335;&#24452;&#26469;&#39044;&#27979;&#20856;&#22411;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#36335;&#24452;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#19982;&#25991;&#29486;&#20013;&#26368;&#20339;&#30340;&#24403;&#21069;&#31574;&#30053;&#30456;&#27604;&#65292;&#26126;&#30830;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#25299;&#25169;&#65288;&#19981;&#65289;&#21487;&#24674;&#22797;&#24615;&#39044;&#27979;&#38382;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#22810;&#23610;&#24230;&#36974;&#25377;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24322;&#24120;&#28857;&#65292;&#24182;&#19988;&#22312;Dark Energy Survey Instrument&#20013;&#30340;&#26143;&#31995;&#20809;&#35889;&#24322;&#24120;&#28857;&#19978;&#21462;&#24471;&#20102;&#26356;&#26131;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20012</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#29305;&#24449;&#24402;&#22240;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Feature Attribution for Outliers. (arXiv:2310.20012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#22810;&#23610;&#24230;&#36974;&#25377;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24322;&#24120;&#28857;&#65292;&#24182;&#19988;&#22312;Dark Energy Survey Instrument&#20013;&#30340;&#26143;&#31995;&#20809;&#35889;&#24322;&#24120;&#28857;&#19978;&#21462;&#24471;&#20102;&#26356;&#26131;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#35782;&#21035;&#28023;&#37327;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#28857;&#65292;&#27604;&#20154;&#24037;&#26816;&#26597;&#35201;&#24555;&#24471;&#22810;&#12290;&#20294;&#26159;&#65292;&#19968;&#26086;&#21457;&#29616;&#36825;&#20123;&#24322;&#24120;&#28857;&#65292;&#24456;&#24555;&#23601;&#20250;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65306;&#21738;&#20123;&#29305;&#24449;&#20351;&#24471;&#36825;&#20010;&#36755;&#20837;&#25104;&#20026;&#24322;&#24120;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;&#36870;&#22810;&#23610;&#24230;&#36974;&#25377;&#65292;&#19987;&#20026;&#24322;&#24120;&#28857;&#35774;&#35745;&#65292;&#22240;&#20026;&#25105;&#20204;&#23545;&#35201;&#35782;&#21035;&#30340;&#29305;&#24449;&#31867;&#22411;&#20102;&#35299;&#24456;&#23569;&#65292;&#24182;&#19988;&#39044;&#35745;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#22240;&#20026;&#24322;&#24120;&#30340;&#27979;&#35797;&#25968;&#25454;&#24456;&#21487;&#33021;&#36229;&#36807;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;Dark Energy Survey Instrument&#26816;&#27979;&#21040;&#30340;&#26143;&#31995;&#20809;&#35889;&#24322;&#24120;&#28857;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#32467;&#26524;&#27604;&#20854;&#20182;&#24402;&#22240;&#26041;&#27861;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36827;&#21270;&#28216;&#25103;&#35774;&#35745;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#29983;&#25104;&#12298;&#39118;&#38505;&#12299;&#28216;&#25103;&#30340;&#26032;&#21464;&#20307;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#26032;&#21464;&#20307;&#25317;&#26377;&#26356;&#23567;&#30340;&#22320;&#22270;&#21644;&#26356;&#30701;&#30340;&#27604;&#36187;&#26102;&#38388;&#65292;&#24182;&#20135;&#29983;&#26356;&#21152;&#24179;&#34913;&#30340;&#28216;&#25103;&#23545;&#23616;&#12290;</title><link>http://arxiv.org/abs/2310.20008</link><description>&lt;p&gt;
&#36827;&#21270;&#26700;&#38754;&#28216;&#25103;&#35774;&#35745;&#65306;&#20197;&#8220;&#39118;&#38505;&#28216;&#25103;&#8221;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Tabletop Game Design: A Case Study in the Risk Game. (arXiv:2310.20008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36827;&#21270;&#28216;&#25103;&#35774;&#35745;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#29983;&#25104;&#12298;&#39118;&#38505;&#12299;&#28216;&#25103;&#30340;&#26032;&#21464;&#20307;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#26032;&#21464;&#20307;&#25317;&#26377;&#26356;&#23567;&#30340;&#22320;&#22270;&#21644;&#26356;&#30701;&#30340;&#27604;&#36187;&#26102;&#38388;&#65292;&#24182;&#20135;&#29983;&#26356;&#21152;&#24179;&#34913;&#30340;&#28216;&#25103;&#23545;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#21019;&#36896;&#21644;&#35780;&#20272;&#28216;&#25103;&#26159;&#19968;&#39033;&#33392;&#24040;&#32780;&#36153;&#26102;&#30340;&#20219;&#21153;&#12290;&#31243;&#24207;&#29983;&#25104;&#20869;&#23481;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#28216;&#25103;&#20803;&#32032;&#26469;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#36890;&#24120;&#19981;&#33021;&#29983;&#25104;&#23436;&#25972;&#30340;&#28216;&#25103;&#12290;&#36827;&#21270;&#28216;&#25103;&#35774;&#35745;&#23558;&#36827;&#21270;&#31639;&#27861;&#19982;&#33258;&#21160;&#21270;&#27979;&#35797;&#30456;&#32467;&#21512;&#65292;&#24050;&#29992;&#20110;&#21019;&#24314;&#20855;&#22791;&#31616;&#21333;&#35774;&#22791;&#30340;&#26032;&#39062;&#26700;&#38754;&#28216;&#25103;&#65307;&#28982;&#32780;&#65292;&#21407;&#26377;&#26041;&#27861;&#24182;&#19981;&#21253;&#25324;&#24102;&#26377;&#39600;&#23376;&#12289;&#21345;&#29260;&#21644;&#22320;&#22270;&#31561;&#22797;&#26434;&#26700;&#38754;&#28216;&#25103;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#26700;&#38754;&#28216;&#25103;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#12298;&#39118;&#38505;&#12299;&#36825;&#27454;&#20891;&#20107;&#31574;&#30053;&#28216;&#25103;&#30340;&#21464;&#20307;&#26469;&#35780;&#20272;&#35813;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#25152;&#36873;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#35268;&#21017;&#20013;&#24515;&#20195;&#29702;&#27979;&#35797;&#28216;&#25103;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#36136;&#37327;&#26631;&#20934;&#35780;&#20272;&#29983;&#25104;&#30340;&#26032;&#21464;&#20307;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#21019;&#36896;&#20102;&#21407;&#22987;&#28216;&#25103;&#30340;&#26032;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20855;&#26377;&#36739;&#23567;&#30340;&#22320;&#22270;&#65292;&#23548;&#33268;&#27604;&#36187;&#26102;&#38388;&#26356;&#30701;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21464;&#20307;&#20135;&#29983;&#20102;&#26356;&#21152;&#24179;&#34913;&#30340;&#27604;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, main
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.20007</link><description>&lt;p&gt;
&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20013;&#27748;&#26222;&#26862;&#37319;&#26679;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#65292;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#31163;&#25955;&#30340;&#20195;&#29702;&#29615;&#22659;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#19968;&#33268;&#24615;&#23545;&#20449;&#24687;&#27604;&#36827;&#34892;&#20102;&#31934;&#30830;&#20998;&#26512;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#20026;$\widetilde{O}(H\sqrt{d_{l_1}T})$&#65292;&#20854;&#20013;$H$&#20026;&#22238;&#21512;&#38271;&#24230;&#65292;$d_{l_1}$&#20026;&#29615;&#22659;&#31354;&#38388;&#30340;Kolmogorov $l_1$&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#25214;&#21040;&#20102;$d_{l_1}$&#30340;&#20855;&#20307;&#30028;&#38480;&#65292;&#27604;&#22914;&#34920;&#26684;&#12289;&#32447;&#24615;&#21644;&#26377;&#38480;&#28151;&#21512;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31105;&#24524;&#25628;&#32034;&#30340;&#31616;&#21333;&#23398;&#20064;&#21551;&#21457;&#24335;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19990</link><description>&lt;p&gt;
&#25581;&#31034;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30340;&#23616;&#38480;&#24615;: &#20320;&#26159;&#26368;&#24378;&#22823;&#30340;&#28201;&#39034;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?. (arXiv:2310.19990v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31105;&#24524;&#25628;&#32034;&#30340;&#31616;&#21333;&#23398;&#20064;&#21551;&#21457;&#24335;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#30456;&#32467;&#21512;&#24050;&#32463;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#21464;&#24471;&#27969;&#34892;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#20294;&#23427;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#24037;&#31243;&#25237;&#20837;&#19979;&#23637;&#29616;&#20986;&#20102;&#24456;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20123;&#25972;&#21512;&#23581;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#20013;&#31561;&#22797;&#26434;&#24615;&#21644;&#24369;&#22522;&#32447;&#30340;&#24773;&#20917;&#22312;&#20934;&#30830;&#35780;&#20272;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#26102;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20854;&#27425;&#65292;&#32570;&#20047;&#28040;&#34701;&#30740;&#31350;&#20351;&#24471;&#20934;&#30830;&#22320;&#37327;&#21270;&#21644;&#24402;&#22240;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#25913;&#36827;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#21518;&#65292;&#22312;&#19981;&#21516;&#20998;&#24067;&#19979;&#23398;&#20064;&#21551;&#21457;&#24335;&#30340;&#27867;&#21270;&#24615;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#34987;&#35782;&#21035;&#20986;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#31105;&#24524;&#25628;&#32034;&#30340;&#31616;&#21333;&#23398;&#20064;&#21551;&#21457;&#24335;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heurist
&lt;/p&gt;</description></item><item><title>BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19975</link><description>&lt;p&gt;
BioInstruct:&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19975
&lt;/p&gt;
&lt;p&gt;
BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#39046;&#22495;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21482;&#21457;&#34920;&#20102;&#24456;&#23569;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#23450;&#21046;&#30340;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;25,000&#20010;&#31034;&#20363;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#20010;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#20197;GPT-4&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#20248;&#21270;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA LLMs (1&amp;2,7B&amp;13B)&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25351;&#20196;&#22914;&#20309;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
&lt;/p&gt;</description></item><item><title>ExPT&#26159;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#24212;&#29992;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#21644;&#26399;&#26395;&#36755;&#20986;&#65292;&#29983;&#25104;&#26368;&#20248;&#30340;&#36755;&#20837;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#25110;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19961</link><description>&lt;p&gt;
ExPT: &#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19961
&lt;/p&gt;
&lt;p&gt;
ExPT&#26159;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#24212;&#29992;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#21644;&#26399;&#26395;&#36755;&#20986;&#65292;&#29983;&#25104;&#26368;&#20248;&#30340;&#36755;&#20837;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#25110;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#35774;&#35745;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#35774;&#35745;&#35780;&#20272;&#30340;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#23433;&#20840;&#25104;&#26412;&#65292;&#26679;&#26412;&#25928;&#29575;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#65292;&#35201;&#20040;&#20381;&#36182;&#23545;&#36807;&#21435;&#23454;&#39564;&#30340;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#65292;&#20854;&#20013;&#21482;&#26377;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#36755;&#20837;&#35774;&#35745;&#26679;&#26412;&#21450;&#20854;&#30456;&#24212;&#30340;&#25968;&#20540;&#21487;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#30475;&#20316;&#26159;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#26681;&#25454;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#21644;&#26399;&#26395;&#30340;&#36755;&#20986;&#26465;&#20214;&#29983;&#25104;&#26368;&#20248;&#30340;&#36755;&#20837;&#35774;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23454;&#39564;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;ExPT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#23454;&#39564;&#35774;&#35745;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#21512;&#25104;&#39044;&#35757;&#32451;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#22312;ExPT&#20013;&#65292;&#25105;&#20204;&#21482;&#20551;&#35774;&#23545;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26377;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.19957</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65306;&#26426;&#36935;&#21644;&#25361;&#25112;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges. (arXiv:2310.19957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#23450;&#20301;&#31995;&#32479;&#12289;&#36965;&#24863;&#21644;&#35745;&#31639;&#27169;&#25311;&#30340;&#36827;&#27493;&#65292;&#26469;&#33258;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22823;&#37327;&#26102;&#31354;&#25968;&#25454;&#27491;&#22312;&#20197;&#36234;&#26469;&#36234;&#24555;&#30340;&#36895;&#24230;&#34987;&#25910;&#38598;&#12290;&#28085;&#30422;&#30340;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#22320;&#29699;&#31185;&#23398;&#12289;&#20892;&#19994;&#12289;&#26234;&#24935;&#22478;&#24066;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#12290;&#36825;&#31181;&#26032;&#20852;&#30340;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#31354;&#22823;&#25968;&#25454;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#35299;&#20915;&#20197;&#24448;&#26080;&#27861;&#23454;&#29616;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#20363;&#22914;&#65292;&#36965;&#24863;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#22320;&#29699;&#22270;&#20687;&#22823;&#25968;&#25454;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#20247;&#22810;&#22303;&#22320;&#35206;&#30422;&#21644;&#22303;&#22320;&#21033;&#29992;&#24314;&#27169;&#20219;&#21153;&#12290;&#27839;&#28023;&#27169;&#25311;&#23398;&#23478;&#21487;&#20197;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#26367;&#20195;&#27169;&#22411;&#21152;&#36895;&#25968;&#20540;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#26102;&#31354;&#22823;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#32473;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23637;&#26395;&#20102;&#26102;&#31354;&#22823;&#25968;&#25454;&#30340;&#21508;&#31181;&#31867;&#22411;&#65292;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#31354;&#22823;&#25968;&#25454;&#20013;&#30340;&#26032;&#30740;&#31350;&#26426;&#20250;&#65292;&#24182;&#21015;&#20030;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19944</link><description>&lt;p&gt;
&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#26159;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#23427;&#23558;&#39550;&#39542;&#29615;&#22659;&#21644;&#30495;&#23454;&#26410;&#26469;&#20851;&#31995;&#24314;&#31435;&#22312;&#27010;&#29575;&#38544;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#27492;&#31354;&#38388;&#29983;&#25104;&#39044;&#27979;&#12290;&#26412;&#25991;&#23545;CVAE&#30340;&#20851;&#38190;&#32452;&#20214;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21457;&#29616;&#21464;&#21270;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#37319;&#26679;&#33021;&#22815;&#26356;&#36866;&#21512;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#32780;&#38543;&#26426;&#37319;&#26679;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20854;&#20182;&#25913;&#36827;&#65292;&#21253;&#25324;&#26356;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#38544;&#31354;&#38388;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#33021;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CVAE&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;INTERACTION&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#24072;&#29983;&#26550;&#26500;&#22312;&#23569;&#26631;&#27880;&#23398;&#20064;&#35774;&#32622;&#20013;&#36991;&#20813;&#20102;&#20381;&#36182;&#25935;&#24863;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#22312;&#26631;&#27880;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.19936</link><description>&lt;p&gt;
&#38754;&#21521;&#23569;&#26631;&#27880;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#65306;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#24072;&#29983;&#26550;&#26500;&#22312;&#23569;&#26631;&#27880;&#23398;&#20064;&#35774;&#32622;&#20013;&#36991;&#20813;&#20102;&#20381;&#36182;&#25935;&#24863;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#22312;&#26631;&#27880;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19987;&#38376;&#30340;&#21644;&#23494;&#38598;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#65292;&#26631;&#35760;&#25968;&#25454;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#26412;&#36739;&#39640;&#65292;&#22240;&#27492;&#23569;&#26679;&#26412;&#21644;&#21322;&#30417;&#30563;&#27169;&#22411;&#25104;&#20026;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#23569;&#26679;&#26412;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#30456;&#20284;&#25968;&#37327;&#30340;&#21442;&#25968;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#27604;&#22522;&#20110;&#21367;&#31215;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#26368;&#26032;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#27809;&#26377;&#37027;&#20040;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;Deformable DETR&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#24072;&#29983;&#26550;&#26500;&#22312;&#23569;&#26631;&#27880;&#23398;&#20064;&#35774;&#32622;&#20013;&#36991;&#20813;&#20102;&#20381;&#36182;&#25935;&#24863;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;COVO&#21644;Pascal VOC&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#29305;&#21035;&#26159;&#26631;&#27880;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#24320;&#21551;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilitie
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#36890;&#36807;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#38271;&#26399;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#20248;&#21270;&#22256;&#38590;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#26799;&#24230;&#26041;&#24046;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19927</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65306;&#29702;&#35770;&#21644;&#23454;&#36341;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19927
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#36890;&#36807;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#38271;&#26399;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#20248;&#21270;&#22256;&#38590;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#26799;&#24230;&#26041;&#24046;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#24212;&#29992;&#20110;&#38271;&#26399;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#21487;&#33021;&#20250;&#36935;&#21040;&#28151;&#20081;&#21644;&#38750;&#24179;&#28369;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#23548;&#33268;&#26799;&#24230;&#26041;&#24046;&#29190;&#28856;&#65292;&#20174;&#32780;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#36825;&#19982;&#20256;&#32479;&#35266;&#24565;&#30456;&#21453;&#65292;&#21363;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31561;&#38382;&#39064;&#20013;&#20855;&#26377;&#36739;&#20302;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#23547;&#25214;&#35299;&#20915;&#20248;&#21270;&#22256;&#38590;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25351;&#20986;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#24179;&#28369;&#24615;&#26159;&#24433;&#21709;&#26799;&#24230;&#20272;&#35745;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#29190;&#28856;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding var
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19917</link><description>&lt;p&gt;
&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#20559;&#35265;&#26816;&#27979;&#21644;&#32531;&#35299;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22312;&#21307;&#30103;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#32508;&#36848;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#12290;&#26041;&#27861;&#65306;&#36981;&#24490;Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)&#20934;&#21017;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#20174;PubMed&#12289;Web of Science&#21644;&#30005;&#27668;&#21644;&#30005;&#23376;&#24037;&#31243;&#24072;&#23398;&#20250;&#20013;&#26816;&#32034;&#20102;2010&#24180;1&#26376;1&#26085;&#33267;2022&#24180;10&#26376;31&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;&#32467;&#26524;&#65306;&#22312;&#26816;&#32034;&#21040;&#30340;252&#31687;&#25991;&#31456;&#20013;&#65292;&#26377;20&#31687;&#31526;&#21512;&#26368;&#32456;&#32508;&#36848;&#30340;&#32435;&#20837;&#26631;&#20934;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#20845;&#31181;&#20559;&#35265;&#20013;&#30340;&#20116;&#31181;&#65306;&#20843;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#36873;&#25321;&#20559;&#35265;&#65307;&#20845;&#39033;&#30740;&#31350;&#38024;&#23545;&#38544;&#24615;&#20559;&#35265;&#65307;&#20116;&#39033;&#30740;&#31350;&#23545;&#28151;&#26434;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#22235;&#39033;&#30740;&#31350;&#23545;&#27979;&#37327;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#20004;&#39033;&#30740;&#31350;&#23545;&#31639;&#27861;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#26041;&#38754;&#65292;&#26377;&#21313;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19906</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#21644;&#23545;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#36825;&#20652;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;&#21407;&#22411;&#30340;&#20351;&#29992;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#21407;&#22411;&#26469;&#26263;&#31034;&#24433;&#21709;&#39044;&#27979;&#30340;&#35757;&#32451;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#32473;&#21407;&#22411;&#25552;&#20379;&#26469;&#33258;&#25972;&#20010;&#22270;&#30340;&#36807;&#22810;&#20449;&#24687;&#65292;&#23548;&#33268;&#20851;&#38190;&#23376;&#32467;&#26500;&#30340;&#25490;&#38500;&#25110;&#26080;&#20851;&#23376;&#32467;&#26500;&#30340;&#21253;&#21547;&#65292;&#36825;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#31216;&#20026;&#35299;&#37322;&#24615;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048; (PGIB)&#65292;&#23558;&#21407;&#22411;&#23398;&#20064;&#32435;&#20837;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65292;&#20026;&#21407;&#22411;&#25552;&#20379;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26234;&#33021;&#32452;&#21512;&#22120;&#65292;&#19968;&#32676;&#24320;&#28304;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;&#19987;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19902</link><description>&lt;p&gt;
Herd&#65306;&#36890;&#36807;&#26234;&#33021;&#32452;&#21512;&#22120;&#20351;&#29992;&#22810;&#20010;&#36739;&#23567;&#30340;LLM&#26469;&#19982;&#19987;&#26377;&#30340;&#22823;&#22411;LLM&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer. (arXiv:2310.19902v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19902
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#32452;&#21512;&#22120;&#65292;&#19968;&#32676;&#24320;&#28304;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;&#19987;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23384;&#22312;&#36229;&#36807;&#19968;&#21315;&#20010;&#22810;&#21151;&#33021;&#30340;LLM&#65292;&#21487;&#20197;&#25191;&#34892;&#23454;&#38469;&#20219;&#21153;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20869;&#23481;&#29983;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#20813;&#36153;&#27169;&#22411;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#35268;&#27169;&#21644;&#21487;&#38752;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26085;&#24120;&#20351;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#35775;&#38382;&#21644;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20687;HuggingFace&#36825;&#26679;&#30340;&#32452;&#32455;&#24050;&#32463;&#21019;&#24314;&#20102;&#27169;&#22411;&#20179;&#24211;&#65292;&#29992;&#25143;&#21487;&#20197;&#19978;&#20256;&#24050;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#37327;&#21270;&#29256;&#26412;&#65292;&#20197;&#21450;&#25551;&#36848;&#35757;&#32451;&#36807;&#31243;&#30340;&#27169;&#22411;&#21345;&#29255;&#12290;&#23613;&#31649;&#19968;&#20123;&#27169;&#22411;&#25253;&#21578;&#20102;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#38750;&#25152;&#26377;&#27169;&#22411;&#37117;&#22914;&#27492;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19982;&#27169;&#22411;&#37096;&#32626;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#30495;&#23454;&#19990;&#30028;&#24433;&#21709;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#32676;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#36335;&#30001;&#22120;&#36798;&#21040;&#25110;&#36229;&#36807;&#19987;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#30340;&#32676;&#33021;&#22815;&#36798;&#21040;ChatGPT&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&amp;A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#25935;&#24863;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30740;&#31350;&#32593;&#32476;&#30340;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#21512;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#33539;&#22260;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#32423;&#21035;&#38598;&#36941;&#21382;&#31639;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19982;&#32473;&#23450;&#28304;&#22270;&#20687;&#30456;&#20284;&#20294;&#23646;&#20110;&#30456;&#21516;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.19889</link><description>&lt;p&gt;
&#25506;&#32034;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#30450;&#28857;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Exploring Geometry of Blind Spots in Vision Models. (arXiv:2310.19889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#25935;&#24863;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30740;&#31350;&#32593;&#32476;&#30340;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#21512;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#33539;&#22260;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#32423;&#21035;&#38598;&#36941;&#21382;&#31639;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19982;&#32473;&#23450;&#28304;&#22270;&#20687;&#30456;&#20284;&#20294;&#23646;&#20110;&#30456;&#21516;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#23545;&#25509;&#36817;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#65292;&#28145;&#24230;&#32593;&#32476;&#20063;&#21487;&#33021;&#20986;&#29616;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#22823;&#24133;&#25200;&#21160;&#19981;&#25935;&#24863;&#30340;&#24773;&#20917;&#65292;&#32780;&#36825;&#24182;&#19981;&#20250;&#23548;&#33268;&#32593;&#32476;&#28608;&#27963;&#21457;&#29983;&#26126;&#26174;&#25913;&#21464;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#22312;CNNs&#21644;Transformers&#31561;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#19981;&#25935;&#24863;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#30740;&#31350;&#36825;&#20123;&#32593;&#32476;&#8220;&#31561;&#32622;&#20449;&#24230;&#8221;&#32423;&#21035;&#38598;&#21512;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#33539;&#22260;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#21035;&#38598;&#36941;&#21382;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#26799;&#24230;&#30340;&#27491;&#20132;&#20998;&#37327;&#26469;&#36845;&#20195;&#22320;&#25506;&#32034;&#19982;&#36755;&#20837;&#31354;&#38388;&#20013;&#39640;&#32622;&#20449;&#24230;&#21306;&#22495;&#12290;&#32473;&#23450;&#19968;&#20010;&#28304;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#31639;&#27861;&#26469;&#35782;&#21035;&#19982;&#28304;&#22270;&#20687;&#23646;&#20110;&#30456;&#21516;&#31561;&#32622;&#20449;&#24230;&#32423;&#21035;&#38598;&#30340;&#36755;&#20837;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#24863;&#30693;&#19978;&#19982;&#20219;&#24847;&#22270;&#20687;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary image
&lt;/p&gt;</description></item><item><title>Res-Tuning&#26159;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#33539;&#24335;&#65292;&#36890;&#36807;&#35299;&#32465;&#35843;&#33410;&#22120;&#19982;&#20027;&#24178;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#39640;&#25928;&#30340;&#35843;&#33410;&#12290;&#36825;&#31181;&#32467;&#26500;&#35299;&#31163;&#20351;&#24471;&#35843;&#33410;&#22120;&#30340;&#35774;&#35745;&#19982;&#32593;&#32476;&#26550;&#26500;&#26080;&#20851;&#65292;&#26041;&#20415;&#20102;&#21508;&#31181;&#35843;&#33410;&#31574;&#30053;&#30340;&#28789;&#27963;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.19859</link><description>&lt;p&gt;
Res-Tuning: &#36890;&#36807;&#35299;&#32465;&#19982;&#20027;&#24178;&#30340;&#35843;&#33410;&#22120;&#26469;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#30340;&#35843;&#33410;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone. (arXiv:2310.19859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19859
&lt;/p&gt;
&lt;p&gt;
Res-Tuning&#26159;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#33539;&#24335;&#65292;&#36890;&#36807;&#35299;&#32465;&#35843;&#33410;&#22120;&#19982;&#20027;&#24178;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#39640;&#25928;&#30340;&#35843;&#33410;&#12290;&#36825;&#31181;&#32467;&#26500;&#35299;&#31163;&#20351;&#24471;&#35843;&#33410;&#22120;&#30340;&#35774;&#35745;&#19982;&#32593;&#32476;&#26550;&#26500;&#26080;&#20851;&#65292;&#26041;&#20415;&#20102;&#21508;&#31181;&#35843;&#33410;&#31574;&#30053;&#30340;&#28789;&#27963;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21442;&#25968;&#35843;&#33410;&#24050;&#25104;&#20026;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#21040;&#19979;&#28216;&#24212;&#29992;&#30340;&#36235;&#21183;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#19968;&#20123;&#36731;&#37327;&#32423;&#35843;&#33410;&#22120;&#23884;&#20837;&#20027;&#24178;&#20013;&#65292;&#35843;&#33410;&#22120;&#30340;&#35774;&#35745;&#21644;&#23398;&#20064;&#37117;&#39640;&#24230;&#20381;&#36182;&#20110;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Res-Tuning&#30340;&#26032;&#30340;&#35843;&#33410;&#33539;&#24335;&#65292;&#23427;&#26377;&#24847;&#23558;&#35843;&#33410;&#22120;&#20174;&#20027;&#24178;&#20013;&#35299;&#32465;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#35843;&#33410;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#35299;&#32465;&#20844;&#24335;&#19979;&#25317;&#26377;&#31561;&#25928;&#30340;&#23545;&#24212;&#29289;&#65292;&#24182;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;&#30001;&#20110;&#32467;&#26500;&#35299;&#31163;&#65292;&#25105;&#20204;&#21487;&#20197;&#33258;&#30001;&#35774;&#35745;&#35843;&#33410;&#22120;&#32780;&#19981;&#21463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#32452;&#21512;&#21508;&#31181;&#35843;&#33410;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;Res-Tuning&#21464;&#20307;&#65292;&#20854;&#20013;&#32469;&#36807;&#65288;&#30001;&#19968;&#31995;&#21015;&#35843;&#33410;&#22120;&#24418;&#25104;&#65289;&#26377;&#25928;&#22320;&#20174;&#20027;&#25903;&#20998;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated o
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.19852</link><description>&lt;p&gt;
AI&#23545;&#40784;: &#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26088;&#22312;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#12290;&#38543;&#30528;&#25317;&#26377;&#36229;&#20154;&#31867;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38169;&#35823;&#23545;&#40784;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22823;&#35268;&#27169;&#39118;&#38505;&#21464;&#24471;&#26126;&#26174;&#12290;&#25968;&#30334;&#21517;AI&#19987;&#23478;&#21644;&#20844;&#20247;&#20154;&#29289;&#37117;&#23545;AI&#39118;&#38505;&#34920;&#36798;&#20102;&#20851;&#27880;&#65292;&#35748;&#20026;&#20943;&#36731;AI&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#24212;&#35813;&#25104;&#20026;&#20840;&#29699;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#19982;&#22823;&#35268;&#27169;&#31038;&#20250;&#39118;&#38505;&#22914;&#22823;&#27969;&#34892;&#30149;&#21644;&#26680;&#25112;&#20105;&#24182;&#21015;&#12290;&#37492;&#20110;AI&#23545;&#40784;&#39046;&#22495;&#32570;&#20047;&#26368;&#26032;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#23545;&#40784;&#30740;&#31350;&#30340;&#26680;&#24515;&#27010;&#24565;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#30446;&#26631;&#21407;&#21017;&#20316;&#20026;AI&#23545;&#40784;&#30340;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#65288;RICE&#65289;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#23545;&#40784;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#12290;&#21069;&#32773;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.19845</link><description>&lt;p&gt;
&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#65306;&#20197;XGBoost&#22312;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#20013;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22403;&#22334;&#37038;&#20214;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#21644;&#21830;&#19994;&#30028;&#30340;&#20851;&#27880;&#12290;Twitter&#24050;&#25104;&#20026;&#20256;&#25773;&#22403;&#22334;&#37038;&#20214;&#20869;&#23481;&#30340;&#39318;&#36873;&#23186;&#20171;&#12290;&#35768;&#22810;&#30740;&#31350;&#21162;&#21147;&#35797;&#22270;&#24212;&#23545;&#31038;&#20132;&#32593;&#32476;&#22403;&#22334;&#37038;&#20214;&#12290;Twitter&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#24120;&#65292;&#30456;&#20851;&#30740;&#31350;&#24037;&#20316;&#20851;&#27880;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#20027;&#35201;&#25361;&#25112;&#65292;&#25110;&#32773;&#20135;&#29983;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#35813;&#31639;&#27861;&#21021;&#22987;&#21270;&#20102;&#19968;&#20010;eXtreme Gradient Boosting&#20998;&#31867;&#22120;&#65292;&#24182;&#20943;&#23569;&#20102;&#25512;&#25991;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;50&#27425;&#37325;&#22797;&#30340;10&#20493;&#20998;&#23618;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#36827;&#34892;&#20998;&#26512;&#12290;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#24179;&#22343;&#36798;&#21040;82.32&#65285;&#21644;92.67&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#26497;&#38480;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#30005;&#35805;&#33829;&#38144;&#36807;&#31243;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#21644;&#25104;&#26412;&#25935;&#24863;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30005;&#35805;&#33829;&#38144;&#25968;&#25454;&#21644;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#23545;&#23458;&#25143;&#24847;&#24895;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26500;&#24314;&#20986;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.19843</link><description>&lt;p&gt;
&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#26497;&#38480;&#22686;&#24378;&#27169;&#22411;&#23545;&#30005;&#35805;&#33829;&#38144;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65306;&#29305;&#24449;&#36873;&#25321;&#21644;&#25104;&#26412;&#25935;&#24863;&#30340;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach. (arXiv:2310.19843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#26497;&#38480;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#30005;&#35805;&#33829;&#38144;&#36807;&#31243;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#21644;&#25104;&#26412;&#25935;&#24863;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30005;&#35805;&#33829;&#38144;&#25968;&#25454;&#21644;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#23545;&#23458;&#25143;&#24847;&#24895;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26500;&#24314;&#20986;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#30452;&#25509;&#33829;&#38144;&#27963;&#21160;&#37117;&#26159;&#36890;&#36807;&#34394;&#25311;&#26041;&#24335;&#32780;&#19981;&#26159;&#38754;&#23545;&#38754;&#36827;&#34892;&#30340;&#65292;&#36825;&#21152;&#24555;&#20102;&#20154;&#38469;&#20132;&#24448;&#25216;&#24039;&#30340;&#34928;&#36864;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#20225;&#19994;&#19968;&#30452;&#22312;&#21162;&#21147;&#24863;&#30693;&#21644;&#20419;&#36827;&#23458;&#25143;&#25509;&#21463;&#33829;&#38144;&#25552;&#26696;&#30340;&#20542;&#21521;&#12290;&#25968;&#23383;&#36716;&#22411;&#21644;&#22686;&#21152;&#30340;&#34394;&#25311;&#23384;&#22312;&#36843;&#20351;&#20225;&#19994;&#23547;&#27714;&#26032;&#30340;&#33829;&#38144;&#30740;&#31350;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30005;&#35805;&#33829;&#38144;&#25968;&#25454;&#24314;&#27169;&#23458;&#25143;&#21150;&#29702;&#23450;&#26399;&#23384;&#27454;&#30340;&#24847;&#24895;&#65292;&#24182;&#25214;&#20986;&#23458;&#25143;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#12290;&#20351;&#29992;&#33889;&#33796;&#29273;&#38134;&#34892;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#22269;&#23478;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#26469;&#24314;&#27169;&#30005;&#35805;&#33829;&#38144;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#36873;&#25321;&#26368;&#20339;&#30340;&#21306;&#20998;&#29305;&#24449;&#21644;&#35843;&#25972;&#20998;&#31867;&#22120;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;COVID-19&#30123;&#33495;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#39539;&#26021;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#21644;&#32463;&#36807;&#31574;&#21010;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#33258;&#21160;&#39539;&#26021;&#34394;&#20551;&#20449;&#24687;&#12290;&#36825;&#20026;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.19834</link><description>&lt;p&gt;
AMIR&#65306;&#22522;&#20110;COVID-19&#30123;&#33495;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#39539;&#26021;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System. (arXiv:2310.19834v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;COVID-19&#30123;&#33495;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#39539;&#26021;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#21644;&#32463;&#36807;&#31574;&#21010;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#33258;&#21160;&#39539;&#26021;&#34394;&#20551;&#20449;&#24687;&#12290;&#36825;&#20026;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#30340;&#32972;&#26223;&#19979;&#65292;&#23427;&#21152;&#21095;&#20102;&#30123;&#33495;&#29369;&#35947;&#19981;&#20915;&#12290;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#33719;&#21462;&#30340;&#29616;&#26377;&#20449;&#24687;&#65292;&#24182;&#19982;&#26356;&#22810;&#32463;&#36807;&#31574;&#21010;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#20197;&#20419;&#36827;&#22823;&#35268;&#27169;&#33258;&#21160;&#39539;&#26021;&#34394;&#20551;&#20449;&#24687;&#12290;&#34429;&#28982;&#36825;&#37324;&#30340;&#24819;&#27861;&#21487;&#20197;&#25512;&#24191;&#24182;&#37325;&#26032;&#24212;&#29992;&#20110;&#20351;&#29992;&#22810;&#31181;&#20449;&#24687;&#26469;&#28304;&#21644;&#28385;&#36275;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20809;&#35889;&#30340;&#36739;&#22823;&#33539;&#22260;&#30340;&#34394;&#20551;&#20449;&#24687;&#32531;&#35299;&#65292;&#20294;&#26412;&#24037;&#20316;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#21463;&#38480;&#20110;&#20854;&#33539;&#22260;&#65292;&#20165;&#38480;&#20110;&#23545;&#25512;&#25991;&#30340;&#21453;&#39539;&#65292;&#19988;&#20165;&#38480;&#20110;COVID-19&#30456;&#20851;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;FaCov&#65288;&#32463;&#20107;&#23454;&#26680;&#26597;&#30340;&#25991;&#31456;&#65289;&#21644;misleading&#65288;&#31038;&#20132;&#23186;&#20307;&#25512;&#25991;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation has emerged as a major societal threat in recent years in general; specifically in the context of the COVID-19 pandemic, it has wrecked havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable solutions for combating misinformation are the need of the hour. This work explored how existing information obtained from social media and augmented with more curated fact checked data repositories can be harnessed to facilitate automated rebuttal of misinformation at scale. While the ideas herein can be generalized and reapplied in the broader context of misinformation mitigation using a multitude of information sources and catering to the spectrum of social media platforms, this work serves as a proof of concept, and as such, it is confined in its scope to only rebuttal of tweets, and in the specific context of misinformation regarding COVID-19. It leverages two publicly available datasets, viz. FaCov (fact-checked articles) and misleading (social media Twitt
&lt;/p&gt;</description></item><item><title>GalliformeSpectra&#26159;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#40481;&#21697;&#31181;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#26377;1010&#24352;&#22270;&#20687;&#65292;&#23637;&#31034;&#20102;&#27599;&#20010;&#21697;&#31181;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#23545;&#20110;&#23478;&#31165;&#31185;&#23398;&#12289;&#36951;&#20256;&#23398;&#21644;&#20892;&#19994;&#30740;&#31350;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19830</link><description>&lt;p&gt;
GalliformeSpectra: &#19968;&#20221;&#40481;&#21697;&#31181;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GalliformeSpectra: A Hen Breed Dataset. (arXiv:2310.19830v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19830
&lt;/p&gt;
&lt;p&gt;
GalliformeSpectra&#26159;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#40481;&#21697;&#31181;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#26377;1010&#24352;&#22270;&#20687;&#65292;&#23637;&#31034;&#20102;&#27599;&#20010;&#21697;&#31181;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#23545;&#20110;&#23478;&#31165;&#31185;&#23398;&#12289;&#36951;&#20256;&#23398;&#21644;&#20892;&#19994;&#30740;&#31350;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21313;&#31181;&#19981;&#21516;&#30340;&#27597;&#40481;&#21697;&#31181;&#65292;&#26469;&#33258;&#19981;&#21516;&#22320;&#21306;&#65292;&#25429;&#25417;&#20102;&#27599;&#20010;&#21697;&#31181;&#30340;&#29420;&#29305;&#29305;&#24449;&#21644;&#29305;&#28857;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;Bielefeld&#12289;Blackorpington&#12289;Brahma&#12289;Buckeye&#12289;Fayoumi&#12289;Leghorn&#12289;Newhampshire&#12289;Plymouthrock&#12289;Sussex&#21644;Turken&#21697;&#31181;&#65292;&#20026;&#19990;&#30028;&#33539;&#22260;&#20869;&#24120;&#35265;&#30340;&#23478;&#31165;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#20195;&#34920;&#24615;&#12290;&#20849;&#25910;&#38598;&#20102;1010&#24352;&#21407;&#22987;&#30340;JPG&#22270;&#29255;&#65292;&#23637;&#31034;&#20102;&#27599;&#20010;&#27597;&#40481;&#21697;&#31181;&#30340;&#20307;&#24577;&#29305;&#24449;&#12289;&#32701;&#27611;&#22270;&#26696;&#21644;&#29420;&#29305;&#29305;&#24449;&#12290;&#36825;&#20123;&#22270;&#29255;&#38543;&#21518;&#34987;&#26631;&#20934;&#21270;&#12289;&#35843;&#25972;&#22823;&#23567;&#24182;&#36716;&#25442;&#20026;PNG&#26684;&#24335;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#38598;&#30340;&#19968;&#33268;&#24615;&#12290;&#34429;&#28982;&#32534;&#21046;&#30340;&#36807;&#31243;&#22312;&#21697;&#31181;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#20294;&#36825;&#20010;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#36164;&#28304;&#65292;&#20026;&#23478;&#31165;&#31185;&#23398;&#12289;&#36951;&#20256;&#23398;&#21644;&#20892;&#19994;&#30740;&#31350;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26377;&#30528;&#37325;&#35201;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#21644;&#20998;&#26512;&#29420;&#29305;&#30340;&#29305;&#24449;&#26469;&#36129;&#29486;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a comprehensive dataset featuring ten distinct hen breeds, sourced from various regions, capturing the unique characteristics and traits of each breed. The dataset encompasses Bielefeld, Blackorpington, Brahma, Buckeye, Fayoumi, Leghorn, Newhampshire, Plymouthrock, Sussex, and Turken breeds, offering a diverse representation of poultry commonly bred worldwide. A total of 1010 original JPG images were meticulously collected, showcasing the physical attributes, feather patterns, and distinctive features of each hen breed. These images were subsequently standardized, resized, and converted to PNG format for consistency within the dataset. The compilation, although unevenly distributed across the breeds, provides a rich resource, serving as a foundation for research and applications in poultry science, genetics, and agricultural studies. This dataset holds significant potential to contribute to various fields by enabling the exploration and analysis of unique characte
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#20449;&#20219;&#21644;&#30693;&#35782;&#30340;&#24418;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#21482;&#26377;&#22312;&#27491;&#30830;&#29305;&#24449;&#19979;&#36328;&#24773;&#26223;&#33391;&#22909;&#36816;&#34892;&#30340;&#31639;&#27861;&#25165;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.19819</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#65306;&#20026;&#20160;&#20040;&#40065;&#26834;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19819
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#20449;&#20219;&#21644;&#30693;&#35782;&#30340;&#24418;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#21482;&#26377;&#22312;&#27491;&#30830;&#29305;&#24449;&#19979;&#36328;&#24773;&#26223;&#33391;&#22909;&#36816;&#34892;&#30340;&#31639;&#27861;&#25165;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#23545;&#20854;&#36755;&#20986;&#26377;&#20449;&#24515;&#12290;&#20449;&#24515;&#36890;&#24120;&#20197;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#37322;&#65292;&#21363;&#27169;&#22411;&#20135;&#29983;&#39640;&#27604;&#20363;&#27491;&#30830;&#36755;&#20986;&#26102;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21487;&#38752;&#24615;&#19981;&#33021;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#27169;&#22411;&#20381;&#36182;&#38169;&#35823;&#29305;&#24449;&#25110;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#21464;&#21270;&#12290;&#25105;&#35748;&#20026;&#65292;&#23545;&#20449;&#20219;&#30340;&#35748;&#35782;&#32500;&#24230;&#21487;&#20197;&#36890;&#36807;&#30693;&#35782;&#30340;&#27010;&#24565;&#26469;&#29702;&#35299;&#65292;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#21462;&#20915;&#20110;&#20854;&#29992;&#25143;&#26159;&#21542;&#33021;&#22815;&#30830;&#35748;&#20854;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#12290;&#30693;&#35782;&#35201;&#27714;&#20449;&#24565;&#22522;&#20110;&#27491;&#30830;&#30340;&#21407;&#22240;&#24418;&#25104;&#65292;&#24182;&#19988;&#23545;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21482;&#26377;&#22312;&#36328;&#21453;&#20107;&#23454;&#24773;&#26223;&#20013;&#33391;&#22909;&#36816;&#34892;&#65292;&#24182;&#22522;&#20110;&#27491;&#30830;&#29305;&#24449;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#25165;&#33021;&#25552;&#20379;&#30693;&#35782;&#12290;&#25105;&#35748;&#20026;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#25105;&#20204;&#24212;&#35813;&#20851;&#24515;&#20687;&#21487;&#35299;&#37322;&#24615;&#36825;&#26679;&#30340;&#27169;&#22411;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25299;&#25169;&#21464;&#21270;&#21644;&#31574;&#30053;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#35757;&#32451;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#20302;&#33021;&#32791;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.19815</link><description>&lt;p&gt;
&#35757;&#32451;&#26080;&#38656;&#28014;&#28857;&#31934;&#24230;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training binary neural networks without floating point precision. (arXiv:2310.19815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25299;&#25169;&#21464;&#21270;&#21644;&#31574;&#30053;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#35757;&#32451;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#20302;&#33021;&#32791;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#35757;&#32451;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#20302;&#33021;&#32791;&#30340;&#29305;&#28857;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25299;&#25169;&#21464;&#21270;&#21644;&#31574;&#30053;&#35757;&#32451;&#65292;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#26159;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main goal of this work is to improve the efficiency of training binary neural networks, which are low latency and low energy networks. The main contribution of this work is the proposal of two solutions comprised of topology changes and strategy training that allow the network to achieve near the state-of-the-art performance and efficient training. The time required for training and the memory required in the process are two factors that contribute to efficient training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36951;&#20256;&#25913;&#36827;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#26469;&#25552;&#39640;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20351;&#29992;LLM&#32534;&#36753;&#30340;&#34917;&#19969;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#25968;&#37327;&#39640;&#36798;75&#65285;&#65292;&#20294;&#30456;&#27604;&#36739;&#26631;&#20934;&#32534;&#36753;&#65292;LLMs&#25214;&#21040;&#30340;&#34917;&#19969;&#36739;&#23569;&#22810;&#26679;&#21270;&#12290;&#23613;&#31649;LLM&#22686;&#24378;&#30340;GI&#25214;&#21040;&#20102;&#35768;&#22810;&#25913;&#36827;&#30340;&#34917;&#19969;&#65292;&#20294;&#26368;&#22909;&#30340;&#25913;&#36827;&#34917;&#19969;&#26159;&#36890;&#36807;&#26631;&#20934;GI&#25214;&#21040;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.19813</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36951;&#20256;&#25913;&#36827;&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
Enhancing Genetic Improvement Mutations Using Large Language Models. (arXiv:2310.19813v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36951;&#20256;&#25913;&#36827;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#26469;&#25552;&#39640;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20351;&#29992;LLM&#32534;&#36753;&#30340;&#34917;&#19969;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#25968;&#37327;&#39640;&#36798;75&#65285;&#65292;&#20294;&#30456;&#27604;&#36739;&#26631;&#20934;&#32534;&#36753;&#65292;LLMs&#25214;&#21040;&#30340;&#34917;&#19969;&#36739;&#23569;&#22810;&#26679;&#21270;&#12290;&#23613;&#31649;LLM&#22686;&#24378;&#30340;GI&#25214;&#21040;&#20102;&#35768;&#22810;&#25913;&#36827;&#30340;&#34917;&#19969;&#65292;&#20294;&#26368;&#22909;&#30340;&#25913;&#36827;&#34917;&#19969;&#26159;&#36890;&#36807;&#26631;&#20934;GI&#25214;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#31243;&#24207;&#20462;&#22797;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36951;&#20256;&#25913;&#36827;&#65288;GI&#65289;&#31561;&#22522;&#20110;&#25628;&#32034;&#30340;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;LLMs&#20316;&#20026;GI&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#20197;&#25913;&#36827;&#25628;&#32034;&#36807;&#31243;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Gin Java GI&#24037;&#20855;&#21253;&#65292;&#20197;&#35843;&#29992;OpenAI&#30340;API&#20026;JCodec&#24037;&#20855;&#29983;&#25104;&#32534;&#36753;&#12290;&#25105;&#20204;&#20351;&#29992;5&#31181;&#19981;&#21516;&#30340;&#32534;&#36753;&#31867;&#22411;&#38543;&#26426;&#25277;&#26679;&#32534;&#36753;&#31354;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;LLM&#32534;&#36753;&#65292;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#34917;&#19969;&#25968;&#37327;&#39640;&#20110;&#20351;&#29992;&#26631;&#20934;&#25554;&#20837;&#32534;&#36753;&#30340;&#34917;&#19969;&#25968;&#37327;&#39640;&#36798;75&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#26631;&#20934;&#32534;&#36753;&#30456;&#27604;&#65292;LLMs&#25214;&#21040;&#30340;&#34917;&#19969;&#36890;&#24120;&#36739;&#23569;&#22810;&#26679;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#36816;&#34892;GI&#20197;&#23547;&#25214;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;&#23613;&#31649;LLM&#22686;&#24378;&#30340;GI&#25214;&#21040;&#20102;&#35768;&#22810;&#25913;&#36827;&#30340;&#34917;&#19969;&#65292;&#20294;&#26368;&#22909;&#30340;&#25913;&#36827;&#34917;&#19969;&#26159;&#36890;&#36807;&#26631;&#20934;GI&#25214;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19812</link><description>&lt;p&gt;
&#33041;&#35299;&#30721;&#65306;&#36208;&#21521;&#23454;&#26102;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20116;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#21644;&#22522;&#30784;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23545;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#35273;&#30693;&#35273;&#65292;&#29616;&#22312;&#21487;&#20197;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#35299;&#30721;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#65288;&#32422;&#20026;0.5 Hz&#65289;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#65288;&#32422;&#20026;5000 Hz&#65289;&#27979;&#37327;&#33041;&#27963;&#21160;&#30340;&#31070;&#32463;&#24433;&#20687;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;MEG&#35299;&#30721;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#21644;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#20174;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;ii&#65289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;MEG&#27169;&#22359;&#20197;&#21450;iii&#65289;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;MEG&#35299;&#30721;&#22120;&#22312;&#32463;&#20856;&#32447;&#24615;&#35299;&#30721;&#22120;&#19978;&#26174;&#31034;&#20986;7&#20493;&#30340;&#22270;&#20687;&#26816;&#32034;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#21518;&#26399;&#33041;&#37096;
&lt;/p&gt;
&lt;p&gt;
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.19806</link><description>&lt;p&gt;
&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#31561;&#20215;&#24615;&#23646;&#24615;&#30340;&#33258;&#21160;&#39564;&#35777;-&#23398;&#22763;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#24037;&#19994;&#24212;&#29992;&#22686;&#21152;&#65292;&#23545;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20851;&#38190;&#24212;&#29992;&#30340;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#31243;&#24207;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#24076;&#26395;&#26377;&#19968;&#31181;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#20248;&#21270;&#30340;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#12290;&#20174;&#24418;&#24335;&#19978;&#35762;&#65292;&#36825;&#23545;&#24212;&#20110;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20102;&#32763;&#35793;&#24037;&#20855;anthem&#12290;&#23427;&#21487;&#20197;&#19982;&#29992;&#20110;&#32463;&#20856;&#36923;&#36753;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#26159;&#21542;&#24378;&#31561;&#20215;&#12290;&#22312;&#24403;&#21069;&#29256;&#26412;&#30340;anthem&#20013;&#65292;&#21482;&#33021;&#39564;&#35777;&#20855;&#26377;&#21463;&#38480;&#36755;&#20837;&#35821;&#35328;&#30340;&#27491;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;&#36825;&#26159;anthem&#20013;&#23454;&#29616;&#30340;&#32763;&#35793;&#964;*&#30340;&#32467;&#26524;&#65292;&#23427;&#29983;&#25104;&#20102;here-and-there&#36923;&#36753;&#20013;&#30340;&#20844;&#24335;&#65292;&#35813;&#36923;&#36753;&#21482;&#23545;&#27491;&#31243;&#24207;&#19982;&#32463;&#20856;&#36923;&#36753;&#30456;&#19968;&#33268;&#12290;&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;anthem&#65292;&#20197;&#20415;&#21487;&#20197;&#39564;&#35777;&#26356;&#24191;&#27867;&#30340;&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in ord
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26680;&#30340;&#35282;&#24230;&#35770;&#36848;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#34892;&#20026;&#24230;&#37327;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#19982;MICo&#36317;&#31163;&#31561;&#20215;&#12290;&#27492;&#22806;&#65292;&#26680;&#30340;&#35270;&#35282;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#30028;&#23450;&#20215;&#20540;&#20989;&#25968;&#24046;&#24322;&#21644;&#23884;&#20837;&#21040;&#20302;&#22833;&#30495;&#35823;&#24046;&#30340;&#27431;&#27663;&#31354;&#38388;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;&#34892;&#20026;&#24230;&#37327;&#26500;&#24314;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19804</link><description>&lt;p&gt;
&#20174;&#26680;&#30340;&#35282;&#24230;&#30475;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#34892;&#20026;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Kernel Perspective on Behavioural Metrics for Markov Decision Processes. (arXiv:2310.19804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26680;&#30340;&#35282;&#24230;&#35770;&#36848;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#34892;&#20026;&#24230;&#37327;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#19982;MICo&#36317;&#31163;&#31561;&#20215;&#12290;&#27492;&#22806;&#65292;&#26680;&#30340;&#35270;&#35282;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#30028;&#23450;&#20215;&#20540;&#20989;&#25968;&#24046;&#24322;&#21644;&#23884;&#20837;&#21040;&#20302;&#22833;&#30495;&#35823;&#24046;&#30340;&#27431;&#27663;&#31354;&#38388;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;&#34892;&#20026;&#24230;&#37327;&#26500;&#24314;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#34892;&#20026;&#24230;&#37327;&#26159;&#26500;&#24314;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#27491;&#23450;&#26680;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#34892;&#20026;&#24230;&#37327;&#30340;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#35270;&#35282;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#65292;&#21487;&#20197;&#34987;&#35777;&#26126;&#19982;&#26368;&#36817;&#24341;&#20837;&#30340;MICo&#36317;&#31163;&#65288;Castro&#31561;&#20154;&#65292;2021&#24180;&#65289;&#31561;&#20215;&#12290;&#26680;&#30340;&#35270;&#35282;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#32467;&#26524;&#21253;&#25324;&#36890;&#36807;&#25105;&#20204;&#30340;&#24230;&#37327;&#26469;&#30028;&#23450;&#20215;&#20540;&#20989;&#25968;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#24230;&#37327;&#21487;&#20197;&#34987;&#35777;&#26126;&#23884;&#20837;&#21040;&#20855;&#26377;&#20302;&#22833;&#30495;&#35823;&#24046;&#30340;&#26377;&#38480;&#32500;&#27431;&#27663;&#31354;&#38388;&#20013;&#12290;&#36825;&#26159;&#22312;&#20351;&#29992;&#34892;&#20026;&#24230;&#37327;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26102;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.
&lt;/p&gt;</description></item><item><title>SyMPox&#26159;&#19968;&#20010;&#22522;&#20110;&#30151;&#29366;&#30340;&#33258;&#21160;&#29492;&#30168;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;XGBoost&#31639;&#27861;&#20998;&#26512;&#30151;&#29366;&#27169;&#24335;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#29492;&#30168;&#35786;&#26029;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.19801</link><description>&lt;p&gt;
&#22522;&#20110;&#30151;&#29366;&#30340;&#33258;&#21160;&#29492;&#30168;&#26816;&#27979;&#31995;&#32479;SyMPox&#65306;&#20351;&#29992;XGBoost&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost. (arXiv:2310.19801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19801
&lt;/p&gt;
&lt;p&gt;
SyMPox&#26159;&#19968;&#20010;&#22522;&#20110;&#30151;&#29366;&#30340;&#33258;&#21160;&#29492;&#30168;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;XGBoost&#31639;&#27861;&#20998;&#26512;&#30151;&#29366;&#27169;&#24335;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#29492;&#30168;&#35786;&#26029;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29492;&#30168;&#26159;&#19968;&#31181;&#20154;&#30044;&#20849;&#24739;&#30149;&#12290;&#21040;2023&#24180;6&#26376;10&#26085;&#65292;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#30830;&#35748;&#20102;&#32422;87000&#20363;&#29492;&#30168;&#30149;&#20363;&#12290;&#30446;&#21069;&#26368;&#24120;&#29992;&#30340;&#37492;&#23450;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#20687;&#35782;&#21035;&#25216;&#26415;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#19988;&#20165;&#20379;&#23569;&#25968;&#20154;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyMPox&#30340;&#29420;&#31435;&#24212;&#29992;&#65292;&#22522;&#20110;&#30151;&#29366;&#35786;&#26029;&#29492;&#30168;&#30149;&#20363;&#12290;SyMPox&#21033;&#29992;&#24378;&#22823;&#30340;XGBoost&#31639;&#27861;&#20998;&#26512;&#30151;&#29366;&#27169;&#24335;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;SyMPox&#20351;&#29992;Gradio&#26694;&#26550;&#24320;&#21457;&#65292;&#20026;&#20010;&#20154;&#25552;&#20379;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#30151;&#29366;&#24182;&#33719;&#24471;&#21487;&#38752;&#30340;&#29492;&#30168;&#35786;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monkeypox is a zoonotic disease. About 87000 cases of monkeypox were confirmed by the World Health Organization until 10th June 2023. The most prevalent methods for identifying this disease are image-based recognition techniques. Still, they are not too fast and could only be available to a few individuals. This study presents an independent application named SyMPox, developed to diagnose Monkeypox cases based on symptoms. SyMPox utilizes the robust XGBoost algorithm to analyze symptom patterns and provide accurate assessments. Developed using the Gradio framework, SyMPox offers a user-friendly platform for individuals to assess their symptoms and obtain reliable Monkeypox diagnoses.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19786</link><description>&lt;p&gt;
&#20174;&#22806;&#37096;&#21040;Swap&#36951;&#25022;2.0&#65306;&#38024;&#23545;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#32422;&#21270;&#21644;&#26080;&#30693;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#21040;&#22806;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#34892;&#20026;&#31354;&#38388;&#30340;&#26377;&#38480;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23384;&#22312;&#26576;&#20010;&#20551;&#35774;&#31867;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#65292;&#23601;&#24517;&#28982;&#23384;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#26080;Swap&#36951;&#25022;&#31639;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#21487;&#20197;&#20445;&#35777;&#22312;$\log(N)^{O(1/\epsilon)}$&#36718;&#21518;&#65292;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(N)$&#30340;&#24773;&#20917;&#19979;&#65292;Swap&#36951;&#25022;&#34987;&#38480;&#23450;&#20026;$\epsilon$&#65292;&#32780;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#38656;&#35201;$O(N/\epsilon^2)$&#36718;&#21644;&#33267;&#23569;$\Omega(N^2)$&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20276;&#38543;&#30528;&#19968;&#20010;&#30456;&#20851;&#30340;&#19979;&#30028;&#65292;&#19982;[BM07]&#19981;&#21516;&#65292;&#36825;&#20010;&#19979;&#30028;&#36866;&#29992;&#20110;&#26080;&#30693;&#21644;$\ell_1$-&#21463;&#38480;&#30340;&#23545;&#25163;&#21644;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#19979;&#30028;&#30340;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#20154;&#26426;&#20132;&#20114;&#21327;&#35758;&#30340;&#20849;&#21516;&#35789;&#27719;&#30340;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;&#35299;AI&#24212;&#35813;&#25552;&#20379;&#20160;&#20040;&#20449;&#24687;&#26469;&#24110;&#21161;&#20154;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19778</link><description>&lt;p&gt;
&#35774;&#35745;AI&#25903;&#25345;&#20154;&#31867;&#21442;&#19982;AI&#36741;&#21161;&#20915;&#31574;&#65306;&#20174;&#31995;&#32479;&#24615;&#22238;&#39038;&#20013;&#20998;&#31867;&#20154;&#26426;&#20132;&#20114;&#30340;&#31246;&#34920;
&lt;/p&gt;
&lt;p&gt;
Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review. (arXiv:2310.19778v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#20154;&#26426;&#20132;&#20114;&#21327;&#35758;&#30340;&#20849;&#21516;&#35789;&#27719;&#30340;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;&#35299;AI&#24212;&#35813;&#25552;&#20379;&#20160;&#20040;&#20449;&#24687;&#26469;&#24110;&#21161;&#20154;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21162;&#21147;&#36807;&#20998;&#20851;&#27880;&#25216;&#26415;&#36827;&#27493;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#31639;&#27861;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20174;&#26356;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#20419;&#36827;AI&#30340;&#21457;&#23637;&#12290;&#30830;&#23450;AI&#24212;&#35813;&#25552;&#20379;&#21738;&#20123;&#20449;&#24687;&#26469;&#24110;&#21161;&#20154;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28982;&#32780;&#65292;&#20449;&#24687;&#22914;&#20309;&#21576;&#29616;&#65292;&#27604;&#22914;&#25512;&#33616;&#30340;&#39034;&#24207;&#21644;&#35299;&#37322;&#30340;&#35201;&#27714;&#21516;&#26679;&#37325;&#35201;&#12290;&#36825;&#25512;&#21160;&#20102;&#26356;&#31934;&#30830;&#22320;&#30740;&#31350;&#20154;&#26426;&#20132;&#20114;&#20316;&#20026;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#25903;&#25345;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#24050;&#26377;&#20960;&#20010;&#23454;&#35777;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#20854;&#20013;&#20132;&#20114;&#24418;&#24335;&#21508;&#24322;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#35789;&#27719;&#26469;&#25551;&#36848;&#20154;&#26426;&#20132;&#20114;&#21327;&#35758;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efforts in levering Artificial Intelligence (AI) in decision support systems have disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. To address this, explainable AI promotes AI development from a more human-centered perspective. Determining what information AI should provide to aid humans is vital, however, how the information is presented, e. g., the sequence of recommendations and the solicitation of interpretations, is equally crucial. This motivates the need to more precisely study Human-AI interaction as a pivotal component of AI-based decision support. While several empirical studies have evaluated Human-AI interactions in multiple application domains in which interactions can take many forms, there is not yet a common vocabulary to describe human-AI interaction protocols. To address this gap, we describe the results of a systematic review of the AI-assisted decision making literature, anal
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.19736</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#21452;&#20995;&#21073;&#19968;&#26679;&#65292;LLMs&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#65292;&#20135;&#29983;&#19981;&#36866;&#24403;&#12289;&#26377;&#23475;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#24555;&#36895;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#21487;&#33021;&#20986;&#29616;&#27809;&#26377;&#36275;&#22815;&#20445;&#38556;&#30340;&#36229;&#26234;&#33021;&#31995;&#32479;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#65292;&#23545;LLMs&#36827;&#34892;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;LLMs&#35780;&#20272;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#35780;&#20272;&#20998;&#20026;&#19977;&#22823;&#31867;&#21035;&#65306;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#65292;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#38500;&#20102;&#20840;&#38754;&#22238;&#39038;&#35780;&#20272;&#26041;&#27861;&#21644;&#25216;&#26415;&#20043;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#33021;&#21147;&#30340;&#36777;&#35770;&#32570;&#20047;&#32454;&#33268;&#30340;&#32771;&#34385;&#12290;&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#19977;&#20010;&#24120;&#35265;&#25209;&#35780;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;LLMs&#29702;&#35299;&#21644;&#24847;&#22270;&#38382;&#39064;&#30340;&#21153;&#23454;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.19671</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#24403;&#21069;&#36777;&#35770;&#30340;&#32454;&#24494;&#24046;&#21035;&#30340;&#38656;&#27714;&#21644;&#23545;&#29702;&#35299;&#30340;&#21153;&#23454;&#35266;&#28857;(arXiv:2310.19671v2 [cs.CL] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding. (arXiv:2310.19671v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19671
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#33021;&#21147;&#30340;&#36777;&#35770;&#32570;&#20047;&#32454;&#33268;&#30340;&#32771;&#34385;&#12290;&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#19977;&#20010;&#24120;&#35265;&#25209;&#35780;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;LLMs&#29702;&#35299;&#21644;&#24847;&#22270;&#38382;&#39064;&#30340;&#21153;&#23454;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#26041;&#38754;&#26080;&#19982;&#20262;&#27604;&#12290;LLMs&#27491;&#22312;&#24555;&#36895;&#20986;&#29616;&#65292;&#24182;&#19988;&#20851;&#20110;LLM&#33021;&#21147;&#30340;&#36777;&#35770;&#24050;&#32463;&#24320;&#22987;&#65292;&#20294;&#21453;&#24605;&#28382;&#21518;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32858;&#28966;&#20110;&#36777;&#35770;&#65292;&#24182;&#23545;LLM&#33021;&#21147;&#30340;&#19977;&#20010;&#37325;&#22797;&#20986;&#29616;&#30340;&#25209;&#35780;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65306;i) LLMs&#21482;&#26159;&#27169;&#20223;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#65307;ii) LLMs&#25484;&#25569;&#20102;&#24418;&#24335;&#20294;&#24182;&#38750;&#21151;&#33021;&#24615;&#35821;&#35328;&#33021;&#21147;&#65307;iii) LLMs&#20013;&#30340;&#35821;&#35328;&#23398;&#20064;&#19981;&#33021;&#20026;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#25552;&#20379;&#20449;&#24687;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#35266;&#28857;&#38656;&#35201;&#26356;&#22810;&#32454;&#24494;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#23545;LLMs&#20013;&#8220;&#30495;&#27491;&#8221;&#30340;&#29702;&#35299;&#21644;&#24847;&#22270;&#38382;&#39064;&#30340;&#21153;&#23454;&#35266;&#28857;&#12290;&#29702;&#35299;&#21644;&#24847;&#22270;&#28041;&#21450;&#21040;&#25105;&#20204;&#20551;&#35774;&#20182;&#20154;&#20855;&#26377;&#30340;&#19981;&#21487;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#65306;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#25277;&#35937;&#20986;&#22797;&#26434;&#30340;&#24213;&#23618;&#26426;&#21046;&#24182;&#39044;&#27979;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviou
&lt;/p&gt;</description></item><item><title>LLMaAA&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#21160;&#25209;&#27880;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;LLM&#30830;&#23450;&#39640;&#25928;&#25209;&#27880;&#20869;&#23481;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#24182;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.19596</link><description>&lt;p&gt;
LLMaAA: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#21160;&#25209;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19596
&lt;/p&gt;
&lt;p&gt;
LLMaAA&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#21160;&#25209;&#27880;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;LLM&#30830;&#23450;&#39640;&#25928;&#25209;&#27880;&#20869;&#23481;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#24182;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#26222;&#36941;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22240;&#38656;&#27714;&#22823;&#37327;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#23454;&#38469;&#19978;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20107;&#19994;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20986;&#33394;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#25512;&#21160;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#20165;&#20174;LLM&#20013;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#36136;&#37327;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#20010;&#25968;&#37327;&#32423;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#28508;&#21147;&#24182;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMaAA&#65292;&#23427;&#23558;LLM&#20316;&#20026;&#25209;&#27880;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#25918;&#20837;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20197;&#39640;&#25928;&#30830;&#23450;&#25209;&#27880;&#20869;&#23481;&#12290;&#20026;&#20102;&#21033;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#31283;&#20581;&#23398;&#20064;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#25209;&#27880;&#21644;&#35757;&#32451;&#36807;&#31243;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20174;&#23567;&#30340;&#31034;&#33539;&#27744;&#20013;&#25277;&#21462;k-NN&#31034;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#65288;2&#65289;&#25105;&#20204;&#37319;&#29992;&#31034;&#20363;&#21152;&#26435;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting techniqu
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#20294;&#20854;&#26222;&#21450;&#21487;&#33021;&#23548;&#33268;&#24066;&#27665;&#30340;&#33258;&#20027;&#20915;&#31574;&#26435;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#22320;&#21306;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#31649;&#21046;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2310.19503</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#20027;&#20915;&#31574;AI&#20013;&#30340;&#20449;&#20219;&#12289;&#38382;&#36131;&#21644;&#33258;&#20027;&#24615;
&lt;/p&gt;
&lt;p&gt;
Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination. (arXiv:2310.19503v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19503
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#20294;&#20854;&#26222;&#21450;&#21487;&#33021;&#23548;&#33268;&#24066;&#27665;&#30340;&#33258;&#20027;&#20915;&#31574;&#26435;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#22320;&#21306;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#31649;&#21046;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25104;&#20026;&#20102;&#25512;&#21160;&#26234;&#33021;&#20915;&#31574;&#21644;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#22522;&#30784;&#24179;&#21488;&#65292;&#34987;&#35895;&#27468;&#12289;&#27779;&#23572;&#29595;&#21644;AirBnb&#31561;&#20027;&#35201;&#20844;&#21496;&#24191;&#27867;&#37319;&#29992;&#12290;&#30693;&#35782;&#22270;&#35889;&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26469;&#34917;&#20805;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#25512;&#29702;&#21644;&#38382;&#31572;&#33021;&#21147;&#12290;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#34987;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#12290;&#23613;&#31649;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24102;&#26469;&#35768;&#22810;&#22909;&#22788;&#65292;&#20294;&#20854;&#22312;&#22312;&#32447;&#26381;&#21153;&#20013;&#36234;&#26469;&#36234;&#26222;&#21450;&#21487;&#33021;&#23548;&#33268;&#24066;&#27665;&#22833;&#21435;&#33258;&#20027;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36234;&#20381;&#36182;&#36825;&#20123;&#36890;&#24120;&#26159;&#38598;&#20013;&#21270;&#30340;&#25216;&#26415;&#65292;&#24066;&#27665;&#23601;&#36234;&#26080;&#27861;&#20915;&#23450;&#33258;&#24049;&#30340;&#21629;&#36816;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#23041;&#32961;&#65292;&#22312;&#26576;&#20123;&#22320;&#21306;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#31649;&#21046;&#65292;&#20363;&#22914;&#27431;&#30431;&#30340;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) have emerged as fundamental platforms for powering intelligent decision-making and a wide range of Artificial Intelligence (AI) services across major corporations such as Google, Walmart, and AirBnb. KGs complement Machine Learning (ML) algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI. Despite the numerous benefits that can be accomplished with KG-based AI, its growing ubiquity within online services may result in the loss of self-determination for citizens as a fundamental societal issue. The more we rely on these technologies, which are often centralised, the less citizens will be able to determine their own destinies. To counter this threat, AI regulation, such as the European Union (EU) AI Act, is being proposed in certain regions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#31867;&#22120;&#20998;&#25968;&#33976;&#39311;&#65288;CSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#38544;&#24335;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#65292;&#35777;&#26126;&#20102;&#20165;&#20973;&#20998;&#31867;&#22120;&#26080;&#20851;&#25351;&#23548;&#23601;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19415</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31867;&#22120;&#20998;&#25968;&#33976;&#39311;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-to-3D with Classifier Score Distillation. (arXiv:2310.19415v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#31867;&#22120;&#20998;&#25968;&#33976;&#39311;&#65288;CSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#38544;&#24335;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#65292;&#35777;&#26126;&#20102;&#20165;&#20973;&#20998;&#31867;&#22120;&#26080;&#20851;&#25351;&#23548;&#23601;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#12290;&#34429;&#28982;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25104;&#21151;&#20248;&#21270;&#30340;&#20851;&#38190;&#65292;&#20294;&#23427;&#34987;&#35270;&#20026;&#36741;&#21161;&#25216;&#24039;&#32780;&#19981;&#26159;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#20998;&#31867;&#22120;&#26080;&#20851;&#25351;&#23548;&#22312;&#20998;&#25968;&#33976;&#39311;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65306;&#20165;&#20973;&#25351;&#23548;&#23601;&#36275;&#20197;&#26377;&#25928;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#21629;&#21517;&#20026;&#20998;&#31867;&#22120;&#20998;&#25968;&#33976;&#39311;&#65288;CSD&#65289;&#65292;&#21487;&#20197;&#23558;&#20854;&#35299;&#37322;&#20026;&#20351;&#29992;&#38544;&#24335;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#36825;&#31181;&#26032;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;CSD&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;3D&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#24418;&#29366;&#29983;&#25104;&#12289;&#32441;&#29702;&#21512;&#25104;&#21644;&#24418;&#29366;&#32534;&#36753;&#65292;&#22312;&#32467;&#26524;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is enough for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights for understanding existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project pa
&lt;/p&gt;</description></item><item><title>TeacherLM-7.1B&#26159;&#19968;&#20010;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25945;&#20250;&#20854;&#20182;&#27169;&#22411;&#8220;&#20026;&#20160;&#20040;&#8221;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;&#23427;&#22312;MMLU&#19978;&#21462;&#24471;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2310.19019</link><description>&lt;p&gt;
TeacherLM: &#25945;&#20154;&#25171;&#40060;&#32780;&#19981;&#26159;&#32473;&#40060;&#65292;&#35821;&#35328;&#24314;&#27169;&#21516;&#29702;
&lt;/p&gt;
&lt;p&gt;
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19019
&lt;/p&gt;
&lt;p&gt;
TeacherLM-7.1B&#26159;&#19968;&#20010;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25945;&#20250;&#20854;&#20182;&#27169;&#22411;&#8220;&#20026;&#20160;&#20040;&#8221;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;&#23427;&#22312;MMLU&#19978;&#21462;&#24471;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#21644;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23567;&#22411;&#27169;&#22411;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TeacherLM-7.1B&#65292;&#33021;&#22815;&#32473;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#30456;&#20851;&#22522;&#30784;&#30693;&#35782;&#12289;&#24605;&#32500;&#38142;&#21644;&#24120;&#35265;&#38169;&#35823;&#30340;&#27880;&#37322;&#65292;&#20351;&#27880;&#37322;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#31572;&#26696;&#65292;&#32780;&#19988;&#20351;&#20854;&#20182;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#8220;&#20026;&#20160;&#20040;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;TeacherLM-7.1B&#27169;&#22411;&#22312;MMLU&#19978;&#23454;&#29616;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#36229;&#36807;&#20102;&#25317;&#26377;100B&#21442;&#25968;&#30340;&#22823;&#22810;&#25968;&#27169;&#22411;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#20854;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#22522;&#20110;TeacherLM-7.1B&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#20351;&#29992;&#20102;&#26469;&#33258;OPT&#21644;BLOOM&#31995;&#21015;&#30340;&#19981;&#21516;&#21442;&#25968;&#30340;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#23545;58&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TeacherLM&#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#23558;&#20316;&#20026;&#24320;&#28304;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;AI for Open Science (AI4OS)&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;AI for Science (AI4Science)&#30340;&#22810;&#26234;&#33021;&#20307;&#25193;&#23637;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#31185;&#23398;&#20225;&#19994;&#20013;&#26368;&#22823;&#21270;&#24320;&#25918;&#30693;&#35782;&#36716;&#21270;&#26469;&#20419;&#36827;&#24320;&#25918;&#31185;&#23398;&#12290;&#30740;&#31350;&#20351;&#29992;&#30693;&#35782;&#21457;&#29616;&#19982;&#25968;&#25454;&#25366;&#25496;&#65288;KDD&#65289;&#30340;&#21407;&#21017;&#26469;&#24418;&#24335;&#21270;AI4OS&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#24212;&#29992;&#24320;&#25918;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18852</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#24320;&#25918;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20174;&#20262;&#29702;&#35282;&#24230;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#30693;&#35782;&#30340;&#22810;&#26234;&#33021;&#20307;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI for Open Science: A Multi-Agent Perspective for Ethically Translating Data to Knowledge. (arXiv:2310.18852v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;AI for Open Science (AI4OS)&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;AI for Science (AI4Science)&#30340;&#22810;&#26234;&#33021;&#20307;&#25193;&#23637;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#31185;&#23398;&#20225;&#19994;&#20013;&#26368;&#22823;&#21270;&#24320;&#25918;&#30693;&#35782;&#36716;&#21270;&#26469;&#20419;&#36827;&#24320;&#25918;&#31185;&#23398;&#12290;&#30740;&#31350;&#20351;&#29992;&#30693;&#35782;&#21457;&#29616;&#19982;&#25968;&#25454;&#25366;&#25496;&#65288;KDD&#65289;&#30340;&#21407;&#21017;&#26469;&#24418;&#24335;&#21270;AI4OS&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#24212;&#29992;&#24320;&#25918;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#33258;&#21160;&#39550;&#39542;&#23454;&#39564;&#23460;&#30340;&#24418;&#24335;&#65292;&#26377;&#21487;&#33021;&#25490;&#38500;&#20154;&#31867;&#21442;&#19982;&#24182;&#38459;&#30861;&#26356;&#24191;&#27867;&#31038;&#21306;&#20869;&#30340;&#31185;&#23398;&#21457;&#29616;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20851;&#27880;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#12289;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#30830;&#20445;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#25105;&#20204;&#36824;&#25552;&#20986;&#24212;&#20180;&#32454;&#32771;&#34385;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#24320;&#25918;&#31185;&#23398;&#39046;&#22495;&#30340;&#21457;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AI for Open Science (AI4OS)&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;AI for Science (AI4Science)&#30340;&#22810;&#26234;&#33021;&#20307;&#25193;&#23637;&#65292;&#20854;&#26680;&#24515;&#21407;&#21017;&#26159;&#36890;&#36807;&#31185;&#23398;&#20225;&#19994;&#20013;&#30340;&#24320;&#25918;&#30693;&#35782;&#36716;&#21270;&#26469;&#26368;&#22823;&#21270;&#24320;&#25918;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#21457;&#29616;&#19982;&#25968;&#25454;&#25366;&#25496;&#65288;KDD&#65289;&#30340;&#24050;&#24314;&#31435;&#21407;&#21017;&#26469;&#24418;&#24335;&#21270;AI4OS&#30340;&#35821;&#35328;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23884;&#20837;&#22312;AI4Science&#31995;&#32479;&#20013;&#30340;&#19977;&#20010;&#30693;&#35782;&#36716;&#21270;&#38454;&#27573;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#24320;&#25918;&#24615;&#21487;&#20197;&#24212;&#29992;&#20110;&#20135;&#29983;AI4OS&#26367;&#20195;&#26041;&#26696;&#30340;&#29305;&#23450;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#37327;&#21270;AI4OS&#30340;&#24320;&#25918;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI for Science (AI4Science), particularly in the form of self-driving labs, has the potential to sideline human involvement and hinder scientific discovery within the broader community. While prior research has focused on ensuring the responsible deployment of AI applications, enhancing security, and ensuring interpretability, we also propose that promoting openness in AI4Science discoveries should be carefully considered. In this paper, we introduce the concept of AI for Open Science (AI4OS) as a multi-agent extension of AI4Science with the core principle of maximizing open knowledge translation throughout the scientific enterprise rather than a single organizational unit. We use the established principles of Knowledge Discovery and Data Mining (KDD) to formalize a language around AI4OS. We then discuss three principle stages of knowledge translation embedded in AI4Science systems and detail specific points where openness can be applied to yield an AI4OS alternative. Lastly, we formul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#12289;&#25991;&#26412;&#21040;Python&#21644;&#25991;&#26412;&#21040;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#37325;&#20889;&#21644;SQL&#22686;&#24378;&#31561;&#25216;&#26415;&#65292;&#23558;&#27169;&#31946;&#20449;&#24687;&#36716;&#21270;&#20026;&#30830;&#20999;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#25191;&#34892;&#21453;&#39304;&#21644;&#26597;&#35810;&#32467;&#26524;&#22686;&#24378;SQL&#26412;&#36523;&#12290;&#35813;&#26041;&#27861;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.18752</link><description>&lt;p&gt;
Reboost&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#12289;&#25991;&#26412;&#21040;Python&#21644;&#25991;&#26412;&#21040;&#20989;&#25968;&#30340;&#26041;&#27861;&#8212;&#8212;&#20197;&#20132;&#36890;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and Text-to-Function -- with Real Applications in Traffic Domain. (arXiv:2310.18752v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#12289;&#25991;&#26412;&#21040;Python&#21644;&#25991;&#26412;&#21040;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#37325;&#20889;&#21644;SQL&#22686;&#24378;&#31561;&#25216;&#26415;&#65292;&#23558;&#27169;&#31946;&#20449;&#24687;&#36716;&#21270;&#20026;&#30830;&#20999;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#25191;&#34892;&#21453;&#39304;&#21644;&#26597;&#35810;&#32467;&#26524;&#22686;&#24378;SQL&#26412;&#36523;&#12290;&#35813;&#26041;&#27861;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;Spider&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#22797;&#29616;&#19994;&#21153;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#22797;&#26434;&#24615;&#21644;&#38382;&#39064;&#24847;&#22270;&#30340;&#26126;&#30830;&#31243;&#24230;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#24046;&#24322;&#23545;&#25552;&#31034;&#26041;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#36866;&#24212;&#24615;&#26356;&#24378;&#12289;&#26356;&#36890;&#29992;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20027;&#35201;&#21253;&#25324;&#26597;&#35810;&#37325;&#20889;&#21644;SQL&#22686;&#24378;&#65292;&#20998;&#21035;&#23558;&#27169;&#31946;&#20449;&#24687;&#36716;&#21270;&#20026;&#30830;&#20999;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#25968;&#25454;&#24211;&#20869;&#23481;&#30340;&#25191;&#34892;&#21453;&#39304;&#21644;&#26597;&#35810;&#32467;&#26524;&#26469;&#22686;&#24378;SQL&#26412;&#36523;&#12290;&#20026;&#20102;&#38450;&#27490;&#20449;&#24687;&#32570;&#22833;&#65292;&#25105;&#20204;&#23558;&#21015;&#30340;&#27880;&#37322;&#12289;&#20540;&#31867;&#22411;&#21644;&#20540;&#31034;&#20363;&#20316;&#20026;&#25968;&#25454;&#24211;&#25551;&#36848;&#30340;&#19968;&#37096;&#20998;&#21253;&#21547;&#22312;&#25552;&#31034;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The previous state-of-the-art (SOTA) method achieved a remarkable execution accuracy on the Spider dataset, which is one of the largest and most diverse datasets in the Text-to-SQL domain. However, during our reproduction of the business dataset, we observed a significant drop in performance. We examined the differences in dataset complexity, as well as the clarity of questions' intentions, and assessed how those differences could impact the performance of prompting methods. Subsequently, We develop a more adaptable and more general prompting method, involving mainly query rewriting and SQL boosting, which respectively transform vague information into exact and precise information and enhance the SQL itself by incorporating execution feedback and the query results from the database content. In order to prevent information gaps, we include the comments, value types, and value samples for columns as part of the database description in the prompt. Our experiments with Large Language Model
&lt;/p&gt;</description></item><item><title>LoRAShear&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#21098;&#26525;&#21644;&#21160;&#24577;&#24494;&#35843;&#65292;&#26377;&#25928;&#20943;&#23569;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#24182;&#19988;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18356</link><description>&lt;p&gt;
LoRAShear: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18356
&lt;/p&gt;
&lt;p&gt;
LoRAShear&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#21098;&#26525;&#21644;&#21160;&#24577;&#24494;&#35843;&#65292;&#26377;&#25928;&#20943;&#23569;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#24182;&#19988;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26684;&#23616;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoRAShear&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;LLMs&#24182;&#24674;&#22797;&#30693;&#35782;&#12290;LoRAShear&#39318;&#20808;&#22312;LoRA&#27169;&#22359;&#19978;&#21019;&#24314;&#20381;&#36182;&#22270;&#65292;&#20197;&#21457;&#29616;&#26368;&#23567;&#21024;&#38500;&#32467;&#26500;&#24182;&#20998;&#26512;&#30693;&#35782;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;LoRA&#36866;&#37197;&#22120;&#19978;&#36827;&#34892;&#28176;&#36827;&#24335;&#32467;&#26500;&#21098;&#26525;&#65292;&#24182;&#23454;&#29616;&#20869;&#22312;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#20887;&#20313;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#24674;&#22797;&#21098;&#26525;&#26399;&#38388;&#20002;&#22833;&#30340;&#30693;&#35782;&#65292;LoRAShear&#20180;&#32454;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#26041;&#26696;&#65292;&#20351;&#29992;&#21160;&#24577;&#25968;&#25454;&#36866;&#37197;&#22120;&#65292;&#20197;&#26377;&#25928;&#32553;&#23567;&#19982;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#20351;&#29992;&#19968;&#22359;GPU&#22312;&#20960;&#22825;&#20869;&#65292;LoRAShear&#23558;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#26377;&#25928;&#20943;&#23569;&#20102;20%&#65292;&#20165;&#26377;1.0%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance d
&lt;/p&gt;</description></item><item><title>BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.18351</link><description>&lt;p&gt;
BioImage.IO Chatbot: &#19968;&#20010;&#20197;&#31038;&#21306;&#30693;&#35782;&#24211;&#22686;&#24378;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#20010;&#20154;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base. (arXiv:2310.18351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18351
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25193;&#23637;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#26223;&#35266;&#32473;&#19987;&#23478;&#21644;&#26032;&#26469;&#32773;&#37117;&#24102;&#26469;&#20102;&#23548;&#33322;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25628;&#32034;&#26041;&#27861;&#22312;&#36825;&#20010;&#22797;&#26434;&#29615;&#22659;&#20013;&#24120;&#24120;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioImage.IO Chatbot&#65292;&#19968;&#20010;&#20026;&#29983;&#29289;&#22270;&#20687;&#31038;&#21306;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35805;&#21161;&#25163;&#12290;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#32858;&#21512;&#21644;&#35299;&#37322;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#29305;&#23450;&#24037;&#20855;&#25991;&#26723;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#32463;&#36807;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;BioImage.IO Chatbot &#19981;&#20165;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20114;&#21160;&#65292;&#36824;&#25552;&#20379;&#20016;&#23500;&#30340;&#30693;&#35782;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#12290;&#23427;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#29983;&#29289;&#23398;&#23478;&#12289;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24072;&#21644;&#24320;&#21457;&#32773;&#23548;&#33322;&#21644;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#26041;&#24335;&#65292;&#20026;&#31038;&#21306;&#39537;&#21160;&#30340;&#21487;&#35775;&#38382;&#31185;&#23398;&#30740;&#31350;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly expanding landscape of bioimage analysis tools presents a navigational challenge for both experts and newcomers. Traditional search methods often fall short in assisting users in this complex environment. To address this, we introduce the BioImage$.$IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and structured data sources. Enhanced by a community-contributed knowledge base and fine-tuned retrieval methods, the BioImage$.$IO Chatbot offers not just a personalized interaction but also a knowledge-enriched, context-aware experience. It fundamentally transforms the way biologists, bioimage analysts, and developers navigate and utilize advanced bioimage analysis tools, setting a new standard for community-driven, accessible scientific research.
&lt;/p&gt;</description></item><item><title>AllTogether&#26159;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#36890;&#36807;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#23545;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26041;&#38754;&#65292;&#20248;&#20110;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.18331</link><description>&lt;p&gt;
AllTogether&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20351;&#29992;&#25340;&#25509;&#25552;&#31034;&#36827;&#34892;Web&#23548;&#33322;&#30340;&#25928;&#26524;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18331
&lt;/p&gt;
&lt;p&gt;
AllTogether&#26159;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#36890;&#36807;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#23545;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26041;&#38754;&#65292;&#20248;&#20110;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#20195;&#29702;&#65292;&#23427;&#20204;&#35299;&#37322;&#30446;&#26631;&#24182;&#19982;Web&#39029;&#38754;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#20013;&#20351;&#29992;&#25340;&#25509;&#25552;&#31034;&#30340;&#25928;&#26524;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AllTogether&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;LLMs&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#26469;&#33258;&#24320;&#28304;Llama-2&#21644;&#21487;&#35775;&#38382;&#30340;GPT&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#21644;&#25351;&#20196;&#24494;&#35843;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;Web&#23548;&#33322;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#19988;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#27604;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26356;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.17811</link><description>&lt;p&gt;
&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#21307;&#23398;&#24433;&#20687;&#20013;&#29983;&#25104;&#25253;&#21578;&#26377;&#26395;&#25913;&#21892;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#20174;&#22270;&#20687;&#29983;&#25104;&#23436;&#25972;&#30340;&#25253;&#21578;&#26469;&#32771;&#34385;&#22270;&#20687;&#21040;&#25253;&#21578;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#28151;&#28102;&#20102;&#25253;&#21578;&#30340;&#20869;&#23481;&#65288;&#22914;&#21457;&#29616;&#21644;&#20854;&#23646;&#24615;&#65289;&#19982;&#20854;&#39118;&#26684;&#65288;&#22914;&#26684;&#24335;&#21644;&#35789;&#27719;&#36873;&#25321;&#65289;&#65292;&#21487;&#33021;&#23548;&#33268;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20869;&#23481;&#65292;&#28982;&#21518;&#23558;&#25552;&#21462;&#30340;&#20869;&#23481;&#36716;&#21270;&#20026;&#19982;&#29305;&#23450;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;RadGraph&#8212;&#8212;&#19968;&#31181;&#25253;&#21578;&#30340;&#22270;&#34920;&#31034;&#8212;&#8212;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;&#23450;&#37327;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#30410;&#22788;&#12290;&#36890;&#36807;&#20020;&#24202;&#35780;&#20272;&#32773;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#25253;&#21578;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#65292;&#26080;&#27861;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
&lt;/p&gt;</description></item><item><title>CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.17680</link><description>&lt;p&gt;
CodeFusion: &#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17680
&lt;/p&gt;
&lt;p&gt;
CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#24320;&#21457;&#32773;&#21482;&#33021;&#20462;&#25913;&#20854;&#26368;&#21518;&#19968;&#34892;&#20195;&#30721;&#65292;&#22312;&#27491;&#30830;&#20043;&#21069;&#65292;&#20182;&#20204;&#38656;&#35201;&#22810;&#23569;&#27425;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#20989;&#25968;&#21602;&#65311;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#26377;&#31867;&#20284;&#30340;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#23481;&#26131;&#37325;&#26032;&#32771;&#34385;&#20043;&#21069;&#29983;&#25104;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeFusion&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#20197;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#23436;&#25972;&#31243;&#24207;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#38024;&#23545;Bash&#12289;Python&#21644;Microsoft Excel&#26465;&#20214;&#26684;&#24335;(CF)&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#23545;CodeFusion&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CodeFusion&#65288;75M&#21442;&#25968;&#65289;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#65288;350M-175B&#21442;&#25968;&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;top-3&#21644;top-5&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#22312;&#22810;&#26679;&#24615;&#19982;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
&lt;/p&gt;</description></item><item><title>FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.17306</link><description>&lt;p&gt;
FormaT5: &#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26465;&#20214;&#34920;&#26684;&#26684;&#24335;&#21270;&#30340;&#25277;&#26679;&#21644;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17306
&lt;/p&gt;
&lt;p&gt;
FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#30340;&#26684;&#24335;&#21270;&#26159;&#21487;&#35270;&#21270;&#12289;&#23637;&#31034;&#21644;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32534;&#20889;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#26469;&#33258;&#21160;&#26684;&#24335;&#21270;&#34920;&#26684;&#12290;&#20294;&#23545;&#29992;&#25143;&#26469;&#35828;&#65292;&#32534;&#20889;&#36825;&#26679;&#30340;&#35268;&#21017;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20182;&#20204;&#29702;&#35299;&#21644;&#23454;&#29616;&#24213;&#23618;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;FormaT5&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#26399;&#26395;&#30340;&#26684;&#24335;&#36923;&#36753;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#25143;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#25110;&#21547;&#31946;&#30340;&#65292;&#36825;&#20351;&#24471;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#38590;&#20197;&#22312;&#19968;&#27493;&#20013;&#20934;&#30830;&#23398;&#20064;&#21040;&#25152;&#38656;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35268;&#33539;&#19981;&#36275;&#30340;&#38382;&#39064;&#24182;&#20943;&#23569;&#21442;&#25968;&#38169;&#35823;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;&#36825;&#20123;&#21344;&#20301;&#31526;&#21487;&#20197;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#25110;&#32773;&#24403;&#21487;&#29992;&#30340;&#34892;&#31034;&#20363;&#26102;&#65292;&#30001;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#32534;&#31243;&#31995;&#32479;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
&lt;/p&gt;</description></item><item><title>CosmosDSR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;YOLOv3&#19982;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#36712;&#36947;&#30862;&#29255;&#12290;&#22312;&#20351;&#29992;SPARK&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;YOLOv3&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;CosmosDSR&#21644;LKF&#37117;&#33021;&#20934;&#30830;&#22320;&#36319;&#36394;&#21355;&#26143;&#12290;</title><link>http://arxiv.org/abs/2310.17158</link><description>&lt;p&gt;
CosmosDSR -- &#19968;&#31181;&#20351;&#29992;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#36712;&#36947;&#30862;&#29255;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter. (arXiv:2310.17158v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17158
&lt;/p&gt;
&lt;p&gt;
CosmosDSR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;YOLOv3&#19982;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#36712;&#36947;&#30862;&#29255;&#12290;&#22312;&#20351;&#29992;SPARK&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;YOLOv3&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;CosmosDSR&#21644;LKF&#37117;&#33021;&#20934;&#30830;&#22320;&#36319;&#36394;&#21355;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kessler&#32508;&#21512;&#24449;&#26159;&#25351;&#39057;&#32321;&#30340;&#22826;&#31354;&#27963;&#21160;&#20135;&#29983;&#30340;&#21319;&#32423;&#30340;&#22826;&#31354;&#30862;&#29255;&#65292;&#23041;&#32961;&#21040;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;YOLO&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#32452;&#21512;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#39033;&#30446;&#24341;&#20837;&#20102;CosmosDSR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;YOLOv3&#19982;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;&#24207;&#21015;&#22270;&#20687;&#20013;&#36319;&#36394;&#21355;&#26143;&#65292;&#19982;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30456;&#27604;&#12290;&#20351;&#29992;&#21346;&#26862;&#22561;&#22823;&#23398;&#30340;SPARK&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;YOLOv3&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#20102;&#25152;&#26377;&#21355;&#26143;&#31867;&#21035;&#65288;mAP=97.18&#65285;&#65292;F1=0.95&#65289;&#65292;&#20986;&#29616;&#20102;&#23569;&#37327;&#38169;&#35823;&#65288;TP=4163&#65292;FP=209&#65292;FN=237&#65289;&#12290;CosmosDSR&#21644;LKF&#37117;&#20934;&#30830;&#36319;&#36394;&#21355;&#26143;&#65288;UKF&#65306;MSE=2.83/RMSE=1.66&#65292;LKF&#65306;...
&lt;/p&gt;
&lt;p&gt;
The Kessler syndrome refers to the escalating space debris from frequent space activities, threatening future space exploration. Addressing this issue is vital. Several AI models, including Convolutional Neural Networks (CNN), Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning (MAML), have been assessed with various data types. Earlier studies highlighted the combination of the YOLO object detector and a linear Kalman filter for object detection and tracking. Building on this, our project introduces CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter for tracking satellites in sequential images, compared to a linear Kalman filter. Using the SPARK dataset from the University of Luxembourg for training and testing, the YOLOv3 precisely detected and classified all satellite categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237). Both CosmosDSR and the LKF tracked satellites accurately (UKF: MSE=2.83/RMSE=1.66, LKF: 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22238;&#28335;&#27861;&#32416;&#27491;&#65292;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#28304;&#25991;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#26082;&#31616;&#27905;&#21448;&#20445;&#30041;&#37325;&#35201;&#20803;&#32032;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65288;&#25110;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#28151;&#28102;&#65289;&#65292;&#21363;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#21547;&#28304;&#25991;&#20214;&#20013;&#27809;&#26377;&#26681;&#25454;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#27493;&#39588;&#65306;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27979;&#37327;&#26377;&#20851;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#31616;&#21333;&#32479;&#35745;&#20449;&#24687;&#21487;&#20197;&#23454;&#29616;&#21069;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24778;&#20154;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
&lt;/p&gt;</description></item><item><title>FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.15421</link><description>&lt;p&gt;
FANToM: &#22312;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15421
&lt;/p&gt;
&lt;p&gt;
FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#32570;&#20047;&#20114;&#21160;&#24615;&#30340;&#34987;&#21160;&#25925;&#20107;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FANToM&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24515;&#26234;&#29702;&#35770;&#30340;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#21512;&#20102;&#24515;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#29702;&#35770;&#35201;&#27714;&#21644;&#23545;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#24517;&#35201;&#30340;&#32463;&#39564;&#32771;&#34385;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#35201;&#27714;&#30456;&#21516;&#30340;&#22522;&#26412;&#25512;&#29702;&#26469;&#35782;&#21035;LLM&#20013;&#19981;&#23384;&#22312;&#25110;&#34394;&#20551;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FANToM&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;LLM&#20063;&#34920;&#29616;&#27604;&#20154;&#31867;&#24046;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14336</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14336
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65288;&#22914;&#20915;&#31574;&#26641;&#65289;&#22312;&#38656;&#35201;&#39640;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#36879;&#26126;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#33391;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31163;&#25955;&#30340;&#21442;&#25968;&#21644;&#32467;&#26500;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#22312;&#20248;&#21270;&#26041;&#38754;&#24456;&#38590;&#24212;&#23545;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#38598;&#25104;&#26041;&#27861;&#21644;&#27169;&#31946;/&#36719;&#35268;&#21017;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#33719;&#24471;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#65292;&#31216;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#34920;&#31034;&#23398;&#20064;&#22120;&#65288;RRL&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#19981;&#21487;&#24494;&#20998;&#30340;RRL&#65292;&#25105;&#20204;&#23558;&#20854;&#26144;&#23556;&#21040;&#36830;&#32493;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#26799;&#24230;&#23884;&#20837;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#30452;&#25509;&#20248;&#21270;&#31163;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#21152;RRL&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#21028;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.09866</link><description>&lt;p&gt;
&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20316;&#20026;&#35768;&#22810;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#38382;&#39064;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#20173;&#23616;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#26080;&#27861;&#28385;&#36275;&#36825;&#20123;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#30340;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#21327;&#20316;&#35299;&#20915;&#19968;&#20010;MOO&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;FMOL&#26694;&#26550;&#20801;&#35768;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#39318;&#27425;&#23558;MOO&#24418;&#24335;&#21270;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#12290;&#23545;&#20110;&#36825;&#20010;FMOL&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;FMOO&#65289;&#31639;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#22810;&#26799;&#24230;&#19979;&#38477;&#24179;&#22343;&#65288;FMGDA&#65289;&#21644;&#32852;&#37030;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;Federated SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
&lt;/p&gt;</description></item><item><title>MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.09833</link><description>&lt;p&gt;
MIR2:&#38754;&#21521;&#36890;&#36807;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#36827;&#34892;&#21487;&#35777;&#26126;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09833
&lt;/p&gt;
&lt;p&gt;
MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#23545;&#20110;&#26410;&#30693;&#30431;&#21451;&#30340;&#19981;&#30830;&#23450;&#25110;&#26368;&#22351;&#24773;&#20917;&#34892;&#21160;&#38656;&#35201;&#20855;&#22791;&#24377;&#24615;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;MARL&#20013;&#30340;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#25216;&#26415;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#30340;&#23545;&#25163;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#38590;&#20197;&#25805;&#20316;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#35797;&#22270;&#31616;&#21270;&#36825;&#31181;&#22797;&#26434;&#24615;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#20110;&#24754;&#35266;&#30340;&#31574;&#30053;&#12289;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#19981;&#36275;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#20154;&#31867;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#34892;&#20026;&#26102;&#33258;&#28982;&#32780;&#28982;&#22320;&#19981;&#38656;&#35201;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIR2&#65292;&#23427;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#35270;&#20026;&#19968;&#20010;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#21382;&#21490;&#21644;&#34892;&#21160;&#20043;&#38388;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#38544;&#21547;&#22320;&#26368;&#22823;&#21270;&#20102;&#40065;&#26834;&#24615;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#24314;&#27169;&#20195;&#29702;&#65292;&#29992;&#20110;&#35299;&#20915;MILP&#38382;&#39064;&#12290;&#35813;&#20195;&#29702;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25968;&#23398;&#27169;&#22411;&#12289;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#20915;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06116</link><description>&lt;p&gt;
OptiMUS: &#20351;&#29992;mip&#27714;&#35299;&#22120;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
OptiMUS: Optimization Modeling Using mip Solvers and large language models. (arXiv:2310.06116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06116
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#24314;&#27169;&#20195;&#29702;&#65292;&#29992;&#20110;&#35299;&#20915;MILP&#38382;&#39064;&#12290;&#35813;&#20195;&#29702;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25968;&#23398;&#27169;&#22411;&#12289;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#20915;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#38382;&#39064;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21046;&#36896;&#21644;&#20998;&#38144;&#21040;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#31867;&#38382;&#39064;&#20173;&#28982;&#26159;&#36890;&#36807;&#25163;&#24037;&#21551;&#21457;&#24335;&#35299;&#20915;&#32780;&#19981;&#26159;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#26368;&#20248;&#35299;&#65292;&#22240;&#20026;&#21046;&#23450;&#21644;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#38480;&#21046;&#20102;&#20248;&#21270;&#24037;&#20855;&#21644;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20195;&#29702;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21046;&#23450;&#21644;&#35299;&#20915;MILP&#38382;&#39064;&#12290;OptiMUS&#33021;&#22815;&#24320;&#21457;&#25968;&#23398;&#27169;&#22411;&#65292;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#24320;&#21457;&#27979;&#35797;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#35299;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23545;&#25105;&#20204;&#30340;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;NLP4LP&#65292;&#36825;&#26159;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;(LP)&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#26412;&#30340;LLM&#25552;&#31034;&#31574;&#30053;&#30456;&#27604;&#65292;OptiMUS&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#38382;&#39064;&#65292;&#35299;&#20915;&#29575;&#25552;&#39640;&#20102;67&#65285;&#12290;OptiMUS&#30340;&#20195;&#30721;&#21644;NLP4LP&#25968;&#25454;&#38598;&#21487;&#22312;\href{https://github&#20013;&#33719;&#24471;.
&lt;/p&gt;
&lt;p&gt;
Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS is able to solve 67\% more problems compared to a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \href{https://github
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#25463;&#24452;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24573;&#30053;&#25463;&#24452;&#32447;&#32034;&#24182;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02230</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#32531;&#35299;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25463;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#25463;&#24452;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24573;&#30053;&#25463;&#24452;&#32447;&#32034;&#24182;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20854;&#20013;&#22810;&#20010;&#32447;&#32034;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#21487;&#33021;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#20110;&#23398;&#20064;&#30340;&#32447;&#32034;&#32780;&#24573;&#30053;&#21487;&#38752;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#20013;&#36825;&#20123;&#32447;&#32034;&#39640;&#24230;&#30456;&#20851;&#65292;DPMs&#20855;&#26377;&#29420;&#31435;&#34920;&#31034;&#22810;&#20010;&#35270;&#35273;&#32447;&#32034;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#23454;&#35777;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#21487;&#20197;&#20351;&#27169;&#22411;&#36991;&#24320;&#25463;&#24452;&#32447;&#32034;&#30340;&#27880;&#24847;&#65292;&#23454;&#29616;&#20102;&#19982;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#20808;&#21069;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#38598;&#25104;&#22810;&#26679;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to shortcut learning phenomena, where a model may rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting the generation of synthetic counterfactuals using Diffusion Probabilistic Models (DPMs). We discover that DPMs have the inherent capability to represent multiple visual cues independently, even when they are largely correlated in the training data. We leverage this characteristic to encourage model diversity and empirically show the efficacy of the approach with respect to several diversification objectives. We show that diffusion-guided diversification can lead models to avert attention from shortcut cues, achieving ensemble diversity performance comparable to previous methods requiring additional data collection.
&lt;/p&gt;</description></item><item><title>NoxTrader&#26159;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#32929;&#31080;&#25910;&#30410;&#21160;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20013;&#38271;&#26399;&#30408;&#21033;&#12290;&#23427;&#36890;&#36807;&#23545;&#21382;&#21490;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#27979;&#24314;&#27169;&#12289;&#21442;&#25968;&#37197;&#32622;&#21644;&#22238;&#27979;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#20132;&#26131;&#27169;&#22411;&#22312;&#29616;&#23454;&#20132;&#26131;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00747</link><description>&lt;p&gt;
NoxTrader: &#22522;&#20110;LSTM&#30340;&#32929;&#31080;&#25910;&#30410;&#21160;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
NoxTrader: LSTM-Based Stock Return Momentum Prediction. (arXiv:2310.00747v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00747
&lt;/p&gt;
&lt;p&gt;
NoxTrader&#26159;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#32929;&#31080;&#25910;&#30410;&#21160;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20013;&#38271;&#26399;&#30408;&#21033;&#12290;&#23427;&#36890;&#36807;&#23545;&#21382;&#21490;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#27979;&#24314;&#27169;&#12289;&#21442;&#25968;&#37197;&#32622;&#21644;&#22238;&#27979;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#20132;&#26131;&#27169;&#22411;&#22312;&#29616;&#23454;&#20132;&#26131;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NoxTrader&#65292;&#23427;&#26088;&#22312;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#26500;&#24314;&#21644;&#20132;&#26131;&#25191;&#34892;&#65292;&#26088;&#22312;&#20135;&#29983;&#30408;&#21033;&#32467;&#26524;&#12290;NoxTrader&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#32929;&#31080;&#24066;&#22330;&#20132;&#26131;&#65292;&#37325;&#28857;&#26159;&#22521;&#20859;&#20013;&#38271;&#26399;&#21033;&#28070;&#12290;NoxTrader&#30340;&#23398;&#20064;&#36807;&#31243;&#22522;&#20110;&#23545;&#21382;&#21490;&#20132;&#26131;&#25968;&#25454;&#30340;&#27934;&#23519;&#21147;&#30340;&#21560;&#25910;&#65292;&#20027;&#35201;&#20381;&#38752;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#22240;&#20026;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#24615;&#36136;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#27979;&#24314;&#27169;&#12289;&#21442;&#25968;&#37197;&#32622;&#12289;&#24314;&#31435;&#20005;&#35880;&#30340;&#22238;&#27979;&#26694;&#26550;&#31561;&#30340;&#39034;&#24207;&#36827;&#23637;&#65292;&#24182;&#26368;&#32456;&#23558;NoxTrader&#23450;&#20301;&#20026;&#31639;&#27861;&#20132;&#26131;&#27169;&#22411;&#22312;&#29616;&#23454;&#20132;&#26131;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#21487;&#34892;&#24615;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NoxTrader, which is designed for portfolio construction and trading execution, aims at generating profitable outcomes. The primary focus of NoxTrader is on stock market trading with an emphasis on cultivating moderate to long-term profits. The underlying learning process of NoxTrader hinges on the assimilation of insights gleaned from historical trading data, primarily hinging on time-series analysis due to the inherent nature of the employed dataset. We delineate the sequential progression encompassing data acquisition, feature engineering, predictive modeling, parameter configuration, establishment of a rigorous backtesting framework, and ultimately position NoxTrader as a testament to the prospective viability of algorithmic trading models within real-world trading scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#21017;&#65292;&#26368;&#23567;&#26465;&#20214;&#20381;&#36182;&#65288;MCD&#65289;&#20934;&#21017;&#65292;&#26469;&#25581;&#31034;&#22240;&#26524;&#35299;&#37322;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36873;&#25321;&#29702;&#30001;&#20505;&#36873;&#39033;&#19978;&#26410;&#36873;&#25321;&#37096;&#20998;&#19982;&#30446;&#26631;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24378;&#21046;&#36873;&#25321;&#25152;&#26377;&#30340;&#26631;&#31614;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.13391</link><description>&lt;p&gt;
&#22240;&#26524;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;D-&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
D-Separation for Causal Self-Explanation. (arXiv:2309.13391v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#21017;&#65292;&#26368;&#23567;&#26465;&#20214;&#20381;&#36182;&#65288;MCD&#65289;&#20934;&#21017;&#65292;&#26469;&#25581;&#31034;&#22240;&#26524;&#35299;&#37322;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36873;&#25321;&#29702;&#30001;&#20505;&#36873;&#39033;&#19978;&#26410;&#36873;&#25321;&#37096;&#20998;&#19982;&#30446;&#26631;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24378;&#21046;&#36873;&#25321;&#25152;&#26377;&#30340;&#26631;&#31614;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26694;&#26550;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26368;&#22823;&#20114;&#20449;&#24687;&#65288;MMI&#65289;&#20934;&#21017;&#26469;&#25214;&#21040;&#26368;&#33021;&#35828;&#26126;&#30446;&#26631;&#26631;&#31614;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20934;&#21017;&#21487;&#33021;&#21463;&#21040;&#19982;&#22240;&#26524;&#35299;&#37322;&#25110;&#30446;&#26631;&#26631;&#31614;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20934;&#21017;&#26469;&#25581;&#31034;&#22240;&#26524;&#35299;&#37322;&#65292;&#31216;&#20026;&#26368;&#23567;&#26465;&#20214;&#20381;&#36182;&#65288;MCD&#65289;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#24314;&#31435;&#22312;&#25105;&#20204;&#21457;&#29616;&#30340;&#38750;&#22240;&#26524;&#29305;&#24449;&#19982;&#30446;&#26631;&#26631;&#31614;&#36890;&#36807;&#22240;&#26524;&#35299;&#37322;&#34987;&#8220;&#20998;&#31163;&#8221;&#20043;&#19978;&#12290;&#36890;&#36807;&#22312;&#36873;&#25321;&#30340;&#29702;&#30001;&#20505;&#36873;&#39033;&#19978;&#32473;&#20986;&#26410;&#36873;&#25321;&#37096;&#20998;&#19982;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#26368;&#23567;&#21270;&#65292;&#24378;&#21046;&#36873;&#25321;&#25152;&#26377;&#30340;&#26631;&#31614;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#20381;&#36182;&#24230;&#37327;&#65292;&#20855;&#20307;&#26159;KL&#25955;&#24230;&#65292;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;MCD&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are \emph{d-separated} by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empir
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;3&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#38480;&#37319;&#26679;&#65292;&#24182;&#34920;&#26126;&#20165;&#20960;&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#29983;&#25104;&#30340;&#26631;&#31614;&#23601;&#36275;&#22815;&#23454;&#29616;&#22270;&#20687;&#30340;&#23457;&#26597;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.02770</link><description>&lt;p&gt;
&#36890;&#36807;3&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#38480;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback. (arXiv:2307.02770v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;3&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#38480;&#37319;&#26679;&#65292;&#24182;&#34920;&#26126;&#20165;&#20960;&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#29983;&#25104;&#30340;&#26631;&#31614;&#23601;&#36275;&#22815;&#23454;&#29616;&#22270;&#20687;&#30340;&#23457;&#26597;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#38169;&#35823;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22909;&#30340;&#22270;&#20687;&#65292;&#20294;&#26377;&#26102;&#20250;&#29983;&#25104;&#19981;&#29702;&#24819;&#30340;&#22270;&#20687;&#12290;&#22914;&#26524;&#26159;&#36825;&#26679;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#38459;&#27490;&#29983;&#25104;&#31967;&#31957;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23457;&#26597;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#32463;&#36807;&#26368;&#23567;&#20154;&#31867;&#21453;&#39304;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23457;&#26597;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26497;&#39640;&#30340;&#20154;&#31867;&#21453;&#39304;&#25928;&#29575;&#21487;&#20197;&#23454;&#29616;&#23457;&#26597;&#65292;&#24182;&#19988;&#20165;&#20960;&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#29983;&#25104;&#30340;&#26631;&#31614;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently shown remarkable success in high-quality image generation. Sometimes, however, a pre-trained diffusion model exhibits partial misalignment in the sense that the model can generate good images, but it sometimes outputs undesirable images. If so, we simply need to prevent the generation of the bad images, and we call this task censoring. In this work, we present censored generation with a pre-trained diffusion model using a reward model trained on minimal human feedback. We show that censoring can be accomplished with extreme human feedback efficiency and that labels generated with a mere few minutes of human feedback are sufficient. Code available at: https://github.com/tetrzim/diffusion-human-feedback.
&lt;/p&gt;</description></item><item><title>SAMAug&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#32467;&#21512;&#21021;&#22987;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;Segment Anything Model&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01187</link><description>&lt;p&gt;
SAMAug: Segment Anything Model&#30340;&#28857;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAMAug: Point Prompt Augmentation for Segment Anything Model. (arXiv:2307.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01187
&lt;/p&gt;
&lt;p&gt;
SAMAug&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#32467;&#21512;&#21021;&#22987;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;Segment Anything Model&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SAMAug&#65292;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26032;&#22411;&#35270;&#35273;&#28857;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;&#12290;SAMAug&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#20449;&#24687;&#32473;SAM&#12290;SAM&#20174;&#19968;&#20010;&#21021;&#22987;&#28857;&#25552;&#31034;&#24320;&#22987;&#29983;&#25104;&#19968;&#20010;&#21021;&#22987;&#25513;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#25552;&#20986;&#30340;SAMAug&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#39069;&#22806;&#30340;&#28857;&#65292;SAM&#21487;&#20197;&#22522;&#20110;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#21644;&#21021;&#22987;&#25552;&#31034;&#29983;&#25104;&#22686;&#24378;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#28857;&#22686;&#24378;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65306;&#38543;&#26426;&#37319;&#26679;&#65292;&#22522;&#20110;&#26368;&#22823;&#24046;&#24322;&#29109;&#30340;&#37319;&#26679;&#65292;&#26368;&#22823;&#36317;&#31163;&#21644;&#26174;&#33879;&#24615;&#12290;&#22312;COCO&#12289;Fundus&#12289;COVID QUEx&#21644;ISIC2018&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;SAMAug&#21487;&#20197;&#25552;&#21319;SAM&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#26368;&#22823;&#36317;&#31163;&#21644;&#26174;&#33879;&#24615;&#12290;SAMAug&#35777;&#26126;&#20102;&#20854;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#36719;&#26426;&#22120;&#20154;&#31995;&#32479;&#26102;&#21046;&#23450;&#35268;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38024;&#23545;&#36141;&#29289;&#26434;&#36135;&#25342;&#21462;&#20219;&#21153;&#30340;&#36719;&#22841;&#25345;&#22120;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#31034;&#20363;&#35268;&#33539;&#65292;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#39044;&#27979;&#24615;&#12289;&#20262;&#29702;&#21644;&#27861;&#35268;&#31561;&#21151;&#33021;&#21644;&#38750;&#21151;&#33021;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.01159</link><description>&lt;p&gt;
&#36719;&#25235;&#21462;&#65306;&#38024;&#23545;&#21487;&#20449;&#24230;&#30340;&#35268;&#33539;&#21270;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Soft Gripping: Specifying for Trustworthiness. (arXiv:2307.01159v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#36719;&#26426;&#22120;&#20154;&#31995;&#32479;&#26102;&#21046;&#23450;&#35268;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38024;&#23545;&#36141;&#29289;&#26434;&#36135;&#25342;&#21462;&#20219;&#21153;&#30340;&#36719;&#22841;&#25345;&#22120;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#31034;&#20363;&#35268;&#33539;&#65292;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#39044;&#27979;&#24615;&#12289;&#20262;&#29702;&#21644;&#27861;&#35268;&#31561;&#21151;&#33021;&#21644;&#38750;&#21151;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#26426;&#22120;&#20154;&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#24037;&#31243;&#24072;&#29992;&#26469;&#21046;&#36896;&#21508;&#31181;&#24212;&#29992;&#30340;&#28789;&#27963;&#35774;&#22791;&#12290;&#20026;&#20102;&#25512;&#21160;&#36719;&#26426;&#22120;&#20154;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65307;&#22914;&#26524;&#36719;&#26426;&#22120;&#20154;&#19981;&#21487;&#20449;&#65292;&#23427;&#20204;&#23601;&#26080;&#27861;&#21457;&#25381;&#20854;&#20840;&#37096;&#28508;&#21147;&#12290;&#20026;&#20102;&#35777;&#26126;&#21487;&#20449;&#24230;&#65292;&#38656;&#35201;&#21046;&#23450;&#19968;&#20010;&#35268;&#33539;&#26469;&#23450;&#20041;&#20160;&#20040;&#26159;&#21487;&#20449;&#30340;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#36719;&#26426;&#22120;&#20154;&#22841;&#25345;&#22120;&#36825;&#26679;&#30340;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26368;&#25104;&#29087;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#36719;&#26426;&#22120;&#20154;&#31038;&#21306;&#22312;&#21046;&#23450;&#35268;&#33539;&#26041;&#38754;&#30340;&#20851;&#27880;&#24230;&#20173;&#28982;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#36719;&#26426;&#22120;&#20154;&#31995;&#32479;&#26399;&#38388;&#24320;&#21457;&#35268;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36141;&#29289;&#26434;&#36135;&#25342;&#21462;&#20219;&#21153;&#30340;&#36719;&#22841;&#25345;&#22120;&#30340;&#24191;&#27867;&#31034;&#20363;&#35268;&#33539;&#12290;&#25152;&#25552;&#35758;&#30340;&#35268;&#33539;&#28085;&#30422;&#20102;&#21151;&#33021;&#21644;&#38750;&#21151;&#33021;&#35201;&#27714;&#65292;&#22914;&#21487;&#38752;&#24615;&#65292;&#23433;&#20840;&#24615;&#65292;&#36866;&#24212;&#24615;&#65292;&#21487;&#39044;&#27979;&#24615;&#65292;&#20262;&#29702;&#21644;&#27861;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications. In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential. In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy. However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications. In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items. The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35266;&#23519;&#21040;&#24403;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#35266;&#27979;&#21040;&#20855;&#26377;&#29421;&#31364;&#33539;&#20540;&#30340;&#36755;&#20837;&#65292;&#36825;&#23545;&#20110;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08687</link><description>&lt;p&gt;
&#22522;&#20110;&#33539;&#25968;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#28508;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Norm-guided latent space exploration for text-to-image generation. (arXiv:2306.08687v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35266;&#23519;&#21040;&#24403;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#35266;&#27979;&#21040;&#20855;&#26377;&#29421;&#31364;&#33539;&#20540;&#30340;&#36755;&#20837;&#65292;&#36825;&#23545;&#20110;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#21508;&#31181;&#27010;&#24565;&#30340;&#26032;&#26500;&#22270;&#21644;&#22330;&#26223;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21021;&#22987;&#31181;&#23376;&#30340;&#28508;&#31354;&#38388;&#20173;&#19981;&#34987;&#24456;&#22909;&#29702;&#35299;&#65292;&#24182;&#19988;&#20854;&#32467;&#26500;&#24050;&#34987;&#35777;&#26126;&#20250;&#24433;&#21709;&#21508;&#31181;&#27010;&#24565;&#30340;&#29983;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#27431;&#20960;&#37324;&#24471;&#25110;&#29699;&#38754;&#24230;&#37327;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25554;&#20540;&#21644;&#23547;&#25214;&#31181;&#23376;&#38598;&#30340;&#36136;&#24515;&#31561;&#31616;&#21333;&#25805;&#20316;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#65292;&#22312;&#24403;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#35266;&#27979;&#21040;&#20855;&#26377;&#29421;&#31364;&#33539;&#20540;&#30340;&#36755;&#20837;&#12290;&#36825;&#23545;&#20110;&#20381;&#36182;&#20110;&#31181;&#23376;&#25805;&#20316;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#21644;&#38271;&#23614;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#31181;&#23376;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#35777;&#26126;&#23427;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#20808;&#39564;&#30340;&#26032;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models show great potential in synthesizing a large variety of concepts in new compositions and scenarios. However, the latent space of initial seeds is still not well understood and its structure was shown to impact the generation of various concepts. Specifically, simple operations like interpolation and finding the centroid of a set of seeds perform poorly when using standard Euclidean or spherical metrics in the latent space. This paper makes the observation that, in current training procedures, diffusion models observed inputs with a narrow range of norm values. This has strong implications for methods that rely on seed manipulation for image generation, with applications to few-shot and long-tail learning tasks. To address this issue, we propose a novel method for interpolating between two seeds and demonstrate that it defines a new non-Euclidean metric that takes into account a norm-based prior on seeds. We describe a simple yet efficient algorithm for ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07916</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#27169;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#28041;&#21450;&#29983;&#29289;&#25968;&#25454;&#12289;&#21307;&#23398;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#35821;&#35328;&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#24182;&#19988;&#20851;&#31995;&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#35266;&#27979;&#21464;&#37327;&#30001;&#19968;&#32452;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#26377;&#20123;&#28508;&#21464;&#37327;&#21487;&#33021;&#27809;&#26377;&#35266;&#23519;&#21040;&#30340;&#21518;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#65306;&#23545;&#20110;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#20801;&#35768;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#22810;&#26465;&#36335;&#24452;&#65292;&#36825;&#25918;&#23485;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#28508;&#21464;&#37327;&#26641;&#20551;&#35774;&#65307;&#23545;&#20110;&#32467;&#26500;&#20989;&#25968;&#65292;&#25105;&#20204;&#27809;&#26377;&#36827;&#34892;&#21442;&#25968;&#20551;&#35774;&#65292;&#22240;&#27492;&#21487;&#20197;&#20801;&#35768;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05584</link><description>&lt;p&gt;
&#22810;&#20307;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#30340;&#30495;&#27491;&#36890;&#29992;&#26041;&#27861;&#23545;&#20110;&#29702;&#35299;&#20851;&#33410;&#29289;&#20307;&#21644;&#31227;&#21160;&#22330;&#26223;&#30340;&#19977;&#32500;&#24433;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#20043;&#38388;&#23494;&#20999;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SE&#65288;3&#65289;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#36731;&#37327;&#32423;&#21644;&#30456;&#20114;&#36830;&#25509;&#30340;&#22836;&#37096;&#65292;&#20351;&#29992;&#28857;&#32423;&#19981;&#21464;&#29305;&#24449;&#21644;&#26469;&#33258;SE&#65288;3&#65289;&#31561;&#21464;&#29305;&#24449;&#30340;&#36816;&#21160;&#20272;&#35745;&#26469;&#39044;&#27979;&#20998;&#21106;&#25513;&#27169;&#65292;&#32780;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#22521;&#35757;&#31574;&#30053;&#21487;&#20197;&#22312;&#32447;&#25191;&#34892;&#65292;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#27969;&#65292;&#20998;&#21106;&#25513;&#27169;&#21644;&#21018;&#24615;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21482;&#26377;0.25M&#21442;&#25968;&#21644;0.92G FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;
A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of ou
&lt;/p&gt;</description></item><item><title>FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05523</link><description>&lt;p&gt;
FACTIFY3M: &#36890;&#36807;5W&#38382;&#31572;&#35299;&#37322;&#30340;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05523
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26159;&#24403;&#21069;&#20127;&#24453;&#35299;&#20915;&#30340;&#31038;&#20250;&#21361;&#26426;&#20043;&#19968;&#8212;&#8212;&#22823;&#32422;67%&#30340;&#32654;&#22269;&#20154;&#35748;&#20026;&#34394;&#20551;&#20449;&#24687;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26377;10%&#30340;&#20154;&#26377;&#24847;&#35782;&#22320;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#35777;&#25454;&#34920;&#26126;&#65292;&#34394;&#20551;&#20449;&#24687;&#21487;&#20197;&#25805;&#32437;&#27665;&#20027;&#36827;&#31243;&#21644;&#20844;&#20247;&#33286;&#35770;&#65292;&#24182;&#22312;&#21361;&#26426;&#26399;&#38388;&#24341;&#36215;&#32929;&#24066;&#21160;&#33633;&#12289;&#31038;&#20250;&#24656;&#24908;&#29978;&#33267;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#24212;&#21450;&#26102;&#35782;&#21035;&#24182;&#23613;&#21487;&#33021;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27599;&#22825;&#20998;&#20139;&#22823;&#32422;32&#20159;&#24352;&#22270;&#20687;&#21644;720,000 &#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#22240;&#27492;&#23545;&#20110;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#30340;&#21487;&#25193;&#23637;&#24615;&#26816;&#27979;&#38656;&#35201;&#39640;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#12290;&#23613;&#31649;&#22312;&#25991;&#26412;&#27169;&#24335;&#19979;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#21462;&#24471;&#20102;&#36827;&#23637;(&#20363;&#22914;&#65292;FEVER, LIAR)&#65292;&#20294;&#23398;&#26415;&#30028;&#22312;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#21162;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FACTIFY3M&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;300&#19975;&#20010;&#26679;&#26412;&#65292;&#36890;&#36807;&#22810;&#31181;&#27169;&#24335;&#21644;5W&#38382;&#31572;&#25552;&#39640;&#20102;&#20107;&#23454;&#39564;&#35777;&#39046;&#22495;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#31034;&#36234;&#30456;&#20284;&#20110;&#33041;&#25104;&#20687;&#30340;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01930</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structural Similarities Between Language Models and Neural Response Measurements. (arXiv:2306.01930v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#31034;&#36234;&#30456;&#20284;&#20110;&#33041;&#25104;&#20687;&#30340;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#22797;&#26434;&#30340;&#20869;&#37096;&#21160;&#24577;&#65292;&#20294;&#21487;&#20197;&#30740;&#31350;&#20854;&#35789;&#27719;&#21644;&#30701;&#35821;&#30340;&#34920;&#31034;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#20063;&#24456;&#38590;&#29702;&#35299;&#65292;&#20294;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#21487;&#20197;&#25552;&#20379;&#22312;&#21548;&#25110;&#35835;&#26102;&#28608;&#27963;&#30340;&#65288;&#22024;&#26434;&#30340;&#65289;&#35760;&#24405;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#25552;&#21462;&#30456;&#20284;&#30340;&#35789;&#27719;&#21644;&#30701;&#35821;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#22312;&#33041;&#35299;&#30721;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36825;&#20123;&#34920;&#31034;&#25152;&#24341;&#21457;&#30340;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#31034;&#19982;&#33041;&#25104;&#20687;&#30340;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#36234;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have complicated internal dynamics, but induce representations of words and phrases whose geometry we can study. Human language processing is also opaque, but neural response measurements can provide (noisy) recordings of activation during listening or reading, from which we can extract similar representations of words and phrases. Here we study the extent to which the geometries induced by these representations, share similarities in the context of brain decoding. We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging. Code is available at \url{https://github.com/coastalcph/brainlm}.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18654</link><description>&lt;p&gt;
&#20449;&#20208;&#19982;&#21629;&#36816;&#65306;Transformer&#22312;&#32452;&#21512;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18654
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21516;&#26102;&#22312;&#19968;&#20123;&#31616;&#21333;&#38382;&#39064;&#19978;&#20063;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#36825;&#24341;&#21457;&#20102;&#30097;&#38382;&#65306;&#36825;&#20123;&#38169;&#35823;&#26159;&#20598;&#28982;&#30340;&#65292;&#36824;&#26159;&#23427;&#20204;&#34920;&#26126;&#20102;&#26356;&#23454;&#36136;&#24615;&#30340;&#38480;&#21046;&#65311;&#20026;&#20102;&#25581;&#31034;Transformer&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#26497;&#38480; - &#22810;&#20301;&#25968;&#20056;&#27861;&#12289;&#36923;&#36753;&#32593;&#26684;&#35868;&#39064;&#21644;&#19968;&#20010;&#32463;&#20856;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290; &#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27493;&#39588;&#65292;&#24182;&#23558;&#36825;&#20123;&#27493;&#39588;&#32508;&#21512;&#25104;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#32452;&#21512;&#22411;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#31639;&#22270;&#65292;&#20197;&#31995;&#32479;&#22320;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#25512;&#29702;&#27493;&#39588;&#20998;&#35299;&#20026;&#20013;&#38388;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#36890;&#36807;&#23558;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#36716;&#21270;&#20026;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#26469;&#35299;&#20915;&#32452;&#21512;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#24773;&#32490;&#20215;&#20540;&#19982;&#24773;&#32490;&#36733;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#32852;&#21512;&#39044;&#27979;&#35774;&#32622;&#20013;&#20351;&#29992;&#21028;&#21035;&#24615;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.17422</link><description>&lt;p&gt;
&#29702;&#35299;&#24773;&#32490;&#20215;&#20540;&#26159;&#19968;&#39033;&#32852;&#21512;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Understanding Emotion Valence is a Joint Deep Learning Task. (arXiv:2305.17422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17422
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#24773;&#32490;&#20215;&#20540;&#19982;&#24773;&#32490;&#36733;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#32852;&#21512;&#39044;&#27979;&#35774;&#32622;&#20013;&#20351;&#29992;&#21028;&#21035;&#24615;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35828;&#35805;&#20154;&#30340;&#35805;&#35821;&#25110;&#20889;&#20316;&#20013;&#24773;&#32490;&#20215;&#20540;&#20998;&#26512;&#26377;&#21161;&#20110;&#29702;&#35299;&#23545;&#35805;&#20013;&#24773;&#32490;&#29366;&#24577;&#30340;&#28608;&#27963;&#21644;&#21464;&#21270;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#24773;&#32490;&#36733;&#20307;&#65288;EC&#65289;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;&#35828;&#35805;&#20154;&#25152;&#24863;&#21463;&#21040;&#30340;&#24773;&#32490;&#21450;&#20854;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#24773;&#32490;&#20215;&#20540;&#21644;EC&#20043;&#38388;&#30340;&#33258;&#28982;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#24773;&#32490;&#20215;&#20540;&#21644;EC&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#21333;&#20219;&#21153;&#12289;&#20004;&#27493;&#27861;&#21644;&#32852;&#21512;&#35774;&#32622;&#12290;&#25105;&#20204;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#29983;&#25104;&#24615;&#65288;GPT-2&#65289;&#21644;&#21028;&#21035;&#24615;&#65288;BERT&#65289;&#26550;&#26500;&#22312;&#27599;&#31181;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20010;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30495;&#23454;&#26631;&#31614;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#21028;&#21035;&#24615;&#27169;&#22411;&#22312;&#32852;&#21512;&#39044;&#27979;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#24773;&#32490;&#20215;&#20540;&#21644;EC&#39044;&#27979;&#20219;&#21153;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#33021;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The valence analysis of speakers' utterances or written posts helps to understand the activation and variations of the emotional state throughout the conversation. More recently, the concept of Emotion Carriers (EC) has been introduced to explain the emotion felt by the speaker and its manifestations. In this work, we investigate the natural inter-dependency of valence and ECs via a multi-task learning approach. We experiment with Pre-trained Language Models (PLM) for single-task, two-step, and joint settings for the valence and EC prediction tasks. We compare and evaluate the performance of generative (GPT-2) and discriminative (BERT) architectures in each setting. We observed that providing the ground truth label of one task improves the prediction performance of the models in the other task. We further observed that the discriminative model achieves the best trade-off of valence and EC prediction tasks in the joint prediction setting. As a result, we attain a single model that perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16397</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26159;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23450;&#24615;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#37492;&#21035;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#32622;&#20110;&#33258;&#21160;&#32454;&#31890;&#24230;&#23450;&#37327;&#35780;&#20272;&#39640;&#32423;&#29616;&#35937;&#65288;&#22914;&#32452;&#21512;&#24615;&#65289;&#30340;&#20219;&#21153;&#20013;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20004;&#39033;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;DiffusionITM&#30340;&#26032;&#26041;&#27861;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#31283;&#23450;&#25193;&#25955;&#65289;&#36716;&#25442;&#20026;&#20219;&#20309;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;(ITM)&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;7&#20010;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12289;&#20559;&#24046;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Stable Diffusion + DiffusionITM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#65288;&#22914;CLEVR&#21644;Winoground&#31561;&#65289;&#19978;&#20248;&#20110;CLIP&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MS-COCO&#19978;&#24494;&#35843;&#20445;&#25345;&#22270;&#20687;&#29305;&#24449;&#30340;&#36716;&#31227;&#35774;&#32622;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#21644;&#29702;&#35770;&#20445;&#35777;&#30340;&#20915;&#31574;&#24863;&#30693;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#32852;&#21512;&#30446;&#26631;&#26469;&#35299;&#20915;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#19988;&#26080;&#35770;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;&#35813;&#31639;&#27861;&#37117;&#20445;&#35777;&#21333;&#35843;&#31574;&#30053;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.15249</link><description>&lt;p&gt;
&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#21644;&#29702;&#35770;&#20445;&#35777;&#30340;&#20915;&#31574;&#24863;&#30693;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees. (arXiv:2305.15249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#21644;&#29702;&#35770;&#20445;&#35777;&#30340;&#20915;&#31574;&#24863;&#30693;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#32852;&#21512;&#30446;&#26631;&#26469;&#35299;&#20915;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#19988;&#26080;&#35770;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;&#35813;&#31639;&#27861;&#37117;&#20445;&#35777;&#21333;&#35843;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21592;&#35780;&#35770;&#23478; (AC) &#26041;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064; (RL) &#20013;&#65292;&#24182;&#20174;&#20351;&#29992;&#20219;&#20309;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20316;&#20026;&#28436;&#21592;&#21644;&#22522;&#20110;&#20540;&#26041;&#27861;&#20316;&#20026;&#35780;&#35770;&#23478;&#30340;&#28789;&#27963;&#24615;&#20013;&#21463;&#30410;&#12290;&#35780;&#35770;&#23478;&#36890;&#24120;&#36890;&#36807;&#26368;&#23567;&#21270; TD &#35823;&#24046;&#26469;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#23454;&#29616;&#39640;&#22870;&#21169;&#30340;&#30495;&#23454;&#30446;&#26631;&#21487;&#33021;&#33073;&#38057;&#30340;&#23458;&#35266;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20915;&#31574;&#24863;&#30693;&#30340;&#32852;&#21512;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#31181;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#30446;&#26631;&#26469;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#30340; AC &#31639;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#20219;&#20309;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#26126;&#30830;&#34920;&#24449;&#20102;&#22312;&#36873;&#25321;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#24471;&#31639;&#27861;&#20445;&#35777;&#21333;&#35843;&#31574;&#30053;&#25913;&#36827;&#30340;&#26465;&#20214;&#12290;&#23454;&#20363;&#21270;&#36890;&#29992;&#31639;&#27861;&#23558;&#23548;&#33268;&#28041;&#21450;&#26368;&#22823;&#21270;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968; (&#31867;&#20284;&#20110; TRPO&#12289;PPO) &#30340;&#28436;&#21592;&#21644;&#28041;&#21450;&#26368;&#23567;&#21270;&#19968;&#20010;&#23494;&#20999;&#30456;&#20851;&#30446;&#26631;&#30340;&#35780;&#35770;&#23478;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#35777;&#26126;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic (AC) methods are widely used in reinforcement learning (RL) and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a decision-aware fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO) and a critic that involves minimizing a closely connected objective. Using simple 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13786</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#12298;&#24863;&#30693;&#27979;&#35797;&#65306;&#22810;&#27169;&#24577;&#35270;&#39057;&#27169;&#22411;&#30340;&#35786;&#26029;&#22522;&#20934;&#12299;
&lt;/p&gt;
&lt;p&gt;
Perception Test: A Diagnostic Benchmark for Multimodal Video Models. (arXiv:2305.13786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#8212;&#8212;&#24863;&#30693;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914; Flamingo&#12289;BEiT-3 &#25110; GPT-4&#65289;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#25216;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20391;&#37325;&#20110;&#35745;&#31639;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#12289;&#26816;&#27979;&#25110;&#36319;&#36394;&#65289;&#19981;&#21516;&#65292;&#24863;&#30693;&#27979;&#35797;&#20391;&#37325;&#20110;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#36328;&#36234;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25512;&#29702;&#31867;&#22411;&#65288;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#65289;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#32780;&#39640;&#25928;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#25110;&#26377;&#38480;&#24494;&#35843;&#19979;&#25361;&#36873;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#65292;&#24863;&#30693;&#27979;&#35797;&#20171;&#32461;&#20102;11.6k&#31181;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#65292;&#24179;&#22343;&#38271;&#24230;&#20026;23&#31186;&#65292;&#26088;&#22312;&#23637;&#31034;&#24863;&#30693;&#19978;&#26377;&#36259;&#30340;&#24773;&#22659;&#65292;&#30001;&#20840;&#29699;&#32422;100&#21517;&#21442;&#19982;&#32773;&#25293;&#25668;&#12290;&#36825;&#20123;&#35270;&#39057;&#23494;&#38598;&#22320;&#24102;&#26377;&#20845;&#31181;&#26631;&#31614;&#65288;&#22810;&#39033;&#36873;&#25321;&#21644;&#22522;&#20110;&#35270;&#39057;&#38382;&#39064;&#22238;&#31572;&#65292;&#23545;&#35937;a&#65289;
&lt;/p&gt;
&lt;p&gt;
We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12162</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;DSIC&#20223;&#23556;&#26497;&#22823;&#20215;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23547;&#25214;&#32463;&#39564;&#19978;&#39640;&#25910;&#20837;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#22810;&#29289;&#21697;&#25293;&#21334;&#24773;&#26223;&#30340;&#24037;&#20316;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;RegretNet&#31867;&#21644;&#20223;&#23556;&#26497;&#22823;&#20215;&#65288;AMAs&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#20005;&#26684;&#20445;&#35777;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#65292;&#32780;&#21518;&#32773;&#22240;&#20026;&#20998;&#37197;&#20505;&#36873;&#20154;&#25968;&#36807;&#22810;&#32780;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMenuNet&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20174;&#20986;&#20215;&#20154;&#21644;&#29289;&#21697;&#34920;&#31034;&#20013;&#26500;&#36896;AMA&#21442;&#25968;&#65288;&#29978;&#33267;&#21253;&#25324;&#20998;&#37197;&#33756;&#21333;&#65289;&#12290;&#30001;&#20110;AMA&#30340;&#23646;&#24615;&#65292;AMenuNet&#22987;&#32456;&#26159;DSIC&#21644;&#20010;&#20154;&#29702;&#24615;&#65288;IR&#65289;&#30340;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#26469;&#22686;&#24378;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;AMenuNet&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#19981;&#21463;&#25293;&#21334;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;AMenuNet&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#24605;&#32771;&#12289;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00833</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#24605;&#32771;&#12289;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#19981;&#33021;&#20445;&#30041;&#20197;&#20379;&#23558;&#26469;&#20351;&#29992;&#30340;&#20808;&#21069;&#25512;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#20801;&#35768;&#27169;&#22411;&#36827;&#34892;&#33258;&#27880;&#35760;&#12290;&#19982;&#26368;&#36817;&#30340;&#24605;&#32500;&#38142;&#25110;&#33609;&#31295;&#26412;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#38543;&#26102;&#20559;&#31163;&#36755;&#20837;&#19978;&#19979;&#25991;&#26469;&#26126;&#30830;&#24605;&#32771;&#21644;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#38405;&#35835;&#19978;&#19979;&#25991;&#26102;&#21363;&#26102;&#25512;&#29702;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35760;&#24518;&#24182;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#32455;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#27880;&#35760;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#24605;&#32500;&#38142;&#21644;&#33609;&#31295;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.
&lt;/p&gt;</description></item><item><title>AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.12479</link><description>&lt;p&gt;
&#29992;&#20110;&#25945;&#32946;&#30340;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12479
&lt;/p&gt;
&lt;p&gt;
AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;GPT-4&#21644;ChatGPT&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20316;&#20026;&#26410;&#26469;&#25216;&#26415;&#24050;&#32463;&#24471;&#21040;&#20840;&#29699;&#35748;&#21487;&#12290;AGI&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#21046;&#20154;&#31867;&#26234;&#33021;&#65292;&#26159;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#38024;&#23545;&#26377;&#38480;&#33539;&#22260;&#30340;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#65292;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#26080;&#27861;&#32771;&#34385;&#25945;&#32946;&#20013;&#22797;&#26434;&#30340;&#20154;&#38469;&#21160;&#24577;&#12290;&#21463;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#39537;&#21160;&#65292;AGI&#20195;&#34920;&#20102;&#26426;&#22120;&#22312;&#25191;&#34892;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#20363;&#22914;&#25512;&#29702;&#12289;&#35299;&#20915;&#38382;&#39064;&#12289;&#20570;&#20986;&#20915;&#31574;&#65292;&#29978;&#33267;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;AGI&#30340;&#20851;&#38190;&#27010;&#24565;&#12289;&#33021;&#21147;&#12289;&#33539;&#22260;&#21644;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#24314;&#31435;e-learning&#24179;&#21488;&#21644;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
&lt;/p&gt;</description></item><item><title>&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07327</link><description>&lt;p&gt;
OpenAssistant Conversations -- &#27665;&#20027;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07327
&lt;/p&gt;
&lt;p&gt;
&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21487;&#29992;&#24615;&#24182;&#25512;&#21160;&#20854;&#24555;&#36895;&#24212;&#29992;&#65292;&#22914;ChatGPT&#25152;&#31034;&#12290; &#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#31561;&#23545;&#40784;&#25216;&#26415;&#22823;&#22823;&#38477;&#20302;&#20102;&#26377;&#25928;&#21457;&#25381;LLM&#33021;&#21147;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290; &#28982;&#32780;&#65292;&#20687;RLHF&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#40784;&#25216;&#26415;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#26114;&#36149;&#19988;&#20445;&#23494;&#12290; &#20026;&#20102;&#27665;&#20027;&#21270;&#22823;&#35268;&#27169;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;161,443&#26465;&#28040;&#24687;&#65292;&#20998;&#24067;&#22312;66,497&#20010;&#23545;&#35805;&#26641;&#20013;&#65292;&#24182;&#22312;35&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#29992;461,292&#20010;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;OpenAssistant Conversations&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#35821;&#26009;&#24211;&#65292;&#20351;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#36827;&#19968;&#27493;&#30740;&#31350;&#27665;&#20027;&#21270;LLM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#31867;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 1
&lt;/p&gt;</description></item><item><title>RoboPianist&#26159;&#19968;&#20010;&#26032;&#30340;&#39640;&#32500;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#39640;&#31934;&#24230;&#12289;&#21327;&#35843;&#21644;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#25509;&#35302;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#36827;&#34892;&#38050;&#29748;&#28436;&#22863;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#24615;&#33021;&#29305;&#24449;&#30340;&#23450;&#37327;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04150</link><description>&lt;p&gt;
RoboPianist&#65306;&#29992;&#20110;&#39640;&#32500;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RoboPianist: A Benchmark for High-Dimensional Robot Control. (arXiv:2304.04150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04150
&lt;/p&gt;
&lt;p&gt;
RoboPianist&#26159;&#19968;&#20010;&#26032;&#30340;&#39640;&#32500;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#39640;&#31934;&#24230;&#12289;&#21327;&#35843;&#21644;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#25509;&#35302;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#36827;&#34892;&#38050;&#29748;&#28436;&#22863;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#24615;&#33021;&#29305;&#24449;&#30340;&#23450;&#37327;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#38024;&#23545;&#27979;&#35797;&#39640;&#31354;&#38388;&#21644;&#26102;&#38388;&#31934;&#24230;&#12289;&#21327;&#35843;&#21644;&#35268;&#21010;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26159;&#22312;&#39057;&#32321;&#36827;&#34892;&#25509;&#35302;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#20013;&#36827;&#34892;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#25361;&#25112;&#26159;&#36890;&#36807;&#21452;&#25163;&#28789;&#24039;&#65292;&#20351;&#29992;&#19968;&#23545;&#20223;&#20154;&#26426;&#22120;&#20154;&#25163;&#26469;&#25484;&#25569;&#38050;&#29748;&#28436;&#22863;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;RoboPianist&#65292;&#26368;&#21021;&#29256;&#26412;&#28085;&#30422;&#20102;150&#39318;&#38590;&#24230;&#19981;&#21516;&#30340;&#27468;&#26354;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#20934;&#27979;&#35797;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#34920;&#24449;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#29305;&#24449;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#26576;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#26576;&#20123;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26576;&#20123;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;RoboPianist&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12289;&#36890;&#36807;&#31616;&#21333;&#22686;&#21152;&#26032;&#27468;&#26354;&#26469;&#25193;&#23637;&#26354;&#30446;&#30340;&#39640;&#26131;&#29992;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new benchmarking suite for high-dimensional control, targeted at testing high spatial and temporal precision, coordination, and planning, all with an underactuated system frequently making-and-breaking contacts. The proposed challenge is mastering the piano through bi-manual dexterity, using a pair of simulated anthropomorphic robot hands. We call it RoboPianist, and the initial version covers a broad set of 150 variable-difficulty songs. We investigate both model-free and model-based methods on the benchmark, characterizing their performance envelopes. We observe that while certain existing methods, when well-tuned, can achieve impressive levels of performance in certain aspects, there is significant room for improvement. RoboPianist provides a rich quantitative benchmarking environment, with human-interpretable results, high ease of expansion by simply augmenting the repertoire with new songs, and opportunities for further research, including in multi-task learning, ze
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03216</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;Pareto&#21069;&#27839;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#32473;&#23450;&#26041;&#21521;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#20854;&#37319;&#26679;&#27604;&#20363;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#36890;&#36807;&#35757;&#32451;200&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#26041;&#21521;&#21644;&#24635;&#20219;&#21153;&#25968;&#37327;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#26102;&#65292;&#26631;&#37327;&#21270;&#23548;&#33268;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#21069;&#27839;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;Pareto&#21069;&#27839;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#24130;&#24459;&#26469;&#39044;&#27979;MNMT&#20013;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#25968;&#25454;&#20805;&#36275;&#24615;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;MNMT&#20013;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#22522;&#20110;&#21452;&#37325;&#24130;&#24459;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12799</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65306;&#29992;&#35270;&#35273;transformer&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#19981;&#35268;&#21017;&#25277;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39640;&#24230;&#23450;&#21046;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#24615;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#23427;&#20204;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#39640;&#31232;&#30095;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#35843;&#25972;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#20197;&#25191;&#34892;&#19982;&#22270;&#20687;&#20998;&#31867;&#30456;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20551;&#35774;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#28508;&#22312;&#22320;&#25193;&#23637;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#20256;&#24863;&#22120;&#35774;&#32622;&#20013;&#65292;&#21363;&#22312;&#27979;&#35797;&#26399;&#38388;&#23631;&#34109;&#21464;&#37327;&#30340;&#23376;&#38598;&#20013;&#65292;&#24615;&#33021;&#27604;&#26368;&#20339;&#22522;&#20934;&#25552;&#39640;&#20102;&#39640;&#36798;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10586</link><description>&lt;p&gt;
&#20998;&#24067;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#22312;&#23569;&#37327;&#26631;&#31614;&#19978;&#20114;&#30456;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#8212;&#8212;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#65292;&#35813;&#31574;&#30053;&#24314;&#31435;&#22312;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#19978;&#12290;DPT&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#20266;&#26631;&#31614;&#65307;&#20351;&#29992;&#36825;&#20123;&#20266;&#26631;&#31614;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#20266;&#22270;&#20687;&#65307;&#24182;&#20351;&#29992;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#28151;&#21512;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;DPT&#22987;&#32456;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#30340;SOTA&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;DPT&#30340;Fr\'echet Inception Distance&#65288;FID&#65289;&#24471;&#20998;&#20998;&#21035;&#20026;3.08&#25110;2.52&#65292;&#36229;&#36807;&#20102;&#20855;&#26377;&#23436;&#25972;&#26631;&#31614;&#30340;&#24378;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;IDDPM&#65292;CDM&#65292;ADM&#21644;LDM&#65289;&#12290;&#27492;&#22806;&#65292;DPT&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#31454;&#20105;&#24615;&#30340;&#21322;&#30417;&#30563;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#39030;&#32423;1&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#32972;&#26223;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#20851;&#38190;&#25361;&#25112;&#21644;&#20248;&#21183;&#65292;&#24182;&#35752;&#35770;&#20102;&#25968;&#25454;&#12289;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2302.10035</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey. (arXiv:2302.10035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#32972;&#26223;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#20851;&#38190;&#25361;&#25112;&#21644;&#20248;&#21183;&#65292;&#24182;&#35752;&#35770;&#20102;&#25968;&#25454;&#12289;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#36890;&#29992;&#28145;&#24230;&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#35768;&#22810;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#25552;&#20986;&#65292;&#20363;&#22914;BERT&#65292;ViT&#65292;GPT&#31561;&#12290;&#21463;&#21040;&#36825;&#20123;&#27169;&#22411;&#22312;&#21333;&#19968;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#36817;&#24180;&#26469;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#24182;&#24076;&#26395;&#26412;&#35770;&#25991;&#33021;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#24110;&#21161;&#26032;&#30740;&#31350;&#20154;&#21592;&#36861;&#36394;&#26368;&#21069;&#27839;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22238;&#39038;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;&#30740;&#31350;&#24037;&#20316;&#65292;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;MM-PTMs&#65289;&#30340;&#20219;&#21153;&#23450;&#20041;&#12289;&#20851;&#38190;&#25361;&#25112;&#21644;&#20248;&#21183;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#25968;&#25454;&#12289;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;MM-PTMs&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21518;&#32493;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36873;&#25321;&#20999;&#21106;&#24179;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#20999;&#21106;&#36873;&#25321;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09166</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25972;&#25968;&#35268;&#21010;&#20013;&#30340;&#20999;&#21106;&#24179;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Cutting Planes in Integer Programming: A Survey. (arXiv:2302.09166v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36873;&#25321;&#20999;&#21106;&#24179;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#20999;&#21106;&#36873;&#25321;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36873;&#25321;&#20999;&#21106;&#24179;&#38754;&#65288;&#25110;&#20999;&#21106;&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#23613;&#31649;&#23384;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#20999;&#21106;&#31867;&#21035;&#65292;&#20294;&#22312;&#20998;&#25903;&#23450;&#30028;&#26641;&#30340;&#32473;&#23450;&#33410;&#28857;&#30340;&#32447;&#24615;&#35268;&#21010;&#25918;&#26494;&#20013;&#36873;&#25321;&#19968;&#32452;&#35201;&#28155;&#21152;&#30340;&#20999;&#21106;&#24179;&#38754;&#30340;&#20219;&#21153;&#36804;&#20170;&#20026;&#27490;&#26080;&#27861;&#24471;&#21040;&#27491;&#24335;&#21644;&#21551;&#21457;&#24335;&#35299;&#27861;&#12290;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#26469;&#35782;&#21035;&#21152;&#36895;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#30340;&#26377;&#21069;&#26223;&#30340;&#20999;&#21106;&#65292;&#20026;&#25913;&#36827;&#20999;&#21106;&#36873;&#25321;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#20027;&#39064;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25968;&#25454;&#25910;&#38598;&#12289;&#35780;&#20272;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25991;&#29486;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#35797;&#22270;&#37327;&#21270;&#24050;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#26368;&#21518;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We survey recent work on machine learning (ML) techniques for selecting cutting planes (or cuts) in mixed-integer linear programming (MILP). Despite the availability of various classes of cuts, the task of choosing a set of cuts to add to the linear programming (LP) relaxation at a given node of the branch-and-bound (B&amp;B) tree has defied both formal and heuristic solutions to date. ML offers a promising approach for improving the cut selection process by using data to identify promising cuts that accelerate the solution of MILP instances. This paper presents an overview of the topic, highlighting recent advances in the literature, common approaches to data collection, evaluation, and ML model architectures. We analyze the empirical results in the literature in an attempt to quantify the progress that has been made and conclude by suggesting avenues for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.07491</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#19982;&#26102;&#38388;&#21644;&#32467;&#26500;&#24378;&#24230;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment. (arXiv:2302.07491v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#26088;&#22312;&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#65292;&#21516;&#26102;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#38745;&#24577;&#22270;&#19981;&#21516;&#65292;&#26102;&#38388;&#22270;&#36890;&#24120;&#20197;&#36830;&#32493;&#26102;&#38388;&#19978;&#30340;&#33410;&#28857;&#20132;&#20114;&#24207;&#21015;&#30340;&#24418;&#24335;&#32452;&#32455;&#65292;&#32780;&#19981;&#26159;&#37051;&#25509;&#30697;&#38453;&#12290;&#22823;&#22810;&#25968;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#26102;&#38388;&#19978;&#32452;&#21512;&#21382;&#21490;&#20449;&#24687;&#26469;&#24314;&#27169;&#24403;&#21069;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#19968;&#38454;&#26102;&#38388;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;S2T&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph learning aims to generate high-quality representations for graph-based tasks along with dynamic information, which has recently drawn increasing attention. Unlike the static graph, a temporal graph is usually organized in the form of node interaction sequences over continuous time instead of an adjacency matrix. Most temporal graph learning methods model current interactions by combining historical information over time. However, such methods merely consider the first-order temporal information while ignoring the important high-order structural information, leading to sub-optimal performance. To solve this issue, by extracting both temporal and structural information to learn more informative node representations, we propose a self-supervised method termed S2T for temporal graph learning. Note that the first-order temporal information and the high-order structural information are combined in different ways by the initial node representations to calculate two conditional 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;ChatGPT-3&#20316;&#20026;&#20889;&#20316;&#36741;&#21161;&#24037;&#20855;&#21644;&#19981;&#20351;&#29992;&#23545;&#23398;&#29983;&#35770;&#25991;&#20889;&#20316;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32452;&#23398;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#30456;&#20284;&#65292;&#20294;&#23454;&#39564;&#32452;&#30340;&#25991;&#26412;&#19981;&#30495;&#23454;&#24615;&#31245;&#39640;&#65292;&#25972;&#20307;&#26679;&#26412;&#20013;&#35770;&#25991;&#30456;&#20284;&#24615;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2302.04536</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT-3&#20316;&#20026;&#23398;&#29983;&#35770;&#25991;&#20889;&#20316;&#36741;&#21161;&#30340;&#25928;&#26524;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Better by you, better than me, chatgpt3 as writing assistance in students essays. (arXiv:2302.04536v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;ChatGPT-3&#20316;&#20026;&#20889;&#20316;&#36741;&#21161;&#24037;&#20855;&#21644;&#19981;&#20351;&#29992;&#23545;&#23398;&#29983;&#35770;&#25991;&#20889;&#20316;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32452;&#23398;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#30456;&#20284;&#65292;&#20294;&#23454;&#39564;&#32452;&#30340;&#25991;&#26412;&#19981;&#30495;&#23454;&#24615;&#31245;&#39640;&#65292;&#25972;&#20307;&#26679;&#26412;&#20013;&#35770;&#25991;&#30456;&#20284;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#27604;&#36739;&#20351;&#29992;ChatGPT-3&#20316;&#20026;&#20889;&#20316;&#36741;&#21161;&#24037;&#20855;&#21644;&#19981;&#20351;&#29992;&#30340;&#23398;&#29983;&#35770;&#25991;&#20889;&#20316;&#34920;&#29616;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#30740;&#31350;&#20013;&#26377;18&#21517;&#23398;&#29983;&#21442;&#19982;&#65288;9&#21517;&#23545;&#29031;&#32452;&#21644;9&#21517;&#20351;&#29992;ChatGPT-3&#30340;&#23454;&#39564;&#32452;&#65289;&#12290;&#25105;&#20204;&#29992;&#25104;&#32489;&#65288;A-D&#65289;&#21644;&#23545;&#24212;&#30340;&#25968;&#20540;&#65288;4-1&#65289;&#23545;&#35770;&#25991;&#35201;&#32032;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35770;&#25991;&#20998;&#25968;&#19982;&#23398;&#29983;&#30340;GPT&#20998;&#25968;&#12289;&#20889;&#20316;&#26102;&#38388;&#12289;&#30495;&#23454;&#24615;&#21644;&#20869;&#23481;&#30456;&#20284;&#24615;&#12290;&#32467;&#26524;&#65306;&#20004;&#32452;&#30340;&#24179;&#22343;&#20998;&#37117;&#26159;C&#65292;&#23545;&#29031;&#32452;&#20026;2.39&#65288;&#26631;&#20934;&#24046;=0.71&#65289;&#65292;&#23454;&#39564;&#32452;&#20026;2.00&#65288;&#26631;&#20934;&#24046;=0.73&#65289;&#12290;&#27809;&#26377;&#20219;&#20309;&#39044;&#27979;&#22240;&#32032;&#23545;&#35770;&#25991;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#65306;&#32452;&#21035;&#65288;P=0.184&#65289;&#65292;&#20889;&#20316;&#25345;&#32493;&#26102;&#38388;&#65288;P=0.669&#65289;&#65292;&#27169;&#22359;&#65288;P=0.388&#65289;&#21644;GPA&#65288;P=0.532&#65289;&#12290;&#23454;&#39564;&#32452;&#20013;&#30340;&#25991;&#26412;&#19981;&#30495;&#23454;&#24615;&#31245;&#39640;&#19968;&#20123;&#65288;11.87%&#65292;&#26631;&#20934;&#24046;=13.45 &#23545;&#27604; 9.96%&#65292;&#26631;&#20934;&#24046;=9.81%&#65289;&#65292;&#20294;&#25972;&#20307;&#26679;&#26412;&#20013;&#35770;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26222;&#36941;&#36739;&#20302;&#65288;Jaccard&#30456;&#20284;&#24230;&#25351;&#25968;&#22312;0&#21040;0.054&#20043;&#38388;&#65289;&#12290;&#22312;&#23454;&#39564;&#32452;&#20013;&#65292;AI&#20998;&#31867;&#22120;&#35782;&#21035;&#20986;&#26356;&#22810;&#28508;&#22312;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aim: To compare students' essay writing performance with or without employing ChatGPT-3 as a writing assistant tool. Materials and methods: Eighteen students participated in the study (nine in control and nine in the experimental group that used ChatGPT-3). We scored essay elements with grades (A-D) and corresponding numerical values (4-1). We compared essay scores to students' GPTs, writing time, authenticity, and content similarity. Results: Average grade was C for both groups; for control (2.39, SD=0.71) and for experimental (2.00, SD=0.73). None of the predictors affected essay scores: group (P=0.184), writing duration (P=0.669), module (P=0.388), and GPA (P=0.532). The text unauthenticity was slightly higher in the experimental group (11.87%, SD=13.45 to 9.96%, SD=9.81%), but the similarity among essays was generally low in the overall sample (the Jaccard similarity index ranging from 0 to 0.054). In the experimental group, AI classifier recognized more potential AI-generated text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10564</link><description>&lt;p&gt;
&#26080;&#35270;&#35273;&#22522;&#32447;&#30340;&#22810;&#27169;&#24335;&#35821;&#27861;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37197;&#23545;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20449;&#21495;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#20013;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;LLM&#30340;C-PCFG&#65288;LC-PCFG&#65289;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#27861;&#24402;&#32435;&#24615;&#33021;&#12290;&#19982;&#24102;&#22270;&#20687;&#30340;&#35821;&#27861;&#24402;&#32435;&#30456;&#27604;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;7.9&#20010;&#28857;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;85&#65285;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;1.7&#20493;&#12290;&#22312;&#19977;&#20010;&#36741;&#21161;&#35270;&#39057;&#30340;&#35821;&#27861;&#24402;&#32435;&#22522;&#20934;&#20013;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26368;&#22810;7.7&#20010;&#28857;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;8.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2212.02648</link><description>&lt;p&gt;
Spuriosity Rankings: &#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20559;&#35265;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#25490;&#21517;&#22270;&#20687;&#30340;&#34394;&#20551;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#20844;&#27491;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#24207;&#25968;&#25454;&#26469;&#27979;&#37327;&#21644;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#32447;&#32034;&#30340;&#20381;&#36182;&#25152;&#24341;&#36215;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#36827;&#34892;&#26114;&#36149;&#30340;&#25913;&#21464;&#65292;&#32780;&#26159;&#26356;&#22909;&#22320;&#21033;&#29992;&#24050;&#26377;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#36890;&#36807;&#21487;&#35299;&#37322;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#31867;&#20869;&#25490;&#24207;&#65292;&#20197;&#34913;&#37327;&#20854;&#34394;&#20551;&#24615;&#65288;&#21363;&#24120;&#35265;&#34394;&#20551;&#32447;&#32034;&#30340;&#23384;&#22312;&#31243;&#24230;&#65289;&#12290;&#36890;&#36807;&#34394;&#20551;&#24615;&#25490;&#21517;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#35782;&#21035;&#20986;&#23569;&#25968;&#23376;&#32676;&#20307;&#65288;&#21363;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#65289;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#29575;&#24046;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#29978;&#33267;&#21487;&#20197;&#36890;&#36807;&#22312;&#34394;&#20551;&#24615;&#36739;&#20302;&#30340;&#22270;&#20687;&#19978;&#24494;&#35843;&#20998;&#31867;&#22836;&#37096;&#65292;&#20197;&#26497;&#23569;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#26469;&#26377;&#25928;&#28040;&#38500;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26679;&#26412;&#30340;&#26356;&#20844;&#27491;&#22788;&#29702;&#65292;&#26080;&#35770;&#34394;&#20551;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27880;&#37322;&#20102;5000&#20010;&#31867;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65288;&#20854;&#20013;630&#20010;&#26159;&#34394;&#20551;&#30340;&#65289;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;325k&#20010;&#36719;&#20998;&#21106;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24037;&#20855;Melting Pot 2.0&#20026;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#19968;&#32452;&#20856;&#22411;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23427;&#20204;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13746</link><description>&lt;p&gt;
&#29076;&#28809;2.0
&lt;/p&gt;
&lt;p&gt;
Melting Pot 2.0. (arXiv:2211.13746v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13746
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24037;&#20855;Melting Pot 2.0&#20026;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#19968;&#32452;&#20856;&#22411;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23427;&#20204;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25215;&#35834;&#24320;&#21457;&#27604;&#8220;&#33258;&#25105;&#20013;&#24515;&#8221;&#26041;&#27861;&#26356;&#20855;&#20154;&#31867;&#29305;&#28857;&#21644;&#26356;&#26131;&#20110;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#26234;&#33021;&#25216;&#26415;&#12290; Melting Pot&#26159;&#20026;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#32780;&#24320;&#21457;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#22312;&#19968;&#32452;&#20856;&#22411;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27599;&#31181;&#24773;&#26223;&#23558;&#19968;&#20010;&#29289;&#29702;&#29615;&#22659;&#65288;&#8220;&#22522;&#26495;&#8221;&#65289;&#19982;&#19968;&#32452;&#21442;&#32771;&#21512;&#20316;&#32773;&#65288;&#8220;&#32972;&#26223;&#20154;&#32676;&#8221;&#65289;&#37197;&#23545;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#38388;&#30456;&#20114;&#20381;&#23384;&#24615;&#30340;&#31038;&#20132;&#24773;&#22659;&#12290;&#20363;&#22914;&#65292;&#19968;&#20123;&#24773;&#24418;&#21463;&#21040;&#20102;&#22522;&#20110;&#21046;&#24230;&#32463;&#27982;&#23398;&#30340;&#33258;&#28982;&#36164;&#28304;&#31649;&#29702;&#21644;&#20844;&#20849;&#29289;&#21697;&#20379;&#32473;&#22256;&#22659;&#30340;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#32780;&#20854;&#20182;&#24773;&#24418;&#21017;&#21463;&#21040;&#20102;&#36827;&#21270;&#29983;&#29289;&#23398;&#12289;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#29983;&#21629;&#31561;&#26041;&#38754;&#30340;&#32771;&#34385;&#25152;&#21551;&#21457;&#12290;Melting Pot&#26088;&#22312;&#28085;&#30422;&#19968;&#32452;&#26368;&#22823;&#22810;&#26679;&#21270;&#30340;&#24773;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent artificial intelligence research promises a path to develop intelligent technologies that are more human-like and more human-compatible than those produced by "solipsistic" approaches, which do not consider interactions between agents. Melting Pot is a research tool developed to facilitate work on multi-agent artificial intelligence, and provides an evaluation protocol that measures generalization to novel social partners in a set of canonical test scenarios. Each scenario pairs a physical environment (a "substrate") with a reference set of co-players (a "background population"), to create a social situation with substantial interdependence between the individuals involved. For instance, some scenarios were inspired by institutional-economics-based accounts of natural resource management and public-good-provision dilemmas. Others were inspired by considerations from evolutionary biology, game theory, and artificial life. Melting Pot aims to cover a maximally diverse set of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32469;&#36807;&#22312;DNN&#25511;&#21046;&#31995;&#32479;&#20013;&#23545;DNN&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;DNN&#20013;&#25554;&#20837;&#19968;&#20010;&#25277;&#35937;&#23618;&#65292;&#23558;&#23454;&#25968;&#25277;&#35937;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#36827;&#32780;&#23454;&#29616;&#23545;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#40657;&#30418;&#21487;&#36798;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2211.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25277;&#35937;&#30340;&#35757;&#32451;&#39535;&#26381;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Taming Reachability Analysis of DNN-Controlled Systems via Abstraction-Based Training. (arXiv:2211.11127v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32469;&#36807;&#22312;DNN&#25511;&#21046;&#31995;&#32479;&#20013;&#23545;DNN&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;DNN&#20013;&#25554;&#20837;&#19968;&#20010;&#25277;&#35937;&#23618;&#65292;&#23558;&#23454;&#25968;&#25277;&#35937;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#36827;&#32780;&#23454;&#29616;&#23545;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#40657;&#30418;&#21487;&#36798;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#20351;&#24471;&#39564;&#35777;&#32593;&#32476;&#26412;&#36523;&#21644;&#25176;&#31649;DNN&#25511;&#21046;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#38754;&#20020;&#30456;&#21516;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#23545;DNN&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25928;&#29575;&#20302;&#19979;&#24182;&#19988;&#36807;&#24230;&#20272;&#35745;&#36739;&#22823;&#65292;&#24182;&#19988;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;DNNs&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#22312;&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#23545;DNNs&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#25277;&#35937;&#23618;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;DNNs&#65292;&#35813;&#25277;&#35937;&#23618;&#23558;&#23454;&#25968;&#25277;&#35937;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;&#36827;&#34892;&#35757;&#32451;&#12290;&#25554;&#20837;&#30340;&#25277;&#35937;&#23618;&#30830;&#20445;&#21306;&#38388;&#34920;&#31034;&#30340;&#20540;&#22312;&#35757;&#32451;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#32593;&#32476;&#19981;&#21487;&#21306;&#20998;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#40657;&#30418;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20854;&#20013;&#21482;&#23545;&#35757;&#32451;&#36807;&#30340;DNN&#36827;&#34892;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intrinsic complexity of deep neural networks (DNNs) makes it challenging to verify not only the networks themselves but also the hosting DNN-controlled systems. Reachability analysis of these systems faces the same challenge. Existing approaches rely on over-approximating DNNs using simpler polynomial models. However, they suffer from low efficiency and large overestimation, and are restricted to specific types of DNNs. This paper presents a novel abstraction-based approach to bypass the crux of over-approximating DNNs in reachability analysis. Specifically, we extend conventional DNNs by inserting an additional abstraction layer, which abstracts a real number to an interval for training. The inserted abstraction layer ensures that the values represented by an interval are indistinguishable to the network for both training and decision-making. Leveraging this, we devise the first black-box reachability analysis approach for DNN-controlled systems, where trained DNNs are only querie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11087</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#22120;&#36741;&#21161;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#23427;&#20204;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26410;&#33021;&#21435;&#20559;&#35265;&#25110;&#32773;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#22120;&#8212;&#8212;&#19968;&#31181;&#36719;&#25237;&#24433;&#26041;&#27861;&#8212;&#8212;&#26469;&#35782;&#21035;&#21644;&#21435;&#38500;&#22914;BERT&#21644;GPT&#31561;LLMs&#20013;&#30340;&#20559;&#35265;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24212;&#29992;&#27010;&#24565;&#22120;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#36890;&#36807;&#21518;&#22788;&#29702;&#36827;&#34892;&#20559;&#35265;&#23376;&#31354;&#38388;&#25237;&#24433;&#65307;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#27010;&#24565;&#22120;&#20171;&#20837;BERT(CI-BERT)&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;LLMs&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#24456;&#31283;&#20581;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#29616;&#26377;&#20559;&#35265;&#23376;&#31354;&#38388;&#30340;&#36923;&#36753;&#25805;&#20316;&#26469;&#26377;&#25928;&#22320;&#20943;&#36731;&#20132;&#38598;&#20559;&#35265;&#12290;&#34429;&#28982;CI-BERT&#30340;&#35757;&#32451;&#32771;&#34385;&#20102;&#25152;&#26377;&#23618;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.02016</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#22312;&#29615;&#22659;&#25200;&#21160;&#19979;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22312;&#20540;&#20989;&#25968;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#31561;&#20215;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#36716;&#25442;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;-&#40065;&#26834;&#24615;&#36716;&#25442;&#22240;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#22120;&#65288;USR&#65289;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#20989;&#25968;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#23454;&#29616;&#12290;&#29305;&#21035;&#26159;&#65292;USR&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;RL&#26694;&#26550;&#20013;&#12290;&#20026;&#20102;&#22788;&#29702;&#26410;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20989;&#25968;&#29983;&#25104;&#30340;&#26032;&#39062;&#23545;&#25239;&#26041;&#27861;&#26469;&#29983;&#25104;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#65288;RWRL&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;USR&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#38236;&#20687;&#19978;&#21319;&#30340;&#26222;&#36866;&#26694;&#26550;(FMA-PG)&#65292;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#65292;&#24182;&#19988;&#19981;&#21463;&#31574;&#30053;&#21442;&#25968;&#21270;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2108.05828</link><description>&lt;p&gt;
&#19968;&#31867;&#31283;&#23450;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#26367;&#20195;&#20989;&#25968;&#30340;&#26222;&#36866;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A general class of surrogate functions for stable and efficient reinforcement learning. (arXiv:2108.05828v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#38236;&#20687;&#19978;&#21319;&#30340;&#26222;&#36866;&#26694;&#26550;(FMA-PG)&#65292;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#65292;&#24182;&#19988;&#19981;&#21463;&#31574;&#30053;&#21442;&#25968;&#21270;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#26367;&#20195;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#26367;&#20195;&#20989;&#25968;&#65292;&#22823;&#22810;&#25968;&#27809;&#26377;&#24378;&#26377;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;TRPO&#12289;PPO&#25110;MPO&#31561;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#19981;&#26159;&#35774;&#35745;&#21478;&#19968;&#20010;&#26367;&#20195;&#20989;&#25968;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#38236;&#20687;&#19978;&#21319;&#30340;&#26222;&#36866;&#26694;&#26550;&#65288;FMA-PG&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#25972;&#22871;&#26367;&#20195;&#20989;&#25968;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#26367;&#20195;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#20445;&#35777;&#31574;&#30053;&#25913;&#36827;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#26367;&#20195;&#20989;&#25968;&#25152;&#27809;&#26377;&#30340;&#29305;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#20123;&#20445;&#35777;&#19981;&#21463;&#31574;&#30053;&#21442;&#25968;&#21270;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;FMA-PG&#30340;&#29305;&#23450;&#23454;&#20363;&#24674;&#22797;&#20102;&#37325;&#35201;&#30340;&#23454;&#29616;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#21069;&#21521;&#21644;&#21453;&#21521;KL&#25955;&#24230;&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20855;&#26377;&#39069;&#22806;&#29702;&#24819;&#24615;&#36136;&#30340;TRPO&#21464;&#31181;&#12290;&#36890;&#36807;&#22312;&#31616;&#21333;&#36125;&#21494;&#26031;&#38382;&#39064;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;FMA-PG&#20135;&#29983;&#30340;&#31639;&#27861;&#23454;&#20363;&#12290;&#35813;&#26694;&#26550;&#20063;&#25903;&#25345;&#20854;&#20182;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple bandit problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#35775;&#20102;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#25351;&#31034;&#20449;&#21495;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#31574;&#30053;&#32593;&#32476;&#20849;&#20139;&#21442;&#25968;&#30340;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#31574;&#30053;&#25110;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24322;&#26500;&#35266;&#27979;&#21644;&#34892;&#21160;&#31354;&#38388;&#23398;&#20064;&#20013;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2005.13625</link><description>&lt;p&gt;
&#37325;&#35775;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.13625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#35775;&#20102;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#25351;&#31034;&#20449;&#21495;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#31574;&#30053;&#32593;&#32476;&#20849;&#20139;&#21442;&#25968;&#30340;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#31574;&#30053;&#25110;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24322;&#26500;&#35266;&#27979;&#21644;&#34892;&#21160;&#31354;&#38388;&#23398;&#20064;&#20013;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#26159;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#24120;&#29992;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#29420;&#31435;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#24182;&#19988;&#25152;&#26377;&#31574;&#30053;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#20849;&#20139;&#21516;&#19968;&#31574;&#30053;&#32593;&#32476;&#65292;&#23427;&#20204;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#30340;&#31574;&#30053;&#25110;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#35266;&#27979;&#20013;&#28155;&#21152;&#26234;&#33021;&#20307;&#29305;&#23450;&#30340;&#25351;&#31034;&#20449;&#21495;&#65288;&#31216;&#20026;&#8220;&#26234;&#33021;&#20307;&#25351;&#31034;&#8221;&#65289;&#26469;&#36827;&#34892;&#23454;&#39564;&#24615;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#20307;&#25351;&#31034;&#30340;&#23616;&#38480;&#22312;&#20110;&#65292;&#22914;&#26524;&#19981;&#36827;&#34892;&#20462;&#25913;&#65292;&#23427;&#26080;&#27861;&#24212;&#29992;&#20110;&#34892;&#21160;&#31354;&#38388;&#21644;/&#25110;&#35266;&#27979;&#31354;&#38388;&#19981;&#21516;&#36136;&#30340;&#29615;&#22659;&#12290;&#26412;&#30740;&#31350;&#27491;&#24335;&#23450;&#20041;&#20102;&#26234;&#33021;&#20307;&#25351;&#31034;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#39318;&#27425;&#23454;&#29616;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#25193;&#23637;&#21442;&#25968;&#20849;&#20139;&#21040;&#24322;&#26500;&#35266;&#27979;&#21644;&#34892;&#21160;&#31354;&#38388;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#24182;&#23545;&#27604;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter sharing, where each agent independently learns a policy with fully shared parameters between all policies, is a popular baseline method for multi-agent deep reinforcement learning. Unfortunately, since all agents share the same policy network, they cannot learn different policies or tasks. This issue has been circumvented experimentally by adding an agent-specific indicator signal to observations, which we term "agent indication". Agent indication is limited, however, in that without modification it does not allow parameter sharing to be applied to environments where the action spaces and/or observation spaces are heterogeneous. This work formalizes the notion of agent indication and proves that it enables convergence to optimal policies for the first time. Next, we formally introduce methods to extend parameter sharing to learning in heterogeneous observation and action spaces, and prove that these methods allow for convergence to optimal policies. Finally, we experimentally
&lt;/p&gt;</description></item></channel></rss>