<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#20013;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#24182;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35843;&#25972;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#35777;&#26126;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#26356;&#22810;&#21453;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11087</link><description>&lt;p&gt;
&#33258;&#21160;&#25512;&#29702;&#39046;&#22495;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Online Learning for Sets of Related Problems in Automated Reasoning. (arXiv:2305.11087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#20013;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#24182;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35843;&#25972;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#35777;&#26126;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#26356;&#22810;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Self-Driven Strategy Learning (sdsl)&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#33258;&#21160;&#25512;&#29702;&#20013;&#38656;&#35201;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;sdsl&#20250;&#22312;&#35299;&#20915;&#26089;&#26399;&#38382;&#39064;&#26102;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#26469;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#23427;&#21033;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25968;&#25454;&#26469;&#35843;&#25972;&#21518;&#32493;&#38382;&#39064;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32447;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25277;&#35937;&#30340;&#36716;&#25442;&#35268;&#21017;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;sdsl&#35745;&#31639;&#23454;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#26465;&#20214;&#37319;&#26679;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Kissat&#27714;&#35299;&#22120;&#19978;&#23454;&#29616;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;Kissat+sdsl&#22312;&#26368;&#26032;&#30340;&#30828;&#20214;&#27169;&#22411;&#26816;&#26597;&#31454;&#36187;&#20013;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26377;&#38480;&#27169;&#22411;&#26816;&#26597;&#26041;&#27861;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#20102;&#26356;&#22810;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Self-Driven Strategy Learning (sdsl), a lightweight online learning methodology for automated reasoning tasks that involve solving a set of related problems. sdsl automatically gathers information, in form of a dataset, while solving earlier problems. It utilizes the learned data to adjust the solving strategy for later problems by fitting a machine learning model to the obtained data on the fly. We formally define the approach as a set of abstract transition rules. We describe a concrete instance of the sdsl calculus which uses conditional sampling for generating data and random forests as the underlying machine learning model. We implement the approach on top of the Kissat solver and show that the combination of Kissat+sdsl certifies larger bounds and finds more counter-examples than other state-of-the-art bounded model checking approaches on benchmarks obtained from the latest Hardware Model Checking Competition.
&lt;/p&gt;</description></item><item><title>Tram&#26159;&#19968;&#31181;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#22312;&#35299;&#30721;&#22120;&#31471;&#31934;&#32454;&#26816;&#32034;&#24110;&#21161;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.11074</link><description>&lt;p&gt;
Tram&#65306;&#19968;&#20010;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization. (arXiv:2305.11074v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11074
&lt;/p&gt;
&lt;p&gt;
Tram&#26159;&#19968;&#31181;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#22312;&#35299;&#30721;&#22120;&#31471;&#31934;&#32454;&#26816;&#32034;&#24110;&#21161;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#25991;&#26412;&#20197;&#25551;&#36848;&#31243;&#24207;&#30340;&#21151;&#33021;&#26159;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#32467;&#21512;&#31070;&#32463;&#27169;&#22411;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26032;&#36235;&#21183;&#27491;&#22312;&#20852;&#36215;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#32034;&#21644;&#32452;&#21512;&#33539;&#24335;&#65288;&#26816;&#32034;&#31867;&#20284;&#30340;&#20195;&#30721;&#29255;&#27573;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;&#20195;&#30721;&#21644;&#25688;&#35201;&#23545;&#26469;&#32534;&#30721;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26159;&#31895;&#31890;&#24230;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#21033;&#29992;&#35299;&#30721;&#22120;&#31471;&#39640;&#36136;&#37327;&#30340;&#26816;&#32034;&#25688;&#35201;&#20196;&#29260;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#22312;&#35299;&#30721;&#22120;&#31471;&#24110;&#21161;&#21407;&#22987;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#22909;&#30340;&#20195;&#30721;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32531;&#35299;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20195;&#30721;&#35821;&#20041;&#38598;&#25104;&#21040;&#25688;&#35201;&#20196;&#29260;&#20013;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Tram&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although Neural Language Models achieve significant performance in this field, an emerging trend is combining neural models with external knowledge. Most previous approaches rely on the sentence-level retrieval and combination paradigm (retrieval of similar code snippets and use of the corresponding code and summary pairs) on the encoder side. However, this paradigm is coarse-grained and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we explore a fine-grained token-level retrieval-augmented mechanism on the decoder side to help the vanilla neural model generate a better code summary. Furthermore, to mitigate the limitation of token-level retrieval on capturing contextual code semantics, we propose to integrate code semantics into summary tokens. Extensive experiments and human evaluation revea
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21487;&#26356;&#22909;&#30340;&#29702;&#35299;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;Pubmed&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11070</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11070
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21487;&#26356;&#22909;&#30340;&#29702;&#35299;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;Pubmed&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#36935;&#21040;&#30340;&#25991;&#26412;&#20855;&#26377;&#30456;&#20114;&#32852;&#31995;&#30340;&#24773;&#20917;&#30456;&#24403;&#22810;&#12290;&#20363;&#22914;&#65292;Wikipedia&#25991;&#31456;&#36890;&#36807;&#36229;&#38142;&#25509;&#24341;&#29992;&#20854;&#20182;&#25991;&#31456;&#65292;&#31185;&#23398;&#35770;&#25991;&#36890;&#36807;&#24341;&#29992;&#25110;&#65288;&#20849;&#21516;&#65289;&#20316;&#32773;&#19982;&#20854;&#20182;&#35770;&#25991;&#30456;&#20851;&#32852;&#65292;&#32780;&#25512;&#25991;&#21017;&#36890;&#36807;&#20851;&#27880;&#24444;&#27492;&#25110;&#36716;&#21457;&#20869;&#23481;&#26469;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#31867;&#20284;&#20110;&#22270;&#24418;&#30340;&#32467;&#26500;&#21487;&#20197;&#34920;&#31034;&#29616;&#26377;&#30340;&#32852;&#31995;&#65292;&#24182;&#34987;&#35270;&#20026;&#25429;&#25417;&#25991;&#26412;&#30340;&#8220;&#19978;&#19979;&#25991;&#8221;&#12290;&#22240;&#27492;&#65292;&#25552;&#21462;&#21644;&#25972;&#21512;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#33258;&#21160;&#29702;&#35299;&#25991;&#26412;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#23558;&#22522;&#20110;&#22270;&#24418;&#30340;&#19978;&#19979;&#25991;&#21270;&#32435;&#20837;BERT&#27169;&#22411;&#20250;&#22686;&#24378;&#20854;&#22312;&#20998;&#31867;&#20219;&#21153;&#31034;&#20363;&#19978;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;Pubmed&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35823;&#24046;&#20174;8.51&#65285;&#38477;&#33267;7.96&#65285;&#65292;&#21516;&#26102;&#20165;&#22686;&#21152;&#20102;1.6&#65285;&#30340;&#21442;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the "context" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert
&lt;/p&gt;</description></item><item><title>ORKG-Leaderboards&#26159;&#19968;&#31181;&#20197;&#30693;&#35782;&#22270;&#35889;&#24418;&#24335;&#25366;&#25496;AI&#39046;&#22495;&#25490;&#34892;&#27036;&#24182;&#25903;&#25345;&#26426;&#22120;&#21487;&#25805;&#20316;&#24615;&#20986;&#29256;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#35753;&#30740;&#31350;&#20154;&#21592;&#36879;&#26126;&#22320;&#20102;&#35299;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#36319;&#36394;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.11068</link><description>&lt;p&gt;
ORKG-Leaderboards: &#19968;&#31181;&#20197;&#30693;&#35782;&#22270;&#35889;&#24418;&#24335;&#25366;&#25496;&#25490;&#34892;&#27036;&#30340;&#31995;&#32479;&#21270;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph. (arXiv:2305.11068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11068
&lt;/p&gt;
&lt;p&gt;
ORKG-Leaderboards&#26159;&#19968;&#31181;&#20197;&#30693;&#35782;&#22270;&#35889;&#24418;&#24335;&#25366;&#25496;AI&#39046;&#22495;&#25490;&#34892;&#27036;&#24182;&#25903;&#25345;&#26426;&#22120;&#21487;&#25805;&#20316;&#24615;&#20986;&#29256;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#35753;&#30740;&#31350;&#20154;&#21592;&#36879;&#26126;&#22320;&#20102;&#35299;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#36319;&#36394;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; ORKG-Leaderboard &#36719;&#20214;&#65292;&#23427;&#26088;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#22823;&#37327;&#23454;&#35777;&#30740;&#31350;&#35770;&#25991;&#20013;&#33258;&#21160;&#25552;&#21462;&#20197;&#20219;&#21153;-&#25968;&#25454;&#38598;-&#24230;&#37327;&#20803;&#32452;&#20026;&#23450;&#20041;&#30340;&#25490;&#34892;&#27036;&#12290;&#35813;&#36719;&#20214;&#25903;&#25345;&#23398;&#26415;&#20986;&#29256;&#30340;&#20027;&#35201;&#24037;&#20316;&#27969;&#31243;&#65292;&#21363; LaTeX &#25991;&#20214;&#25110; PDF &#25991;&#20214;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#36824;&#19982; Open Research Knowledge Graph (ORKG) &#24179;&#21488;&#38598;&#25104;&#65292;&#35813;&#24179;&#21488;&#20419;&#36827;&#20102;&#23398;&#26415;&#21457;&#29616;&#30340;&#26426;&#22120;&#21487;&#25805;&#20316;&#24615;&#20986;&#29256;&#12290;&#22240;&#27492;&#65292;&#24403;&#31995;&#32479;&#36755;&#20986;&#19982; ORKG &#25903;&#25345;&#30340;&#35821;&#20041; web &#22522;&#30784;&#35774;&#26045;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#20004;&#20010;&#26041;&#38754;&#30340;&#21151;&#33021;&#65306;1&#65289;&#27178;&#36328;&#20840;&#29699;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#35777;&#30740;&#31350;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#26377;&#21487;&#33021;&#26159;&#23436;&#25972;&#30340;&#65307;2&#65289;&#35753;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this work is to describe the Orkg-Leaderboard software designed to extract leaderboards defined as Task-Dataset-Metric tuples automatically from large collections of empirical research papers in Artificial Intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the Open Research Knowledge Graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus the system output, when integrated within the ORKG's supported Semantic Web infrastructure of representing machine-actionable 'resources' on the Web, enables: 1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and 2) specifically, enables researchers to track the progress in AI with an overview 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11067</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#20869;&#23481;&#20016;&#23500;&#12289;&#25925;&#20107;&#36830;&#36143;&#30340;&#28459;&#30011;
&lt;/p&gt;
&lt;p&gt;
Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#20445;&#25345;&#38899;&#20048;&#23478;&#38899;&#20048;&#39118;&#26684;&#30340;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#26410;&#23436;&#25104;&#30340;&#38899;&#20048;&#20316;&#21697;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#26377;&#36259;&#30340;&#28459;&#30011;&#25925;&#20107;&#65292;&#24182;&#20445;&#25345;&#33402;&#26415;&#23478;&#30340;&#33402;&#26415;&#39118;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#29983;&#25104;&#24773;&#33410;&#21644;&#23545;&#35805;&#65292;&#28982;&#21518;&#20351;&#29992;stable diffusion&#29983;&#25104;&#28459;&#30011;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;AI&#29983;&#25104;&#25925;&#20107;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#23545;stable diffusion&#36827;&#34892;fine-tuning&#65292;&#36798;&#21040;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25991;&#26412;&#36716;SQL&#26041;&#27861;: SPSQL&#65292;&#23427;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#38590;&#24230;&#21644;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35201;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11061</link><description>&lt;p&gt;
SPSQL: &#22522;&#20110;&#36880;&#27493;&#35299;&#26512;&#30340;&#25991;&#26412;&#36716;SQL&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation. (arXiv:2305.11061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25991;&#26412;&#36716;SQL&#26041;&#27861;: SPSQL&#65292;&#23427;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#38590;&#24230;&#21644;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35201;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;(Text2SQL)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#24211;&#30340;&#20351;&#29992;&#24050;&#32463;&#28183;&#36879;&#21040;&#21508;&#20010;&#39046;&#22495;&#65292;&#20854;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#35268;&#27169;&#22823;&#12289;&#31181;&#31867;&#22810;&#26679;&#12289;&#33539;&#22260;&#24191;&#27867;&#65292;&#20351;&#24471;&#25968;&#25454;&#26597;&#35810;&#32321;&#29712;&#20302;&#25928;&#65292;&#23545;Text2SQL&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#39640;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;Text2SQL&#26041;&#27861;: SPSQL&#65292;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#36807;&#31243;&#20998;&#35299;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#8212;&#8212;&#34920;&#36873;&#25321;&#12289;&#21015;&#36873;&#25321;&#12289;SQL&#29983;&#25104;&#21644;&#20540;&#22635;&#20805;&#65292;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#21487;&#35299;&#26512;&#30340;&#21477;&#27861;&#26641;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting text into the structured query language (Text2SQL) is a research hotspot in the field of natural language processing (NLP), which has broad application prospects. In the era of big data, the use of databases has penetrated all walks of life, in which the collected data is large in scale, diverse in variety, and wide in scope, making the data query cumbersome and inefficient, and putting forward higher requirements for the Text2SQL model. In practical applications, the current mainstream end-to-end Text2SQL model is not only difficult to build due to its complex structure and high requirements for training data, but also difficult to adjust due to massive parameters. In addition, the accuracy of the model is hard to achieve the desired result. Based on this, this paper proposes a pipelined Text2SQL method: SPSQL. This method disassembles the Text2SQL task into four subtasks--table selection, column selection, SQL generation, and value filling, which can be converted into a te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#21160;&#24577;&#30340;&#24605;&#24819;&#20070;&#20889;&#26041;&#24335;&#65292;&#21363;&#21518;&#26399;&#32465;&#23450;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;AI&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#36825;&#31181;&#26032;&#24418;&#24335;&#20063;&#20250;&#24102;&#26469;&#30456;&#20851;&#27861;&#24459;&#21644;&#35268;&#33539;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11058</link><description>&lt;p&gt;
AI&#26102;&#20195;&#30340;&#21518;&#26399;&#32465;&#23450;&#23398;&#26415;&#30740;&#31350;&#65306;&#24212;&#23545;&#30693;&#35782;&#29983;&#20135;&#20013;&#30340;&#27861;&#24459;&#19982;&#35268;&#33539;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Late-Binding Scholarship in the Age of AI: Navigating Legal and Normative Challenges of a New Form of Knowledge Production. (arXiv:2305.11058v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#21160;&#24577;&#30340;&#24605;&#24819;&#20070;&#20889;&#26041;&#24335;&#65292;&#21363;&#21518;&#26399;&#32465;&#23450;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;AI&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#36825;&#31181;&#26032;&#24418;&#24335;&#20063;&#20250;&#24102;&#26469;&#30456;&#20851;&#27861;&#24459;&#21644;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#26395;&#20419;&#36827;&#23398;&#26415;&#20869;&#23481;&#30340;&#21019;&#26032;&#24615;&#21457;&#23637;&#12290;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3&#65289;&#30340;&#21512;&#20316;&#31561;&#26032;&#30340;&#19982;AI&#31995;&#32479;&#20114;&#21160;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#25913;&#21464;&#23398;&#26415;&#30740;&#31350;&#21450;&#20854;&#20135;&#20986;&#29289;&#36136;&#24615;&#36136;&#30340;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#21160;&#24577;&#32534;&#20889;&#12289;&#20998;&#21457;&#12289;&#38405;&#35835;&#12289;&#32452;&#32455;&#21644;&#23384;&#20648;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#27604;&#24403;&#21069;&#30340;&#23398;&#26415;&#23454;&#36341;&#26356;&#21152;&#26377;&#25928;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#19968;&#31181;&#8220;&#21518;&#26399;&#32465;&#23450;&#8221;&#30340;&#36807;&#31243;&#65288;&#21363;&#22312;&#24605;&#24819;&#31163;&#24320;&#20316;&#32773;&#26700;&#23376;&#21069;&#19981;&#23436;&#20840;&#24402;&#32435;&#20986;&#19968;&#31181;&#26368;&#32456;&#30340;&#20070;&#38754;&#24418;&#24335;&#30340;&#36807;&#31243;&#65289;&#65292;&#32780;&#38750;&#24403;&#21069;&#30340;&#8220;&#26089;&#26399;&#32465;&#23450;&#8221;&#36807;&#31243;&#12290;&#21453;&#36807;&#26469;&#65292;&#22312;&#38405;&#35835;&#26102;&#23558;&#24605;&#24819;&#21363;&#26102;&#20070;&#20889;&#21487;&#20197;&#22312;&#23398;&#26415;&#24037;&#20316;&#20013;&#20135;&#29983;&#8220;&#21160;&#24577;&#8221;&#30340;&#21464;&#21270;&#65292;&#36825;&#23558;&#21487;&#33021;&#24418;&#25104;&#8220;&#26410;&#32465;&#23450;&#8221;&#30340;&#30693;&#35782;&#20256;&#36882;&#26041;&#24335;&#12290;&#36825;&#19968;&#20840;&#26032;&#30340;&#30693;&#35782;&#27169;&#24335;&#21487;&#33021;&#20652;&#29983;&#24418;&#24577;&#21508;&#24322;&#30340;&#23398;&#26415;&#20316;&#21697;&#22312;&#35835;&#32773;&#38754;&#21069;&#23454;&#26102;&#29983;&#25104;&#30340;&#23616;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26032;&#24418;&#24335;&#30340;&#30693;&#35782;&#29983;&#20135;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#37325;&#22823;&#30340;&#27861;&#24459;&#21644;&#35268;&#33539;&#38382;&#39064;&#65292;&#22914;&#25152;&#26377;&#26435;&#12289;&#20316;&#32773;&#36523;&#20221;&#21644;&#36136;&#37327;&#25511;&#21046;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#36127;&#36131;&#20219;&#22320;&#20351;&#29992;AI&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is poised to enable a new leap in the creation of scholarly content. New forms of engagement with AI systems, such as collaborations with large language models like GPT-3, offer affordances that will change the nature of both the scholarly process and the artifacts it produces. This article articulates ways in which those artifacts can be written, distributed, read, organized, and stored that are more dynamic, and potentially more effective, than current academic practices. Specifically, rather than the current "early-binding" process (that is, one in which ideas are fully reduced to a final written form before they leave an author's desk), we propose that there are substantial benefits to a "late-binding" process, in which ideas are written dynamically at the moment of reading. In fact, the paradigm of "binding" knowledge may transition to a new model in which scholarship remains ever "unbound" and evolving. An alternative form for a scholarly work could b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#20840;&#29699;&#27700;&#36164;&#28304;&#31232;&#32570;&#24615;&#21644;&#36136;&#37327;&#38382;&#39064;&#30340;&#39033;&#30446;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#21270;&#30340;&#27700;&#20581;&#24247;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#65288;WHOW-KG&#65289;&#65292;&#21487;&#20197;&#23545;&#27700;&#30340;&#28040;&#32791;&#12289;&#27745;&#26579;&#12289;&#20256;&#26579;&#30149;&#29575;&#21644;&#33647;&#21697;&#20998;&#24067;&#31561;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25903;&#25345;&#30693;&#35782;&#21457;&#29616;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2305.11051</link><description>&lt;p&gt;
&#27700;&#20581;&#24247;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
The Water Health Open Knowledge Graph. (arXiv:2305.11051v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#20840;&#29699;&#27700;&#36164;&#28304;&#31232;&#32570;&#24615;&#21644;&#36136;&#37327;&#38382;&#39064;&#30340;&#39033;&#30446;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#21270;&#30340;&#27700;&#20581;&#24247;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#65288;WHOW-KG&#65289;&#65292;&#21487;&#20197;&#23545;&#27700;&#30340;&#28040;&#32791;&#12289;&#27745;&#26579;&#12289;&#20256;&#26579;&#30149;&#29575;&#21644;&#33647;&#21697;&#20998;&#24067;&#31561;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25903;&#25345;&#30693;&#35782;&#21457;&#29616;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#27700;&#21644;&#20581;&#24247;&#36164;&#28304;&#30340;&#31649;&#29702;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#31181;&#20852;&#36259;&#28304;&#20110;&#20840;&#29699;&#21487;&#25345;&#32493;&#21457;&#23637;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#27700;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#21644;&#36136;&#37327;&#25104;&#20026;&#20102;&#20854;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#12289;&#26377;&#24847;&#20041;&#21644;&#24320;&#25918;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#32852;&#21512;&#22269;&#25152;&#21046;&#23450;&#30340;&#28165;&#27905;&#27700;&#21644;&#21355;&#29983;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27700;&#20581;&#24247;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#65288;WHOW-KG&#65289;&#21450;&#20854;&#35774;&#35745;&#26041;&#27861;&#21644;&#24433;&#21709;&#20998;&#26512;&#12290;WHOW-KG&#26159;&#19968;&#20010;&#35821;&#20041;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#21487;&#23545;&#27700;&#30340;&#28040;&#32791;&#12289;&#27745;&#26579;&#12289;&#20256;&#26579;&#30149;&#29575;&#21644;&#33647;&#21697;&#20998;&#24067;&#31561;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;WHOW-KG&#26159;&#22312;&#27431;&#30431;&#36164;&#21161;&#30340;WHOW&#65288;&#27700;&#20581;&#24247;&#24320;&#25918;&#30693;&#35782;&#65289;&#39033;&#30446;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#25903;&#25345;&#24191;&#27867;&#30340;&#24212;&#29992;&#65306;&#20174;&#30693;&#35782;&#21457;&#29616;&#21040;&#20915;&#31574;&#21046;&#23450;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#20174;&#19994;&#20154;&#21592;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, an increasing interest in the management of water and health resources has been recorded. This interest is fed by the global sustainability challenges posed to the humanity that have water scarcity and quality at their core. Thus, the availability of effective, meaningful and open data is crucial to address those issues in the broader context of the Sustainable Development Goals of clean water and sanitation as targeted by the United Nations. In this paper, we present the Water Health Open Knowledge Graph (WHOW-KG) along with its design methodology and analysis on impact. WHOW-KG is a semantic knowledge graph that models data on water consumption, pollution, infectious disease rates and drug distribution. The WHOW-KG is developed in the context of the EU-funded WHOW (Water Health Open Knowledge) project and aims at supporting a wide range of applications: from knowledge discovery to decision-making, making it a valuable resource for researchers, policymakers, and practitioner
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26032;&#22411;&#30340;&#37327;&#23376;&#24863;&#30693;&#22120;QVP-G&#12290;&#23545;&#27604;&#32463;&#20856;&#24863;&#30693;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;QVP-G&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;QVP&#26356;&#21152;&#20934;&#30830;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.11040</link><description>&lt;p&gt;
&#20351;&#29992;Grover&#31639;&#27861;&#27169;&#25311;&#21464;&#20998;&#37327;&#23376;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Simulation of a Variational Quantum Perceptron using Grover's Algorithm. (arXiv:2305.11040v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26032;&#22411;&#30340;&#37327;&#23376;&#24863;&#30693;&#22120;QVP-G&#12290;&#23545;&#27604;&#32463;&#20856;&#24863;&#30693;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;QVP-G&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;QVP&#26356;&#21152;&#20934;&#30830;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24863;&#30693;&#22120;&#12289;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#34987;&#35748;&#20026;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#32452;&#20214;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#30340;&#26032;&#22411;&#37327;&#23376;&#24863;&#30693;&#22120;&#12290;&#36890;&#36807;&#35745;&#31639;&#23427;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#20998;&#26512;&#23427;&#20204;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23558;&#36825;&#20004;&#20010;&#37327;&#23376;&#27169;&#22411;&#19982;&#32463;&#20856;&#24863;&#30693;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;QVP&#21644;QVP-G&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#37327;&#23376;&#27169;&#22411;&#27604;CP&#26356;&#26377;&#25928;&#29575;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#24314;&#35758;&#27169;&#22411;QVP-G&#32988;&#36807;QVP&#65292;&#35777;&#26126;&#20102;Grover&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#38500;&#20102;&#38750;&#32467;&#26500;&#21270;&#25628;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum perceptron, the variational circuit, and the Grover algorithm have been proposed as promising components for quantum machine learning. This paper presents a new quantum perceptron that combines the quantum variational circuit and the Grover algorithm. However, this does not guarantee that this quantum variational perceptron with Grover's algorithm (QVPG) will have any advantage over its quantum variational (QVP) and classical counterparts. Here, we examine the performance of QVP and QVP-G by computing their loss function and analyzing their accuracy on the classification task, then comparing these two quantum models to the classical perceptron (CP). The results show that our two quantum models are more efficient than CP, and our novel suggested model QVP-G outperforms the QVP, demonstrating that the Grover can be applied to the classification task and even makes the model more accurate, besides the unstructured search problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.11033</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65306;&#26368;&#36817;&#25991;&#29486;&#20013;&#25216;&#26415;&#21644;&#24120;&#35265;&#36235;&#21183;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#38656;&#35201;&#31639;&#27861;&#22238;&#31572;&#26377;&#20851;&#29305;&#23450;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;25&#39033;&#26368;&#26032;&#30740;&#31350;&#21644;6&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#12290;&#20316;&#32773;&#28145;&#20837;&#35843;&#30740;&#20102;&#35813;&#39046;&#22495;&#30340;&#22810;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#26512;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11029</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#25512;&#26029;&#25991;&#26723;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#35821;&#20041;&#20851;&#31995;&#12290;&#36828;&#31243;&#30417;&#30563;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#20250;&#24102;&#26469;&#26032;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#28155;&#21152;&#34394;&#20551;&#30340;&#20266;&#26631;&#31614;&#21644;&#22833;&#21435;&#27491;&#30830;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#36873;&#25321;&#26377;&#25928;&#30340;&#20266;&#26631;&#31614;&#26469;&#21435;&#22122;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#20173;&#28982;&#26159;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#26469;&#30830;&#23450;&#20266;&#26631;&#31614;&#26159;&#21542;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#26631;&#31614;&#21435;&#22122;&#30340;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;UGDRE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#27979;&#37327;&#20102;&#20855;&#26377;&#37325;&#21472;&#20851;&#31995;&#30340;&#20266;&#26631;&#31614;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#23454;&#20363;&#32423;&#21644;&#20851;&#31995;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26631;&#31614;&#21435;&#22122;&#32452;&#20214;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further consider
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.11019</link><description>&lt;p&gt;
&#26080;&#26631;&#27880;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#20998;&#21106;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20934;&#30830;&#22320;&#39044;&#27979;&#20687;&#32032;&#32423;&#20998;&#21106;&#25513;&#30721;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#23450;&#20301;&#22768;&#38899;&#23545;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#31867;&#21035;&#26631;&#31614;&#12289;&#22270;&#20687;&#25513;&#27169;&#23545;&#21644;&#38899;&#39057;&#26679;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#32452;&#21512;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#65288;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#25513;&#27169;&#65289;&#19977;&#20803;&#32452;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#24863;&#30693;&#21464;&#21387;&#22120;&#65288;AuTR&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#26550;&#26500;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#65307;&#65288;iii&#65289;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Audio-Visual Segmentation (AVS) is to locate sounding objects within visual scenes by accurately predicting pixelwise segmentation masks. In this paper, we present the following contributions: (i), we propose a scalable and annotation-free pipeline for generating artificial data for the AVS task. We leverage existing image segmentation and audio datasets to draw links between category labels, image-mask pairs, and audio samples, which allows us to easily compose (image, audio, mask) triplets for training AVS models; (ii), we introduce a novel Audio-Aware Transformer (AuTR) architecture that features an audio-aware query-based transformer decoder. This architecture enables the model to search for sounding objects with the guidance of audio signals, resulting in more accurate segmentation; (iii), we present extensive experiments conducted on both synthetic and real datasets, which demonstrate the effectiveness of training AVS models with synthetic data generated by our p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#21046;&#25513;&#30721;&#26426;&#21046;&#26469;&#23454;&#29616;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#29305;&#23450;&#30693;&#35782;&#20849;&#20139;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#32456;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.10997</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#21046;&#25513;&#30721;&#20998;&#20139;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks. (arXiv:2305.10997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10997
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#21046;&#25513;&#30721;&#26426;&#21046;&#26469;&#23454;&#29616;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#29305;&#23450;&#30693;&#35782;&#20849;&#20139;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#32456;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#20195;&#29702;&#26088;&#22312;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#36880;&#28176;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#36825;&#38656;&#35201;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#21033;&#29992;&#20197;&#21069;&#30340;&#30693;&#35782;&#24182;&#36991;&#20813;&#36951;&#24536;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#21442;&#25968;&#38548;&#31163;&#26041;&#27861;&#8212;&#8212;&#35843;&#21046;&#25513;&#30721;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#34429;&#28982;&#32456;&#36523;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#22312;&#21333;&#20010;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22810;&#20010;&#20195;&#29702;&#22914;&#20309;&#24444;&#27492;&#20998;&#20139;&#32456;&#36523;&#23398;&#20064;&#30693;&#35782;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35843;&#21046;&#25513;&#30721;&#25152;&#20351;&#29992;&#30340;&#21442;&#25968;&#38548;&#31163;&#26426;&#21046;&#29305;&#21035;&#36866;&#21512;&#22312;&#20998;&#24067;&#24335;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#32456;&#36523;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20195;&#29702;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#38548;&#31163;&#21040;&#29305;&#23450;&#30340;&#25513;&#30721;&#20013;&#65292;&#35753;&#20195;&#29702;&#21487;&#20197;&#25353;&#38656;&#36716;&#31227;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#23454;&#29616;&#24378;&#22823;&#32780;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#12290;&#25105;&#20204;&#20551;&#35774;&#21160;&#24577;&#24180;&#40836;&#20840;&#20998;&#24067;&#24335;&#24322;&#27493;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning agents aim to learn multiple tasks sequentially over a lifetime. This involves the ability to exploit previous knowledge when learning new tasks and to avoid forgetting. Modulating masks, a specific type of parameter isolation approach, have recently shown promise in both supervised and reinforcement learning. While lifelong learning algorithms have been investigated mainly within a single-agent approach, a question remains on how multiple agents can share lifelong learning knowledge with each other. We show that the parameter isolation mechanism used by modulating masks is particularly suitable for exchanging knowledge among agents in a distributed and decentralized system of lifelong learners. The key idea is that the isolation of specific task knowledge to specific masks allows agents to transfer only specific knowledge on-demand, resulting in robust and effective distributed lifelong learning. We assume fully distributed and asynchronous scenarios with dynamic age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#23545;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#20854;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10992</link><description>&lt;p&gt;
&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#23545;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#20854;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#38656;&#35201;&#39044;&#27979;&#26367;&#25442;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;token&#30340;&#25513;&#30721;&#12290;&#23613;&#31649;&#26368;&#36817;&#20351;&#29992;&#26356;&#31616;&#21333;&#19988;&#35745;&#31639;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#30340;&#31532;&#19968;&#20010;&#23383;&#31526;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20294;&#20351;&#29992;&#25513;&#30721;&#26041;&#26696;&#30340;&#20219;&#21153;&#30446;&#21069;&#36824;&#27809;&#26377;&#36229;&#36234;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#12290;&#26412;&#25991;&#20551;&#35774;&#32570;&#20047;&#22797;&#26434;&#24615;&#26159;&#36896;&#25104;&#20854;&#24615;&#33021;&#19979;&#38477;&#30340;&#20851;&#38190;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#26356;&#22797;&#26434;&#30340;&#25513;&#30721;&#20219;&#21153;&#26159;&#21542;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#38656;&#35201;&#36798;&#21040;&#22810;&#23569;&#25165;&#33021;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#20351;&#29992;GLUE&#12289;SQuAD&#21644;Universal Dependencies&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20542;&#21521;&#20110;&#23637;&#29616;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#33267;&#23569;&#38656;&#35201;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22797;&#26434;&#24230;&#30340;&#19968;&#21322;&#25165;&#33021;&#19982;&#20854;&#34920;&#29616;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#25513;&#30721;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; KgV &#30340; sigmoid &#38376;&#25511;&#26426;&#21046;&#65292;&#36890;&#36807;&#23884;&#20837;&#23618;&#21098;&#26525;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102; H-SoftPOS &#23618;&#27425;&#23884;&#20837;&#23618;&#36827;&#19968;&#27493;&#25913;&#36827;&#23884;&#20837;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312; WMT14 &#33521;&#24503;&#39564;&#35777;&#38598;&#19978;&#20351; perplexity &#20943;&#23569;&#20102;&#19977;&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.10991</link><description>&lt;p&gt;
&#26356;&#23569;&#21363;&#26159;&#26356;&#22909;&#65281;&#19968;&#31181;&#20248;&#21270;&#35821;&#35328;&#32763;&#35793;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#12290;(arXiv:2305.10991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; KgV &#30340; sigmoid &#38376;&#25511;&#26426;&#21046;&#65292;&#36890;&#36807;&#23884;&#20837;&#23618;&#21098;&#26525;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102; H-SoftPOS &#23618;&#27425;&#23884;&#20837;&#23618;&#36827;&#19968;&#27493;&#25913;&#36827;&#23884;&#20837;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312; WMT14 &#33521;&#24503;&#39564;&#35777;&#38598;&#19978;&#20351; perplexity &#20943;&#23569;&#20102;&#19977;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Softmax &#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#24320;&#21457;&#65292;&#26500;&#24314;&#22312; Transformer &#26550;&#26500;&#30340;&#25104;&#21151;&#22522;&#30784;&#20043;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#23384;&#20648;&#22120;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; KgV&#65292;&#19968;&#31181; sigmoid &#38376;&#25511;&#26426;&#21046;&#65292;&#19982; softmax &#27880;&#24847;&#21147;&#19968;&#36215;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#12290;&#20026;&#20102;&#20462;&#27491;&#22823;&#23567;&#35201;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#24352;&#37327;&#38142;&#26469;&#35782;&#21035;&#21644;&#20462;&#21098;&#22810;&#20313;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#22810;&#20313;&#20027;&#35201;&#23384;&#22312;&#20110;&#23884;&#20837;&#23618;&#20013;&#65292;&#32780;&#19981;&#26159;&#36755;&#20986;&#32447;&#24615;&#23618;&#20013;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#23884;&#20837;&#21644;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; H-SoftPOS&#65292;&#19968;&#31181;&#23618;&#27425;&#23884;&#20837;&#23618;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312; WMT14 &#33521;&#24503;&#39564;&#35777;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351; perplexity &#20943;&#23569;&#20102;&#19977;&#20493;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The softmax attention mechanism has emerged as a noteworthy development in the field of Artificial Intelligence research, building on the successes of Transformer-based architectures. However, their ever increasing sizes necessitate ever increasing computational memory, that limits their usage. We propose KgV, a sigmoid gating mechanism that, in conjunction with softmax attention, significantly boosts performance without increasing architecture size. To amend the size requirements, we leverage Tensor Chains to identify and prune the excess parameters. We find that such excess resides primarily within the embedding layer, and not in the output linear layer. To further improve embedding and significantly reduce parameters, we introduce H-SoftPOS, a hierarchical embedding layer which simultaneously enhances performance. Remarkably, on the WMT14 English-German validation set, our approach yields a threefold reduction in perplexity, surpassing the current state-of-the-art, while reducing pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#23545;&#20110;&#20174;&#30524;&#24213;&#22270;&#20687;&#20013;&#26816;&#27979;OT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#20998;&#21106;&#32593;&#32476;&#22312;&#22270;&#20687;&#20013;&#20998;&#21106;&#30149;&#21464;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30340;DL&#25216;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.10975</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30524;&#37096;&#24339;&#24418;&#34411;&#30149;&#33258;&#21160;&#35786;&#26029;&#22522;&#20934;&#27979;&#35797;&#65306;&#23545;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;&#20840;&#38754;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Deep Learning Frameworks for Automated Diagnosis of Ocular Toxoplasmosis: A Comprehensive Approach to Classification and Segmentation. (arXiv:2305.10975v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#23545;&#20110;&#20174;&#30524;&#24213;&#22270;&#20687;&#20013;&#26816;&#27979;OT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#20998;&#21106;&#32593;&#32476;&#22312;&#22270;&#20687;&#20013;&#20998;&#21106;&#30149;&#21464;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30340;DL&#25216;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#37096;&#24339;&#24418;&#34411;&#30149;&#65288;OT&#65289;&#26159;&#30001;&#24339;&#24418;&#34411;&#24341;&#36215;&#30340;&#24120;&#35265;&#30524;&#37096;&#24863;&#26579;&#65292;&#21487;&#23548;&#33268;&#35270;&#21147;&#38382;&#39064;&#12290;&#35786;&#26029;&#36890;&#24120;&#36890;&#36807;&#20020;&#24202;&#26816;&#26597;&#21644;&#25104;&#20687;&#36827;&#34892;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#22797;&#26434;&#19988;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#35757;&#32451;&#26377;&#32032;&#30340;&#20154;&#21592;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#30740;&#31350;&#65292;&#35780;&#20272;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#29616;&#26377;&#39044;&#35757;&#32451;&#32593;&#32476;&#26816;&#27979;&#20174;&#30524;&#24213;&#22270;&#20687;&#20013;&#26816;&#27979;OT&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#20998;&#21106;&#32593;&#32476;&#22312;&#22270;&#20687;&#20013;&#20998;&#21106;&#30149;&#21464;&#30340;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#25351;&#21335;&#65292;&#24110;&#21161;&#20854;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#19968;&#31181;&#24265;&#20215;&#12289;&#33258;&#21160;&#21270;&#12289;&#26131;&#20110;&#20351;&#29992;&#19988;&#20934;&#30830;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#20998;&#26512;&#65292;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#29992;&#20110;OT&#20998;&#31867;&#21644;&#30149;&#21464;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocular Toxoplasmosis (OT), is a common eye infection caused by T. gondii that can cause vision problems. Diagnosis is typically done through a clinical examination and imaging, but these methods can be complicated and costly, requiring trained personnel. To address this issue, we have created a benchmark study that evaluates the effectiveness of existing pre-trained networks using transfer learning techniques to detect OT from fundus images. Furthermore, we have also analysed the performance of transfer-learning based segmentation networks to segment lesions in the images. This research seeks to provide a guide for future researchers looking to utilise DL techniques and develop a cheap, automated, easy-to-use, and accurate diagnostic method. We have performed in-depth analysis of different feature extraction techniques in order to find the most optimal one for OT classification and segmentation of lesions. For classification tasks, we have evaluated pre-trained models such as VGG16, Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#37325;&#39033;&#30446;&#21644;&#21306;&#38388;&#25209;&#20934;&#25237;&#31080;&#30340;&#21442;&#19982;&#24335;&#39044;&#31639;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25928;&#29992;&#27010;&#24565;&#24182;&#35777;&#26126;&#20102;&#31215;&#26497;&#32467;&#26524;&#30340;&#25193;&#23637;&#24615;&#65292;&#20998;&#26512;&#20102;&#38382;&#39064;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#36319;&#36394;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#20844;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.10972</link><description>&lt;p&gt;
&#22810;&#37325;&#39033;&#30446;&#21644;&#21306;&#38388;&#25209;&#20934;&#25237;&#31080;&#30340;&#21442;&#19982;&#24335;&#39044;&#31639;
&lt;/p&gt;
&lt;p&gt;
Participatory Budgeting With Multiple Degrees of Projects And Ranged Approval Votes. (arXiv:2305.10972v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#37325;&#39033;&#30446;&#21644;&#21306;&#38388;&#25209;&#20934;&#25237;&#31080;&#30340;&#21442;&#19982;&#24335;&#39044;&#31639;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25928;&#29992;&#27010;&#24565;&#24182;&#35777;&#26126;&#20102;&#31215;&#26497;&#32467;&#26524;&#30340;&#25193;&#23637;&#24615;&#65292;&#20998;&#26512;&#20102;&#38382;&#39064;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#36319;&#36394;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#20844;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21487;&#20998;&#37197;&#30340;&#21442;&#19982;&#24335;&#39044;&#31639;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#26377;&#19968;&#20010;&#26377;&#38480;&#30340;&#39044;&#31639;&#38656;&#35201;&#20998;&#37197;&#32473;&#19968;&#32452;&#39033;&#30446;&#65292;&#36890;&#36807;&#27719;&#38598;&#36873;&#27665;&#23545;&#39033;&#30446;&#30340;&#20559;&#22909;&#26469;&#20998;&#37197;&#12290;&#20851;&#20110;&#19981;&#21487;&#20998;&#37197;&#30340;&#21442;&#19982;&#24335;&#39044;&#31639;&#30340;&#25152;&#26377;&#20808;&#21069;&#24037;&#20316;&#37117;&#20551;&#35774;&#27599;&#20010;&#39033;&#30446;&#21482;&#26377;&#19968;&#31181;&#21487;&#33021;&#30340;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#27599;&#20010;&#39033;&#30446;&#20855;&#26377;&#19968;&#32452;&#21487;&#20801;&#35768;&#30340;&#25104;&#26412;&#65292;&#27599;&#20010;&#25104;&#26412;&#21453;&#26144;&#35813;&#39033;&#30446;&#21487;&#33021;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;&#27599;&#20010;&#36873;&#27665;&#20026;&#27599;&#20010;&#39033;&#30446;&#25209;&#20934;&#19968;&#23450;&#25104;&#26412;&#33539;&#22260;&#65292;&#36890;&#36807;&#32473;&#20986;&#22905;&#35748;&#20026;&#35813;&#39033;&#30446;&#24212;&#35813;&#20855;&#26377;&#30340;&#25104;&#26412;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;PB&#35268;&#21017;&#30340;&#32467;&#26524;&#36873;&#25321;&#19968;&#32452;&#39033;&#30446;&#65292;&#24182;&#25351;&#23450;&#23427;&#20204;&#23545;&#24212;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#30340;&#25928;&#29992;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#22312;&#27599;&#20010;&#39033;&#30446;&#21482;&#26377;&#19968;&#31181;&#21487;&#20801;&#35768;&#30340;&#25104;&#26412;&#26102;&#29616;&#26377;&#30340;&#31215;&#26497;&#32467;&#26524;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20854;&#20013;&#19968;&#20010;&#39033;&#30446;&#20855;&#26377;&#22810;&#20010;&#21487;&#20801;&#35768;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#38382;&#39064;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#36319;&#36394;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#19988;&#30452;&#35266;&#30340;&#20844;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an indivisible participatory budgeting (PB) framework, we have a limited budget that is to be distributed among a set of projects, by aggregating the preferences of voters for the projects. All the prior work on indivisible PB assumes that each project has only one possible cost. In this work, we let each project have a set of permissible costs, each reflecting a possible degree of sophistication of the project. Each voter approves a range of costs for each project, by giving an upper and lower bound on the cost that she thinks the project deserves. The outcome of a PB rule selects a subset of projects and also specifies their corresponding costs. We study different utility notions and prove that the existing positive results when every project has exactly one permissible cost can also be extended to our framework where a project has several permissible costs. We also analyze the fixed parameter tractability of the problem. Finally, we propose some important and intuitive axioms and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197; Kaggle &#31454;&#36187;&#20026;&#20363;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#65292;&#39044;&#38450;&#25968;&#25454;&#37319;&#38598;&#21644;&#26631;&#27880;&#38454;&#27573;&#30340;&#38382;&#39064;&#35201;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20462;&#27491;&#26356;&#21152;&#37325;&#35201;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#19981;&#24179;&#34913;&#27979;&#35797;&#65292;&#30417;&#25511;&#25968;&#25454;&#21644;&#27169;&#22411;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.10961</link><description>&lt;p&gt;
&#39044;&#38450;&#32988;&#20110;&#27835;&#30103;&#65306;&#33016;&#37096;&#24322;&#24120;&#26816;&#27979;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prevention is better than cure: a case study of the abnormalities detection in the chest. (arXiv:2305.10961v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197; Kaggle &#31454;&#36187;&#20026;&#20363;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#65292;&#39044;&#38450;&#25968;&#25454;&#37319;&#38598;&#21644;&#26631;&#27880;&#38454;&#27573;&#30340;&#38382;&#39064;&#35201;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20462;&#27491;&#26356;&#21152;&#37325;&#35201;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#19981;&#24179;&#34913;&#27979;&#35797;&#65292;&#30417;&#25511;&#25968;&#25454;&#21644;&#27169;&#22411;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#39044;&#38450;&#32988;&#20110;&#27835;&#30103;&#8221; &#19981;&#20165;&#36866;&#29992;&#20110;&#30142;&#30149;&#39044;&#38450;&#65292;&#20063;&#36866;&#29992;&#20110;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38382;&#39064;&#30340;&#39044;&#38450;&#12290;&#39044;&#27979;&#27169;&#22411;&#22833;&#25928;&#30340;&#26681;&#28304;&#24448;&#24448;&#19981;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#65292;&#32780;&#26159;&#25968;&#25454;&#37319;&#38598;&#25110;&#23454;&#39564;&#35774;&#35745;&#38454;&#27573;&#12290;&#26412;&#25991;&#20197; Kaggle &#31454;&#36187;&#20026;&#20363;&#65292;&#35814;&#32454;&#20998;&#26512; X-&#20809;&#32954;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#27979;&#35797;&#22914;&#20309;&#26292;&#38706;&#20986;&#25968;&#25454;&#37319;&#38598;&#21644;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#22797;&#26434;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#36825;&#26679;&#30340;&#20266;&#30456;&#65292;&#23558;&#20854;&#31227;&#38500;&#25110;&#25913;&#27491;&#21518;&#30340;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#38454;&#27573;&#20986;&#29616;&#30340;&#38169;&#35823;&#20351;&#24471;&#27169;&#22411;&#27491;&#30830;&#39564;&#35777;&#21464;&#24471;&#22256;&#38590;&#12290;&#22522;&#20110;&#27492;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#39044;&#27979;&#27169;&#22411;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#30417;&#25511;&#25968;&#25454;&#21644;&#27169;&#22411;&#24179;&#34913;&#65288;&#20844;&#24179;&#24615;&#65289;&#65292;&#20174;&#25968;&#25454;&#37319;&#38598;&#21040;&#27169;&#22411;&#20998;&#25968;&#30340;&#24179;&#34913;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevention is better than cure. This old truth applies not only to the prevention of diseases but also to the prevention of issues with AI models used in medicine. The source of malfunctioning of predictive models often lies not in the training process but reaches the data acquisition phase or design of the experiment phase.  In this paper, we analyze in detail a single use case - a Kaggle competition related to the detection of abnormalities in X-ray lung images. We demonstrate how a series of simple tests for data imbalance exposes faults in the data acquisition and annotation process. Complex models are able to learn such artifacts and it is difficult to remove this bias during or after the training. Errors made at the data collection stage make it difficult to validate the model correctly.  Based on this use case, we show how to monitor data and model balance (fairness) throughout the life cycle of a predictive model, from data acquisition to parity analysis of model scores.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#21830;&#29992; VR &#30028;&#38754;&#39640;&#25928;&#36828;&#31243;&#25805;&#20316;&#24037;&#19994;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#28388;&#27874;&#26041;&#27861;&#20197;&#35299;&#20915; VR &#25511;&#21046;&#26426;&#22120;&#20154;&#33218;&#26102;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.10960</link><description>&lt;p&gt;
&#24037;&#19994;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#34394;&#25311;&#29616;&#23454;&#36828;&#31243;&#25805;&#20316;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
A Virtual Reality Teleoperation Interface for Industrial Robot Manipulators. (arXiv:2305.10960v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#21830;&#29992; VR &#30028;&#38754;&#39640;&#25928;&#36828;&#31243;&#25805;&#20316;&#24037;&#19994;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#28388;&#27874;&#26041;&#27861;&#20197;&#35299;&#20915; VR &#25511;&#21046;&#26426;&#22120;&#20154;&#33218;&#26102;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20351;&#29992;&#21830;&#29992;&#34394;&#25311;&#29616;&#23454;(VR)&#30028;&#38754;&#36828;&#31243;&#25805;&#20316;&#24037;&#19994;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20197;&#24448;&#30340;VR&#26426;&#22120;&#20154;&#25805;&#32437;&#36828;&#31243;&#25805;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21327;&#20316;&#25110;&#30740;&#31350;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#65288;&#20854;&#21160;&#21147;&#23398;&#21644;&#32422;&#26463;&#19981;&#21516;&#20110;&#24037;&#19994;&#26426;&#22120;&#33218;&#65289;&#65292;&#25110;&#20165;&#22788;&#29702;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#19981;&#22826;&#37325;&#35201;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#65306;&#25342;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21830;&#29992;VR&#30028;&#38754;&#26377;&#25928;&#22320;&#36828;&#31243;&#25805;&#20316;&#24037;&#19994;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#22788;&#29702;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#26631;&#20934;&#30340;VR&#26426;&#22120;&#20154;&#33218;&#25511;&#21046;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;&#24179;&#21488;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25197;&#30697;&#21644;&#36895;&#24230;&#25511;&#21046;&#27809;&#26377;&#26292;&#38706;&#65292;&#20301;&#32622;&#25511;&#21046;&#26159;&#36890;&#36807;&#40657;&#30418;&#25511;&#21046;&#22120;&#20171;&#23548;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#28388;&#27874;&#26041;&#27861;&#26469;&#22788;&#29702;&#21629;&#20196;&#20449;&#21495;&#65292;&#20197;&#20351;&#25805;&#20316;&#21592;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;VR&#30028;&#38754;&#36828;&#31243;&#25805;&#20316;&#24037;&#19994;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of teleoperating an industrial robot manipulator via a commercially available Virtual Reality (VR) interface. Previous works on VR teleoperation for robot manipulators focus primarily on collaborative or research robot platforms (whose dynamics and constraints differ from industrial robot arms), or only address tasks where the robot's dynamics are not as important (e.g: pick and place tasks). We investigate the usage of commercially available VR interfaces for effectively teleoeprating industrial robot manipulators in a variety of contact-rich manipulation tasks. We find that applying standard practices for VR control of robot arms is challenging for industrial platforms because torque and velocity control is not exposed, and position control is mediated through a black-box controller. To mitigate these problems, we propose a simplified filtering approach to process command signals to enable operators to effectively teleoperate industrial robot arms with VR inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33041;&#21551;&#21457;AI&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#24341;&#21457;&#30340;&#19968;&#20123;&#27010;&#24565;&#24615;&#12289;&#25216;&#26415;&#24615;&#21644;&#20262;&#29702;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10938</link><description>&lt;p&gt;
&#33041;&#21551;&#21457;AI&#20262;&#29702;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A method for the ethical analysis of brain-inspired AI. (arXiv:2305.10938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33041;&#21551;&#21457;AI&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#24341;&#21457;&#30340;&#19968;&#20123;&#27010;&#24565;&#24615;&#12289;&#25216;&#26415;&#24615;&#21644;&#20262;&#29702;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#30446;&#26631;&#26041;&#38754;&#65292;&#23427;&#20173;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#12290;&#36825;&#20123;&#38480;&#21046;&#21487;&#20197;&#35828;&#26159;&#27010;&#24565;&#19978;&#30340;&#65288;&#20363;&#22914;&#65292;&#19982;&#24213;&#23618;&#29702;&#35770;&#27169;&#22411;&#26377;&#20851;&#65292;&#22914;&#31526;&#21495;&#22411;&#19982;&#36830;&#25509;&#22411;&#65289;&#65292;&#20063;&#21487;&#20197;&#26159;&#25805;&#20316;&#19978;&#30340;&#65288;&#20363;&#22914;&#65292;&#19982;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30456;&#20851;&#65289;&#12290;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;AI&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;AI&#65292;&#25215;&#35834;&#25552;&#20379;&#36229;&#36234;&#24050;&#20256;&#32479;&#21253;&#21547;&#22312;AI&#20013;&#30340;&#29983;&#29289;&#26041;&#38754;&#65292;&#20174;&#32780;&#21487;&#20197;&#35780;&#20272;&#21644;&#21487;&#33021;&#20811;&#26381;&#20854;&#29616;&#26377;&#32570;&#28857;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33041;&#21551;&#21457;AI&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#24341;&#21457;&#30340;&#19968;&#20123;&#27010;&#24565;&#24615;&#12289;&#25216;&#26415;&#24615;&#21644;&#20262;&#29702;&#24615;&#38382;&#39064;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#33041;&#21551;&#21457;AI&#26159;&#21542;&#23384;&#22312;&#20262;&#29702;&#19978;&#30340;&#29420;&#29305;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its successes, to date Artificial Intelligence (AI) is still characterized by a number of shortcomings with regards to different application domains and goals. These limitations are arguably both conceptual (e.g., related to underlying theoretical models, such as symbolic vs. connectionist), and operational (e.g., related to robustness and ability to generalize). Biologically inspired AI, and more specifically brain-inspired AI, promises to provide further biological aspects beyond those that are already traditionally included in AI, making it possible to assess and possibly overcome some of its present shortcomings. This article examines some conceptual, technical, and ethical issues raised by the development and use of brain-inspired AI. Against this background, the paper asks whether there is anything ethically unique about brain-inspired AI. The aim of the paper is to introduce a method that has a heuristic nature and that can be applied to identify and address the ethical 
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10930</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10930
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#23384;&#22312;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#21363;&#23558;&#32763;&#35793;&#36755;&#20986;&#21040;&#38169;&#35823;&#30340;&#35821;&#35328;&#20013;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24403;&#32534;&#30721;&#30446;&#26631;&#35821;&#35328;&#20449;&#21495;&#26102;&#22833;&#25928;&#65292;&#20250;&#23548;&#33268;&#31163;&#35889;&#38382;&#39064;&#65292;&#24182;&#19988;&#20004;&#31181;&#35821;&#35328;&#35789;&#27719;&#20043;&#38388;&#26356;&#25509;&#36817;&#30340;&#35789;&#27719;&#36317;&#31163;&#65288;&#21363;KL&#20998;&#27495;&#65289;&#19982;&#26356;&#39640;&#30340;&#31163;&#35889;&#29575;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#65292;&#20165;&#38548;&#31163;&#35299;&#30721;&#22120;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35789;&#27719;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;Language Aware Vocabulary Sharing (LAVS)&#26469;&#26500;&#24314;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#65292;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#32763;&#35793;&#27169;&#22411;&#30340;&#31163;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;90&#20010;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;LAVS&#30340;&#31163;&#35889;&#29575;&#38477;&#20302;&#20102;37&#65285;&#33267;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10924</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#30340;&#36716;&#22411;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#37117;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Pruning&#65292;&#19968;&#31181;&#19987;&#20026;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;Diff-Pruning&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#21098;&#26525;&#26102;&#38388;&#27493;&#38271;&#30340;Taylor&#23637;&#24320;&#65292;&#22312;&#36807;&#28388;&#25481;&#26080;&#36129;&#29486;&#25193;&#25955;&#27493;&#39588;&#21644;&#25972;&#21512;&#26377;&#20449;&#24687;&#30340;&#26799;&#24230;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;1&#65289;&#25928;&#29575;&#65306;&#23427;&#21487;&#20197;&#20197;&#21407;&#22987;&#35757;&#32451;&#25237;&#20837;&#30340;&#20165;10&#65285;&#21040;20&#65285;&#30340;&#20195;&#20215;&#23454;&#29616;&#32422;50&#65285;&#30340;FLOPs&#20943;&#23569;; 2&#65289;&#19968;&#33268;&#24615;: &#21098;&#26525;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#24314;&#27169;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#20195;&#29702;&#38388;&#22914;&#20309;&#23454;&#29616;&#26356;&#22909;&#22320;&#32039;&#24613;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#32452;&#25104;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#32039;&#24613;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.10920</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32039;&#24613;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Emergent Communication with Attention. (arXiv:2305.10920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#20195;&#29702;&#38388;&#22914;&#20309;&#23454;&#29616;&#26356;&#22909;&#22320;&#32039;&#24613;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#32452;&#25104;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#32039;&#24613;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24320;&#21457;&#20986;&#26356;&#22909;&#22320;&#20351;&#29992;&#33258;&#24049;&#30340;&#32039;&#24613;&#35821;&#35328;&#36827;&#34892;&#36890;&#20449;&#30340;&#35745;&#31639;&#20195;&#29702;&#65292;&#25105;&#20204;&#36171;&#20104;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#20851;&#27880;&#29305;&#23450;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#20154;&#31867;&#32463;&#24120;&#23558;&#19968;&#20010;&#23545;&#35937;&#25110;&#22330;&#26223;&#29702;&#35299;&#20026;&#27010;&#24565;&#30340;&#32452;&#21512;&#65292;&#24182;&#23558;&#36825;&#20123;&#27010;&#24565;&#36827;&#19968;&#27493;&#26144;&#23556;&#21040;&#21333;&#35789;&#19978;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#30452;&#35273;&#23454;&#29616;&#20026;"Speaker"&#21644;"Listener"&#20195;&#29702;&#20013;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#24341;&#29992;&#28216;&#25103;&#20013;&#26174;&#31034;&#27880;&#24847;&#21147;&#23548;&#33268;&#26356;&#20855;&#32452;&#25104;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#32039;&#24613;&#35821;&#35328;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35843;&#26597;&#27599;&#20010;&#28040;&#24687;&#31526;&#21495;&#30456;&#20851;&#30340;&#27880;&#24847;&#26435;&#37325;&#20197;&#21450;"Speaker"&#21644;"Listener"&#20195;&#29702;&#20043;&#38388;&#27880;&#24847;&#26435;&#37325;&#30340;&#23545;&#40784;&#65292;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#23398;&#20064;&#21040;&#30340;&#36890;&#20449;&#21327;&#35758;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#26159;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#21270;&#30340;&#32039;&#24613;&#35821;&#35328;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
To develop computational agents that better communicate using their own emergent language, we endow the agents with an ability to focus their attention on particular concepts in the environment. Humans often understand an object or scene as a composite of concepts and those concepts are further mapped onto words. We implement this intuition as cross-modal attention mechanisms in Speaker and Listener agents in a referential game and show attention leads to more compositional and interpretable emergent language. We also demonstrate how attention aids in understanding the learned communication protocol by investigating the attention weights associated with each message symbol and the alignment of attention weights between Speaker and Listener agents. Overall, our results suggest that attention is a promising mechanism for developing more human-like emergent language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10913</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#23398;&#20064;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#38590;&#24230;&#26356;&#22823;&#65292;&#22240;&#20026;&#26080;&#27861;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#25991;&#26412;&#30701;&#35821;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#27169;&#22411;&#65288;SPRM&#65289;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#24471;&#21040;&#30340;&#12290;&#31532;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22359;&#26088;&#22312;&#36820;&#22238;&#25991;&#26412;&#30701;&#35821;&#21644;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#31895;&#30053;&#23545;&#40784;&#12290;&#31532;&#20108;&#20010;&#35757;&#32451;&#36807;&#30340;&#27169;&#22359;&#30001;&#20004;&#20010;&#23376;&#32452;&#20214;&#32452;&#25104;&#65292;&#29992;&#20110;&#32454;&#21270;&#31895;&#30053;&#30340;&#23545;&#40784;&#20197;&#25552;&#39640;&#26368;&#32456;&#30701;&#35821;-&#36793;&#30028;&#26694;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#22270;&#20687;&#21644;&#21477;&#23376;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#20351;&#21516;&#19968;&#21477;&#23376;&#21644;&#19968;&#20010;&#26032;&#30340;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#26368;&#23567;&#21270;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Transformer&#24207;&#21015;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDMs)&#65292;&#35777;&#26126;TDMs&#34920;&#29616;&#33391;&#22909;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20316;&#20026;&#31574;&#30053;&#36890;&#29992;&#26368;&#20248;&#34892;&#20026;&#65292;&#27867;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21487;&#20197;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.10912</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#29992;&#21160;&#21147;&#23398;&#27169;&#22411;&#29992;&#20110;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Generalist Dynamics Model for Control. (arXiv:2305.10912v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Transformer&#24207;&#21015;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDMs)&#65292;&#35777;&#26126;TDMs&#34920;&#29616;&#33391;&#22909;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20316;&#20026;&#31574;&#30053;&#36890;&#29992;&#26368;&#20248;&#34892;&#20026;&#65292;&#27867;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21487;&#20197;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Transformer&#24207;&#21015;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDMs)&#12290;&#25105;&#20204;&#22312;DeepMind&#25511;&#21046;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#39318;&#20808;&#65292;&#22312;&#21333;&#29615;&#22659;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;TDMs&#34920;&#29616;&#33391;&#22909;&#12290;&#20854;&#27425;&#65292;TDMs&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;few-shot&#23398;&#20064;&#21644;zero-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20316;&#20026;&#31574;&#30053;&#36890;&#29992;&#26368;&#20248;&#34892;&#20026;&#65292;&#27867;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21487;&#20197;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;&#36825;&#20351;&#24471;TDMs&#25104;&#20026;&#25511;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the use of transformer sequence models as dynamics models (TDMs) for control. In a number of experiments in the DeepMind control suite, we find that first, TDMs perform well in a single-environment learning setting when compared to baseline models. Second, TDMs exhibit strong generalization capabilities to unseen environments, both in a few-shot setting, where a generalist model is fine-tuned with small amounts of data from the target environment, and in a zero-shot setting, where a generalist model is applied to an unseen environment without any further training. We further demonstrate that generalizing system dynamics can work much better than generalizing optimal behavior directly as a policy. This makes TDMs a promising ingredient for a foundation model of control.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10906</link><description>&lt;p&gt;
RobustFair: &#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#25932;&#23545;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (arXiv:2305.10906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21487;&#20449;&#24230;&#32463;&#24120;&#21463;&#21040;&#36731;&#24494;&#25932;&#23545;&#25200;&#21160;&#30340;&#25361;&#25112;&#65292;&#36825;&#19981;&#20165;&#20250;&#30772;&#22351;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#32780;&#19988;&#21487;&#33021;&#20026;&#31867;&#20284;&#30340;&#36755;&#20837;&#23548;&#33268;&#26377;&#20559;&#39044;&#27979;&#65288;&#20010;&#20307;&#20844;&#24179;&#24615;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#20934;&#30830;&#20844;&#27491;&#24230;&#26469;&#24378;&#21046;&#23454;&#26045;&#20934;&#30830;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#20043;&#38388;&#30340;&#35856;&#21644;&#24179;&#34913;&#12290;&#23427;&#24341;&#20837;&#20102;&#20844;&#24179;&#28151;&#28102;&#30697;&#38453;&#30340;&#27010;&#24565;&#26469;&#23558;&#39044;&#27979;&#20998;&#31867;&#20026;&#30495;&#27491;&#20844;&#24179;&#12289;&#30495;&#27491;&#26377;&#20559;&#12289;&#20551;&#27491;&#20844;&#24179;&#21644;&#20551;&#26377;&#20559;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#20351;&#29992;&#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#21046;&#20316;&#30340;&#25932;&#23545;&#25200;&#21160;&#65292;&#23545;DNN&#30340;&#20934;&#30830;&#20844;&#27491;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#26469;&#36817;&#20284;&#25932;&#23545;&#23454;&#20363;&#30340;&#22522;&#26412;&#30495;&#23454;&#24615;&#65292;RobustFair&#21487;&#20197;&#29305;&#21035;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#36825;&#36890;&#24120;&#22312;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#38590;&#20197;&#25417;&#25720;&#65292;&#22312;&#20010;&#20307;&#20844;&#24179;&#35780;&#20272;&#20013;&#32570;&#22833;&#12290;RobustFair&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The trustworthiness of DNNs is often challenged by their vulnerability to minor adversarial perturbations, which may not only undermine prediction accuracy (robustness) but also cause biased predictions for similar inputs (individual fairness). Accurate fairness has been recently proposed to enforce a harmonic balance between accuracy and individual fairness. It induces the notion of fairness confusion matrix to categorize predictions as true fair, true biased, false fair, and false biased. This paper proposes a harmonic evaluation approach, RobustFair, for the accurate fairness of DNNs, using adversarial perturbations crafted through fairness confusion directed gradient search. By using Taylor expansions to approximate the ground truths of adversarial instances, RobustFair can particularly identify the robustness defects entangled for spurious fairness, which are often elusive in robustness evaluation, and missing in individual fairness evaluation. RobustFair can boost robustness and 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IRB-AF&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;IRB&#21644;ArtFlow&#26041;&#27861;&#26469;&#35299;&#20915;&#21475;&#21693;&#22120;&#23448;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22495;&#24046;&#24322;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10883</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21475;&#21693;&#22120;&#23448;sim-to-real&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs. (arXiv:2305.10883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IRB-AF&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;IRB&#21644;ArtFlow&#26041;&#27861;&#26469;&#35299;&#20915;&#21475;&#21693;&#22120;&#23448;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22495;&#24046;&#24322;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#36741;&#21161;&#19979;&#30340;&#32463;&#21475;&#27668;&#31649;&#25554;&#31649;&#38656;&#35201;&#20351;&#29992;&#25903;&#25345;&#21307;&#29983;&#23558;&#27668;&#31649;&#23548;&#31649;&#25554;&#20837;&#22768;&#38376;&#32780;&#19981;&#26159;&#39135;&#31649;&#30340;&#20869;&#31397;&#38236;&#12290;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;&#26426;&#22120;&#20154;&#30340;&#27668;&#31649;&#25554;&#31649;&#38656;&#35201;&#21307;&#30103;&#26426;&#22120;&#20154;&#20687;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#29983;&#19968;&#26679;&#21306;&#20998;&#35299;&#21078;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#27169;&#20223;&#12290;&#28982;&#32780;&#21475;&#21693;&#22120;&#23448;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#24448;&#24448;&#30001;&#20110;&#24320;&#28304;&#25968;&#25454;&#21463;&#38480;&#21644;&#24739;&#32773;&#38544;&#31169;&#38382;&#39064;&#32780;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;Sim-to-Real&#26694;&#26550;&#65292;&#31216;&#20026;IoU-Ranking Blend-ArtFlow (IRB-AF)&#65292;&#29992;&#20110;&#21475;&#21693;&#22120;&#23448;&#30340;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#31181;&#31216;&#20026;IoU-Ranking Blend (IRB)&#30340;&#22270;&#20687;&#34701;&#21512;&#31574;&#30053;&#21644;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;ArtFlow&#12290;&#20854;&#20013;&#65292;IRB&#36890;&#36807;&#20943;&#36731;&#25968;&#25454;&#38598;&#20043;&#38388;&#37325;&#35201;&#30340;&#39046;&#22495;&#24046;&#24322;&#25152;&#23548;&#33268;&#30340;&#23454;&#29616;&#20998;&#21106;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#32780;ArtFlow&#21017;&#21487;&#36827;&#19968;&#27493;&#38477;&#20302;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#24615;&#12290; &#26412;&#30740;&#31350;&#37319;&#29992;&#19968;&#31181;&#34394;&#25311;&#21475;&#21693;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#22909;&#30340;&#21475;&#21693;&#22120;&#23448;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-assisted transoral tracheal intubation (TI) necessitates using an endoscope that helps the physician insert a tracheal tube into the glottis instead of the esophagus. The growing trend of robotic-assisted TI would require a medical robot to distinguish anatomical features like an experienced physician which can be imitated by utilizing supervised deep-learning techniques. However, the real datasets of oropharyngeal organs are often inaccessible due to limited open-source data and patient privacy. In this work, we propose a domain adaptive Sim-to-Real framework called IoU-Ranking Blend-ArtFlow (IRB-AF) for image segmentation of oropharyngeal organs. The framework includes an image blending strategy called IoU-Ranking Blend (IRB) and style-transfer method ArtFlow. Here, IRB alleviates the problem of poor segmentation performance caused by significant datasets domain differences; while ArtFlow is introduced to reduce the discrepancies between datasets further. A virtual oropharynx i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10865</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22411;MARL&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#30528;&#37325;&#20110;&#36866;&#24403;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#33258;&#21160;&#23376;&#30446;&#26631;&#29983;&#25104;&#65288;ASG&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#21487;&#34892;&#30340;MARL&#26041;&#27861;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20869;&#22312;&#39537;&#21160;&#30340;&#22686;&#24378;&#23398;&#20064;&#20013;&#21033;&#29992;&#23376;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#22870;&#21169;&#20013;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26080;&#30097;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#35299;&#32806;"&#20915;&#31574;&#26041;&#27861;&#65292;&#21363;&#22312;MARL&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;&#65288;SAMA&#65289;&#65292;&#21463;&#21040;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
&lt;/p&gt;</description></item><item><title>Quiver &#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#22522;&#20110; GPU &#30340; GNN &#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24037;&#20316;&#36127;&#36733;&#25351;&#26631;&#26469;&#39044;&#27979; GNN &#35831;&#27714;&#30340;&#19981;&#35268;&#21017;&#35745;&#31639;&#65292;&#24182;&#31649;&#29702; GPU &#29992;&#20110;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#65292;&#27604;&#29616;&#26377;&#31995;&#32479;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798; 15x&#12290;</title><link>http://arxiv.org/abs/2305.10863</link><description>&lt;p&gt;
Quiver: &#22522;&#20110;&#24037;&#20316;&#36127;&#36733;&#24863;&#30693;&#30340;&#20302;&#24310;&#36831;&#12289;&#39640;&#21534;&#21520;&#37327;&#30340; GNN &#26381;&#21153;&#25903;&#25345; GPU
&lt;/p&gt;
&lt;p&gt;
Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving with Workload Awareness. (arXiv:2305.10863v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10863
&lt;/p&gt;
&lt;p&gt;
Quiver &#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#22522;&#20110; GPU &#30340; GNN &#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24037;&#20316;&#36127;&#36733;&#25351;&#26631;&#26469;&#39044;&#27979; GNN &#35831;&#27714;&#30340;&#19981;&#35268;&#21017;&#35745;&#31639;&#65292;&#24182;&#31649;&#29702; GPU &#29992;&#20110;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#65292;&#27604;&#29616;&#26377;&#31995;&#32479;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798; 15x&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#30340;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#24517;&#39035;&#22312;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20294;&#30001;&#20110;&#37319;&#26679;&#30340;&#22270;&#33410;&#28857;&#21644;&#32858;&#21512;&#30340; GNN &#29305;&#24449;&#23384;&#22312;&#20559;&#24046;&#65292;&#31995;&#32479;&#38754;&#20020;&#19981;&#35268;&#21017;&#35745;&#31639;&#30340;&#25361;&#25112;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#21033;&#29992; GPU &#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#20165;&#20351;&#29992; GPU &#23545;&#23569;&#37327;&#22270;&#33410;&#28857;&#36827;&#34892;&#37319;&#26679;&#30340;&#24615;&#33021;&#20302;&#20110;&#22522;&#20110; CPU &#30340;&#37319;&#26679;&#65307;&#32780;&#23545;&#35768;&#22810;&#29305;&#24449;&#36827;&#34892;&#32858;&#21512;&#20250;&#20135;&#29983; GPU &#21644; CPU &#20043;&#38388;&#30340;&#39640;&#25968;&#25454;&#31227;&#21160;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340; GNN &#26381;&#21153;&#31995;&#32479;&#20351;&#29992; CPU &#36827;&#34892;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65292;&#38480;&#21046;&#20102;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102; Quiver&#65292;&#19968;&#31181;&#20998;&#24067;&#24335;&#22522;&#20110; GPU &#30340; GNN &#26381;&#21153;&#31995;&#32479;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;Quiver &#30340;&#20851;&#38190;&#24605;&#36335;&#26159;&#21033;&#29992;&#24037;&#20316;&#36127;&#36733;&#25351;&#26631;&#26469;&#39044;&#27979; GNN &#35831;&#27714;&#30340;&#19981;&#35268;&#21017;&#35745;&#31639;&#65292;&#24182;&#31649;&#29702; GPU &#29992;&#20110;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65306;(1) &#23545;&#20110;&#22270;&#37319;&#26679;&#65292;Quiver &#35745;&#31639;&#27010;&#29575;&#37319;&#26679;&#30340;&#22270;&#22823;&#23567;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#33410;&#28857;&#37319;&#26679;&#24182;&#34892;&#24230;&#30340;&#25351;&#26631;&#65307;(2) &#23545;&#20110;&#29305;&#24449;&#32858;&#21512;&#65292;Quiver &#37319;&#29992;&#21015;&#27714;&#21644;&#30340;&#26041;&#24335;&#28040;&#38500;&#25968;&#25454;&#31227;&#21160;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Quiver &#22312;&#23454;&#29616; 94&#65285; &#30340; GPU &#21033;&#29992;&#29575;&#30340;&#21516;&#26102;&#65292;&#27604;&#29616;&#26377;&#30340; GNN &#26381;&#21153;&#31995;&#32479;&#24615;&#33021;&#25552;&#39640;&#20102;&#22810;&#36798; 15 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems for serving inference requests on graph neural networks (GNN) must combine low latency with high throughout, but they face irregular computation due to skew in the number of sampled graph nodes and aggregated GNN features. This makes it challenging to exploit GPUs effectively: using GPUs to sample only a few graph nodes yields lower performance than CPU-based sampling; and aggregating many features exhibits high data movement costs between GPUs and CPUs. Therefore, current GNN serving systems use CPUs for graph sampling and feature aggregation, limiting throughput.  We describe Quiver, a distributed GPU-based GNN serving system with low-latency and high-throughput. Quiver's key idea is to exploit workload metrics for predicting the irregular computation of GNN requests, and governing the use of GPUs for graph sampling and feature aggregation: (1) for graph sampling, Quiver calculates the probabilistic sampled graph size, a metric that predicts the degree of parallelism in graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#30830;&#23450;&#24615;&#36817;&#20284;&#31639;&#23376;&#30340;&#26497;&#38480;&#36817;&#20284;&#12289;&#21322;&#24179;&#34913;&#35821;&#20041;&#30340;&#20195;&#25968;&#20844;&#24335;&#20197;&#21450;&#24102;&#26377;&#32858;&#21512;&#30340;&#19981;&#30456;&#20132;&#36923;&#36753;&#31243;&#24207;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.10846</link><description>&lt;p&gt;
&#38750;&#30830;&#23450;&#24615;&#36817;&#20284;&#31639;&#23376;&#65306;&#26497;&#38480;&#31639;&#23376;&#12289;&#21322;&#24179;&#34913;&#35821;&#20041;&#21644;&#32858;&#21512;&#65288;&#23436;&#25972;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Non-deterministic approximation operators: ultimate operators, semi-equilibrium semantics and aggregates (full version). (arXiv:2305.10846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#30830;&#23450;&#24615;&#36817;&#20284;&#31639;&#23376;&#30340;&#26497;&#38480;&#36817;&#20284;&#12289;&#21322;&#24179;&#34913;&#35821;&#20041;&#30340;&#20195;&#25968;&#20844;&#24335;&#20197;&#21450;&#24102;&#26377;&#32858;&#21512;&#30340;&#19981;&#30456;&#20132;&#36923;&#36753;&#31243;&#24207;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#19981;&#21160;&#28857;&#29702;&#35770;&#65288;AFT&#65289;&#26159;&#30740;&#31350;&#38750;&#21333;&#35843;&#36923;&#36753;&#35821;&#20041;&#30340;&#25277;&#35937;&#21644;&#36890;&#29992;&#20195;&#25968;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;AFT&#34987;&#25512;&#24191;&#21040;&#38750;&#30830;&#23450;&#24615;&#31639;&#23376;&#65292;&#21363;&#20854;&#33539;&#22260;&#26159;&#20803;&#32032;&#30340;&#38598;&#21512;&#32780;&#19981;&#26159;&#21333;&#20010;&#20803;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#38750;&#30830;&#23450;&#24615;AFT&#36827;&#34892;&#20102;&#19977;&#20010;&#36827;&#19968;&#27493;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23450;&#20041;&#21644;&#30740;&#31350;&#20102;&#38750;&#30830;&#23450;&#24615;&#31639;&#23376;&#30340;&#26497;&#38480;&#36817;&#20284;&#65292;&#65288;2&#65289;&#25105;&#20204;&#32473;&#20986;&#20102;Amendola&#31561;&#20154;&#25552;&#20986;&#30340;&#21322;&#24179;&#34913;&#35821;&#20041;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#65288;3&#65289;&#25105;&#20204;&#23558;&#19981;&#30456;&#20132;&#36923;&#36753;&#31243;&#24207;&#30340;&#29305;&#24449;&#25512;&#24191;&#21040;&#24102;&#26377;&#32858;&#21512;&#30340;&#19981;&#30456;&#20132;&#36923;&#36753;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximation fixpoint theory (AFT) is an abstract and general algebraic framework for studying the semantics of non-monotonic logics. In recent work, AFT was generalized to non-deterministic operators, i.e.\ operators whose range are sets of elements rather than single elements. In this paper, we make three further contributions to non-deterministic AFT: (1) we define and study ultimate approximations of non-deterministic operators, (2) we give an algebraic formulation of the semi-equilibrium semantics by Amendola, et al., and (3) we generalize the characterisations of disjunctive logic programs to disjunctive logic programs with aggregates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-IQE&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;&#23427;&#20855;&#26377;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12289;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#35780;&#20272;&#22270;&#20687;&#32654;&#23398;&#31561;&#20248;&#28857;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28145;&#24230;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10843</link><description>&lt;p&gt;
X-IQE&#65306;&#21033;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models. (arXiv:2305.10843v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-IQE&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;&#23427;&#20855;&#26377;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12289;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#35780;&#20272;&#22270;&#20687;&#32654;&#23398;&#31561;&#20248;&#28857;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28145;&#24230;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;X-IQE&#65292;&#23427;&#21033;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;X-IQE&#21033;&#29992;&#20998;&#23618;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#20351;MiniGPT-4&#33021;&#22815;&#20135;&#29983;&#33258;&#27965;&#12289;&#26080;&#20559;&#30340;&#25991;&#26412;&#65292;&#19982;&#20154;&#31867;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#23427;&#20855;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#33021;&#22815;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#12289;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#35780;&#20272;&#22270;&#20687;&#32654;&#23398;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#22411;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#27604;&#65292;X-IQE&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#26174;&#33879;&#22686;&#24378;&#20102;&#28145;&#24230;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#22522;&#20934;&#30340;&#26377;&#25928;&#24615;&#12290;X-IQE&#22312;COCO Caption&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#35780;&#20272;&#26041;&#27861;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations. X-IQE utilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation. It offers several advantages, including the ability to distinguish between real and generated images, evaluate text-image alignment, and assess image aesthetics without requiring model training or fine-tuning. X-IQE is more cost-effective and efficient compared to human evaluation, while significantly enhancing the transparency and explainability of deep image quality evaluation models. We validate the effectiveness of our method as a benchmark using images generated by prevalent diffusion models. X-IQE demonstrates similar performance to state-of-the-art (SOTA) evaluation methods on COCO Caption, while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#34920;&#24449;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#25968;&#25454;&#28857;&#30340;&#31934;&#30830;&#24230;&#24182;&#24110;&#21161;&#33258;&#21160;&#20390;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.10840</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Deep Neural Networks through Statistical Inference on Latent Space. (arXiv:2305.10840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#34920;&#24449;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#25968;&#25454;&#28857;&#30340;&#31934;&#30830;&#24230;&#24182;&#24110;&#21161;&#33258;&#21160;&#20390;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#34920;&#24449;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#22788;&#29702;&#25968;&#25454;&#28857;&#30340;&#31934;&#30830;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#35299;&#20915;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#8220;&#36807;&#20110;&#33258;&#20449;&#8221;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#32593;&#32476;&#33021;&#22815;&#27491;&#30830;&#20998;&#31867;&#30340;&#37096;&#20998;&#35757;&#32451;&#38598;&#25152;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#20197;&#27492;&#24314;&#31435;&#20102;&#33021;&#22815;&#25429;&#25417;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#24615;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#24120;&#29992;&#26041;&#27861;&#19968;&#33324;&#23384;&#22312;&#8220;&#36807;&#20110;&#33258;&#20449;&#8221;&#30340;&#38382;&#39064;&#65292;&#29978;&#33267;&#23545;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#28857;&#20381;&#28982;&#22914;&#27492;&#12290;&#19982;&#20043;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#31934;&#24230;&#27424;&#20339;&#30340;&#25968;&#25454;&#28857;&#65292;&#22240;&#27492;&#23545;&#20110;&#24322;&#24120;&#20540;&#30340;&#33258;&#21160;&#20390;&#27979;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-quantification methods are applied to estimate the confidence of deep-neural-networks classifiers over their predictions. However, most widely used methods are known to be overconfident. We address this problem by developing an algorithm that exploits the latent-space representation of data points fed into the network, to assess the accuracy of their prediction. Using the latent-space representation generated by the fraction of training set that the network classifies correctly, we build a statistical model that is able to capture the likelihood of a given prediction. We show on a synthetic dataset that commonly used methods are mostly overconfident. Overconfidence occurs also for predictions made on data points that are outside the distribution that generated the training data. In contrast, our method can detect such out-of-distribution data points as inaccurately predicted, thus aiding in the automatic detection of outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#22363;&#35752;&#35770;&#20102;&#22522;&#20110;AI&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#23545;&#25968;&#23383;&#33402;&#26415;&#21644;&#30005;&#23376;&#25991;&#23398;&#39046;&#22495;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#21508;&#31181;&#20316;&#21697;&#37117;&#20174;&#25991;&#23398;&#35282;&#24230;&#32771;&#34385;&#65292;&#31361;&#20986;&#20102;&#21019;&#20316;&#36807;&#31243;&#30340;&#20132;&#20114;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10834</link><description>&lt;p&gt;
&#12298;AI&#20889;&#20316;&#65306;&#22270;&#20687;&#29983;&#25104;&#21644;&#25968;&#23383;&#20889;&#20316;&#30340;&#20851;&#31995;&#12299;
&lt;/p&gt;
&lt;p&gt;
AIwriting: Relations Between Image Generation and Digital Writing. (arXiv:2305.10834v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#22363;&#35752;&#35770;&#20102;&#22522;&#20110;AI&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#23545;&#25968;&#23383;&#33402;&#26415;&#21644;&#30005;&#23376;&#25991;&#23398;&#39046;&#22495;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#21508;&#31181;&#20316;&#21697;&#37117;&#20174;&#25991;&#23398;&#35282;&#24230;&#32771;&#34385;&#65292;&#31361;&#20986;&#20102;&#21019;&#20316;&#36807;&#31243;&#30340;&#20132;&#20114;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;&#65292;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#30340;AI&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#65288;&#22914;GPT-3&#65289;&#21644;&#22522;&#20110;AI&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#65288;&#22914;DALL-E 2&#21644;&#31283;&#23450;&#25193;&#25955;&#65289;&#37117;&#21462;&#24471;&#20102;&#25351;&#25968;&#32423;&#30340;&#39134;&#36291;&#65292;&#26080;&#30097;&#27491;&#22312;&#25913;&#21464;&#25968;&#23383;&#33402;&#26415;&#21644;&#30005;&#23376;&#25991;&#23398;&#39046;&#22495;&#12290;&#22312;&#26412;&#35770;&#22363;&#20013;&#65292;&#19968;&#32452;&#30005;&#23376;&#25991;&#23398;&#20316;&#32773;&#21644;&#29702;&#35770;&#23478;&#32771;&#34385;&#36825;&#20123;&#31995;&#32479;&#24102;&#26469;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#26032;&#26426;&#36935;&#65292;&#24182;&#23637;&#31034;&#20102;&#20182;&#20204;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#21019;&#20316;&#30340;&#20316;&#21697;&#65292;&#36825;&#20123;&#20316;&#21697;&#36890;&#36807;&#36845;&#20195;&#30340;&#23545;&#35805;&#36807;&#31243;&#36716;&#21270;&#20026;&#35270;&#35273;&#34920;&#29616;&#12290;&#36825;&#20123;&#28436;&#31034;&#30340;&#21069;&#25552;&#26159;&#65292;&#36825;&#20123;&#31995;&#32479;&#21644;&#25152;&#29983;&#25104;&#30340;&#20316;&#21697;&#24517;&#39035;&#20174;&#25991;&#23398;&#35282;&#24230;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#20204;&#36215;&#28304;&#20110;&#20154;&#31867;&#20889;&#20316;&#12290;&#20174;&#20010;&#20154;&#20581;&#24247;&#21361;&#26426;&#30340;&#35270;&#35273;&#22238;&#24518;&#24405;&#65292;&#21040;&#20114;&#21160;&#32593;&#39029;&#28459;&#30011;&#65292;&#21040;&#22522;&#20110;&#25277;&#35937;&#35799;&#24847;&#35821;&#35328;&#30340;&#24314;&#31569;&#65292;&#20877;&#21040;&#25919;&#27835;&#33832;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
During 2022, both transformer-based AI text generation sys-tems such as GPT-3 and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion made exponential leaps forward and are unquestionably altering the fields of digital art and electronic literature. In this panel a group of electronic literature authors and theorists consider new oppor-tunities for human creativity presented by these systems and present new works have produced during the past year that specifically address these systems as environments for literary expressions that are translated through iterative interlocutive processes into visual representations. The premise that binds these presentations is that these systems and the works gener-ated must be considered from a literary perspective, as they originate in human writing. In works ranging from a visual memoir of the personal experience of a health crisis, to interac-tive web comics, to architectures based on abstract poetic language, to political sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#20010;&#24615;&#21270;&#21098;&#21147;&#22681;&#24067;&#32622;&#26234;&#33021;&#21161;&#25163;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#20154;&#21592;&#24555;&#36895;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#29983;&#25104;&#25928;&#26524;&#30340;&#21098;&#21147;&#22681;&#24067;&#32622;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.10830</link><description>&lt;p&gt;
&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#20010;&#24615;&#21270;&#21098;&#21147;&#22681;&#24067;&#32622;&#26234;&#33021;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Constructing a personalized AI assistant for shear wall layout using Stable Diffusion. (arXiv:2305.10830v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#20010;&#24615;&#21270;&#21098;&#21147;&#22681;&#24067;&#32622;&#26234;&#33021;&#21161;&#25163;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#20154;&#21592;&#24555;&#36895;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#29983;&#25104;&#25928;&#26524;&#30340;&#21098;&#21147;&#22681;&#24067;&#32622;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#21147;&#22681;&#32467;&#26500;&#24191;&#27867;&#29992;&#20110;&#39640;&#23618;&#20303;&#23429;&#24314;&#31569;&#65292;&#21098;&#21147;&#22681;&#30340;&#24067;&#32622;&#38656;&#35201;&#22810;&#24180;&#30340;&#35774;&#35745;&#32463;&#39564;&#21644;&#36845;&#20195;&#35797;&#38169;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26041;&#27861;&#23384;&#22312;&#32467;&#26524;&#29983;&#25104;&#36807;&#24930;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#20165;&#33021;&#29983;&#25104;&#21333;&#19968;&#30340;&#24067;&#32622;&#26041;&#26696;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30001;&#20110;&#31283;&#23450;&#25193;&#25955;&#26041;&#27861;&#30446;&#21069;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#26469;&#24494;&#35843;&#22823;&#27169;&#22411;&#65292;&#21033;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#23601;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#20010;&#24615;&#21270;&#21098;&#21147;&#22681;&#24067;&#32622;&#26234;&#33021;&#21161;&#25163;&#65292;&#36890;&#36807;&#27979;&#35797;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#33391;&#22909;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shear wall structures are widely used in high-rise residential buildings, and the layout of shear walls requires many years of design experience and iterative trial and error. Currently, there are methods based on heuristic algorithms, but they generate results too slowly. Those based on Generative Adversarial Networks (GANs) or Graph Neural Networks (GNNs) can only generate single arrangements and require large amounts of training data. At present, Stable Diffusion is being widely used, and by using the Low-Rank Adaptation (LoRA) method to fine-tune large models with small amounts of data, good generative results can be achieved. Therefore, this paper proposes a personalized AI assistant for shear wall layout based on Stable Diffusion, which has been proven to produce good generative results through testing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#27169;&#24335;&#20132;&#20114;&#21644;&#24773;&#24863;&#35745;&#31639;&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#24773;&#24863;&#29616;&#35937;&#22312;&#30740;&#31350;&#20013;&#30340;&#35282;&#33394;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#20173;&#26377;&#19968;&#20123;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.10827</link><description>&lt;p&gt;
&#24773;&#24863;&#29616;&#35937;&#22312;&#22810;&#27169;&#24335;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Expanding the Role of Affective Phenomena in Multimodal Interaction Research. (arXiv:2305.10827v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#27169;&#24335;&#20132;&#20114;&#21644;&#24773;&#24863;&#35745;&#31639;&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#24773;&#24863;&#29616;&#35937;&#22312;&#30740;&#31350;&#20013;&#30340;&#35282;&#33394;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#20173;&#26377;&#19968;&#20123;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24773;&#24863;&#35745;&#31639;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#20154;&#26426;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#30340;AI&#31995;&#32479;&#35782;&#21035;&#21644;&#34920;&#36798;&#24773;&#24863;&#29616;&#35937;&#65288;&#22914;&#24773;&#24863;&#21644;&#24773;&#32490;&#65289;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#23545;&#22810;&#27169;&#24335;&#20132;&#20114;&#21644;&#24773;&#24863;&#35745;&#31639;&#20132;&#21449;&#39046;&#22495;&#30740;&#31350;&#30340;&#23457;&#26597;&#65292;&#26088;&#22312;&#35266;&#23519;&#36235;&#21183;&#24182;&#30830;&#23450;&#26410;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#26469;&#33258;&#22810;&#27169;&#24335;&#20132;&#20114;&#12289;&#24773;&#24863;&#35745;&#31639;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36873;&#25321;&#24615;&#20250;&#35758;&#19978;&#30340;&#36229;&#36807;16000&#31687;&#35770;&#25991;&#65306;ACM&#22269;&#38469;&#22810;&#27169;&#24335;&#20132;&#20114;&#20250;&#35758;&#12289;AAAC&#22269;&#38469;&#24773;&#24863;&#35745;&#31639;&#21644;&#26234;&#33021;&#20132;&#20114;&#20250;&#35758;&#12289;&#21327;&#20250;&#35745;&#31639;&#35821;&#35328;&#23398;&#24180;&#20250;&#21644;&#32463;&#39564;&#26041;&#27861;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20250;&#35758;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;910&#31687;&#28041;&#21450;&#24773;&#24863;&#30340;&#35770;&#25991;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#36825;&#20123;&#35770;&#25991;&#20013;&#24773;&#24863;&#29616;&#35937;&#20316;&#29992;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;&#30740;&#31350;&#30340;&#20027;&#20307;...
&lt;/p&gt;
&lt;p&gt;
In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10824</link><description>&lt;p&gt;
&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Integrating Item Relevance in Training Loss for Sequential Recommender Systems. (arXiv:2305.10824v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21487;&#33021;&#19982;&#20043;&#20132;&#20114;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#21463;&#21040;&#26469;&#33258;&#24080;&#25143;&#20849;&#20139;&#12289;&#19981;&#19968;&#33268;&#30340;&#20559;&#22909;&#25110;&#24847;&#22806;&#28857;&#20987;&#31561;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#65288;ii&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#20351;&#20854;&#23545;&#22122;&#22768;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#27169;&#22411;&#22312;&#20256;&#32479;&#35780;&#20272;&#21327;&#35758;&#20013;&#25552;&#39640;&#20102;NDCG@10&#32422;1.2%&#21644;HR&#32422;0.88%&#65292;&#32780;&#22312;&#26032;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#25913;&#36827;&#30340;NDCG@10&#32422;1.63%&#21644;HR&#32422;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#22686;&#24378;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SESRec&#65289;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#25628;&#32034;&#20852;&#36259;&#36827;&#34892;&#25512;&#33616;&#65292;&#36890;&#36807;&#21306;&#20998;S&#65286;R&#34892;&#20026;&#20013;&#30340;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#34920;&#31034;&#65292;&#20351;S&#65286;R&#29305;&#24449;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#25381;&#20854;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.10822</link><description>&lt;p&gt;
&#24403;&#25628;&#32034;&#36935;&#35265;&#25512;&#33616;&#65306;&#23398;&#20064;&#21306;&#20998;&#25628;&#32034;&#34920;&#31034;&#20197;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation. (arXiv:2305.10822v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10822
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#22686;&#24378;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SESRec&#65289;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#25628;&#32034;&#20852;&#36259;&#36827;&#34892;&#25512;&#33616;&#65292;&#36890;&#36807;&#21306;&#20998;S&#65286;R&#34892;&#20026;&#20013;&#30340;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#34920;&#31034;&#65292;&#20351;S&#65286;R&#29305;&#24449;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#25381;&#20854;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#65292;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#36890;&#24120;&#25552;&#20379;&#25628;&#32034;&#21644;&#25512;&#33616;&#65288;S&#65286;R&#65289;&#26381;&#21153;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;&#24456;&#23569;&#26377;&#20219;&#20309;&#26377;&#25928;&#30340;&#25163;&#27573;&#23558;&#26469;&#33258;S&#65286;R&#26381;&#21153;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#23558;S&#65286;R&#34892;&#20026;&#21333;&#29420;&#22788;&#29702;&#65292;&#35201;&#20040;&#36890;&#36807;&#32858;&#21512;&#20004;&#20010;&#26381;&#21153;&#30340;&#25968;&#25454;&#26469;&#32852;&#21512;&#20248;&#21270;&#23427;&#20204;&#65292;&#24573;&#30053;&#20102;S&#65286;R&#20013;&#29992;&#25143;&#24847;&#22270;&#21487;&#20197;&#26377;&#25130;&#28982;&#19981;&#21516;&#30340;&#20107;&#23454;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#22686;&#24378;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SESRec&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21306;&#20998;S&#65286;R&#34892;&#20026;&#20013;&#30340;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#34920;&#31034;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#25628;&#32034;&#20852;&#36259;&#36827;&#34892;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SESRec&#39318;&#20808;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#26469;&#23545;&#40784;&#26597;&#35810;&#21644;&#39033;&#30446;&#23884;&#20837;&#20197;&#35745;&#31639;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20004;&#20010;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;S&#65286;R&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#35774;&#35745;&#20102;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20197;&#23398;&#20064;&#25628;&#32032;&#29305;&#24449;&#34920;&#31034;&#21644;&#25512;&#33616;&#29305;&#24449;&#34920;&#31034;&#30340;&#30456;&#20284;&#24230;&#36317;&#31163;&#65292;&#20351;&#24471;S&#65286;R&#29305;&#24449;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#25381;&#20854;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern online service providers such as online shopping platforms often provide both search and recommendation (S&amp;R) services to meet different user needs. Rarely has there been any effective means of incorporating user behavior data from both S&amp;R services. Most existing approaches either simply treat S&amp;R behaviors separately, or jointly optimize them by aggregating data from both services, ignoring the fact that user intents in S&amp;R can be distinctively different. In our paper, we propose a Search-Enhanced framework for the Sequential Recommendation (SESRec) that leverages users' search interests for recommendation, by disentangling similar and dissimilar representations within S&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings based on users' query-item interactions for the computations of their similarities. Two transformer encoders are used to learn the contextual representations of S&amp;R behaviors independently. Then a contrastive learning task is designed to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#20247;&#21253;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#20855;&#36523;&#20195;&#29702;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.10783</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#35328;&#25351;&#20196;&#37325;&#26032;&#23450;&#20041;&#20855;&#36523;&#20195;&#29702;&#33021;&#21147;&#65306;&#25913;&#21464;&#20154;&#26426;&#20013;&#24515;&#30340;AI&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Transforming Human-Centered AI Collaboration: Redefining Embodied Agents Capabilities through Interactive Grounded Language Instructions. (arXiv:2305.10783v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#20247;&#21253;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#20855;&#36523;&#20195;&#29702;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#30340;&#36866;&#24212;&#24615;&#21313;&#20998;&#20986;&#33394;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#12290;&#20174;&#24180;&#36731;&#26102;&#26399;&#24320;&#22987;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#20223;&#20182;&#20154;&#25110;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#33719;&#21462;&#26032;&#33021;&#21147;&#24182;&#35299;&#20915;&#38382;&#39064;&#12290;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#24320;&#23637;&#20132;&#20114;&#24335;&#8220;&#20855;&#36523;&#20195;&#29702;&#8221;&#24320;&#21457;&#65292;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#24182;&#21327;&#21161;&#20182;&#20204;&#23436;&#25104;&#23454;&#38469;&#20219;&#21153;&#12290;&#36825;&#20123;&#20195;&#29702;&#24517;&#39035;&#20855;&#22791;&#21450;&#26102;&#35831;&#27714;&#21453;&#39304;&#30340;&#33021;&#21147;&#65292;&#20197;&#38450;&#36890;&#20449;&#20013;&#26029;&#25110;&#25351;&#20196;&#19981;&#28165;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24517;&#39035;&#23637;&#29616;&#20986;&#23398;&#20064;&#26576;&#19968;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;(1)&#19968;&#31181;&#20247;&#21253;&#24037;&#20855;&#65292;&#29992;&#20110;&#25910;&#38598;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#35328;&#25351;&#20196;; (2)&#26368;&#22823;&#30340;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;; (3)&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;&#36825;&#20123;&#36129;&#29486;&#36866;&#29992;&#20110;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence's adaptability is remarkable, allowing us to adjust to new tasks and multi-modal environments swiftly. This skill is evident from a young age as we acquire new abilities and solve problems by imitating others or following natural language instructions. The research community is actively pursuing the development of interactive "embodied agents" that can engage in natural conversations with humans and assist them with real-world tasks. These agents must possess the ability to promptly request feedback in case communication breaks down or instructions are unclear. Additionally, they must demonstrate proficiency in learning new vocabulary specific to a given domain.  In this paper, we made the following contributions: (1) a crowd-sourcing tool for collecting grounded language instructions; (2) the largest dataset of grounded language instructions; and (3) several state-of-the-art baselines. These contributions are suitable as a foundation for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23383;&#22823;&#23567;&#27604;&#36739;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#36798;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10782</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#20540;&#22823;&#23567;&#27604;&#36739;&#25928;&#24212;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numeric Magnitude Comparison Effects in Large Language Models. (arXiv:2305.10782v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23383;&#22823;&#23567;&#27604;&#36739;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#36798;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24182;&#27809;&#26377;&#21306;&#20998;&#20986;&#25991;&#23383;&#20013;&#30340;&#25968;&#23383;&#65292;&#32780;&#25968;&#23383;&#22312;&#25991;&#26412;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#23545;&#25968;&#23383;&#21644;&#21333;&#35789;&#26377;&#30528;&#19981;&#21516;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#34892;&#20026;&#35282;&#24230;&#25506;&#31350;&#27969;&#34892;&#30340;LLMs&#33021;&#22815;&#22810;&#22909;&#22320;&#25429;&#25417;&#25968;&#23383;&#30340;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;$4&lt;5$&#65289;&#12290;&#20197;&#24448;&#23545;LLMs&#34920;&#24449;&#33021;&#21147;&#30340;&#30740;&#31350;&#21697;&#35780;&#20182;&#20204;&#26159;&#21542;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#27604;&#22914;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#25972;&#20307;&#20934;&#30830;&#29575;&#36739;&#39640;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#19982;&#35748;&#30693;&#31185;&#23398;&#30456;&#20851;&#30340;&#19981;&#21516;&#38382;&#39064;&#65306;LLMs&#25968;&#23383;&#34920;&#24449;&#19982;&#20154;&#31867;&#35821;&#35328;&#29992;&#25143;&#30340;&#34920;&#29616;&#26377;&#22810;&#25509;&#36817;&#65292;&#20182;&#20204;&#36890;&#24120;&#34920;&#29616;&#20986;&#36317;&#31163;&#12289;&#22823;&#23567;&#21644;&#27604;&#20363;&#25928;&#24212;? &#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#36830;&#25509;&#20551;&#35774;&#23558;&#25968;&#23383;&#21333;&#35789;&#21644;&#25968;&#23383;&#30340;&#27169;&#22411;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26144;&#23556;&#21040;&#20154;&#31867;&#21453;&#24212;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#31034;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#20855;&#26377;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4 &lt; 5$) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#22810;&#27169;&#25968;&#25454;&#33258;&#36866;&#24212;&#32534;&#30721;&#26426;&#21046;&#65292;&#37319;&#29992;NN-based&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#25552;&#21462;&#19981;&#21516;&#27169;&#24577;&#20013;&#21253;&#21547;&#30340;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.10773</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#22810;&#27169;&#25968;&#25454;&#33258;&#36866;&#24212;&#32534;&#30721;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Rate-Adaptive Coding Mechanism for Semantic Communications With Multi-Modal Data. (arXiv:2305.10773v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#22810;&#27169;&#25968;&#25454;&#33258;&#36866;&#24212;&#32534;&#30721;&#26426;&#21046;&#65292;&#37319;&#29992;NN-based&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#25552;&#21462;&#19981;&#21516;&#27169;&#24577;&#20013;&#21253;&#21547;&#30340;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#36890;&#20449;&#31995;&#32479;&#23545;&#24102;&#23485;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#34987;&#24212;&#29992;&#20110;&#22810;&#27169;&#22330;&#26223;&#20013;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#33410;&#30465;&#36890;&#20449;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#19981;&#24102;&#20449;&#36947;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#30340;&#29616;&#26377;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26694;&#26550;&#19982;&#29616;&#20195;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#19981;&#20860;&#23481;&#12290;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#35774;&#35745;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#65292;&#38656;&#35201;&#20026;&#26032;&#20219;&#21153;&#37325;&#26032;&#35774;&#35745;&#21644;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#27169;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;&#20449;&#36947;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;NN&#30340;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#25552;&#21462;&#19981;&#21516;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#20013;&#21253;&#21547;&#30340;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#22810;&#27169;&#33258;&#36866;&#24212;&#32534;&#30721;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the ever-increasing demand for bandwidth in multi-modal communication systems requires a paradigm shift. Powered by deep learning, semantic communications are applied to multi-modal scenarios to boost communication efficiency and save communication resources. However, the existing end-to-end neural network (NN) based framework without the channel encoder/decoder is incompatible with modern digital communication systems. Moreover, most end-to-end designs are task-specific and require re-design and re-training for new tasks, which limits their applications. In this paper, we propose a distributed multi-modal semantic communication framework incorporating the conventional channel encoder/decoder. We adopt NN-based semantic encoder and decoder to extract correlated semantic information contained in different modalities, including speech, text, and image. Based on the proposed framework, we further establish a general rate-adaptive coding mechanism for various types of multi-modal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20462;&#25913; (AdvAmd) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#25552;&#39640;&#20854;&#22312;&#33391;&#24615;&#26679;&#26412;&#19978;&#30340;&#21407;&#22987;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.10766</link><description>&lt;p&gt;
&#23545;&#25239;&#20462;&#25913;&#26159;&#23558;&#25932;&#20154;&#36716;&#21270;&#20026;&#26379;&#21451;&#30340;&#21807;&#19968;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Amendment is the Only Force Capable of Transforming an Enemy into a Friend. (arXiv:2305.10766v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20462;&#25913; (AdvAmd) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#25552;&#39640;&#20854;&#22312;&#33391;&#24615;&#26679;&#26412;&#19978;&#30340;&#21407;&#22987;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#36890;&#24120;&#34987;&#35748;&#20026;&#23545;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35823;&#23548;&#34892;&#20026;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#21453;&#30340;&#35266;&#28857;&#65306;&#22914;&#26524;&#20462;&#25913;&#24471;&#24403;&#65292;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#31070;&#32463;&#27169;&#22411;&#12290;&#19982;&#26088;&#22312;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#20256;&#32479;&#23545;&#25239;&#38450;&#24481;&#25110;&#23545;&#25239;&#35757;&#32451;&#26041;&#26696;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#23545;&#25239;&#20462;&#25913;&#65288;AdvAmd&#65289;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#31070;&#32463;&#27169;&#22411;&#22312;&#33391;&#24615;&#26679;&#26412;&#19978;&#30340;&#21407;&#22987;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#33391;&#24615;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#36825;&#31181;&#20998;&#24067;&#19981;&#21305;&#37197;&#21644;&#19982;&#20808;&#21069;&#30340;&#38450;&#24481;&#31574;&#30053;&#24212;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#27604;&#29575;&#30340;&#30456;&#20114;&#23398;&#20064;&#26426;&#21046;&#26159;&#23548;&#33268;&#33391;&#24615;&#26679;&#26412;&#20934;&#30830;&#24230;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#25152;&#25552;&#20986;&#30340;AdvAmd&#34987;&#35777;&#26126;&#21487;&#20197;&#31283;&#23450;&#22320;&#24674;&#22797;&#20934;&#30830;&#24230;&#19979;&#38477;&#65292;&#29978;&#33267;&#25552;&#39640;&#24120;&#35265;&#31070;&#32463;&#27169;&#22411;&#22312;&#33391;&#24615;&#20998;&#31867;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attack is commonly regarded as a huge threat to neural networks because of misleading behavior. This paper presents an opposite perspective: adversarial attacks can be harnessed to improve neural models if amended correctly. Unlike traditional adversarial defense or adversarial training schemes that aim to improve the adversarial robustness, the proposed adversarial amendment (AdvAmd) method aims to improve the original accuracy level of neural models on benign samples. We thoroughly analyze the distribution mismatch between the benign and adversarial samples. This distribution mismatch and the mutual learning mechanism with the same learning ratio applied in prior art defense strategies is the main cause leading the accuracy degradation for benign samples. The proposed AdvAmd is demonstrated to steadily heal the accuracy degradation and even leads to a certain accuracy boost of common neural models on benign classification, object detection, and segmentation tasks. The eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10748</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#29702;&#35299;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#26680;&#21487;&#20197;&#23558;&#20808;&#39564;&#26377;&#20851;&#28508;&#22312;&#20989;&#25968;&#30340;&#20449;&#24565;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;(GP)&#20013;&#20197;&#24418;&#25104;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#38500;&#20102;&#20869;&#26680;&#36873;&#25321;&#22806;&#65292;GP&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#24456;&#38590;&#29702;&#35299;&#12290;&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#23545;GP&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#28436;&#31034;&#20102;Matern&#20869;&#26680;&#30340;&#957;&#36830;&#32493;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#26799;&#24230;&#22330;&#20851;&#38190;&#28857;&#30340;&#28798;&#21464;&#29702;&#35770;&#26041;&#38754;&#12290;&#36890;&#36807;&#23558;&#957;&#30452;&#25509;&#21253;&#21547;&#22312;Matern&#20869;&#26680;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#957;&#30340;&#20856;&#22411;&#20540;&#22686;&#21152;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#20294;&#20854;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#38750;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20107;&#20808;&#35780;&#20272;GP&#38598;&#21512;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;&#25439;&#22833;&#26223;&#35266;&#29289;&#29702;&#23646;&#24615;&#30340;&#21508;&#31181;&#25237;&#31080;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#22312;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;GP&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian Process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from physics. We demonstrate $\nu$-continuity for Matern kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including $\nu$ in the hyperparameter optimisation for Matern kernels, we find that typical values of $\nu$ are far from optimal in terms of performance, yet prevail in the literature due to the increased computational speed. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide an enhanced und
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10738</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#22270;&#32858;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861; - &#21487;&#20197;&#25429;&#33719;&#20851;&#38190;&#30340;&#21160;&#24577;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#38754;&#21521;&#32858;&#31867;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#26469;&#22788;&#29702;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#21160;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20063;&#24341;&#21457;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TGC&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#22270;&#28145;&#24230;&#32858;&#31867;&#65292;&#23427;&#35843;&#25972;&#20102;&#28145;&#24230;&#32858;&#31867;&#25216;&#26415;&#65288;&#32858;&#31867;&#20998;&#37197;&#20998;&#24067;&#21644;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#65289;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#22270;&#22522;&#20110;&#20132;&#20114;&#24207;&#21015;&#30340;&#25209;&#22788;&#29702;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#20102;&#26102;&#38388;&#22270;&#32858;&#31867;&#19982;&#29616;&#26377;&#38745;&#24577;&#22270;&#32858;&#31867;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;TGC&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoFactSum&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#20943;&#36731;&#21407;&#22240;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10736</link><description>&lt;p&gt;
&#23545;&#29983;&#25104;&#20107;&#23454;&#19968;&#33268;&#24615;&#25991;&#26412;&#25688;&#35201;&#36827;&#34892;&#21453;&#20107;&#23454;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoFactSum&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#20943;&#36731;&#21407;&#22240;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#25152;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#26500;&#24314;&#22240;&#26524;&#22270;&#65292;&#24182;&#30830;&#23450;&#20102;&#36896;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#20869;&#22312;&#21407;&#22240;&#65292;&#21363;&#35821;&#35328;&#20559;&#35265;&#21644;&#26080;&#20851;&#24615;&#20559;&#35265;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoFactSum&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#20943;&#36731;&#36825;&#20123;&#20559;&#24046;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;CoFactSum&#25552;&#20379;&#20102;&#20004;&#31181;&#21453;&#20107;&#23454;&#20272;&#35745;&#31574;&#30053;&#65292;&#21363;&#26126;&#30830;&#21453;&#20107;&#23454;&#36974;&#34109;&#20855;&#26377;&#26126;&#30830;&#30340;&#21160;&#24577;&#36974;&#34109;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#38544;&#24335;&#37492;&#21035;&#20132;&#21449;&#20851;&#27880;&#26426;&#21046;&#30340;&#38544;&#24335;&#21453;&#20107;&#23454;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21435;&#20559;&#24230;&#35843;&#25972;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#21160;&#24577;&#36866;&#24212;&#21435;&#20559;&#31243;&#24230;&#12290;&#23545;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;CoFactSum&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#30340;&#25688;&#35201;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. In this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation. Specifically, the proposed CoFactSum provides two counterfactual estimation strategies, i.e., Explicit Counterfactual Masking with an explicit dynamic masking strategy, and Implicit Counterfactual Training with an implicit discriminative cross-attention mechanism. Meanwhile, we design a Debiasing Degree Adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. Extensive experiments on two widely-used summarization datasets dem
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29615;&#22659;&#25216;&#26415;&#21644;&#26234;&#33021;&#22914;&#20309;&#25913;&#21892;&#27531;&#30142;&#20154;&#30340;&#25252;&#29702;&#38656;&#27714;&#21450;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10726</link><description>&lt;p&gt;
&#29615;&#22659;&#25216;&#26415;&#19982;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Ambient Technology &amp; Intelligence. (arXiv:2305.10726v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29615;&#22659;&#25216;&#26415;&#21644;&#26234;&#33021;&#22914;&#20309;&#25913;&#21892;&#27531;&#30142;&#20154;&#30340;&#25252;&#29702;&#38656;&#27714;&#21450;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25105;&#20204;&#26377;&#24180;&#36731;&#20154;&#21644;&#32769;&#24180;&#20154;&#12289;&#26377;&#29305;&#27530;&#38656;&#27714;&#30340;&#20154;&#20197;&#21450;&#21487;&#20197;&#33258;&#29702;&#30340;&#20154;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#20840;&#29699;&#26377;&#36229;&#36807;10&#20159;&#20272;&#35745;&#26377;&#27531;&#30142;&#30340;&#20154;&#65292;&#21344;&#19990;&#30028;&#20154;&#21475;&#30340;&#32422;15&#65285;&#65292;&#20854;&#20013;15&#23681;&#21450;&#20197;&#19978;&#30340;&#20154;&#21475;&#21344;3.8&#65285;&#65288;&#32452;&#32455;&#65292;2011&#65289;&#12290;&#24739;&#26377;&#27531;&#30142;&#30340;&#20154;&#25968;&#22240;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#31561;&#22240;&#32032;&#32780;&#19978;&#21319;&#12290;&#36825;&#20123;&#21644;&#20854;&#20182;&#22240;&#32032;&#20351;&#24471;&#24403;&#20170;&#31038;&#20250;&#24613;&#38656;&#36866;&#24403;&#30340;&#25252;&#29702;&#35774;&#26045;&#12290;&#24314;&#31435;&#20102;&#20960;&#20010;&#25252;&#29702;&#35774;&#26045;&#26469;&#24110;&#21161;&#27531;&#30142;&#20154;&#36807;&#19978;&#26085;&#24120;&#29983;&#27963;&#65292;&#19981;&#20250;&#34987;&#31038;&#21306;&#25490;&#38500;&#22312;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, we have a mixture of young and older individuals, people with special needs, and people who can care for themselves. Over 1 billion people are estimated to be disabled; this figure corresponds to about 15% of the world's population, with 3.8% (approximately 190 million people) accounting for people aged 15 and up (Organization, 2011). The number of people with disabilities is upward due to the increase in chronic health conditions and many other things. These and other factors have made the need for proper care facilities urgent in today's society. Several care facilities are built to help people with disabilities live their everyday lives and not be left out of the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAA+&#26694;&#26550;&#65292;&#37319;&#29992;&#28151;&#21512;&#25552;&#31034;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10724</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25552;&#31034;&#27491;&#21017;&#21270;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20998;&#21106;&#20219;&#20309;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
Segment Any Anomaly without Training via Hybrid Prompt Regularization. (arXiv:2305.10724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAA+&#26694;&#26550;&#65292;&#37319;&#29992;&#28151;&#21512;&#25552;&#31034;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#8220;Segment Any Anomaly + (SAA+)&#8221;&#65292;&#37319;&#29992;&#28151;&#21512;&#25552;&#31034;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#65292;&#20197;&#25552;&#39640;&#29616;&#20195;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#29616;&#26377;&#30340;&#24322;&#24120;&#20998;&#21106;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26080;&#25968;&#24322;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#20110;&#24322;&#24120;&#20998;&#21106;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#21644;&#30446;&#26631;&#22270;&#20687;&#19978;&#19979;&#25991;&#30340;&#28151;&#21512;&#25552;&#31034;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#12290;&#25105;&#20204;&#30340;SAA+&#27169;&#22411;&#22312;&#21253;&#25324;VisA&#12289;MVTec-AD&#12289;MTD&#21644;KSDD2&#22312;&#20869;&#30340;&#22810;&#20010;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#22312;\href{https://github.com/caoyunkang/Segment-Any-An}{https://github.com/caoyunkang/Segment-Any-An}&#21457;&#24067;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for zero-shot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models. Existing anomaly segmentation models typically rely on domain-specific fine-tuning, limiting their generalization across countless anomaly patterns. In this work, inspired by the great zero-shot generalization ability of foundation models like Segment Anything, we first explore their assembly to leverage diverse multi-modal prior knowledge for anomaly localization. For non-parameter foundation model adaptation to anomaly segmentation, we further introduce hybrid prompts derived from domain expert knowledge and target image context as regularization. Our proposed SAA+ model achieves state-of-the-art performance on several anomaly segmentation benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting. We will release the code at \href{https://github.com/caoyunkang/Segment-Any-An
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#32447;&#24615;&#26144;&#23556;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;RevIN&#21644;CI&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#32447;&#24615;&#26144;&#23556;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21608;&#26399;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.10721</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#32447;&#24615;&#26144;&#23556;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping. (arXiv:2305.10721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#32447;&#24615;&#26144;&#23556;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;RevIN&#21644;CI&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#32447;&#24615;&#26144;&#23556;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21608;&#26399;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#19987;&#38376;&#35774;&#35745;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22797;&#26434;&#30340;&#26550;&#26500;&#30456;&#27604;&#65292;&#21333;&#20010;&#32447;&#24615;&#23618;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#26368;&#36817;&#26041;&#27861;&#30340;&#20869;&#22312;&#26377;&#25928;&#24615;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#32467;&#35770;&#65306;1&#65289;&#32447;&#24615;&#26144;&#23556;&#23545;&#20110;&#20808;&#21069;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65307;2&#65289;RevIN&#65288;&#21487;&#36870;&#35268;&#33539;&#21270;&#65289;&#21644;CI&#65288;&#36890;&#36947;&#29420;&#31435;&#65289;&#22312;&#25552;&#39640;&#24635;&#20307;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65307;3&#65289;&#24403;&#22686;&#21152;&#36755;&#20837;&#35270;&#37326;&#26102;&#65292;&#32447;&#24615;&#26144;&#23556;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21608;&#26399;&#29305;&#24449;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#36890;&#36947;&#19981;&#21516;&#21608;&#26399;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#35299;&#37322;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#20195;&#30721;&#21487;&#22312;\url{https://git}&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting has gained significant attention in recent years. While there are various specialized designs for capturing temporal dependency, previous studies have demonstrated that a single linear layer can achieve competitive forecasting performance compared to other complex architectures. In this paper, we thoroughly investigate the intrinsic effectiveness of recent approaches and make three key observations: 1) linear mapping is critical to prior long-term time series forecasting efforts; 2) RevIN (reversible normalization) and CI (Channel Independent) play a vital role in improving overall forecasting performance; and 3) linear mapping can effectively capture periodic features in time series and has robustness for different periods across channels when increasing the input horizon. We provide theoretical and experimental explanations to support our findings and also discuss the limitations and future works. Our framework's code is available at \url{https://git
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10716</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Time-Series Pre-Trained Models. (arXiv:2305.10716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#26500;&#24314;&#22823;&#35268;&#27169;&#12289;&#33391;&#22909;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36880;&#28176;&#24341;&#36215;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;TS-PTMs&#65289;&#65292;&#26088;&#22312;&#25351;&#23548;&#20102;&#35299;&#12289;&#24212;&#29992;&#21644;&#30740;&#31350;TS-PTMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;TSM&#20013;&#20351;&#29992;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#39044;&#35757;&#32451;&#25216;&#26415;&#27010;&#36848;&#20102;TS-PTMs&#12290;&#25105;&#20204;&#25506;&#35752;&#30340;&#20027;&#35201;&#31867;&#21035;&#21253;&#25324;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;TS-PTMs&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantage
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#8220;&#21307;&#30103;&#20445;&#38505;&#25512;&#33616;&#31995;&#32479;&#8221;&#65292;&#37319;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22320;&#28857;&#21644;&#20215;&#26684;&#31579;&#36873;&#21307;&#30103;&#31649;&#29702;&#32452;&#32455;&#65288;HMO&#65289;&#30340;&#25968;&#25454;&#65292;&#24182;&#25512;&#33616;&#30456;&#20284;&#26381;&#21153;&#30340;&#21069;&#19977;&#20010;HMO&#65292;&#26088;&#22312;&#24110;&#21161;&#23612;&#26085;&#21033;&#20122;&#20154;&#26041;&#20415;&#22320;&#25214;&#21040;&#26368;&#36866;&#21512;&#20182;&#20204;&#30340;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2305.10708</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21307;&#30103;&#20445;&#38505;&#20915;&#31574;&#25512;&#33616;&#31995;&#32479;&#22312;&#23612;&#26085;&#21033;&#20122;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Recommendation System For Health Insurance Decision Making In Nigeria. (arXiv:2305.10708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#8220;&#21307;&#30103;&#20445;&#38505;&#25512;&#33616;&#31995;&#32479;&#8221;&#65292;&#37319;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22320;&#28857;&#21644;&#20215;&#26684;&#31579;&#36873;&#21307;&#30103;&#31649;&#29702;&#32452;&#32455;&#65288;HMO&#65289;&#30340;&#25968;&#25454;&#65292;&#24182;&#25512;&#33616;&#30456;&#20284;&#26381;&#21153;&#30340;&#21069;&#19977;&#20010;HMO&#65292;&#26088;&#22312;&#24110;&#21161;&#23612;&#26085;&#21033;&#20122;&#20154;&#26041;&#20415;&#22320;&#25214;&#21040;&#26368;&#36866;&#21512;&#20182;&#20204;&#30340;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23612;&#26085;&#21033;&#20122;&#65292;&#20581;&#24247;&#20445;&#38505;&#30340;&#26222;&#21450;&#29575;&#24456;&#20302;&#65292;&#25552;&#39640;&#20854;&#26222;&#21450;&#24230;&#30340;&#37325;&#35201;&#27493;&#39588;&#21253;&#25324;&#25552;&#39640;&#24847;&#35782;&#12289;&#33719;&#21462;&#20449;&#24687;&#21644;&#25903;&#25345;&#20915;&#31574;&#30340;&#24037;&#20855;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#22312;&#24110;&#21161;&#20010;&#20154;&#22312;&#20114;&#32852;&#32593;&#19978;&#25214;&#21040;&#30005;&#24433;&#12289;&#20070;&#31821;&#12289;&#38899;&#20048;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#20135;&#21697;&#26041;&#38754;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12290;&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#65288;&#22522;&#20110;&#39033;&#30446;&#30340;&#26041;&#27861;&#65289;&#26469;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#31639;&#27861;&#65292;&#32463;&#36807;&#22810;&#27425;&#35780;&#20272;&#21644;&#19982;&#39046;&#22495;&#30693;&#35782;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#36873;&#25321;&#20313;&#24358;&#30456;&#20284;&#24230;&#20316;&#20026;&#25105;&#20204;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#35813;&#25512;&#33616;&#31995;&#32479;&#32771;&#34385;&#20102;&#29992;&#25143;&#36755;&#20837;&#30340;&#36873;&#25321;&#65292;&#36890;&#36807;&#22320;&#28857;&#21644;&#20215;&#26684;&#26469;&#31579;&#36873;&#21307;&#30103;&#31649;&#29702;&#32452;&#32455;&#65288;HMO&#65289;&#30340;&#25968;&#25454;&#65292;&#28982;&#21518;&#25512;&#33616;&#30456;&#20284;&#26381;&#21153;&#30340;&#21069;&#19977;&#20010;HMO&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#25512;&#33616;&#24037;&#20855;&#21487;&#24110;&#21161;&#20154;&#20204;&#26041;&#20415;&#22320;&#25214;&#21040;&#21644;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#20204;&#30340;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
The uptake of health insurance has been poor in Nigeria, a significant step to improving this includes improved awareness, access to information and tools to support decision making. Artificial intelligence (AI) based recommender systems have gained popularity in helping individuals find movies, books, music, and different types of products on the internet including diverse applications in healthcare. The content-based methodology (item-based approach) was employed in the recommender system. We applied both the K-Nearest Neighbor (KNN) and Cosine similarity algorithm. We chose the Cosine similarity as our chosen algorithm after several evaluations based of their outcomes in comparison with domain knowledge. The recommender system takes into consideration the choices entered by the user, filters the health management organization (HMO) data by location and chosen prices. It then recommends the top 3 HMOs with closest similarity in services offered. A recommendation tool to help people f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Brainstorm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22836;&#33041;&#39118;&#26292;&#27493;&#39588;&#29983;&#25104;&#24182;&#36873;&#25321;&#20851;&#20110;&#38382;&#39064;&#30340;&#19981;&#21516;&#24819;&#27861;&#65292;&#21487;&#26174;&#33879;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#31454;&#20105;&#32423;&#21035;&#32534;&#31243;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#22312;CodeContests&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;pass@$k$&#25351;&#26631;&#22686;&#21152;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.10679</link><description>&lt;p&gt;
&#36229;&#36234;&#32534;&#30721;&#65306;&#22836;&#33041;&#39118;&#26292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation. (arXiv:2305.10679v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Brainstorm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22836;&#33041;&#39118;&#26292;&#27493;&#39588;&#29983;&#25104;&#24182;&#36873;&#25321;&#20851;&#20110;&#38382;&#39064;&#30340;&#19981;&#21516;&#24819;&#27861;&#65292;&#21487;&#26174;&#33879;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#31454;&#20105;&#32423;&#21035;&#32534;&#31243;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#22312;CodeContests&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;pass@$k$&#25351;&#26631;&#22686;&#21152;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#26088;&#22312;&#20174;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#28304;&#20195;&#30721;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#36719;&#20214;&#24037;&#31243;&#30340;&#29983;&#20135;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#22312;&#31616;&#21333;&#20219;&#21153;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#26356;&#22797;&#26434;&#20219;&#21153;&#30340;&#20195;&#30721;&#65288;&#22914;&#31454;&#20105;&#32423;&#21035;&#30340;&#38382;&#39064;&#65289;&#20173;&#28982;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Brainstorm&#26694;&#26550;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#22836;&#33041;&#39118;&#26292;&#27493;&#39588;&#65292;&#29983;&#25104;&#24182;&#36873;&#25321;&#20851;&#20110;&#38382;&#39064;&#30340;&#19981;&#21516;&#24819;&#27861;&#20197;&#20419;&#36827;&#31639;&#27861;&#25512;&#29702;&#65292;&#20854;&#20013;&#36825;&#20123;&#24605;&#32771;&#26159;&#35299;&#20915;&#38382;&#39064;&#30340;&#21487;&#33021;&#34013;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;Brainstorm&#26174;&#33879;&#22686;&#24378;&#20102;LLMs&#35299;&#20915;&#31454;&#20105;&#32423;&#21035;&#32534;&#31243;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#22312;CodeContests&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;pass@$k$&#25351;&#26631;&#22686;&#21152;&#20102;50&#65285;&#20197;&#19978;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LeetCode&#31454;&#36187;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;o
&lt;/p&gt;
&lt;p&gt;
Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;&#26694;&#26550;&#65292;&#21517;&#20026;STEP&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21160;&#24577;&#22270;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10673</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#21160;&#24577;&#22270;&#30340;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs. (arXiv:2305.10673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;&#26694;&#26550;&#65292;&#21517;&#20026;STEP&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21160;&#24577;&#22270;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#30340;&#26222;&#21450;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#38754;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#26102;&#38388;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#21407;&#22987;&#22270;&#21098;&#26525;&#25104;&#19968;&#20010;&#23567;&#32780;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#22270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#23545;&#20462;&#21098;&#21518;&#30340;&#22270;&#21644;&#22823;&#22270;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#32463;&#39564;&#26377;&#25928;&#65292;&#20294;&#24403;&#21069;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#38745;&#24577;&#25110;&#38750;&#26102;&#38388;&#22270;&#65292;&#36825;&#20123;&#22270;&#23545;&#21160;&#24577;&#22330;&#26223;&#30340;&#30452;&#25509;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38656;&#35201;&#26631;&#31614;&#20316;&#20026;&#22522;&#26412;&#20107;&#23454;&#26469;&#23398;&#20064;&#20449;&#24687;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26631;&#31614;&#38590;&#20197;&#33719;&#24471;&#30340;&#26032;&#38382;&#39064;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;STEP&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#26102;&#38388;&#21098;&#26525;&#26694;&#26550;&#65292;&#23427;&#23398;&#20064;&#20174;&#36755;&#20837;&#30340;&#21160;&#24577;&#22270;&#20013;&#21435;&#38500;&#28508;&#22312;&#20887;&#20313;&#30340;&#36793;&#32536;&#12290;&#20174;&#25216;&#26415;&#21644;&#24037;&#19994;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#21160;&#24577;&#22270;&#21098;&#26525;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20197;&#21098;&#26525;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10668</link><description>&lt;p&gt;
MetaGAD&#65306;&#23398;&#20064;&#20803;&#36716;&#31227;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#20010;&#39046;&#22495;&#20449;&#24687;&#23433;&#20840;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#37329;&#34701;&#27450;&#35784;&#12289;&#31038;&#20250;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#20837;&#20405;&#31561;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#25191;&#34892;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24448;&#24448;&#22826;&#26114;&#36149;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#24322;&#24120;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21487;&#33021;&#20250;&#23558;&#34987;&#35782;&#21035;&#30340;&#24322;&#24120;&#35270;&#20026;&#25968;&#25454;&#22122;&#22768;&#25110;&#19981;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#21487;&#33719;&#21462;&#26377;&#38480;&#30340;&#26631;&#35760;&#24322;&#24120;&#65292;&#36825;&#20123;&#26631;&#35760;&#24322;&#24120;&#20855;&#26377;&#25512;&#36827;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#21644;&#22823;&#37327;&#26080;&#26631;&#35760;&#33410;&#28857;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24037;&#20316;&#30456;&#24403;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;MetaGAD&#65292;&#23398;&#20064;&#20803;&#36716;&#31227;&#30693;&#35782;&#26469;&#36827;&#34892;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#21487;&#29983;&#25104;&#39640;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#33021;&#26377;&#25928;&#25915;&#20987;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2305.10665</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Content-based Unrestricted Adversarial Attack. (arXiv:2305.10665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#21487;&#29983;&#25104;&#39640;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#33021;&#26377;&#25928;&#25915;&#20987;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#20250;&#25805;&#32437;&#22270;&#20687;&#30340;&#35821;&#20041;&#20869;&#23481;&#65288;&#22914;&#39068;&#33394;&#25110;&#32441;&#29702;&#65289;&#26469;&#21019;&#24314;&#26082;&#26377;&#25928;&#21448;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#20855;&#26377;&#27450;&#39575;&#20154;&#31867;&#24863;&#30693;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34109;&#24615;&#21644;&#25104;&#21151;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20316;&#21697;&#36890;&#24120;&#20250;&#29306;&#29298;&#19981;&#21463;&#38480;&#21046;&#30340;&#31243;&#24230;&#24182;&#20027;&#35266;&#22320;&#36873;&#25321;&#19968;&#20123;&#22270;&#20687;&#20869;&#23481;&#26469;&#20445;&#35777;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#36924;&#30495;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;&#20026;&#20445;&#35777;&#23545;&#25239;&#26679;&#26412;&#30340;&#36924;&#30495;&#24615;&#21644;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#21463;&#38480;&#21046;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#34920;&#31034;&#33258;&#28982;&#22270;&#20687;&#30340;&#20302;&#32500;&#27969;&#24418;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#27969;&#24418;&#19978;&#65292;&#24182;&#27839;&#30528;&#20854;&#23545;&#25239;&#26041;&#21521;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#23545;&#25239;&#20869;&#23481;&#25915;&#20987;&#65292;&#24182;&#29983;&#25104;&#20102;&#39640;&#21487;&#36716;&#31227;&#30340;&#36924;&#30495;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#21487;&#20197;&#26377;&#25928;&#25915;&#20987;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unrestricted adversarial attacks typically manipulate the semantic content of an image (e.g., color or texture) to create adversarial examples that are both effective and photorealistic, demonstrating their ability to deceive human perception and deep neural networks with stealth and success. However, current works usually sacrifice unrestricted degrees and subjectively select some image content to guarantee the photorealism of unrestricted adversarial examples, which limits its attack performance. To ensure the photorealism of adversarial examples and boost attack performance, we propose a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack. By leveraging a low-dimensional manifold that represents natural images, we map the images onto the manifold and optimize them along its adversarial direction. Therefore, within this framework, we implement Adversarial Content Attack based on Stable Diffusion and can generate high transferable unrestricted adve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28034;&#40486;&#27880;&#37322;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#36741;&#21161;&#27169;&#22359;&#25110;&#39069;&#22806;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#32467;&#26500;&#32422;&#26463;&#26469;&#35299;&#20915;&#36965;&#24863;&#22270;&#20687;&#30446;&#26631;&#25552;&#21462;&#20013;&#30340;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10661</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#32467;&#26500;&#32422;&#26463;&#30340;&#36965;&#24863;&#22270;&#20687;&#30446;&#26631;&#25552;&#21462;&#26041;&#27861;&#30340;&#28034;&#40486;&#30417;&#30563;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Scribble-Supervised Target Extraction Method Based on Inner Structure-Constraint for Remote Sensing Images. (arXiv:2305.10661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28034;&#40486;&#27880;&#37322;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#36741;&#21161;&#27169;&#22359;&#25110;&#39069;&#22806;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#32467;&#26500;&#32422;&#26463;&#26469;&#35299;&#20915;&#36965;&#24863;&#22270;&#20687;&#30446;&#26631;&#25552;&#21462;&#20013;&#30340;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28034;&#40486;&#27880;&#37322;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#22312;&#36965;&#24863;&#22270;&#20687;&#30446;&#26631;&#25552;&#21462;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#28034;&#40486;&#20855;&#26377;&#25551;&#36848;&#26354;&#25240;&#29289;&#20307;&#21644;&#25163;&#21160;&#26631;&#27880;&#25104;&#26412;&#20302;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#28034;&#40486;&#36807;&#20110;&#31232;&#30095;&#65292;&#38590;&#20197;&#35782;&#21035;&#23545;&#35937;&#32467;&#26500;&#21644;&#35814;&#32454;&#20449;&#24687;&#65292;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#36793;&#30028;&#25551;&#36848;&#26041;&#38754;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#31181;&#20869;&#37096;&#32467;&#26500;&#32422;&#26463;&#12289;&#21464;&#24418;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#21487;&#35757;&#32451;&#30340;&#20027;&#21160;&#36718;&#24275;&#25439;&#22833;&#65292;&#20197;&#21450;&#28034;&#40486;&#32422;&#26463;&#65292;&#36890;&#36807;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#20248;&#21270;&#65292;&#32780;&#19981;&#24341;&#20837;&#20219;&#20309;&#22522;&#20110;&#20808;&#39564;&#32447;&#32034;&#30340;&#36741;&#21161;&#27169;&#22359;&#25110;&#39069;&#22806;&#25805;&#20316;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#35813;&#39046;&#22495;&#20013;&#30340;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/yitongli123/ISC-TE &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning based on scribble annotations in target extraction of remote sensing images has drawn much interest due to scribbles' flexibility in denoting winding objects and low cost of manually labeling. However, scribbles are too sparse to identify object structure and detailed information, bringing great challenges in target localization and boundary description. To alleviate these problems, in this paper, we construct two inner structure-constraints, a deformation consistency loss and a trainable active contour loss, together with a scribble-constraint to supervise the optimization of the encoder-decoder network without introducing any auxiliary module or extra operation based on prior cues. Comprehensive experiments demonstrate our method's superiority over five state-of-the-art algorithms in this field. Source code is available at https://github.com/yitongli123/ISC-TE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#30340;&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.10659</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#38556;&#30861;&#31243;&#24230;&#36827;&#34892;&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Use of Speech Impairment Severity for Dysarthric Speech Recognition. (arXiv:2305.10659v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#30340;&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#22240;&#32032;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#31561;&#22240;&#32032;&#25152;&#23548;&#33268;&#30340;&#35828;&#35805;&#20154;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#35828;&#35805;&#20154;&#36523;&#20221;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#65306;a&#65289;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#21253;&#25324;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#39044;&#27979;&#35823;&#24046;&#65307;b&#65289;&#20197;&#35828;&#35805;&#20154;-&#38556;&#30861;&#31243;&#24230;&#20026;&#37325;&#28857;&#30340;&#36741;&#21161;&#29305;&#24449;&#35843;&#25972;&#65307;c&#65289;&#20165;&#38024;&#23545;&#35828;&#35805;&#20154;&#36523;&#20221;&#21644;&#38556;&#30861;&#31243;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#21270;LHUC&#21464;&#25442;&#12290;&#22312;UASpeech&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#39069;&#22806;&#30340;&#35821;&#38899;&#38556;&#30861;&#31243;&#24230;&#32435;&#20837;&#26368;&#20808;&#36827;&#30340;&#28151;&#21512;DNN&#12289;E2E Conformer&#21644;&#39044;&#35757;&#32451;&#30340;Wav2vec 2.0 ASR&#31995;&#32479;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#65292;&#26368;&#39640;&#21487;&#36798;4.78%&#65288;&#30456;&#23545;&#20110;14.03%&#30340;WER&#38477;&#20302;&#65289;&#12290;&#20351;&#29992;&#26368;&#20339;&#31995;&#32479;&#65292;&#22312;UASpeech&#19978;&#21487;&#20197;&#33719;&#24471;&#24050;&#21457;&#24067;&#30340;&#26368;&#20302;WER&#20026;17.82%&#65288;&#23545;&#20110;&#38750;&#24120;&#20302;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#20026;51.25%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity associated factors such as gender, and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#35745;&#31639;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#65292;&#38416;&#26126;&#20102;Systm-1&#21644;System-2&#30340;&#24213;&#23618;&#26426;&#21046;&#65292;&#28040;&#38500;&#20102;&#23545;&#23427;&#20204;&#30340;&#35823;&#35299;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#23545;&#20803;&#35748;&#30693;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10654</link><description>&lt;p&gt;
&#36890;&#36807;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#38416;&#26126;&#31995;&#32479;1&#21644;&#31995;&#32479;2
&lt;/p&gt;
&lt;p&gt;
Clarifying System 1 &amp; 2 through the Common Model of Cognition. (arXiv:2305.10654v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#35745;&#31639;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#65292;&#38416;&#26126;&#20102;Systm-1&#21644;System-2&#30340;&#24213;&#23618;&#26426;&#21046;&#65292;&#28040;&#38500;&#20102;&#23545;&#23427;&#20204;&#30340;&#35823;&#35299;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#23545;&#20803;&#35748;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;Systm-1&#21644;System-2&#30340;&#21452;&#31995;&#32479;&#25551;&#36848;&#38754;&#20020;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25361;&#25112;&#65292;&#25351;&#36131;&#23427;&#20204;&#19981;&#22815;&#31934;&#30830;&#24182;&#19988;&#23481;&#26131;&#24341;&#36215;&#35823;&#35299;&#12290;&#26412;&#25991;&#37319;&#29992;Dennett&#30340;&#35745;&#31639;&#24605;&#32500;&#26041;&#27861;&#20316;&#20026;&#20998;&#26512;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21407;&#26412;&#34987;&#35748;&#20026;&#26159;System-1&#21644;System-2&#29305;&#26377;&#30340;&#29305;&#24449;&#23454;&#38469;&#19978;&#26500;&#25104;&#20102;&#19968;&#20010;&#35748;&#30693;&#23646;&#24615;&#30340;&#35889;&#31995;&#12290;&#36890;&#36807;&#23558;System-1&#21644;System-2&#22522;&#20110;&#36890;&#29992;&#27169;&#22411;&#36827;&#34892;&#23454;&#29616;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#38416;&#26126;&#23427;&#20204;&#30340;&#24213;&#23618;&#26426;&#21046;&#12289;&#28040;&#38500;&#35823;&#35299;&#20197;&#21450;&#38416;&#26126;&#23427;&#20204;&#23545;&#20803;&#35748;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been increasing challenges to dual-system descriptions of System-1 and System-2, critiquing them as imprecise and fostering misconceptions. We address these issues here by way of Dennett's appeal to use computational thinking as an analytical tool, specifically we employ the Common Model of Cognition. Results show that the characteristics thought to be distinctive of System-1 and System-2 instead form a spectrum of cognitive properties. By grounding System-1 and System-2 in the Common Model we aim to clarify their underlying mechanisms, persisting misconceptions, and implications for metacognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#21644;&#20004;&#31181;logit&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10648</link><description>&lt;p&gt;
&#23545;&#39640;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;logit&#36827;&#34892;&#39640;&#26031;&#24418;&#24335;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition. (arXiv:2305.10648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#21644;&#20004;&#31181;logit&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#38271;&#23614;&#20998;&#24067;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#65292;&#30001;&#20110;&#38590;&#20197;&#27491;&#30830;&#20998;&#31867;&#23614;&#37096;&#31867;&#21035;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#20998;&#31867;&#22120;&#20559;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21069;&#25552;&#26159;&#29992;&#38271;&#23614;&#25968;&#25454;&#33719;&#24471;&#30340;&#29305;&#24449;&#36275;&#22815;&#20195;&#34920;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#22343;&#21248;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22836;&#31867;&#30340;&#23884;&#20837;&#31354;&#38388;&#20005;&#37325;&#21387;&#32553;&#23614;&#31867;&#65292;&#36825;&#23545;&#20110;&#21518;&#32493;&#30340;&#20998;&#31867;&#22120;&#23398;&#20064;&#26159;&#19981;&#21033;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;&#29305;&#24449;&#27700;&#24179;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#22686;&#24378;&#26469;&#24179;&#34913;&#23884;&#20837;&#20998;&#24067;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#29305;&#24449;&#20197;&#39640;&#26031;&#24418;&#24335;&#20855;&#26377;&#19981;&#21516;&#25391;&#24133;&#30340;&#25200;&#21160;&#12290;&#22522;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;logit&#35843;&#25972;&#26041;&#27861;&#26469;&#25552;&#39640;&#23614;&#37096;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is not uncommon that real-world data are distributed with a long tail. For such data, the learning of deep neural networks becomes challenging because it is hard to classify tail classes correctly. In the literature, several existing methods have addressed this problem by reducing classifier bias provided that the features obtained with long-tailed data are representative enough. However, we find that training directly on long-tailed data leads to uneven embedding space. That is, the embedding space of head classes severely compresses that of tail classes, which is not conducive to subsequent classifier learning. %further improving model performance. This paper therefore studies the problem of long-tailed visual recognition from the perspective of feature level. We introduce feature augmentation to balance the embedding distribution. The features of different classes are perturbed with varying amplitudes in Gaussian form. Based on these perturbed features, two novel logit adjustment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;BioAug&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.10647</link><description>&lt;p&gt;
BioAug&#65306;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER. (arXiv:2305.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;BioAug&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(BioNER)&#26159;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#30001;&#20110;&#27880;&#37322;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#21270;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;BioNER &#36973;&#21463;&#30528;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22256;&#25200;&#12290;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#19981;&#33021;&#20026;BioNER&#29983;&#25104;&#30495;&#23454;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;BioNER&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#37325;&#26500;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#24182;&#22312;&#19982;&#35757;&#32451;&#38454;&#27573;&#31867;&#20284;&#30340;&#26377;&#36873;&#25321;&#24615;&#22320;&#25439;&#22351;&#25991;&#26412;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;BioAug&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;BioAug&#27604;&#25152;&#26377;&#22522;&#32447;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;ChatGPT&#25552;&#20986;&#20102;&#20855;&#20307;&#20262;&#29702;&#20851;&#27880;&#28857;&#65292;&#23545;&#20854;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#25552;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;ChatGPT&#30456;&#20851;&#26041;&#30340;&#23454;&#29992;&#35819;&#21629;&#65292;&#20197;&#20419;&#36827;&#20854;&#20262;&#29702;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10646</link><description>&lt;p&gt;
ChatGPT&#30340;&#20262;&#29702;&#20851;&#27880;&#12289;&#25361;&#25112;&#21644;&#35819;&#21629;
&lt;/p&gt;
&lt;p&gt;
Ethical ChatGPT: Concerns, Challenges, and Commandments. (arXiv:2305.10646v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;ChatGPT&#25552;&#20986;&#20102;&#20855;&#20307;&#20262;&#29702;&#20851;&#27880;&#28857;&#65292;&#23545;&#20854;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#25552;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;ChatGPT&#30456;&#20851;&#26041;&#30340;&#23454;&#29992;&#35819;&#21629;&#65292;&#20197;&#20419;&#36827;&#20854;&#20262;&#29702;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#20363;&#22914;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#26497;&#22823;&#22320;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#21450;&#65292;&#29305;&#21035;&#26159;&#22312;&#26222;&#36890;&#27665;&#20247;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#30340;&#24320;&#21457;&#26159;&#20026;&#20102;&#25903;&#25345;&#20154;&#31867;&#20043;&#38388;&#30340;&#33258;&#28982;&#35821;&#35328;&#27807;&#36890;&#12290;&#38382;&#39064;&#26159;&#65292;&#23427;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#20010;&#8220;&#32479;&#35745;&#30456;&#20851;&#24615;&#26426;&#22120;&#8221;&#65288;&#30456;&#20851;&#24615;&#32780;&#38750;&#22240;&#26524;&#20851;&#31995;&#65289;&#65292;&#23545;&#20110;&#20351;&#29992;AI&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#23384;&#22312;&#20262;&#29702;&#20851;&#27880;&#20107;&#39033;&#65292;&#22914;&#20559;&#35265;&#12289;&#38544;&#31169;&#21644;&#28389;&#29992;&#12290;&#26412;&#25991;&#37325;&#28857;&#27010;&#25324;&#20102;ChatGPT&#30340;&#20855;&#20307;&#20262;&#29702;&#20851;&#27880;&#28857;&#65292;&#24182;&#38024;&#23545;ChatGPT&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#25552;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19981;&#21516;ChatGPT&#30456;&#20851;&#26041;&#30340;&#23454;&#29992;&#35819;&#21629;&#65292;&#21487;&#20197;&#20316;&#20026;&#26816;&#26597;&#28165;&#21333;&#25351;&#21335;&#65292;&#20419;&#20351;ChatGPT&#30340;&#20262;&#29702;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, e.g. ChatGPT are currently contributing enormously to make artificial intelligence even more popular, especially among the general population. However, such chatbot models were developed as tools to support natural language communication between humans. Problematically, it is very much a ``statistical correlation machine" (correlation instead of causality) and there are indeed ethical concerns associated with the use of AI language models such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights specific ethical concerns on ChatGPT and articulates key challenges when ChatGPT is used in various applications. Practical commandments for different stakeholders of ChatGPT are also proposed that can serve as checklist guidelines for those applying ChatGPT in their applications. These commandment examples are expected to motivate the ethical use of ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10638</link><description>&lt;p&gt;
&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#36827;&#34892;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#30340;&#20219;&#21153;&#26159;&#20998;&#26512;&#31995;&#32479;&#30417;&#25511;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#31995;&#32479;&#25925;&#38556;/&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26377;&#25928;&#30340;RCA&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#31995;&#32479;&#25925;&#38556;&#24674;&#22797;&#65292;&#24182;&#20943;&#36731;&#31995;&#32479;&#25439;&#22833;&#25110;&#36130;&#21153;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#24320;&#21457;&#31163;&#32447;RCA&#31639;&#27861;&#19978;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#21551;&#21160;RCA&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#31995;&#32479;&#25925;&#38556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;RCA&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;RCA&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;RCA&#27169;&#22411;&#12290;CORAL&#21253;&#25324;&#35302;&#21457;&#28857;&#26816;&#27979;&#12289;&#22686;&#37327;&#35299;&#32544;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;&#35302;&#21457;&#28857;&#26816;&#27979;&#32452;&#20214;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#29366;&#24577;&#36716;&#25442;&#24182;&#36827;&#34892;&#20934;&#23454;&#26102;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;m&#30340;&#22312;&#32447;&#35302;&#21457;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.10601</link><description>&lt;p&gt;
Tree of Thoughts: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#36890;&#29992;&#38382;&#39064;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20173;&#28982;&#21463;&#38480;&#20110;&#22522;&#20110;&#26631;&#35760;&#12289;&#20174;&#24038;&#21040;&#21491;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#21069;&#30651;&#25110;&#21021;&#22987;&#20915;&#31574;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#23427;&#23558;&#36890;&#24120;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#26041;&#27861;&#27867;&#21270;&#65292;&#24182;&#20351;&#29992;&#19968;&#33268;&#30340;&#25991;&#26412;&#21333;&#20301;&#65288;&#24605;&#32500;&#65289;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#24605;&#32500;&#20316;&#20026;&#35299;&#20915;&#38382;&#39064;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#24605;&#32500;&#20043;&#26641;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#33258;&#25105;&#35780;&#20272;&#26469;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#24182;&#20915;&#23450;&#19979;&#19968;&#27493;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#22312;&#24517;&#35201;&#26102;&#21521;&#21069;&#25110;&#21521;&#21518;&#36319;&#36394;&#20197;&#36827;&#34892;&#20840;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ToT&#26174;&#33879;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1550&#20010;&#29289;&#20307;&#21644;1100&#19975;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;Sim-MEES&#65292;&#23427;&#33021;&#22312;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#27169;&#22359;&#21270;&#26411;&#31471;&#25191;&#34892;&#22120;&#31995;&#32479;&#25552;&#20379;&#31934;&#30830;&#30340;&#25235;&#21462;&#26631;&#31614;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.10580</link><description>&lt;p&gt;
Sim-MEES: &#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#26426;&#22120;&#20154;&#22312;&#26434;&#20081;&#29615;&#22659;&#19979;&#36827;&#34892;&#25235;&#21462;&#30340;&#27169;&#22359;&#21270;&#26411;&#31471;&#25191;&#34892;&#22120;&#31995;&#32479;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Sim-MEES: Modular End-Effector System Grasping Dataset for Mobile Manipulators in Cluttered Environments. (arXiv:2305.10580v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1550&#20010;&#29289;&#20307;&#21644;1100&#19975;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;Sim-MEES&#65292;&#23427;&#33021;&#22312;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#27169;&#22359;&#21270;&#26411;&#31471;&#25191;&#34892;&#22120;&#31995;&#32479;&#25552;&#20379;&#31934;&#30830;&#30340;&#25235;&#21462;&#26631;&#31614;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Sim-MEES&#65306;&#19968;&#20010;&#21253;&#21547;1550&#20010;&#29289;&#20307;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#21644;&#29289;&#29702;&#23646;&#24615;&#20197;&#21450;1100&#19975;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#22841;&#29226;&#27169;&#24335;&#35268;&#21010;&#26426;&#22120;&#20154;&#25235;&#21462;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#36807;&#31243;&#32467;&#21512;&#20102;&#20998;&#26512;&#27169;&#22411;&#21644;&#25972;&#20010;&#26434;&#20081;&#29615;&#22659;&#30340;&#21160;&#24577;&#27169;&#25311;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#25235;&#21462;&#26631;&#31614;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#29992;&#20110;&#24179;&#34892;&#22841;&#29226;&#22841;&#20855;&#21644;&#21560;&#30424;&#22841;&#20855;&#30340;&#26631;&#35760;&#36807;&#31243;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;Sim-MEES&#22914;&#20309;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25552;&#20379;&#31934;&#30830;&#30340;&#25235;&#21462;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Sim-MEES: a large-scale synthetic dataset that contains 1,550 objects with varying difficulty levels and physics properties, as well as 11 million grasp labels for mobile manipulators to plan grasps using different gripper modalities in cluttered environments. Our dataset generation process combines analytic models and dynamic simulations of the entire cluttered environment to provide accurate grasp labels. We provide a detailed study of our proposed labeling process for both parallel jaw grippers and suction cup grippers, comparing them with state-of-the-art methods to demonstrate how Sim-MEES can provide precise grasp labels in cluttered environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23457;&#26597;&#20102; DALL-E 2 &#29983;&#25104;&#30340; 15,300 &#24352;&#22270;&#20687;&#20013;&#30340;&#20004;&#31181;&#32844;&#19994;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#23427;&#22312;&#30007;&#24615;&#20027;&#23548;&#39046;&#22495;&#20013;&#20302;&#20272;&#22899;&#24615;&#65292;&#22312;&#22899;&#24615;&#20027;&#23548;&#32844;&#19994;&#20013;&#39640;&#20272;&#22899;&#24615;&#12290;&#27492;&#22806;&#65292;DALL-E 2 &#22270;&#20687;&#25551;&#32472;&#26356;&#22810;&#24102;&#26377;&#24494;&#31505;&#21644;&#21521;&#19979;&#30475;&#30340;&#22899;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22899;&#24615;&#20027;&#23548;&#30340;&#32844;&#19994;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.10566</link><description>&lt;p&gt;
&#24494;&#31505;&#22899;&#24615;&#21521;&#19979;&#30475;&#65306;&#23457;&#26597;&#29983;&#25104;&#22270;&#20687; AI &#20013;&#30340;&#20195;&#34920;&#24615;&#21644;&#23637;&#31034;&#24615;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Smiling Women Pitching Down: Auditing Representational and Presentational Gender Biases in Image Generative AI. (arXiv:2305.10566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23457;&#26597;&#20102; DALL-E 2 &#29983;&#25104;&#30340; 15,300 &#24352;&#22270;&#20687;&#20013;&#30340;&#20004;&#31181;&#32844;&#19994;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#23427;&#22312;&#30007;&#24615;&#20027;&#23548;&#39046;&#22495;&#20013;&#20302;&#20272;&#22899;&#24615;&#65292;&#22312;&#22899;&#24615;&#20027;&#23548;&#32844;&#19994;&#20013;&#39640;&#20272;&#22899;&#24615;&#12290;&#27492;&#22806;&#65292;DALL-E 2 &#22270;&#20687;&#25551;&#32472;&#26356;&#22810;&#24102;&#26377;&#24494;&#31505;&#21644;&#21521;&#19979;&#30475;&#30340;&#22899;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22899;&#24615;&#20027;&#23548;&#30340;&#32844;&#19994;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335; AI &#27169;&#22411;&#65292;&#22914; DALL-E 2&#65292;&#33021;&#22815;&#35299;&#37322;&#25991;&#26412;&#25552;&#31034;&#24182;&#29983;&#25104;&#23637;&#29616;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#23613;&#31649;&#20844;&#20247;&#28909;&#24773;&#39640;&#28072;&#65292;&#20294;&#23545; AI &#29983;&#25104;&#22270;&#20687;&#20013;&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#31995;&#32479;&#23457;&#26597;&#20173;&#28982;&#24456;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#35206;&#30422; 153 &#20010;&#32844;&#19994;&#30340; 15,300 &#24352; DALL-E 2 &#22270;&#20687;&#20013;&#20004;&#31181;&#32844;&#19994;&#24615;&#21035;&#20559;&#35265;&#65288;&#20195;&#34920;&#24615;&#20559;&#35265;&#21644;&#23637;&#31034;&#24615;&#20559;&#35265;&#65289;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#26681;&#25454;2021&#24180;&#20154;&#21475;&#26222;&#26597;&#21171;&#21160;&#21147;&#32479;&#35745;&#25968;&#25454;&#21644; Google &#22270;&#20687;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102; DALL-E 2 &#22312;&#30007;&#24615;&#20027;&#23548;&#39046;&#22495;&#20013;&#20302;&#20272;&#22899;&#24615;&#65292;&#22312;&#22899;&#24615;&#20027;&#23548;&#32844;&#19994;&#20013;&#39640;&#20272;&#22899;&#24615;&#12290;&#27492;&#22806;&#65292;DALL-E 2 &#30340;&#22270;&#20687;&#20542;&#21521;&#20110;&#25551;&#32472;&#26356;&#22810;&#24102;&#26377;&#24494;&#31505;&#21644;&#21521;&#19979;&#30475;&#30340;&#22899;&#24615;&#32780;&#19981;&#26159;&#30007;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22899;&#24615;&#20027;&#23548;&#30340;&#32844;&#19994;&#20013;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#31639;&#27861;&#23457;&#26597;&#30740;&#31350;&#23637;&#31034;&#20102; DALL-E 2 &#20013;&#26356;&#26126;&#26174;&#30340;&#20195;&#34920;&#24615;&#21644;&#23637;&#31034;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models like DALL-E 2 can interpret textual prompts and generate high-quality images exhibiting human creativity. Though public enthusiasm is booming, systematic auditing of potential gender biases in AI-generated images remains scarce. We addressed this gap by examining the prevalence of two occupational gender biases (representational and presentational biases) in 15,300 DALL-E 2 images spanning 153 occupations, and assessed potential bias amplification by benchmarking against 2021 census labor statistics and Google Images. Our findings reveal that DALL-E 2 underrepresents women in male-dominated fields while overrepresenting them in female-dominated occupations. Additionally, DALL-E 2 images tend to depict more women than men with smiling faces and downward-pitching heads, particularly in female-dominated (vs. male-dominated) occupations. Our computational algorithm auditing study demonstrates more pronounced representational and presentational biases in DALL-E 2 compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2305.10564</link><description>&lt;p&gt;
&#23545;&#25918;&#24323;&#20998;&#31867;&#22120;&#36827;&#34892;&#21453;&#20107;&#23454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#24323;&#20998;&#31867;&#22120;&#21487;&#20197;&#36873;&#25321;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#38382;&#39064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20445;&#30041;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#40657;&#30418;&#25918;&#24323;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#32771;&#34385;&#20998;&#31867;&#22120;&#22312;&#23427;&#30340;&#25918;&#24323;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#25918;&#23556;&#31185;&#21307;&#29983;&#19981;&#30830;&#23450;&#20854;&#35786;&#26029;&#25110;&#24403;&#39550;&#39542;&#21592;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#19981;&#27880;&#24847;&#26102;&#65292;&#36825;&#20123;&#32570;&#22833;&#30340;&#39044;&#27979;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#22260;&#32469;&#30528;&#23450;&#20041;&#19968;&#20010;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#21363;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#30340;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25351;&#23450;&#20102;&#26465;&#20214;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10563</link><description>&lt;p&gt;
&#25506;&#31350;&#30828;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGEs&#65289;&#30340;&#36136;&#37327;&#65292;&#23427;&#20381;&#36182;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29992;&#36127;&#19977;&#20803;&#32452;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#22312;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#39640;&#36136;&#37327;&#65288;&#21363;&#30828;&#65289;&#36127;&#37319;&#26679;&#30340;&#21551;&#21457;&#24335;&#29983;&#25104;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;InfoNCE&#25439;&#22833;&#65292;&#26174;&#24335;&#32771;&#34385;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#30828;&#36127;&#26679;&#26412;&#26368;&#23567;&#21270;InfoNCE&#25439;&#22833;&#21487;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#19977;&#20803;&#32452;&#21644;&#36127;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#30828;&#36127;&#26679;&#26412;&#20250;&#23548;&#33268;&#20551;&#36127;&#26679;&#26412;&#65288;&#21363;&#38169;&#35823;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#65289;&#24182;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#32467;&#26500;&#21435;&#38500;&#20551;&#36127;&#19977;&#20803;&#32452;&#30340;&#26032;&#22411;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#31216;&#20026;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\textbf{HaSa}) contrasti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#38656;&#27714;&#33021;&#21147;&#24179;&#34913;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#35299;&#20915;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#65288;UAM&#65289;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#22312;&#39044;&#22788;&#29702;&#27969;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25112;&#26415;&#23433;&#20840;&#20998;&#31163;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10556</link><description>&lt;p&gt;
&#38754;&#21521;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#30340;&#19968;&#20307;&#21270;&#20914;&#31361;&#31649;&#29702;&#65306;&#25112;&#30053;&#38656;&#27714;&#33021;&#21147;&#24179;&#34913;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#25112;&#26415;&#20914;&#31361;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Integrated Conflict Management for UAM with Strategic Demand Capacity Balancing and Learning-based Tactical Deconfliction. (arXiv:2305.10556v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10556
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#38656;&#27714;&#33021;&#21147;&#24179;&#34913;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#35299;&#20915;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#65288;UAM&#65289;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#22312;&#39044;&#22788;&#29702;&#27969;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25112;&#26415;&#23433;&#20840;&#20998;&#31163;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#65288;UAM&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#26085;&#24120;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#21644;&#21608;&#22260;&#30340;&#19987;&#38376;&#20301;&#32622;&#20043;&#38388;&#24555;&#36895;&#12289;&#26377;&#25928;&#22320;&#36816;&#36755;&#20056;&#23458;&#21644;&#36135;&#29289;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26032;&#20852;&#20132;&#36890;&#27169;&#24335;&#21830;&#19994;&#21270;&#21644;&#24191;&#27867;&#37319;&#29992;&#20043;&#21069;&#65292;&#24517;&#39035;&#20445;&#35777;&#33322;&#31354;&#23433;&#20840;&#65292;&#21363;&#36890;&#36807;&#25112;&#30053;&#21644;&#25112;&#26415;&#25490;&#38500;&#20914;&#31361;&#26469;&#23433;&#20840;&#35268;&#36991;&#25152;&#26377;&#39134;&#34892;&#22120;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#33322;&#29677;&#30340;&#25112;&#26415;&#38450;&#25252;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#34920;&#29616;&#21457;&#29616;&#21462;&#20915;&#20110;&#20132;&#36890;&#23494;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#38656;&#27714;&#33021;&#21147;&#24179;&#34913;&#65288;DCB&#65289;&#29992;&#20110;&#25112;&#30053;&#24615;&#20914;&#31361;&#31649;&#29702;&#65292;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#25112;&#26415;&#25490;&#38500;&#20914;&#31361;&#12290;&#36890;&#36807;&#20351;&#29992;DCB&#23558;&#27969;&#37327;&#39044;&#32622;&#21040;&#36866;&#24403;&#30340;&#23494;&#24230;&#27700;&#24179;&#65292;&#25105;&#20204;&#34920;&#26126;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20026;&#25112;&#26415;&#23433;&#20840;&#25490;&#38500;&#20914;&#31361;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Urban air mobility (UAM) has the potential to revolutionize our daily transportation, offering rapid and efficient deliveries of passengers and cargo between dedicated locations within and around the urban environment. Before the commercialization and adoption of this emerging transportation mode, however, aviation safety must be guaranteed, i.e., all the aircraft have to be safely separated by strategic and tactical deconfliction. Reinforcement learning has demonstrated effectiveness in the tactical deconfliction of en route commercial air traffic in simulation. However, its performance is found to be dependent on the traffic density. In this project, we propose a novel framework that combines demand capacity balancing (DCB) for strategic conflict management and reinforcement learning for tactical separation. By using DCB to precondition traffic to proper density levels, we show that reinforcement learning can achieve much better performance for tactical safety separation. Our results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#36870;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#65292;&#33258;&#21160;&#21457;&#29616;&#22870;&#21169;&#20989;&#25968;&#24182;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#23547;&#25214;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.10548</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21457;&#29616;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning. (arXiv:2305.10548v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#36870;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#65292;&#33258;&#21160;&#21457;&#29616;&#22870;&#21169;&#20989;&#25968;&#24182;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#23547;&#25214;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#65288;&#20363;&#22914;&#40060;&#32676;&#21644;&#32454;&#33740;&#32676;&#33853;&#65289;&#20013;&#30340;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#30446;&#26631;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;&#28041;&#21450;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#22810;&#20010;&#20132;&#20114;&#20195;&#29702;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31163;&#32447;&#36870;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IMARL&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;ReF-ER&#25216;&#26415;&#21644;&#23548;&#21521;&#25104;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#22870;&#21169;&#20989;&#25968;&#24182;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#25429;&#25417;&#21040;&#20102;&#25552;&#20379;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#65292;&#24182;&#22312;&#21253;&#25324;OpenAI gym&#20013;&#30340;&#21333;&#20195;&#29702;&#27169;&#22411;&#21644;&#28041;&#21450;&#22810;&#20195;&#29702;&#30340;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#22495;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The pr
&lt;/p&gt;</description></item><item><title>GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10544</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#30340;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10544
&lt;/p&gt;
&lt;p&gt;
GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476; (GSPN)&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#12290;&#21463;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#39030;&#28857;&#24341;&#36215;&#30340;&#35745;&#31639;&#26641;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#21644;&#31215;&#32593;&#32476;&#65288;SPN&#65289;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#29238;SPN&#30340;&#21442;&#25968;&#26159;&#20854;&#23376;&#32423;&#30340;&#21518;&#39564;&#28151;&#21512;&#27010;&#29575;&#30340;&#21487;&#23398;&#20064;&#21464;&#25442;&#12290;&#30001;&#20110;&#26435;&#37325;&#20849;&#20139;&#21644;GSPN&#30340;&#26641;&#29366;&#35745;&#31639;&#22270;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#32570;&#20047;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;Tsetlin Machines&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10538</link><description>&lt;p&gt;
&#21033;&#29992;Tsetlin Machines&#20174;&#25968;&#25454;&#20013;&#29983;&#25104;&#36125;&#21494;&#26031;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generating Bayesian Network Models from Data Using Tsetlin Machines. (arXiv:2305.10538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;Tsetlin Machines&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#26159;&#19968;&#31181;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#27169;&#22411;&#65292;&#22240;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#27010;&#29575;&#25512;&#29702;&#21644;&#22240;&#26524;&#24314;&#27169;&#30340;&#20248;&#28857;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#32473;&#23450;&#19968;&#32452;&#25968;&#25454;&#65292;&#20351;&#29992;BN&#30340;&#19968;&#20010;&#38590;&#28857;&#22312;&#20110;&#20174;&#25968;&#25454;&#26500;&#24314;&#32593;&#32476;&#22270;&#65292;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#30456;&#20851;&#25110;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin Machines&#21457;&#29616;&#32593;&#32476;&#32467;&#26500;&#30340;&#21021;&#27493;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian networks (BN) are directed acyclic graphical (DAG) models that have been adopted into many fields for their strengths in transparency, interpretability, probabilistic reasoning, and causal modeling. Given a set of data, one hurdle towards using BNs is in building the network graph from the data that properly handles dependencies, whether correlated or causal. In this paper, we propose an initial methodology for discovering network structures using Tsetlin Machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#26469;&#39564;&#35777;&#12289;&#20445;&#25252;&#21644;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#32463;&#39564;&#36830;&#36143;&#24615;&#21644;&#25919;&#31574;&#25913;&#36827;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10528</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#32570;&#38519;&#34892;&#20026;&#30340;&#21487;&#25193;&#23637;&#21644;&#23433;&#20840;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#26469;&#39564;&#35777;&#12289;&#20445;&#25252;&#21644;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#32463;&#39564;&#36830;&#36143;&#24615;&#21644;&#25919;&#31574;&#25913;&#36827;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#39537;&#21160;&#21147;&#65292;&#25913;&#21892;&#20102;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#20195;&#29702;&#20154;&#19982;&#20154;&#20043;&#38388;&#26356;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#65292;&#20294;&#22312;&#22823;&#22411;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#24179;&#34913;&#25919;&#31574;&#25913;&#36827;&#21644;&#32463;&#39564;&#36830;&#36143;&#24615;&#32463;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#20013;&#30340;&#39640;&#31934;&#24230;&#26679;&#26412;&#23545;&#25919;&#31574;&#36827;&#34892;&#39564;&#35777;&#12289;&#23433;&#20840;&#20445;&#25252;&#21644;&#25913;&#36827;&#65292;&#20197;&#20415;&#22312;&#22312;&#32447;&#37096;&#32626;&#21069;&#36827;&#34892;&#20462;&#27491;&#12290;&#20316;&#32773;&#23545;&#30495;&#23454;&#30340;&#23545;&#35805;&#31995;&#32479;&#21644;&#23454;&#38469;&#30340;&#22238;&#24402;&#20107;&#20214;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20182;&#20204;&#30340;&#29983;&#20135;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-Policy reinforcement learning has been a driving force for the state-of-the-art conversational AIs leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#32763;&#35793;&#20013;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#65292;&#24573;&#30053;&#38750;&#24615;&#21035;&#20195;&#35789;&#65292;&#20250;&#23558;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#36716;&#25442;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#65292;&#29978;&#33267;&#26080;&#27861;&#23558;&#33521;&#35821;&#20013;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#32763;&#35793;&#20026;&#20854;&#20182;&#35821;&#35328;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#12290;</title><link>http://arxiv.org/abs/2305.10510</link><description>&lt;p&gt;
ChatGPT&#22312;&#20845;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#65292;&#24573;&#30053;&#38750;&#24615;&#21035;&#20195;&#35789;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages. (arXiv:2305.10510v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10510
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#32763;&#35793;&#20013;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#65292;&#24573;&#30053;&#38750;&#24615;&#21035;&#20195;&#35789;&#65292;&#20250;&#23558;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#36716;&#25442;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#65292;&#29978;&#33267;&#26080;&#27861;&#23558;&#33521;&#35821;&#20013;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#32763;&#35793;&#20026;&#20854;&#20182;&#35821;&#35328;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#22810;&#20803;&#25991;&#21270;&#26102;&#20195;&#65292;&#35821;&#35328;&#32763;&#35793;&#26159;&#26368;&#24120;&#35265;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#30001;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35843;&#33410;&#21644;&#33258;&#21160;&#21270;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;ChatGPT&#22768;&#31216;&#22312;&#36825;&#26679;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#22768;&#26126;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#20165;&#20351;&#29992;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#30340;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20013;&#24515;&#36827;&#34892;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#23391;&#21152;&#25289;&#35821;&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#25105;&#20204;&#36824;&#27010;&#25324;&#20102;&#25105;&#20204;&#22312;&#27874;&#26031;&#35821;&#12289;&#39532;&#26469;&#35821;&#12289;&#22612;&#21152;&#27931;&#35821;&#12289;&#27888;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#31561;&#20116;&#31181;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#24310;&#32493;&#20102;&#23545;&#26576;&#20123;&#32844;&#19994;&#65288;&#20363;&#22914;&#30007;&#20154;=&#21307;&#29983;&#65292;&#22899;&#20154;=&#25252;&#22763;&#65289;&#25110;&#34892;&#21160;&#65288;&#22899;&#20154;=&#28921;&#39274;&#65292;&#30007;&#20154;=&#21435;&#24037;&#20316;&#65289;&#36171;&#20104;&#24615;&#21035;&#40664;&#35748;&#20540;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#20559;&#35265;&#65292;&#22240;&#20026;&#23427;&#23558;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#36716;&#25442;&#20026;&#8220;&#20182;&#8221;&#25110;&#8220;&#22905;&#8221;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;ChatGPT&#23436;&#20840;&#26080;&#27861;&#23558;&#33521;&#35821;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#8220;they&#8221;&#32763;&#35793;&#20026;&#20854;&#20182;&#35821;&#35328;&#20013;&#30456;&#24212;&#30340;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in such translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT's accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. We also observe ChatGPT completely failing to translate the English gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languag
&lt;/p&gt;</description></item><item><title>ReasonNet&#26159;&#19968;&#31181;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#26102;&#38388;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22478;&#24066;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#19979;&#30340;&#31232;&#26377;&#19981;&#21033;&#20107;&#20214;&#65292;&#22914;&#31361;&#28982;&#20986;&#29616;&#30340;&#36974;&#25377;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.10507</link><description>&lt;p&gt;
ReasonNet: &#20855;&#26377;&#26102;&#38388;&#21644;&#20840;&#23616;&#25512;&#29702;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
ReasonNet: End-to-End Driving with Temporal and Global Reasoning. (arXiv:2305.10507v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10507
&lt;/p&gt;
&lt;p&gt;
ReasonNet&#26159;&#19968;&#31181;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#26102;&#38388;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22478;&#24066;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#19979;&#30340;&#31232;&#26377;&#19981;&#21033;&#20107;&#20214;&#65292;&#22914;&#31361;&#28982;&#20986;&#29616;&#30340;&#36974;&#25377;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#23578;&#26410;&#23454;&#29616;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#22478;&#24066;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#19979;&#30340;&#39044;&#27979;&#21644;&#24212;&#23545;&#31232;&#26377;&#19981;&#21033;&#20107;&#20214;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ReasonNet&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#26694;&#26550;&#65292;&#24191;&#27867;&#21033;&#29992;&#39550;&#39542;&#22330;&#26223;&#30340;&#26102;&#38388;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#29289;&#20307;&#30340;&#26102;&#38388;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19981;&#21516;&#24103;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#20851;&#31995;&#12290;&#25512;&#29702;&#22330;&#26223;&#30340;&#20840;&#23616;&#20449;&#24687;&#20063;&#21487;&#25552;&#39640;&#25972;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#24182;&#26377;&#30410;&#20110;&#26816;&#27979;&#19981;&#21033;&#20107;&#20214;&#65292;&#29305;&#21035;&#26159;&#23545;&#36974;&#25377;&#29289;&#30340;&#28508;&#22312;&#21361;&#38505;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36974;&#25377;&#20107;&#20214;&#65292;&#25105;&#20204;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#39550;&#39542;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797; DriveOcclusion&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios. In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene. By reasoning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlusion events, we also release publicly a driving simulation benchmark DriveOcclusi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#27169;&#22411;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20316;&#20026;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.10504</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Free Robust Average-Reward Reinforcement Learning. (arXiv:2305.10504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#27169;&#22411;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20316;&#20026;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36890;&#36807;&#22312;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#26469;&#35299;&#20915;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#26080;&#27169;&#22411;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#25551;&#36848;&#20102;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;Bellman&#26041;&#31243;&#30340;&#35299;&#32467;&#26500;&#65292;&#36825;&#23545;&#25105;&#20204;&#21518;&#38754;&#30340;&#25910;&#25947;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#40065;&#26834;&#30456;&#23545;&#20215;&#20540;&#36845;&#20195;(TD)&#21644;&#40065;&#26834;RVI Q-learning&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#31034;&#20363;&#65292;&#21253;&#25324;&#27745;&#26579;&#27169;&#22411;&#12289;&#24635;&#21464;&#24046;&#12289;&#21345;&#26041;&#25955;&#24230;&#12289;KL&#25955;&#24230;&#21644;Wasserstein&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10502</link><description>&lt;p&gt;
EENED&#65306;&#22522;&#20110;&#21367;&#31215;&#21464;&#21387;&#22120;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;EEG&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#65288;Transformer&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#32780;CNN&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#22914;&#38191;&#40831;&#27874;&#20043;&#31867;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#22312;Transformer&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#21367;&#31215;&#27169;&#22359;&#65292;EENED&#21487;&#20197;&#23398;&#20064;&#24739;&#32773;EEG&#20449;&#21495;&#29305;&#24449;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#27880;&#24847;&#21040;&#19982;&#30315;&#30187;&#23494;&#20999;&#30456;&#20851;&#30340;&#23616;&#37096;EEG&#24322;&#24120;&#31361;&#21464;&#65292;&#22914;&#23574;&#38160;&#27874;&#30340;&#20986;&#29616;&#21644;&#32531;&#24930;&#27874;&#30340;&#25955;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;Transformer&#21644;CNN&#25429;&#25417;EEG&#20449;&#21495;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#23558;&#24456;&#24555;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10496</link><description>&lt;p&gt;
&#34701;&#21512;&#24402;&#22240;&#37325;&#35201;&#24615;&#20197;&#25552;&#39640;&#24544;&#23454;&#24230;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26159;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#39044;&#27979;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19968;&#20010;&#26356;&#21152;&#20934;&#30830;&#30340;&#24402;&#22240;&#26041;&#27861;&#26631;&#24535;&#30528;&#23427;&#26356;&#21152;&#24544;&#23454;&#65292;&#23427;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#21453;&#26144;&#21738;&#20123;&#37096;&#20998;&#30340;&#36755;&#20837;&#23545;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#22914;&#20805;&#20998;&#24615;&#21644;&#20840;&#38754;&#24615;&#65292;&#21482;&#20351;&#29992;&#19968;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#65292;&#21363;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#30001;&#32473;&#23450;&#24402;&#22240;&#26041;&#27861;&#25490;&#21517;&#26368;&#39640;&#30340;&#39030;&#37096;&#26631;&#35760;&#65292;&#24182;&#35266;&#23519;&#39044;&#27979;&#21487;&#33021;&#24615;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#24573;&#30053;&#20102;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#65292;&#25226;&#23427;&#20204;&#20840;&#37096;&#31561;&#21516;&#22320;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36719;&#21024;&#38500;&#26631;&#20934;&#12290;&#25105;&#20204;&#19981;&#20250;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#36755;&#20837;&#20013;&#30340;&#26631;&#35760;&#65292;&#32780;&#26159;&#38543;&#26426;&#22320;&#36974;&#30422;&#20195;&#34920;&#24402;&#22240;&#26041;&#27861;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#26631;&#35760;&#21521;&#37327;&#34920;&#31034;&#12290;&#22522;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our sof
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Fisher&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#20316;&#20026;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26576;&#20010;&#31995;&#32479;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#20010;&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.10491</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Renormalization. (arXiv:2305.10491v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Fisher&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#20316;&#20026;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26576;&#20010;&#31995;&#32479;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#20010;&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;Fisher&#24230;&#37327;&#26469;&#23450;&#20041;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#65292;&#36825;&#20010;&#38271;&#24230;&#36215;&#21040;&#20102;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#12290;&#36825;&#20010;RG&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#23545;&#20110;&#19968;&#20010;&#32473;&#23450;&#31995;&#32479;&#21487;&#20197;&#24471;&#21040;&#30340;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#30340;&#20316;&#29992;&#26159;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;&#22312;&#23558;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29289;&#29702;&#31995;&#32479;&#26102;&#65292;&#36825;&#20010;&#30001;&#20449;&#24687;&#35770;&#20986;&#29616;&#30340;RG&#23610;&#24230;&#33258;&#28982;&#22320;&#34987;&#35782;&#21035;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#65292;&#22240;&#27492;&#65292;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we present a fully information theoretic approach to renormalization inspired by Bayesian statistical inference, which we refer to as Bayesian Renormalization. The main insight of Bayesian Renormalization is that the Fisher metric defines a correlation length that plays the role of an emergent RG scale quantifying the distinguishability between nearby points in the space of probability distributions. This RG scale can be interpreted as a proxy for the maximum number of unique observations that can be made about a given system during a statistical inference experiment. The role of the Bayesian Renormalization scheme is subsequently to prepare an effective model for a given system up to a precision which is bounded by the aforementioned scale. In applications of Bayesian Renormalization to physical systems, the emergent information theoretic scale is naturally identified with the maximum energy that can be probed by current experimental apparatus, and thus Bayesian Renormali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#24687;&#30340; nomogram&#65292;&#29992;&#20110;&#35780;&#20272;&#24739;&#26377; 8 &#27627;&#31859;&#20197;&#19979; SPNs &#30340;&#32954;&#30284;&#30340;&#21487;&#33021;&#24615;&#65292;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#27604;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10466</link><description>&lt;p&gt;
&#20351;&#29992; nomogram &#21644;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#24739;&#26377;&#32954;&#30284;&#30340;&#32954;&#32467;&#33410;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Solitary pulmonary nodules prediction for lung cancer patients using nomogram and machine learning. (arXiv:2305.10466v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#24687;&#30340; nomogram&#65292;&#29992;&#20110;&#35780;&#20272;&#24739;&#26377; 8 &#27627;&#31859;&#20197;&#19979; SPNs &#30340;&#32954;&#30284;&#30340;&#21487;&#33021;&#24615;&#65292;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#27604;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#36215;&#28304;&#20110;&#25903;&#27668;&#31649;&#31896;&#33180;&#25110;&#33146;&#20307;&#30340;&#24694;&#24615;&#32959;&#30244;&#12290;&#23396;&#31435;&#24615;&#32954;&#32467;&#33410;&#65288;SPNs&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#32954;&#37096;&#30149;&#21464;&#65292;&#24403;&#20854;&#30452;&#24452;&#22823;&#20110;8&#27627;&#31859;&#26102;&#65292;&#24694;&#24615;&#21487;&#33021;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#20294;&#26159;&#65292;&#24403;&#30452;&#24452;&#23567;&#20110;8&#27627;&#31859;&#26102;&#20173;&#26377;&#24739;&#32954;&#30284;&#30340;&#39118;&#38505;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#24687;&#20026;8&#27627;&#31859;&#20197;&#19979;SPNs&#24739;&#32773;&#21019;&#24314;&#19968;&#20010;&#35780;&#20272;&#32954;&#30284;&#21487;&#33021;&#24615;&#30340; nomogram&#12290;&#24180;&#40836;&#12289;&#21069;&#20307;&#32963;&#27852;&#32032;&#37322;&#25918;&#32957;&#65288;ProGRP&#65289;&#12289;&#24615;&#21035;&#12289;&#30284;&#32986;&#25239;&#21407;&#65288;CEA&#65289;&#21644;&#24212;&#21147;&#33104;&#34432;&#24320;&#35010;&#65288;SCC&#65289;&#26159;&#29420;&#31435;&#30340;&#20851;&#38190;&#32959;&#30244;&#26631;&#24535;&#29289;&#65292;&#34987;&#36755;&#20837;&#21040; nomogram &#20013;&#12290;&#35813; nomogram &#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#39044;&#27979;&#32954;&#30284;&#39118;&#38505;&#30340;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#20869;&#37096;&#39564;&#35777;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.869&#12290;&#19982; nomogram &#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24212;&#29992;&#20110;&#27604;&#36739;&#20854;&#39044;&#27979;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126; nomogram &#20855;&#26377;&#26356;&#22909;&#30340;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010; nomogram &#26469;&#39044;&#27979;8&#27627;&#31859;&#20197;&#19979;&#30340; SPNs &#24739;&#32773;&#30340;&#32954;&#30284;&#21487;&#33021;&#24615;&#65292;&#26174;&#31034;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#34920;&#29616;&#27604;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer(LC) is a type of malignant neoplasm that originates in the bronchial mucosa or glands.As a clinically common nodule,solitary pulmonary nodules(SPNs) have a significantly higher probability of malignancy when they are larger than 8 mm in diameter.But there is also a risk of lung cancer when the diameter is less than 8mm,the purpose of this study was to create a nomogram for estimating the likelihood of lung cancer in patients with SPNs of 8 mm or smaller using computed tomography(CT) scans and biomarker information.Use CT scans and various biomarkers as input to build predictive models for the likelihood of lung cancer in patients with SPNs of 8 mm or less.The age,precursor gastrin-releasing peptide (ProGRP),gender,Carcinoembryonic Antigen(CEA),and stress corrosion cracking(SCC) were independent key tumor markers and were entered into the nomogram.The developed nomogram demonstrated strong accuracy in predicting lung cancer risk,with an internal validation area under the rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;SO(3)&#31283;&#20581;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23481;&#24525;&#19981;&#23436;&#32654;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.10465</link><description>&lt;p&gt;
&#22522;&#20110;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;SO(3)&#31283;&#20581;&#27010;&#29575;&#24314;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution. (arXiv:2305.10465v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;SO(3)&#31283;&#20581;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23481;&#24525;&#19981;&#23436;&#32654;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#24352;RGB&#22270;&#20687;&#20272;&#35745;&#19977;&#32500;&#33258;&#30001;&#26059;&#36716;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27010;&#29575;&#26059;&#36716;&#24314;&#27169;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#21333;&#39044;&#27979;&#26059;&#36716;&#22238;&#24402;&#21487;&#20197;&#39069;&#22806;&#25552;&#20379;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#23545;&#20110;SO(3)&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#65292;&#20351;&#29992;&#31867;&#20284;&#20110;&#39640;&#26031;&#30340;Bingham&#20998;&#24067;&#21644;&#30697;&#38453;Fisher&#20998;&#24067;&#26159;&#33258;&#28982;&#30340;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#24322;&#24120;&#39044;&#27979;&#24456;&#25935;&#24863;&#65292;&#20363;&#22914;180&#24230;&#35823;&#24046;&#65292;&#22240;&#27492;&#19981;&#22826;&#21487;&#33021;&#20197;&#26368;&#20339;&#24615;&#33021;&#25910;&#25947;&#12290;&#26412;&#25991;&#20174;&#22810;&#20803;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SO(3)&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#23545;&#24322;&#24120;&#20540;&#30340;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#24378;&#21046;&#26045;&#21152;&#26799;&#24230;&#21040;&#20302;&#35823;&#24046;&#21306;&#22495;&#65292;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23567;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#23481;&#24525;&#19981;&#23436;&#32654;&#30340;&#27880;&#37322;&#12290;&#21033;&#29992;&#36825;&#20010;&#20248;&#21183;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21322;&#30417;&#30563;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. As a popular approach, probabilistic rotation modeling additionally carries prediction uncertainty information, compared to single-prediction rotation regression. For modeling probabilistic distribution over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix Fisher, however they are shown to be sensitive to outlier predictions, e.g. $180^\circ$ error and thus are unlikely to converge with optimal performance. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region that it can improve. In addition, we show that our method also exhibits robustness to small noises and thus tolerates imperfect annotations. With this benefit, we demonstrate its advantages in semi-supervised r
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10449</link><description>&lt;p&gt;
&#21512;&#20316;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#12290; &#65288;arXiv:2305.10449v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Cooperation Is All You Need. (arXiv:2305.10449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10449
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#36234;&#8220;&#26641;&#31361;&#27665;&#20027;&#8221;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Cooperator&#30340;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;Transformers&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;ChatGPT&#65289;&#22312;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#21151;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290; Transformers&#22522;&#20110;&#38271;&#26399;&#20197;&#26469;&#30340;&#8220;&#31215;&#20998;-&#21457;&#23556;&#8221;&#8220;&#28857;&#8221;&#31070;&#32463;&#20803;&#30340;&#27010;&#24565;&#65292;&#32780;Cooperator&#21017;&#21463;&#21040;&#26368;&#36817;&#31070;&#32463;&#29983;&#29289;&#23398;&#31361;&#30772;&#30340;&#21551;&#31034;&#65292;&#36825;&#20123;&#31361;&#30772;&#34920;&#26126;&#65292;&#31934;&#31070;&#29983;&#27963;&#30340;&#32454;&#32990;&#22522;&#30784;&#21462;&#20915;&#20110;&#26032;&#30382;&#23618;&#20013;&#20855;&#26377;&#20004;&#20010;&#21151;&#33021;&#19978;&#19981;&#21516;&#28857;&#30340;&#19978;&#30382;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29992;&#20110;RL&#26102;&#65292;&#22522;&#20110;Cooperator&#30340;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#27604;&#22522;&#20110;Transformer&#30340;&#31639;&#27861;&#24555;&#24471;&#22810;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#37319;&#29992;&#36328;&#19977;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#25513;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32467;&#26500;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36755;&#20986;&#26684;&#24335;&#12290;&#27169;&#22411;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#21516;&#26102;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#32467;&#21512;&#20998;&#35299;&#27880;&#24847;&#21147;&#21644;&#27169;&#24577;&#19987;&#23478;&#32452;&#21512;&#31574;&#30053;&#20197;&#25552;&#39640;&#20449;&#24687;&#25429;&#33719;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10448</link><description>&lt;p&gt;
&#38754;&#21521;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#32479;&#19968;&#27169;&#24577;&#25513;&#30721;&#24207;&#21015;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding. (arXiv:2305.10448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#37319;&#29992;&#36328;&#19977;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#25513;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32467;&#26500;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36755;&#20986;&#26684;&#24335;&#12290;&#27169;&#22411;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#21516;&#26102;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#32467;&#21512;&#20998;&#35299;&#27880;&#24847;&#21147;&#21644;&#27169;&#24577;&#19987;&#23478;&#32452;&#21512;&#31574;&#30053;&#20197;&#25552;&#39640;&#20449;&#24687;&#25429;&#33719;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenDoc&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#65292;&#20351;&#29992;&#36328;&#19977;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#25513;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#65306;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#19982;&#25991;&#26723;&#29702;&#35299;&#20013;&#24120;&#29992;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#21508;&#31181;&#20135;&#29983;&#19981;&#21516;&#36755;&#20986;&#26684;&#24335;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19981;&#20165;&#21253;&#25324;&#20197;&#24448;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#20256;&#32479;&#25991;&#26412;&#22635;&#20805;&#20219;&#21153;&#65292;&#36824;&#21253;&#25324;&#23631;&#34109;&#30340;&#22270;&#20687;&#20196;&#29260;&#39044;&#27979;&#21644;&#23631;&#34109;&#30340;&#24067;&#23616;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#27169;&#24577;&#29305;&#23450;&#30340;&#25351;&#23548;&#21644;&#37319;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#21644;&#27169;&#24577;&#19987;&#23478;&#32452;&#21512;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#27599;&#31181;&#27169;&#24577;&#25152;&#21033;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents GenDoc, a general sequence-to-sequence document understanding model pre-trained with unified masking across three modalities: text, image, and layout. The proposed model utilizes an encoder-decoder architecture, which allows for increased adaptability to a wide range of downstream tasks with diverse output formats, in contrast to the encoder-only models commonly employed in document understanding. In addition to the traditional text infilling task used in previous encoder-decoder models, our pre-training extends to include tasks of masked image token prediction and masked layout prediction. We also design modality-specific instruction and adopt both disentangled attention and the mixture-of-modality-experts strategy to effectively capture the information leveraged by each modality. Evaluation of the proposed model through extensive experiments on several downstream tasks in document understanding demonstrates its ability to achieve superior or competitive performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35780;&#20998;&#31995;&#32479;&#22312;&#39044;&#27979;&#20540;&#30340;&#21516;&#26102;&#23545;&#27491;&#30830;&#30340;&#20998;&#24067;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#32780;&#19981;&#29306;&#29298;&#20219;&#20309;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10447</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35780;&#20998;&#20013;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring. (arXiv:2305.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35780;&#20998;&#31995;&#32479;&#22312;&#39044;&#27979;&#20540;&#30340;&#21516;&#26102;&#23545;&#27491;&#30830;&#30340;&#20998;&#24067;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#32780;&#19981;&#29306;&#29298;&#20219;&#20309;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#21035;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#20026;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#35768;&#22810;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#27169;&#22411;&#65292;&#24403;&#27169;&#22411;&#21482;&#39044;&#27979;&#35757;&#32451;&#25968;&#25454;&#30340;&#24179;&#22343;&#20540;&#26102;&#65292;&#21487;&#33021;&#20250;&#23481;&#26131;&#20986;&#29616;&#27424;&#25311;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#20026;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#28608;&#21169;&#65292;&#20351;&#20854;&#39044;&#27979;&#27491;&#30830;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#39044;&#27979;&#27491;&#30830;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19981;&#38477;&#20302;&#20219;&#20309;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;Automated Student Assessment Prize Automated Essay Scoring&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.752&#30340;&#20108;&#27425;&#21152;&#26435;kappa&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks and in particular the attention mechanism have brought significant advances to the field of Automated Essay Scoring. Many of these systems use a regression-based model which may be prone to underfitting when the model only predicts the mean of the training data. In this paper, we present a dynamic loss function that creates an incentive for the model to predict with the correct distribution, as well as predicting the correct values. Our loss function achieves this goal without sacrificing any performance achieving a Quadratic Weighted Kappa score of 0.752 on the Automated Student Assessment Prize Automated Essay Scoring dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#24773;&#32490;&#35843;&#33410;&#25351;&#23548;&#30340;&#21465;&#36848;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27861;&#35821;&#24773;&#24863;&#21465;&#36848;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#20102;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65288;&#34892;&#20026;&#12289;&#24863;&#35273;&#12289;&#24605;&#32771;&#21644;&#39046;&#22495;&#65289;&#23545;&#24773;&#24863;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20849;&#21516;&#32771;&#34385;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10446</link><description>&lt;p&gt;
&#22522;&#20110;&#24515;&#29702;&#32452;&#25104;&#37096;&#20998;&#30340;&#24773;&#24863;&#35782;&#21035;&#8212;&#8212;&#22522;&#20110;&#24773;&#32490;&#35843;&#33410;&#25351;&#23548;&#21465;&#36848;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation. (arXiv:2305.10446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#24773;&#32490;&#35843;&#33410;&#25351;&#23548;&#30340;&#21465;&#36848;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27861;&#35821;&#24773;&#24863;&#21465;&#36848;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#20102;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65288;&#34892;&#20026;&#12289;&#24863;&#35273;&#12289;&#24605;&#32771;&#21644;&#39046;&#22495;&#65289;&#23545;&#24773;&#24863;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20849;&#21516;&#32771;&#34385;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35843;&#33410;&#26159;&#22788;&#29702;&#24773;&#24863;&#20107;&#20214;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#24182;&#23545;&#24515;&#29702;&#20581;&#24247;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#24773;&#24863;&#21465;&#36848;&#35821;&#26009;&#24211;&#65292;&#35813;&#35821;&#26009;&#24211;&#26159;&#20351;&#29992;&#24773;&#32490;&#35843;&#33410;&#38382;&#21367;&#25910;&#38598;&#32780;&#26469;&#30340;&#65292;&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20107;&#20214;&#29702;&#35299;&#12290;&#25105;&#20204;&#36981;&#24490;&#32452;&#25104;&#36807;&#31243;&#27169;&#22411;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#24773;&#32490;&#35270;&#20026;&#30001;&#22235;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#32452;&#25104;&#37096;&#20998;&#65288;&#34892;&#20026;&#12289;&#24863;&#35273;&#12289;&#24605;&#32771;&#21644;&#39046;&#22495;&#65289;&#32452;&#25104;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#27599;&#20010;&#21465;&#36848;&#37117;&#19982;&#19968;&#31181;&#31163;&#25955;&#24773;&#24863;&#30456;&#20851;&#65292;&#24182;&#26681;&#25454;&#20316;&#32773;&#30340;&#25152;&#26377;&#24773;&#24863;&#32452;&#25104;&#37096;&#20998;&#26500;&#24314;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#21450;&#20854;&#23545;&#24773;&#24863;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#37117;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#20849;&#21516;&#32771;&#34385;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion regulation is a crucial element in dealing with emotional events and has positive effects on mental health. This paper aims to provide a more comprehensive understanding of emotional events by introducing a new French corpus of emotional narratives collected using a questionnaire for emotion regulation. We follow the theoretical framework of the Component Process Model which considers emotions as dynamic processes composed of four interrelated components (behavior, feeling, thinking and territory). Each narrative is related to a discrete emotion and is structured based on all emotion components by the writers. We study the interaction of components and their impact on emotion classification with machine learning methods and pre-trained language models. Our results show that each component improves prediction performance, and that the best results are achieved by jointly considering all components. Our results also show the effectiveness of pre-trained language models in predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10445</link><description>&lt;p&gt;
&#35760;&#24518;&#26377;&#30410;&#65306;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#20197;&#35760;&#24518;&#21644;&#32972;&#35829;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#31181;&#35760;&#24518;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#19981;&#33391;&#23646;&#24615;&#65292;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#21644;&#20449;&#24687;&#27844;&#28431;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#35760;&#24518;&#35270;&#20026;LM&#30340;&#19968;&#31181;&#26410;&#24320;&#21457;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#33258;&#22238;&#24402;LM&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#12290;&#34429;&#28982;SELM&#19981;&#26131;&#21463;&#20256;&#32479;&#21152;&#23494;&#20998;&#26512;&#26041;&#27861;&#25915;&#30772;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#35777;&#21464;&#20307;&#65292;&#30740;&#31350;&#23427;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/OSU-NLP-Group/SELM &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterized neural language models (LMs) can memorize and recite long sequences of training data. While such memorization is normally associated with undesired properties such as overfitting and information leaking, our work casts memorization as an unexplored capability of LMs. We propose the first symmetric encryption algorithm with autoregressive language models (SELM). We show that autoregressive LMs can encode arbitrary data into a compact real-valued vector (i.e., encryption) and then losslessly decode the vector to the original message (i.e., decryption) via random subspace optimization and greedy decoding. While SELM is not amenable to conventional cryptanalysis, we investigate its security through a novel empirical variant of the classic IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and datasets are available at https://github.com/OSU-NLP-Group/SELM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#27169;&#22411;SuperDriver AI&#65292;&#22312;&#20445;&#35777;&#36947;&#36335;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#30340;&#23454;&#38469;&#34920;&#29616;&#22312;&#26085;&#26412;&#19996;&#20140;&#30340;&#29305;&#23450;&#39550;&#39542;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#39044;&#26399;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10443</link><description>&lt;p&gt;
SuperDriverAI: &#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#35774;&#35745;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
SuperDriverAI: Towards Design and Implementation for End-to-End Learning-based Autonomous Driving. (arXiv:2305.10443v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#27169;&#22411;SuperDriver AI&#65292;&#22312;&#20445;&#35777;&#36947;&#36335;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#30340;&#23454;&#38469;&#34920;&#29616;&#22312;&#26085;&#26412;&#19996;&#20140;&#30340;&#29305;&#23450;&#39550;&#39542;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#39044;&#26399;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#19988;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21608;&#22260;&#20154;&#31867;&#39550;&#39542;&#21592;&#21644;&#34892;&#20154;&#36896;&#25104;&#30340;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#31181;&#33258;&#20027;&#39550;&#39542;&#23578;&#26410;&#22312;&#20844;&#20849;&#36947;&#36335;&#19978;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperDriver AI&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65292;&#20854;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20174;&#26377;&#32463;&#39564;&#30340;&#20154;&#31867;&#39550;&#39542;&#21592;&#20013;&#23398;&#20064;&#39550;&#39542;&#21160;&#20316;&#21644;&#31574;&#30053;&#65292;&#24182;&#30830;&#23450;&#35201;&#37319;&#21462;&#30340;&#39550;&#39542;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#35777;&#36947;&#36335;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35010;&#27169;&#22411;&#21644;&#35270;&#35273;&#27880;&#24847;&#27169;&#22359;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#30495;&#23454;&#30828;&#20214;&#30340;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;&#21644;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;SuperDriver AI&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26085;&#26412;&#19996;&#20140;&#30340;&#19968;&#20010;&#39550;&#39542;&#22330;&#26223;&#20013;&#25910;&#38598;&#20102;150&#27425;&#27979;&#35797;&#65292;&#24182;&#28436;&#31034;&#20102;SuperDriver AI&#19982;&#30495;&#23454;&#36710;&#36742;&#30340;&#37197;&#21512;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully autonomous driving has been widely studied and is becoming increasingly feasible. However, such autonomous driving has yet to be achieved on public roads, because of various uncertainties due to surrounding human drivers and pedestrians. In this paper, we present an end-to-end learningbased autonomous driving system named SuperDriver AI, where Deep Neural Networks (DNNs) learn the driving actions and policies from the experienced human drivers and determine the driving maneuvers to take while guaranteeing road safety. In addition, to improve robustness and interpretability, we present a slit model and a visual attention module. We build a datacollection system and emulator with real-world hardware, and we also test the SuperDriver AI system with real-world driving scenarios. Finally, we have collected 150 runs for one driving scenario in Tokyo, Japan, and have shown the demonstration of SuperDriver AI with the real-world vehicle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#36335;&#30001;&#31639;&#27861;&#65288;DRL-PPONSA&#65289;&#65292;&#22522;&#20110;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#32593;&#32476;&#26550;&#26500;&#19979;&#12289;&#20855;&#26377;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#30340;&#36817;&#20284;&#31574;&#30053;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#24179;&#38754;&#21644;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#25968;&#25454;&#36716;&#21457;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33719;&#21462;&#32593;&#32476;&#29366;&#24577;&#20449;&#24687;&#12289;&#28789;&#27963;&#36716;&#21457;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#26381;&#21153;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10441</link><description>&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;SDWN&#36335;&#30001;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Intelligent SDWN Routing Algorithm Based on Network Situational Awareness and Deep Reinforcement Learning. (arXiv:2305.10441v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#36335;&#30001;&#31639;&#27861;&#65288;DRL-PPONSA&#65289;&#65292;&#22522;&#20110;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#32593;&#32476;&#26550;&#26500;&#19979;&#12289;&#20855;&#26377;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#30340;&#36817;&#20284;&#31574;&#30053;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#24179;&#38754;&#21644;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#25968;&#25454;&#36716;&#21457;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33719;&#21462;&#32593;&#32476;&#29366;&#24577;&#20449;&#24687;&#12289;&#28789;&#27963;&#36716;&#21457;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#26381;&#21153;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26080;&#32447;&#32593;&#32476;&#25299;&#25169;&#39640;&#24230;&#21160;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#39640;&#25928;&#33719;&#21462;&#32593;&#32476;&#29366;&#24577;&#20449;&#24687;&#12289;&#28789;&#27963;&#36716;&#21457;&#25968;&#25454;&#20197;&#25552;&#39640;&#36890;&#20449;&#26381;&#21153;&#36136;&#37327;&#25104;&#20026;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#32593;&#32476;&#26550;&#26500;&#19979;&#12289;&#20855;&#26377;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#30340;&#36817;&#20284;&#31574;&#30053;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#36335;&#30001;&#31639;&#27861;&#65288;DRL-PPONSA&#65289;&#12290;&#39318;&#20808;&#65292;&#20026;&#32593;&#32476;&#25299;&#25169;&#26500;&#24314;&#21644;&#25968;&#25454;&#36716;&#21457;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#24179;&#38754;&#12290;&#25511;&#21046;&#24179;&#38754;&#25910;&#38598;&#32593;&#32476;&#27969;&#37327;&#20449;&#24687;&#65292;&#21457;&#36865;&#27969;&#37327;&#34920;&#65292;&#24182;&#20351;&#29992;GCN-GRU&#39044;&#27979;&#26426;&#21046;&#24863;&#30693;&#26410;&#26469;&#27969;&#37327;&#21464;&#21270;&#36235;&#21183;&#65292;&#20197;&#23454;&#29616;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#12290;&#20854;&#27425;&#65292;&#22312;&#30693;&#35782;&#24179;&#38754;&#20013;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#30340;&#25968;&#25454;&#36716;&#21457;&#26426;&#21046;&#65292;&#39044;&#27979;&#30340;&#32593;&#32476;&#27969;&#37327;&#30697;&#38453;&#21644;&#25299;&#25169;&#20449;&#24687;&#30697;&#38453;&#34987;&#35270;&#20026;DRL&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#32780;&#19979;&#19968;&#36339;&#30456;&#37051;&#33410;&#28857;&#21017;&#34987;&#35270;&#20026;&#21487;&#25191;&#34892;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the highly dynamic changes in wireless network topologies, efficiently obtaining network status information and flexibly forwarding data to improve communication quality of service are important challenges. This article introduces an intelligent routing algorithm (DRL-PPONSA) based on proximal policy optimization deep reinforcement learning with network situational awareness under a software-defined wireless networking architecture. First, a specific data plane is designed for network topology construction and data forwarding. The control plane collects network traffic information, sends flow tables, and uses a GCN-GRU prediction mechanism to perceive future traffic change trends to achieve network situational awareness. Second, a DRL-based data forwarding mechanism is designed in the knowledge plane. The predicted network traffic matrix and topology information matrix are treated as the environment for DRL agents, while next-hop adjacent nodes are treated as executable actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32452;&#25773;&#36335;&#30001;&#26041;&#27861;&#65292;&#22312;SDWN&#29615;&#22659;&#19979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#25773;&#36335;&#30001;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;QoS&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10440</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;SDWN&#26234;&#33021;&#32452;&#25773;&#36335;&#30001;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intelligent multicast routing method based on multi-agent deep reinforcement learning in SDWN. (arXiv:2305.10440v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32452;&#25773;&#36335;&#30001;&#26041;&#27861;&#65292;&#22312;SDWN&#29615;&#22659;&#19979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#25773;&#36335;&#30001;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;QoS&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25773;&#36890;&#20449;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39640;&#35774;&#22791;&#23494;&#24230;&#30340;&#26080;&#32447;&#29615;&#22659;&#20013;&#12290;&#20256;&#32479;&#26080;&#32447;&#32593;&#32476;&#26550;&#26500;&#38590;&#20197;&#28789;&#27963;&#22320;&#33719;&#21462;&#21644;&#32500;&#25252;&#20840;&#23616;&#32593;&#32476;&#29366;&#24577;&#20449;&#24687;&#65292;&#24182;&#19988;&#19981;&#33021;&#24555;&#36895;&#21709;&#24212;&#32593;&#32476;&#29366;&#24577;&#21464;&#21270;&#65292;&#20174;&#32780;&#24433;&#21709;&#29616;&#26377;&#22810;&#25773;&#35299;&#20915;&#26041;&#26696;&#30340;&#21534;&#21520;&#37327;&#12289;&#24310;&#36831;&#21644;&#20854;&#20182;QoS&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#32593;&#32476;&#65288;SDWN&#65289;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL-MR&#65289;&#30340;&#26032;&#22411;&#32452;&#25773;&#36335;&#30001;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;SDWN&#25216;&#26415;&#65292;&#28789;&#27963;&#37197;&#32622;&#32593;&#32476;&#65292;&#24182;&#20197;&#27969;&#37327;&#30697;&#38453;&#30340;&#24418;&#24335;&#33719;&#21462;&#32593;&#32476;&#29366;&#24577;&#20449;&#24687;&#65292;&#34920;&#31034;&#20840;&#23616;&#32593;&#32476;&#38142;&#36335;&#20449;&#24687;&#65292;&#22914;&#38142;&#36335;&#24102;&#23485;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#12290;&#20854;&#27425;&#65292;&#23558;&#32452;&#25773;&#36335;&#30001;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#35299;&#20915;&#12290;&#20026;&#20102;&#20351;&#27599;&#20010;&#20195;&#29702;&#33021;&#22815;&#20934;&#30830;&#20102;&#35299;&#24403;&#21069;&#32593;&#32476;&#29366;&#24577;&#21644;&#23398;&#20064;&#26412;&#22320;&#31574;&#30053;&#65292;&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#31639;&#27861;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#20223;&#30495;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#25773;&#36335;&#30001;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;QoS&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multicast communication technology is widely applied in wireless environments with a high device density. Traditional wireless network architectures have difficulty flexibly obtaining and maintaining global network state information and cannot quickly respond to network state changes, thus affecting the throughput, delay, and other QoS requirements of existing multicasting solutions. Therefore, this paper proposes a new multicast routing method based on multiagent deep reinforcement learning (MADRL-MR) in a software-defined wireless networking (SDWN) environment. First, SDWN technology is adopted to flexibly configure the network and obtain network state information in the form of traffic matrices representing global network links information, such as link bandwidth, delay, and packet loss rate. Second, the multicast routing problem is divided into multiple subproblems, which are solved through multiagent cooperation. To enable each agent to accurately understand the current network st
&lt;/p&gt;</description></item><item><title>IMAGINATOR&#26159;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#21333;&#35789;&#32423;&#21035;&#22270;&#20687;&#26412;&#20307;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;+&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#65292;&#33021;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#32534;&#30721;&#20026;&#30690;&#37327;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.10438</link><description>&lt;p&gt;
IMAGINATOR&#65306;&#20351;&#29992;&#22522;&#20110;&#21333;&#35789;&#32423;&#21035;&#22270;&#20687;&#26412;&#20307;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;+&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10438
&lt;/p&gt;
&lt;p&gt;
IMAGINATOR&#26159;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#21333;&#35789;&#32423;&#21035;&#22270;&#20687;&#26412;&#20307;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;+&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#65292;&#33021;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#32534;&#30721;&#20026;&#30690;&#37327;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#23884;&#20837;&#26159;&#19968;&#31181;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#21521;&#37327;&#34920;&#31034;&#65292;&#20027;&#35201;&#21463;&#21040;&#20998;&#24067;&#20551;&#35774;&#8220;&#20320;&#24212;&#35813;&#36890;&#36807;&#23427;&#30340;&#20276;&#20387;&#26469;&#35748;&#35782;&#19968;&#20010;&#21333;&#35789;&#8221;&#65288;Harris&#65292;1954&#65289;&#30340;&#24433;&#21709;&#65292;&#32780;&#29616;&#20195;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21017;&#20381;&#36182;&#20110;&#35774;&#35745;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;IMAGINATOR&#30340;&#39044;&#35757;&#32451;&#32852;&#21512;&#23884;&#20837;&#65288;JE&#65289;&#65292;&#23427;&#26159;&#22312;1M&#20010;&#22270;&#20687;+&#25991;&#26412;&#23545;&#20013;&#20174;21K&#20010;&#19981;&#21516;&#30340;&#22270;&#20687;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;JE&#26159;&#19968;&#31181;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#32534;&#30721;&#20026;&#30690;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#22522;&#30784;&#20851;&#38190;&#35789;&#65292;&#32780;&#34917;&#20805;&#27169;&#24577;&#65288;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20026;&#22270;&#20687;&#65289;&#21017;&#19982;&#20043;&#30456;&#36830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embeddings, i.e., semantically meaningful vector representation of words, are largely influenced by the distributional hypothesis "You shall know a word by the company it keeps" (Harris, 1954), whereas modern prediction-based neural network embeddings rely on design choices and hyperparameter optimization. Word embeddings like Word2Vec, GloVe etc. well capture the contextuality and real-world analogies but contemporary convolution-based image embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge. The popular king-queen analogy does not hold true for most commonly used vision embeddings.  In this paper, we introduce a pre-trained joint embedding (JE), named IMAGINATOR, trained on 21K distinct image objects level from 1M image+text pairs. JE is a way to encode multimodal data into a vector space where the text modality serves as the ground-ing key, which the complementary modality (in this case, the image) is anchored with. IMAGINATOR encapsulates three indivi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#35268;&#33539;&#30340;&#29289;&#32852;&#32593;&#27169;&#22411;&#26469;&#37096;&#32626;&#26377;&#25928;&#30340;&#29289;&#32852;&#32593;&#26550;&#26500;&#65292;&#21253;&#25324;&#23558;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26356;&#38752;&#36817;&#25968;&#25454;&#28304;&#30340;&#26032;&#26550;&#26500;&#38654;&#35745;&#31639;&#21644;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#22312;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2305.10437</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#24102;&#21040;&#36793;&#32536;&#65306;&#37096;&#32626;&#26377;&#25928;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#27491;&#24335;&#24314;&#27169;&#19982;&#35268;&#33539; (arXiv:2305.10437v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
Bringing AI to the edge: A formal M&amp;S specification to deploy effective IoT architectures. (arXiv:2305.10437v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#35268;&#33539;&#30340;&#29289;&#32852;&#32593;&#27169;&#22411;&#26469;&#37096;&#32626;&#26377;&#25928;&#30340;&#29289;&#32852;&#32593;&#26550;&#26500;&#65292;&#21253;&#25324;&#23558;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26356;&#38752;&#36817;&#25968;&#25454;&#28304;&#30340;&#26032;&#26550;&#26500;&#38654;&#35745;&#31639;&#21644;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#22312;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#27491;&#22312;&#25913;&#21464;&#25105;&#20204;&#30340;&#31038;&#20250;&#65292;&#25552;&#20379;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#21644;&#36164;&#28304;&#31649;&#29702;&#30340;&#26032;&#26381;&#21153;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#22522;&#20110;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#21644;&#30005;&#28304;&#30340;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#26222;&#36941;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#26102;&#25910;&#38598;&#21644;&#23384;&#20648;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#36991;&#20813;&#32593;&#32476;&#39281;&#21644;&#21644;&#39640;&#24310;&#36831;&#65292;&#26032;&#30340;&#26550;&#26500;&#22914;&#38654;&#35745;&#31639;&#27491;&#22312;&#20986;&#29616;&#65292;&#23558;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26356;&#38752;&#36817;&#25968;&#25454;&#28304;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#26032;&#30340;&#25968;&#25454;&#20013;&#24515;&#65292;&#22312;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#23454;&#26102;&#22823;&#25968;&#25454;&#21644;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#65292;&#38656;&#35201;&#32771;&#34385;&#33021;&#28304;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#22312;&#20154;&#31867;&#27963;&#21160;&#21306;&#22495;&#21487;&#25345;&#32493;&#19988;&#26377;&#25928;&#22320;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#35268;&#33539;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#27169;&#22411;&#21270;&#31995;&#32479;&#24037;&#31243;&#30340;&#21407;&#21017;&#30340;&#29289;&#32852;&#32593;&#27169;&#22411;&#12290;&#25152;&#25552;&#20379;&#30340;&#25968;&#23398;&#35268;&#33539;&#35206;&#30422;&#20102;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things is transforming our society, providing new services that improve the quality of life and resource management. These applications are based on ubiquitous networks of multiple distributed devices, with limited computing resources and power, capable of collecting and storing data from heterogeneous sources in real-time. To avoid network saturation and high delays, new architectures such as fog computing are emerging to bring computing infrastructure closer to data sources. Additionally, new data centers are needed to provide real-time Big Data and data analytics capabilities at the edge of the network, where energy efficiency needs to be considered to ensure a sustainable and effective deployment in areas of human activity. In this research, we present an IoT model based on the principles of Model-Based Systems Engineering defined using the Discrete Event System Specification formalism. The provided mathematical formalism covers the description of the entire archite
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10435</link><description>&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65306;&#21551;&#29992;&#25216;&#26415;&#12289;&#28508;&#22312;&#24212;&#29992;&#12289;&#26032;&#20852;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10435
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#23558;&#25105;&#20204;&#25512;&#21521;&#24320;&#21457;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#21644;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#20132;&#27969;&#30340;&#26426;&#22120;&#12290;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22312;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#31038;&#21306;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#30693;&#21517;&#24230;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21450;&#30456;&#20851;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#36825;&#20419;&#20351;&#36827;&#34892;&#20102;&#26412;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65292;&#21253;&#25324;&#20854;&#26550;&#26500;&#12289;&#24037;&#20316;&#36807;&#31243;&#12289;&#35757;&#32451;&#36807;&#31243;&#12289;&#21551;&#29992;&#25216;&#26415;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#35813;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Generative Pre-trained Transformer models represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. Generative Pre-trained Transformer models are based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, Generative Pre-trained Transformer models have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10434</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#20250;&#22312;&#20154;&#20204;&#30340;&#33041;&#28023;&#20013;&#21576;&#29616;&#22270;&#20687;&#65292;&#32780;&#38750;&#35270;&#35273;&#25991;&#26412;&#21017;&#26080;&#27861;&#36798;&#21040;&#27492;&#25928;&#26524;&#12290;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#23558;&#26377;&#21161;&#20110;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3620&#20010;&#33521;&#35821;&#21477;&#23376;&#21450;&#20854;&#22810;&#20010;&#20154;&#31867;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#35270;&#35273;&#24615;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#25991;&#26412;&#21644;&#35270;&#35273;&#36164;&#20135;&#30340;&#25991;&#26723;&#26469;&#21019;&#24314;&#36828;&#31243;&#30417;&#30563;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
&lt;/p&gt;</description></item><item><title>MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10250</link><description>&lt;p&gt;
MemoryBank: &#29992;&#38271;&#26399;&#35760;&#24518;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10250
&lt;/p&gt;
&lt;p&gt;
MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#19981;&#36275;&#20043;&#22788;&#26159;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#36825;&#22312;&#38656;&#35201;&#25345;&#32493;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20363;&#22914;&#20010;&#20154;&#20276;&#20387;&#31995;&#32479;&#21644;&#24515;&#29702;&#21672;&#35810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoryBank&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;LLM&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#12290;MemoryBank&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#24182;&#26377;&#36873;&#25321;&#22320;&#20445;&#23384;&#35760;&#24518;&#65292;MemoryBank&#37319;&#29992;&#20102;&#21463;Ebbinghaus&#36951;&#24536;&#26354;&#32447;&#29702;&#35770;&#21551;&#21457;&#30340;&#35760;&#24518;&#26356;&#26032;&#26426;&#21046;&#65292;&#36825;&#26679;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#21644;&#35760;&#24518;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#36951;&#24536;&#21644;&#21152;&#24378;&#35760;&#24518;&#65292;&#20174;&#32780;&#20026;LLM&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.10089</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20013;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#27169;&#20223;&#19987;&#23478;&#30340;&#22870;&#21169;&#20540;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35789;&#20856;&#24207;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35753;&#23398;&#20064;&#32773;&#30340;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.09860</link><description>&lt;p&gt;
Epsilon Sampling Rocks: &#30740;&#31350;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#24050;&#32463;&#26174;&#31034;&#20986;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26102;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;MBR&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20363;&#22914;&#31062;&#20808;&#37319;&#26679;&#65292;&#26680;&#37319;&#26679;&#21644;top-k&#37319;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#23427;&#20204;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;epsilon&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25152;&#26377;&#23567;&#20110;epsilon&#30340;&#26631;&#35760;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#33719;&#24471;&#20844;&#24179;&#30340;&#27010;&#29575;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;epsilon&#37319;&#26679;&#30340;MBR&#35299;&#30721;&#26174;&#33879;&#20248;&#20110;&#19981;&#20165;&#26159;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#32780;&#19988;&#36824;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;MBR&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.09557</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#23398;&#20064;&#65306;&#31934;&#36873;&#21253;&#19982;&#38543;&#26426;&#21253;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#37096;&#32626;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#36825;&#20123;&#31995;&#32479;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#32858;&#21512;&#30340;&#24418;&#24335;&#25910;&#38598;&#21644;&#21457;&#24067;&#25968;&#25454;&#26631;&#31614;&#65292;&#20174;&#32780;&#21487;&#20197;&#23558;&#21333;&#20010;&#29992;&#25143;&#30340;&#20449;&#24687;&#19982;&#20854;&#20182;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#21512;&#25968;&#25454;&#26631;&#31614;&#32780;&#38750;&#21333;&#20010;&#26631;&#31614;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#12290;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#65306;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#20043;&#21644;&#21487;&#20197;&#34920;&#31034;&#20026;&#27599;&#20010;&#21253;&#30340;&#26799;&#24230;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#21253;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
&lt;/p&gt;</description></item><item><title>LoViT&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#26469;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#20998;&#26512;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08989</link><description>&lt;p&gt;
LoViT: &#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
LoViT: Long Video Transformer for Surgical Phase Recognition. (arXiv:2305.08989v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08989
&lt;/p&gt;
&lt;p&gt;
LoViT&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#26469;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#20998;&#26512;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#33021;&#22815;&#37327;&#21270;&#34920;&#29616;&#24182;&#30417;&#30563;&#25163;&#26415;&#27969;&#31243;&#25191;&#34892;&#30340;&#19978;&#19979;&#25991;&#24037;&#20855;&#26041;&#38754;&#65292;&#22312;&#32447;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#22522;&#20110;&#24103;&#32423;&#30417;&#30563;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#30456;&#20284;&#24103;&#22312;&#19981;&#21516;&#38454;&#27573;&#20986;&#29616;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#19988;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32780;&#26410;&#33021;&#24456;&#22909;&#22320;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#25163;&#26415;&#24178;&#39044;&#20013;&#36890;&#24120;&#36935;&#21040;&#30340;&#38271;&#35270;&#39057;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoViT&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#34701;&#21512;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#20449;&#24687;&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#65292;&#21518;&#32773;&#30001;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#20004;&#20010;&#32423;&#32852;L-Trans&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;ProbSparse&#33258;&#27880;&#24847;&#21147;&#30340;G-Informer&#27169;&#22359;&#32452;&#25104;&#65292;&#29992;&#20110;&#22788;&#29702;&#20840;&#23616;&#26102;&#38388;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#22810;&#23610;&#24230;&#26102;&#38388;&#22836;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35782;&#21035;&#38271;&#35270;&#39057;&#20013;&#30340;&#25163;&#26415;&#38454;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LoViT&#22312;&#20004;&#20010;&#20844;&#20849;&#25163;&#26415;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT) for fusing short- and long-term temporal information that combines a temporally-rich spatial feature extractor and a multi-scale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then combines loc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08285</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65306;&#22522;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;LoRA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#30340;&#19981;&#26029;&#22686;&#38271;&#24341;&#36215;&#20102;&#23545;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312; MIMIC-IV-Note&#19978;&#30340;&#20004;&#20010;&#21307;&#30103;&#25253;&#21578;&#27010;&#36848;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#20844;&#20849;&#21307;&#30103;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#30340;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20943;&#23569;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#33258;&#30001;&#25991;&#26412;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36229;&#36807;92%&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of language models raises great research interests in parameter-efficient fine-tuning such as LoRA that freezes the pre-trained model, and injects small-scale trainable parameters for multiple downstream tasks (e.g., summarization, question answering and translation). To further enhance the efficiency of fine-tuning, we propose a framework that integrates LoRA and structured layer pruning. The integrated framework is validated on two created deidentified medical report summarization datasets based on MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6% parameters of the original model and pruning over 30% Transformer-layers, our framework can reduce 50% of GPU memory usage and speed up 100% of the training phase, while preserving over 92% generation qualities on free-text sequence-to-sequence tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08208</link><description>&lt;p&gt;
&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#19968;&#30452;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21463;&#21040;&#20102;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#37117;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#21487;&#20197;&#22686;&#24378;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1&#24471;&#20998;&#24179;&#22343;&#25552;&#39640;&#20102;4.5%-7.9%&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#24182;&#20026;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#36328;&#22495;&#38382;&#31572;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GitHub&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;*&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.08018</link><description>&lt;p&gt;
DRew&#65306;&#24102;&#24310;&#36831;&#30340;&#21160;&#24577;&#37325;&#36830;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#23384;&#22312;&#36807;&#24230;&#21387;&#32553;&#29616;&#35937;&#65292;&#23548;&#33268;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#21482;&#22312;&#33410;&#28857;&#30340;&#30456;&#37051;&#23621;&#20043;&#38388;&#36827;&#34892;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;&#35797;&#22270;&#20351;&#22270;&#24418;&#8220;&#26356;&#36830;&#36890;&#8221;&#24182;&#19988;&#26356;&#36866;&#21512;&#38271;&#31243;&#20219;&#21153;&#30340;&#37325;&#36830;&#26041;&#27861;&#36890;&#24120;&#20250;&#22833;&#21435;&#22522;&#20110;&#22270;&#24418;&#36317;&#31163;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20351;&#36828;&#31243;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#20013;&#31435;&#21363;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#26550;&#26500;&#65292;&#20197;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#37325;&#36830;&#65292;&#20197;&#30830;&#20445;&#36880;&#28176;&#21152;&#23494;&#22270;&#24418;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#23618;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#36317;&#31163;&#22312;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#36339;&#36291;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#31243;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#20854;&#20248;&#20110;&#22270;&#24418;&#21464;&#25442;&#22120;&#21644;&#22810;&#36339;MPNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07854</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#29305;&#24449;&#25552;&#21462;&#30340;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#38656;&#35201;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#24320;&#21457;&#20934;&#30830;&#21487;&#38752;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20005;&#26684;&#30340;&#25968;&#25454;&#38544;&#31169;&#27861;&#24459;&#21644;&#20016;&#23500;&#30340;&#36793;&#32536;&#24037;&#19994;&#25968;&#25454;&#38656;&#35201;&#20998;&#25955;&#24335;&#25968;&#25454;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#20998;&#25955;&#24335;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#30001;&#20110;&#19981;&#21516;&#30340;&#36864;&#21270;&#26426;&#21046;&#21644;&#19981;&#24179;&#31561;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#24320;&#21457;&#31934;&#24230;&#39640;&#30340;&#35757;&#32451;&#27169;&#22411;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#32479;&#35745;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;FL&#22312;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#21442;&#25968;&#32858;&#21512;&#31639;&#27861;&#30340; FL &#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.07031</link><description>&lt;p&gt;
&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hawkes&#36807;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#30340;&#24207;&#36143;&#20107;&#20214;&#21457;&#29983;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#25429;&#25417;&#36825;&#31181;&#22797;&#26434;&#30340;&#19981;&#35268;&#21017;&#21160;&#24577;&#65292;&#32780;&#19988;&#36824;&#20250;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#35745;&#31639;&#20107;&#20214;&#30340;&#23545;&#25968;&#20284;&#28982;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#22810;&#22522;&#20110;&#35774;&#35745;&#29992;&#20110;&#35268;&#21017;&#31163;&#25955;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;(CDE)&#30340;Hawkes&#36807;&#31243;&#27010;&#24565;&#65292;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#36830;&#32493;RNN&#30340;&#31070;&#32463;CDE&#25216;&#26415;&#12290;&#30001;&#20110;HP-CDE&#19981;&#26029;&#22320;&#35835;&#21462;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#23427;&#20204;&#30340;&#19981;&#22343;&#21248;&#26102;&#38388;&#31354;&#38388;&#65292;&#24182;&#19988;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Hawkes&#36807;&#31243;&#21644;&#31070;&#32463;CDE&#37117;&#26159;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#22495;&#20013;&#39318;&#20808;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#20855;&#26377;&#30456;&#20284;&#30340;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;HP-CDE&#20855;&#26377;&#36879;&#26126;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#22330;&#26223;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#65292;&#20854;&#20013;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#31038;&#20132;&#25193;&#25955;&#21644;&#22320;&#38663;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06137</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#31561;&#25928;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;WIRL&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#19982;&#25237;&#24433;&#27425;&#26799;&#24230;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;WIRL&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;&#26368;&#22823;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#23548;&#24341;&#25104;&#26412;&#23398;&#20064;&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#22270;&#20687;&#26816;&#32034;&#30340;&#36328;&#22495;&#21644;&#35821;&#20041;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#37325;&#26032;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#23545;&#40784;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#20197;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05144</link><description>&lt;p&gt;
&#25913;&#36827;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval. (arXiv:2305.05144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#22270;&#20687;&#26816;&#32034;&#30340;&#36328;&#22495;&#21644;&#35821;&#20041;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#37325;&#26032;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#23545;&#40784;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#20197;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#30340;&#22270;&#20687;&#26816;&#32034;(ZS-SBIR)&#30001;&#20110;&#33609;&#22270;&#21644;&#29031;&#29255;&#20043;&#38388;&#30340;&#36328;&#22495;&#26412;&#36136;&#20197;&#21450;&#24050;&#30693;&#21644;&#26410;&#30693;&#22270;&#20687;&#20998;&#24067;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#21508;&#31181;&#36741;&#21161;&#20449;&#24687;&#21644;&#23398;&#20064;&#31574;&#30053;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388; (\romannumeral 1)&#22312;&#33609;&#22270;&#21644;&#29031;&#29255;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#65292;(\romannumeral 2) &#26725;&#25509;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#22312;&#36866;&#24212;&#39046;&#22495;&#21644;&#20174;&#24050;&#30693;&#31867;&#21035;&#20256;&#36882;&#30693;&#35782;&#26041;&#38754;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#8220;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#8221;&#26041;&#27861;&#26469;&#35299;&#20915;&#20851;&#38190;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#65292;&#20197;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;(CLIP)&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#19982;&#27169;&#22411;&#26126;&#30830;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen image distributions. Previous methods fine-tune pre-trained models with various side information and learning strategies to learn a compact feature space that (\romannumeral1) is shared between the sketch and photo domains and (\romannumeral2) bridges seen and unseen classes. However, these efforts are inadequate in adapting domains and transferring knowledge from seen to unseen classes. In this paper, we present an effective \emph{``Adapt and Align''} approach to address the key challenges. Specifically, we insert simple and lightweight domain adapters to learn new abstract concepts of the sketch domain and improve cross-domain representation capabilities. Inspired by recent advances in image-text foundation models (\textit{e.g.}, CLIP) on zero-shot scenarios, we explicitly align the learned image embedding with a mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2305.05126</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#20869;&#26680;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#28041;&#21450;&#22312;&#21508;&#31181;&#31574;&#21010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32858;&#21512;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24230;&#37327;&#25351;&#26631;&#30340;&#36873;&#25321;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#29702;&#24819;&#24230;&#37327;&#19981;&#26126;&#26174;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#22270;&#29702;&#35770;&#65292;&#24182;&#20419;&#36827;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35825;&#23548;&#19968;&#32452;&#37197;&#22791;&#26377;&#19982;&#19968;&#20123;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#27169;&#22411;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;LLMs&#35013;&#22791;&#20449;&#24687;&#24341;&#23548;&#27169;&#22359;&#26469;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;LLMs&#30340;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#40657;&#30418;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04757</link><description>&lt;p&gt;
&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04757
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;LLMs&#35013;&#22791;&#20449;&#24687;&#24341;&#23548;&#27169;&#22359;&#26469;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;LLMs&#30340;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#40657;&#30418;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20197;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23545;&#30456;&#20851;&#25968;&#25454;&#30340;&#26377;&#38480;&#25509;&#35302;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340; LLM &#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#21482;&#33021;&#36890;&#36807; API &#35775;&#38382;, &#36825;&#38459;&#27490;&#20102;&#36827;&#19968;&#27493;&#29992;&#39046;&#22495;&#23450;&#21046;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#21521; LLM &#25152;&#26377;&#32773;&#25552;&#20379;&#31169;&#26377;&#25968;&#25454;&#20250;&#23548;&#33268;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#22411;&#30340;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548; (PKG) &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026; LLM &#37197;&#22791;&#20102;&#30693;&#35782;&#24341;&#23548;&#27169;&#22359;&#65292;&#20197;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464; LLM &#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340; PKG &#22522;&#20110;&#24320;&#28304;&#30340;&#8220;&#30333;&#30418;&#8221;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#31163;&#32447;&#23384;&#20648; LLM &#38656;&#35201;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340; PKG &#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#8220;&#40657;&#30418;&#8221;LLM&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source "white-box" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of "black-box" LLMs on a range o
&lt;/p&gt;</description></item><item><title>LLM2Loss&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35786;&#26029;&#65292;&#36890;&#36807;&#25552;&#21462;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25581;&#31034;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.03212</link><description>&lt;p&gt;
LLM2Loss: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics. (arXiv:2305.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03212
&lt;/p&gt;
&lt;p&gt;
LLM2Loss&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35786;&#26029;&#65292;&#36890;&#36807;&#25552;&#21462;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25581;&#31034;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#25277;&#35937;&#31354;&#38388;&#20013;&#23545;&#30456;&#24403;&#22797;&#26434;&#30340;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#20174;&#32780;&#25104;&#20026;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#26469;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#35273;&#22495;&#65292;&#20174;&#32780;&#21487;&#20197;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#27169;&#24335;&#30340;&#35821;&#20041;&#35265;&#35299;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#20219;&#21153;&#23450;&#20041;&#65292;&#25105;&#20204;&#39318;&#20808;&#35745;&#31639;&#20854;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#20219;&#21153;&#30456;&#20851;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#21462;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#26469;&#33258;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;CLIP&#23884;&#20837;&#65289;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#23558;&#36825;&#20010;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#20854;&#20219;&#21153;&#25439;&#22833;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#23545;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#28857;&#65292;&#32780;&#19988;&#21487;&#20197;&#35782;&#21035;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.  In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.02777</link><description>&lt;p&gt;
&#21508;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26681;&#25454;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;(&#20363;&#22914;&#65292;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;)&#30340;&#25968;&#25454;&#24320;&#21457;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#27599;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#23384;&#20648;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#24456;&#40635;&#28902;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#32763;&#35793;&#20219;&#21153;&#32479;&#19968;&#21040;&#26356;&#26222;&#36941;&#30340;&#35774;&#32622;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22810;&#25165;&#22810;&#33402;&#8221;&#30340;&#27169;&#22411;&#65292;&#21363;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#25968;&#25454;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;(NMT)&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#31181;&#29615;&#22659;&#19979;&#36827;&#34892;&#33391;&#22909;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23613;&#21487;&#33021;&#22810;&#22320;&#25193;&#23637;&#12290;&#36890;&#36807;&#32479;&#19968;&#23398;&#20064;&#65292;UMLNMT&#33021;&#22815;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#12290;&#22312;&#19971;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32763;&#35793;&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#32763;&#35793;&#12289;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;&#20013;&#65292;&#25105;&#20204;&#30340;UMLNMT&#30456;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01486</link><description>&lt;p&gt;
ARBEx&#65306;&#29992;&#20110;&#40065;&#26834;&#24615;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#30340;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#19982;&#21487;&#38752;&#24615;&#24179;&#34913;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#30001;Vision Transformer&#39537;&#21160;&#30340;&#26032;&#22411;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#24102;&#26377;&#21487;&#38752;&#24615;&#24179;&#34913;&#65292;&#20197;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36739;&#24046;&#31867;&#20998;&#24067;&#12289;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#31934;&#21270;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#31383;&#21475;&#30340;&#20132;&#21449;&#20851;&#27880;ViT&#26469;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#65292;&#21152;&#19978;&#26631;&#31614;&#20998;&#24067;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36890;&#36807;&#21487;&#38752;&#24615;&#24179;&#34913;&#20248;&#21270;&#23545;&#24369;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#26631;&#31614;&#39044;&#27979;&#38887;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#30340;&#26631;&#31614;&#20998;&#31867;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#25439;&#22833;&#65292;&#40723;&#21169;&#38170;&#28857;&#20043;&#38388;&#30340;&#22823;&#38388;&#38548;&#12290;&#21478;&#22806;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#20063;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#25552;&#21319;&#22312;FEL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ARBEx&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.00477</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#36739;&#20302;&#65306;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#26469;&#25214;&#21040;&#22909;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#30340;&#29615;&#22659;&#27169;&#22411;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#26159;&#36825;&#26679;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#30001;&#20110;&#20854;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#65288;PSDRL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#21487;&#25193;&#23637;&#30340;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#20854;&#22522;&#20110;&#27169;&#22411;&#30340;&#26412;&#36136;&#29305;&#24449;&#12290;PSDRL&#23558;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19978;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#22522;&#20110;&#20540;&#20989;&#25968;&#36924;&#36817;&#30340;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#23545;Atari&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;PSDRL&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12550</link><description>&lt;p&gt;
&#35757;&#32451;&#20013;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20581;&#22766;&#24615;&#30340;&#26377;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#22312;&#26356;&#19968;&#33324;&#30340;&#25200;&#21160;&#33539;&#22260;&#19979;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25506;&#32034;&#34920;&#26126;&#65292;&#23558;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163; (&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;) &#32467;&#21512;&#22312;&#35757;&#32451;&#20013;&#65292;&#22312;&#19968;&#20123;&#20856;&#22411;&#30340;&#23398;&#20064;&#22330;&#26223; (&#22914;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;) &#20013;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#31867;&#21035;&#38388;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.10770</link><description>&lt;p&gt;
DEIR: &#22522;&#20110;&#21306;&#20998;&#24615;&#27169;&#22411;&#30340;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#65292;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#65292;&#20854;&#26377;&#25928;&#24615;&#20851;&#38190;&#22320;&#24433;&#21709;&#30528;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38754;&#23545;&#31232;&#30095;&#30340;&#22806;&#37096;&#22870;&#21169;&#26102;&#26356;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#35266;&#27979;&#20013;&#20272;&#35745;&#26032;&#39062;&#24615;&#30340;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#26377;&#25928;&#40723;&#21169;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#33021;&#20250;&#24433;&#21709;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#27492;&#19968;&#20010;&#35266;&#27979;&#30340;&#26032;&#39062;&#24615;&#19982;&#25506;&#32034;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#20934;&#30830;&#20272;&#35745;&#25506;&#32034;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DEIR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20174;&#26465;&#20214;&#20114;&#20449;&#24687;&#39033;&#20013;&#29702;&#35770;&#19978;&#23548;&#20986;&#20869;&#22312;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#20027;&#35201;&#19982;&#20195;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#25152;&#36129;&#29486;&#30340;&#26032;&#39062;&#24615;&#25104;&#27604;&#20363;&#65292;&#24182;&#20511;&#21161;&#21306;&#20998;&#24615;&#30340;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;MiniGrid&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#26631;&#20934;&#21644;&#30828;&#26680;&#25506;&#32034;&#28216;&#25103;&#65292;&#22312;&#32467;&#26524;&#19978;DEIR&#27604;&#22522;&#32447;&#23398;&#20064;&#26356;&#24555;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#24212;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval
&lt;/p&gt;</description></item><item><title>H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04403</link><description>&lt;p&gt;
H2RBox-v2&#65306;&#36890;&#36807;&#23545;&#31216;&#23398;&#20064;&#25552;&#39640;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning. (arXiv:2304.04403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04403
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#31561;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#38656;&#27714;&#30340;&#26085;&#30410;&#22686;&#38271;&#65292;&#26377;&#21521;&#27880;&#37322;&#21464;&#24471;&#38750;&#24120;&#36153;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30340;&#27700;&#24179;&#27880;&#37322;&#25968;&#25454;&#38598;&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#26816;&#27979;&#22120;H2RBox&#65292;&#29992;&#20110;&#20174;&#27700;&#24179;&#26694;Box&#20013;&#23398;&#20064;&#26059;&#36716;&#26694;RBox&#65292;&#24182;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;H2RBox-v2&#30340;&#26032;&#29256;&#26412;&#65292;&#20197;&#36827;&#19968;&#27493;&#24357;&#21512;HBox&#30417;&#30563;&#21644;RBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21033;&#29992;&#32763;&#36716;&#21644;&#26059;&#36716;&#19968;&#33268;&#24615;&#26469;&#24320;&#21457;&#36724;&#23545;&#31216;&#24615;&#26159;&#21487;&#34892;&#30340;&#65292;H2RBox-v2&#21017;&#37319;&#29992;&#19982;H2RBox&#31867;&#20284;&#30340;&#24369;&#30417;&#30563;&#20998;&#25903;&#65292;&#24182;&#23884;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#20998;&#25903;&#65292;&#23427;&#21487;&#20197;&#20174;&#23545;&#35937;&#22270;&#20687;&#20013;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#26041;&#21521;&#12290;&#36890;&#36807;&#22788;&#29702;&#21608;&#36793;&#38382;&#39064;&#30340;&#27169;&#22359;&#65288;&#20363;&#22914;&#35282;&#21608;&#26399;&#24615;&#65289;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#31283;&#23450;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing demand for oriented object detection e.g. in autonomous driving and remote sensing, the oriented annotation has become a labor-intensive work. To make full use of existing horizontally annotated datasets and reduce the annotation cost, a weakly-supervised detector H2RBox for learning the rotated box (RBox) from the horizontal box (HBox) has been proposed and received great attention. This paper presents a new version, H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. While exploiting axisymmetry via flipping and rotating consistencies is available through our theoretical analysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, is embedded with a novel self-supervised branch that learns orientations from the symmetry inherent in the image of objects. Complemented by modules to cope with peripheral issues, e.g. angular periodicity, a stable and effective solution is achieved. To our knowledge, H
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17995</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#65306;&#22522;&#20110;&#29109;&#29305;&#24449;&#30340;&#33041;&#30005;&#20449;&#21495;&#21644;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#20998;&#31163;&#65292;&#29992;&#20110;NNetEn&#35745;&#31639;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation. (arXiv:2303.17995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#27979;&#37327;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#20256;&#32479;&#30340;&#29109;&#27979;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#39321;&#20892;&#29109;&#65292;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#65292;&#38656;&#35201;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;&#26469;&#34920;&#24449;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#27010;&#24565;&#26159;&#22522;&#20110;&#29305;&#27530;&#25968;&#25454;&#38598;(MNIST-10&#21644;SARS-CoV-2-RBV1)&#30340;&#20998;&#31867;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#35760;&#24405;&#22312;LogNNet&#31070;&#32463;&#32593;&#32476;&#20648;&#23618;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#29109;&#30456;&#20851;&#12290;NNetEn&#20197;&#21407;&#22987;&#26041;&#24335;&#20272;&#35745;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#22522;&#20110;NNetEn&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20998;&#31867;&#24230;&#37327;&#65306;R2&#25928;&#29575;&#21644;&#30382;&#23572;&#36874;&#25928;&#29575;&#12290;NNetEn&#30340;&#25928;&#29575;&#22312;&#20351;&#29992;&#31163;&#25955;&#20998;&#26512;(ANOVA)&#20998;&#31163;&#27491;&#24358;&#26144;&#23556;&#30340;&#20004;&#20010;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#24471;&#21040;&#39564;&#35777;&#12290;&#23545;&#20110;&#20004;&#20010;&#25509;&#36817;&#30340;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015; (r=1.1918&#21644;r=1.2243)&#65292;F&#27604;&#20540;&#36798;&#21040;&#20102;124&#30340;&#20540;&#65292;&#21453;&#26144;&#20102;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets (MNIST-10 and SARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the reservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics of time series in an original way. Based on the NNetEn algorithm, we propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis (ANOVA). For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#21270;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#65292;&#20801;&#35768;&#20195;&#29702;&#20154;&#20851;&#27880;&#19968;&#20123;&#21407;&#23376;&#20844;&#24335;&#30340;&#23376;&#38598;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#26080;&#27880;&#24847;&#30606;&#35270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.13494</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#65281;&#65288;&#19981;&#65289;&#19987;&#27880;&#20195;&#29702;&#20154;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attention! Dynamic Epistemic Logic Models of (In)attentive Agents. (arXiv:2303.13494v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#21270;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#65292;&#20801;&#35768;&#20195;&#29702;&#20154;&#20851;&#27880;&#19968;&#20123;&#21407;&#23376;&#20844;&#24335;&#30340;&#23376;&#38598;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#26080;&#27880;&#24847;&#30606;&#35270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#38480;&#21046;&#21644;&#36873;&#25321;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#65288;DEL&#65289;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#35201;&#20040;&#23436;&#20840;&#19987;&#27880;&#65292;&#35201;&#20040;&#23436;&#20840;&#19981;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#21270;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#20195;&#29702;&#20154;&#20851;&#27880;&#19968;&#20123;&#21407;&#23376;&#20844;&#24335;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21629;&#39064;&#27880;&#24847;&#21147;&#30340;&#30456;&#24212;&#36923;&#36753;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20844;&#29702;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#26080;&#27880;&#24847;&#30606;&#35270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the crucial cognitive ability that limits and selects what information we observe. Previous work by Bolander et al. (2016) proposes a model of attention based on dynamic epistemic logic (DEL) where agents are either fully attentive or not attentive at all. While introducing the realistic feature that inattentive agents believe nothing happens, the model does not represent the most essential aspect of attention: its selectivity. Here, we propose a generalization that allows for paying attention to subsets of atomic formulas. We introduce the corresponding logic for propositional attention, and show its axiomatization to be sound and complete. We then extend the framework to account for inattentive agents that, instead of assuming nothing happens, may default to a specific truth-value of what they failed to attend to (a sort of prior concerning the unattended atoms). This feature allows for a more cognitively plausible representation of the inattentional blindness phenomenon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.07551</link><description>&lt;p&gt;
&#21512;&#24182;&#20915;&#31574;Transformer&#65306;&#22810;&#20219;&#21153;&#31574;&#30053;&#24418;&#25104;&#30340;&#26435;&#37325;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#31574;&#30053;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#38598;&#20013;&#30340;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#21019;&#24314;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#12289;&#21333;&#29420;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#21017;&#36825;&#26679;&#20570;&#23601;&#27604;&#36739;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#25110;&#24179;&#22343;&#19981;&#21516;MuJoCo&#36816;&#21160;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;Decision Transformer&#30340;&#23376;&#38598;&#26469;&#36808;&#20986;&#36825;&#20010;&#26041;&#21521;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#24418;&#25104;&#27809;&#26377;&#38598;&#20013;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22312;&#21512;&#24182;&#31574;&#30053;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22914;&#26524;&#25152;&#26377;&#31574;&#30053;&#37117;&#20174;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#24182;&#22312;&#38382;&#39064;&#29305;&#23450;&#30340;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#38899;&#39057;&#20998;&#31867;&#25152;&#38656;&#30340;&#26631;&#27880;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05668</link><description>&lt;p&gt;
UNFUSED: &#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation. (arXiv:2303.05668v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#38899;&#39057;&#20998;&#31867;&#25152;&#38656;&#30340;&#26631;&#27880;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; UnFuSeD &#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#20943;&#23569;&#38899;&#39057;&#20998;&#31867;&#25152;&#38656;&#30340;&#26631;&#27880;&#25968;&#25454;&#37327;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#30452;&#25509;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#23545;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;&#20026;&#26080;&#30417;&#30563;&#24494;&#35843;&#20135;&#29983;&#20266;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#26410;&#26631;&#27880;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25552;&#21462;&#30340;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#36825;&#20123;&#20266;&#26631;&#31614;&#29992;&#20110;&#25351;&#23548;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#19978;&#30340;&#33258;&#33976;&#39311;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26080;&#30417;&#30563;&#24494;&#35843;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#32534;&#30721;&#22120;&#22312;&#30446;&#26631;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807; UnFuSeD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19982;&#25991;&#29486;&#20013;&#30340;&#36890;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#21363;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#26469;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce UnFuSeD, a novel approach to leverage self-supervised learning and reduce the need for large amounts of labeled data for audio classification. Unlike prior works, which directly fine-tune a self-supervised pre-trained encoder on a target dataset, we use the encoder to generate pseudo-labels for unsupervised fine-tuning before the actual fine-tuning step. We first train an encoder using a novel self-supervised learning algorithm (SSL) on an unlabeled audio dataset. Then, we use that encoder to generate pseudo-labels on our target task dataset via clustering the extracted representations. These pseudo-labels are then used to guide self-distillation on a randomly initialized model, which we call unsupervised fine-tuning. Finally, the resultant encoder is then fine-tuned on our target task dataset. Through UnFuSeD, we propose the first system that moves away from generic SSL paradigms in literature, which pre-train and fine-tune the same encoder, and present a n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#35748;&#35777;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#23545;&#20004;&#31181;&#24120;&#35265;&#30340;&#27745;&#26579;&#31867;&#22411;&#36827;&#34892;&#25269;&#25239;&#65292;&#24182;&#30830;&#20445;&#27867;&#21270;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02251</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35748;&#35777;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#65306;&#27867;&#21270;&#21644;&#25239;&#27745;&#26579;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robust Neural Networks: Generalization and Corruption Resistance. (arXiv:2303.02251v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#35748;&#35777;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#23545;&#20004;&#31181;&#24120;&#35265;&#30340;&#27745;&#26579;&#31867;&#22411;&#36827;&#34892;&#25269;&#25239;&#65292;&#24182;&#30830;&#20445;&#27867;&#21270;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#65288;&#23545;&#8220;&#27745;&#26579;&#8221;&#30340;&#25269;&#25239;&#33021;&#21147;&#65289;&#21487;&#33021;&#19982;&#27867;&#21270;&#23384;&#22312;&#30683;&#30462;&#12290;&#20363;&#22914;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#26088;&#22312;&#20943;&#23569;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#23545;&#23567;&#25968;&#25454;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36807;&#25311;&#21512;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#23613;&#31649;&#22312;&#26631;&#20934;&#35757;&#32451;&#20013;&#20960;&#20046;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#31181;&#22855;&#29305;&#30340;&#8220;&#40065;&#26834;&#36807;&#25311;&#21512;&#8221;&#29616;&#35937;&#30340;&#29702;&#35770;&#35777;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;&#35813;&#25439;&#22833;&#20855;&#26377;&#35748;&#35777;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#25269;&#25239;&#20004;&#31181;&#24120;&#35265;&#30340;&#27745;&#26579;&#31867;&#22411;&#8212;&#8212;&#25968;&#25454;&#36867;&#36991;&#21644;&#25915;&#20987;&#8212;&#8212;&#21516;&#26102;&#30830;&#20445;&#27867;&#21270;&#20445;&#35777;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#23383;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#23436;&#25972;&#40065;&#26834;&#65288;HR&#65289;&#35757;&#32451;&#31243;&#24207;&#20855;&#26377;SOTA&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;HR&#35757;&#32451;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#24182;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#20110;GAN&#21644;RL&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work have demonstrated that robustness (to "corruption") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar "robust overfitting" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;DCMDPs&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20381;&#36182;&#21382;&#21490;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;&#20854;&#20013;&#30340;&#36923;&#36753;DCMDPs&#36890;&#36807;&#21033;&#29992;&#32858;&#21512;&#20989;&#25968;&#30830;&#23450;&#19978;&#19979;&#25991;&#36716;&#25442;&#65292;&#25171;&#30772;&#20102;&#23545;&#21382;&#21490;&#38271;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02061</link><description>&lt;p&gt;
&#21382;&#21490;&#20381;&#36182;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with History-Dependent Dynamic Contexts. (arXiv:2302.02061v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02061
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;DCMDPs&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20381;&#36182;&#21382;&#21490;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;&#20854;&#20013;&#30340;&#36923;&#36753;DCMDPs&#36890;&#36807;&#21033;&#29992;&#32858;&#21512;&#20989;&#25968;&#30830;&#23450;&#19978;&#19979;&#25991;&#36716;&#25442;&#65292;&#25171;&#30772;&#20102;&#23545;&#21382;&#21490;&#38271;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DCMDPs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20381;&#36182;&#21382;&#21490;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;&#23427;&#25512;&#24191;&#20102;&#19978;&#19979;&#25991;MDP&#26694;&#26550;&#65292;&#20197;&#22788;&#29702;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#30528;&#37325;&#20110;&#36923;&#36753;DCMDPs&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#32858;&#21512;&#20989;&#25968;&#30830;&#23450;&#19978;&#19979;&#25991;&#36716;&#25442;&#26469;&#25171;&#30772;&#23545;&#21382;&#21490;&#38271;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#12290;&#36825;&#31181;&#29305;&#27530;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#19968;&#31181;&#31867;&#20284;&#20110;&#19978;&#38480;&#32622;&#20449;&#30028;&#31639;&#27861;&#30340;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36951;&#25022;&#30028;&#12290;&#21463;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36923;&#36753;DCMDPs&#65292;&#36825;&#20010;&#31639;&#27861;&#22312;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#20351;&#29992;&#21382;&#21490;&#20381;&#36182;&#29305;&#24449;&#19978;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65288;&#20351;&#29992;MovieLens&#25968;&#25454;&#38598;&#65289;&#65292;&#20854;&#20013;&#29992;&#25143;&#34892;&#20026;&#21160;&#24577;&#22320;&#38543;&#30528;&#25512;&#33616;&#30340;&#21464;&#21270;&#32780;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features. We demonstrate the efficacy of our approach on a recommendation task (using MovieLens data) where user behavior dynamics evolve in response to recommendations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13287</link><description>&lt;p&gt;
MILO: &#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#21644;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#21644;&#35843;&#20248;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#20043;&#19968;&#26159;&#36890;&#36807;&#36873;&#25321;&#24456;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#19982;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#22522;&#20934;&#30456;&#27604;&#65292;&#29616;&#26377;&#30340;&#26234;&#33021;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#30001;&#20110;&#32791;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#27493;&#39588;&#32780;&#19981;&#20855;&#31454;&#20105;&#21147;&#65292;&#35813;&#27493;&#39588;&#28041;&#21450;&#35745;&#31639;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#26799;&#24230;&#21644;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#23376;&#27169;&#22359;&#30446;&#26631;&#30340;&#36138;&#24515;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#28040;&#38500;&#23545;&#19979;&#28216;&#27169;&#22411;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MILO&#65292;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#23427;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#31867;&#36923;&#36753;&#35884;&#35823;&#30340;&#26032;&#26696;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#26816;&#32034;&#21644;&#21382;&#21490;&#26696;&#20363;&#30340;&#35843;&#25972;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11879</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26696;&#20363;&#25512;&#29702;&#22312;&#36923;&#36753;&#35884;&#35823;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#31867;&#36923;&#36753;&#35884;&#35823;&#30340;&#26032;&#26696;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#26816;&#32034;&#21644;&#21382;&#21490;&#26696;&#20363;&#30340;&#35843;&#25972;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#19978;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#30340;&#23481;&#26131;&#21644;&#24555;&#25463;&#24615;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#30340;&#35884;&#35823;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#36923;&#36753;&#35884;&#35823;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#26816;&#32034;&#21644;&#21382;&#21490;&#26696;&#20363;&#30340;&#35843;&#25972;&#26469;&#20998;&#31867;&#36923;&#36753;&#35884;&#35823;&#30340;&#26032;&#26696;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#20114;&#34917;&#30340;&#31574;&#30053;&#65292;&#22522;&#20110;&#26377;&#20851;&#30446;&#26631;&#12289;&#35299;&#37322;&#12289;&#21453;&#39539;&#21644;&#35770;&#35777;&#32467;&#26500;&#30340;&#22806;&#37096;&#20449;&#24687;&#26469;&#20016;&#23500;&#25105;&#20204;&#27169;&#22411;&#30340;&#36755;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#31867;&#20284;&#26696;&#20363;&#30340;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#65292;&#36739;&#23569;&#30340;&#21382;&#21490;&#26696;&#20363;&#20063;&#33021;&#20351;&#27169;&#22411;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewe
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#24067;&#23572;&#32422;&#26463;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#32422;&#26463;&#23436;&#32654;&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Tutte&#23450;&#29702;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#26032;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#22909;&#22320;&#25193;&#23637;&#32422;&#26463;&#21305;&#37197;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.09833</link><description>&lt;p&gt;
&#36890;&#36807;Tutte&#23450;&#29702;&#22522;&#20110;&#28151;&#21512;&#24067;&#23572;&#32422;&#26463;&#35299;&#20915;&#37327;&#23376;&#21551;&#21457;&#24335;&#23436;&#32654;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Quantum-Inspired Perfect Matching Problems via Tutte's Theorem-Based Hybrid Boolean Constraints. (arXiv:2301.09833v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#24067;&#23572;&#32422;&#26463;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#32422;&#26463;&#23436;&#32654;&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Tutte&#23450;&#29702;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#26032;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#22909;&#22320;&#25193;&#23637;&#32422;&#26463;&#21305;&#37197;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#32422;&#26463;&#26465;&#20214;(&#21363;&#28151;&#21512;&#32422;&#26463;&#26465;&#20214;)&#30340;&#24067;&#23572;&#32422;&#26463;&#28385;&#36275;&#24615;&#38382;&#39064;&#26159;&#19968;&#20010;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#24182;&#26377;&#37325;&#35201;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28151;&#21512;&#24067;&#23572;&#32422;&#26463;&#30340;&#19968;&#20010;&#26032;&#24212;&#29992;&#65292;&#21363;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#20986;&#29616;&#30340;&#32422;&#26463;&#23436;&#32654;&#21305;&#37197;&#38382;&#39064;&#12290;&#34429;&#28982;&#36890;&#29992;&#30340;&#28151;&#21512;&#32422;&#26463;&#27714;&#35299;&#22120;&#21487;&#20197;&#24456;&#24378;&#22823;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23558;&#32422;&#26463;&#21305;&#37197;&#38382;&#39064;&#30452;&#25509;&#32534;&#30721;&#20026;&#28151;&#21512;&#32422;&#26463;&#30340;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#25193;&#23637;&#65292;&#24182;&#19988;&#20173;&#38656;&#35201;&#29305;&#27530;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#20013;&#30340;Tutte&#23450;&#29702;&#20197;&#21450;&#20248;&#21270;&#25216;&#26415;&#30340;&#26032;&#32534;&#30721;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32534;&#30721;&#22312;&#36866;&#24403;&#30340;&#35821;&#35328;&#21644;&#20808;&#36827;&#30340;SAT&#27714;&#35299;&#22120;&#20013;&#65292;&#27604;&#35768;&#22810;&#31454;&#20105;&#26041;&#27861;&#26356;&#33021;&#25193;&#23637;&#32422;&#26463;&#21305;&#37197;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#24212;&#29992;&#24378;&#22823;&#30340;&#36890;&#29992;&#32422;&#26463;&#27714;&#35299;&#22120;&#26102;&#38656;&#35201;&#35774;&#35745;&#38382;&#39064;&#29305;&#23450;&#30340;&#32534;&#30721;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the satisfiability of Boolean constraint-satisfaction problems with different types of constraints, that is hybrid constraints, is a well-studied problem with important applications. We study here a new application of hybrid Boolean constraints, which arises in quantum computing. The problem relates to constrained perfect matching in edge-colored graphs. While general-purpose hybrid constraint solvers can be powerful, we show that direct encodings of the constrained-matching problem as hybrid constraints scale poorly and special techniques are still needed. We propose a novel encoding based on Tutte's Theorem in graph theory as well as optimization techniques. Empirical results demonstrate that our encoding, in suitable languages with advanced SAT solvers, scales significantly better than a number of competing approaches on constrained-matching benchmarks. Our study identifies the necessity of designing problem-specific encodings when applying powerful general-purpose const
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.16468</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#20013;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16468
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#23454;&#35777;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#31995;&#32479;&#20013;&#28041;&#21450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#36825;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21069;&#38376;&#35843;&#25972;&#8212;&#8212;&#19968;&#31181;&#32463;&#20856;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#34429;&#28982;&#21069;&#38376;&#20272;&#35745;&#30340;&#32479;&#35745;&#29305;&#24615;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#23427;&#30340;&#31639;&#27861;&#26041;&#38754;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#26368;&#36817;&#65292;Jeong&#65292;Tian&#21644;Barenboim [NeurIPS 2022]&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#25214;&#21040;&#28385;&#36275;&#21069;&#38376;&#20934;&#21017;&#30340;&#38598;&#21512;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O&#65288;n^3&#65288;n+m&#65289;&#65289;$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;$m$&#34920;&#31034;&#22240;&#26524;&#22270;&#30340;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#21363;$O&#65288;n+m&#65289;$&#65292;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#28176;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLICER&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#20219;&#21153;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.01519</link><description>&lt;p&gt;
SLICER: &#21033;&#29992;&#20302;&#36164;&#28304;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLICER&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#20219;&#21153;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#38899;&#39057;&#21644;&#35821;&#38899;&#20998;&#31867;&#25152;&#38656;&#30340;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#20302;&#36164;&#28304;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#29615;&#22659;&#20013;&#23398;&#20064;&#21487;&#20197;&#27010;&#25324;&#22823;&#37327;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#20219;&#21153;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#21463;&#21040;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#22312;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLICER&#65288;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#39640;&#25928;&#34920;&#24449;&#30340;&#23545;&#31216;&#23398;&#20064;&#65289;&#65292;&#23558;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#23398;&#29983;&#21644;&#25945;&#24072;&#32534;&#30721;&#22120;&#20043;&#38388;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#31216;&#25439;&#22833;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#30340;&#39057;&#35889;&#22270;&#25237;&#24433;&#21040;&#19982;&#32858;&#31867;&#25968;&#30446;&#30456;&#21516;&#30340;&#36755;&#20986;&#23376;&#31354;&#38388;&#20013;&#26469;&#22312;&#32447;&#33719;&#24471;&#32858;&#31867;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new Self-Supervised Learning (SSL) approach to pre-train encoders on unlabeled audio data that reduces the need for large amounts of labeled data for audio and speech classification. Our primary aim is to learn audio representations that can generalize across a large variety of speech and non-speech tasks in a low-resource un-labeled audio pre-training setting. Inspired by the recent success of clustering and contrasting learning paradigms for SSL-based speech representation learning, we propose SLICER (Symmetrical Learning of Instance and Cluster-level Efficient Representations), which brings together the best of both clustering and contrasting learning paradigms. We use a symmetric loss between latent representations from student and teacher encoders and simultaneously solve instance and cluster-level contrastive learning tasks. We obtain cluster representations online by just projecting the input spectrogram into an output subspace with dimensions equal to the number of
&lt;/p&gt;</description></item><item><title>MAST&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;&#65292;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20998;&#23618;&#27010;&#24565;&#65292;&#21516;&#26102;&#25193;&#23637;&#23884;&#20837;&#32500;&#24230;&#65292;&#38477;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#12290;&#36890;&#36807;&#37329;&#23383;&#22612;&#32467;&#26500;&#23454;&#29616;&#26089;&#26399;&#23618;&#21644;&#28145;&#23618;&#30340;&#24314;&#27169;&#65292;&#25193;&#23637;&#26041;&#27861;&#20026;SS-MAST&#12290;</title><link>http://arxiv.org/abs/2211.01515</link><description>&lt;p&gt;
MAST:&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01515
&lt;/p&gt;
&lt;p&gt;
MAST&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;&#65292;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20998;&#23618;&#27010;&#24565;&#65292;&#21516;&#26102;&#25193;&#23637;&#23884;&#20837;&#32500;&#24230;&#65292;&#38477;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#12290;&#36890;&#36807;&#37329;&#23383;&#22612;&#32467;&#26500;&#23454;&#29616;&#26089;&#26399;&#23618;&#21644;&#28145;&#23618;&#30340;&#24314;&#27169;&#65292;&#25193;&#23637;&#26041;&#27861;&#20026;SS-MAST&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#30340;&#22810;&#23610;&#24230;&#38899;&#39057;&#35889;&#22270;&#21464;&#21387;&#22120;&#65288;MAST&#65289;&#65292;&#23558;&#22810;&#23610;&#24230;&#29305;&#24449;&#20998;&#23618;&#27010;&#24565;&#24341;&#20837;&#38899;&#39057;&#35889;&#22270;&#21464;&#25442;&#22120;&#65288;AST&#65289;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#30340;&#38899;&#39057;&#35889;&#22270;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#35009;&#21098;&#25104;&#21021;&#27493;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#23884;&#20837;&#32500;&#24230;&#65292;&#38543;&#21518;MAST&#20013;&#30340;&#22810;&#20010;&#38454;&#27573;&#36880;&#28176;&#25193;&#23637;&#23884;&#20837;&#32500;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#36755;&#20837;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#20351;&#24471;MAST&#30340;&#26089;&#26399;&#23618;&#22312;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#20294;&#20302;&#23884;&#20837;&#31354;&#38388;&#19979;&#24314;&#27169;&#31616;&#21333;&#30340;&#20302;&#32423;&#22768;&#23398;&#20449;&#24687;&#65292;&#32780;&#36739;&#28145;&#30340;&#26102;&#38388;&#31895;&#31961;&#23618;&#21017;&#29992;&#39640;&#32500;&#23884;&#20837;&#26469;&#24314;&#27169;&#39640;&#32423;&#22768;&#23398;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;SS-MAST&#65292;&#23427;&#35745;&#31639;&#20102;&#19968;&#20010;&#23545;&#31216;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#21033;&#29992;patch-drop - &#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#22686;&#24378;&#25216;&#26415;&#26469;&#33258;&#23398;&#20064;&#21644;&#25945;&#24072;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Multiscale Audio Spectrogram Transformer (MAST) for audio classification, which brings the concept of multiscale feature hierarchies to the Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we first patchify and project it into an initial temporal resolution and embedding dimension, post which the multiple stages in MAST progressively expand the embedding dimension while reducing the temporal resolution of the input. We use a pyramid structure that allows early layers of MAST operating at a high temporal resolution but low embedding space to model simple low-level acoustic information and deeper temporally coarse layers to model high-level acoustic information with high-dimensional embeddings. We also extend our approach to present a new Self-Supervised Learning (SSL) method called SS-MAST, which calculates a symmetric contrastive loss between latent representations from a student and a teacher encoder, leveraging patch-drop, a novel audio augmentation a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#30340;&#8220;&#34920;&#31034;&#22797;&#26434;&#24230;&#8221;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#36328;&#22810;&#20010;&#31070;&#32463;&#20803;&#25193;&#25955;&#30340;&#20449;&#24687;&#35775;&#38382;&#38590;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.10438</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#30340;&#31070;&#32463;&#34920;&#31034;&#22797;&#26434;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Measure of the Complexity of Neural Representations based on Partial Information Decomposition. (arXiv:2209.10438v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#30340;&#8220;&#34920;&#31034;&#22797;&#26434;&#24230;&#8221;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#36328;&#22810;&#20010;&#31070;&#32463;&#20803;&#25193;&#25955;&#30340;&#20449;&#24687;&#35775;&#38382;&#38590;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#36890;&#24120;&#26159;&#30001;&#31070;&#32463;&#20803;&#32676;&#32852;&#21512;&#34920;&#31034;&#30340;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#20998;&#31867;&#26631;&#31614;&#30340;&#20114;&#20449;&#24687;&#22914;&#20309;&#22312;&#21333;&#20010;&#31070;&#32463;&#20803;&#20043;&#38388;&#20998;&#37197;&#30340;&#32454;&#33410;&#23578;&#19981;&#28165;&#26970;&#65306;&#34429;&#28982;&#37096;&#20998;&#20114;&#20449;&#24687;&#21482;&#33021;&#20174;&#29305;&#23450;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#20013;&#33719;&#24471;&#65292;&#20294;&#20854;&#20182;&#37096;&#20998;&#21017;&#30001;&#22810;&#20010;&#31070;&#32463;&#20803;&#20887;&#20313;&#25110;&#21327;&#21516;&#25215;&#36733;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#26469;&#20998;&#31163;&#36825;&#20123;&#19981;&#21516;&#30340;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#34920;&#31034;&#22797;&#26434;&#24230;&#8221;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#36328;&#22810;&#20010;&#31070;&#32463;&#20803;&#25193;&#25955;&#30340;&#20449;&#24687;&#35775;&#38382;&#38590;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#20309;&#30452;&#25509;&#35745;&#31639;&#36739;&#23567;&#23618;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#38024;&#23545;&#36739;&#22823;&#23618;&#25552;&#20986;&#20102;&#23376;&#25277;&#26679;&#21644;&#31895;&#31890;&#21270;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#24212;&#30340;&#19978;&#38480;&#12290;&#22312;MNIST&#21644;CIFAR10&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#34920;&#31034;&#22797;&#26434;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of "Representational Complexity", which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;</title><link>http://arxiv.org/abs/2207.05631</link><description>&lt;p&gt;
DGPO: &#20351;&#29992;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#21457;&#29616;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37117;&#35797;&#22270;&#23547;&#25214;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#21333;&#20010;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20363;&#22914;&#65292;&#20351;&#26234;&#33021;&#20307;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#26356;&#21152;&#26377;&#36259;&#65292;&#25110;&#32773;&#25552;&#39640;&#31574;&#30053;&#23545;&#24847;&#22806;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#65288;DGPO&#65289;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#22810;&#31181;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#36890;&#36807;&#22312;&#21333;&#27425;&#36816;&#34892;&#20013;&#35757;&#32451;&#20849;&#20139;&#31574;&#30053;&#32593;&#32476;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#30446;&#26631;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#20132;&#26367;&#32422;&#26463;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#22806;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#25512;&#26029;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#36845;&#20195;&#26469;&#26368;&#22823;&#21270;&#24471;&#21040;&#30340;&#19979;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324; Atari &#28216;&#25103;&#21644; Mujoco &#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#31995;&#21015;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Bamboo&#37197;&#32622;&#31574;&#30053;&#65292;&#22522;&#20110;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.10505</link><description>&lt;p&gt;
Transformer&#37197;&#32622;&#19982;&#35757;&#32451;&#30446;&#26631;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Bamboo&#37197;&#32622;&#31574;&#30053;&#65292;&#22522;&#20110;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22312;&#35768;&#22810;&#27169;&#22411;&#35757;&#32451;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#37319;&#29992;&#20256;&#32479;&#30340;&#37197;&#32622;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#20256;&#32479;&#37197;&#32622;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;Bamboo&#30340;&#37197;&#32622;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#21270;SVM&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#32908;&#30005;&#20449;&#21495;&#30340;&#39640;&#25928;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21151;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2204.02179</link><description>&lt;p&gt;
&#22522;&#20110;&#36827;&#21270;&#35745;&#31639;&#30340;&#32908;&#30005;&#25511;&#21046;&#22120;&#30340;&#21151;&#32791;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Power-Efficient Design of Myoelectric Controller based on Evolutionary Computation. (arXiv:2204.02179v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#21270;SVM&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#32908;&#30005;&#20449;&#21495;&#30340;&#39640;&#25928;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21151;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#26159;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#19978;&#32930;&#20551;&#32930;&#21644;&#29983;&#29289;&#26426;&#22120;&#20154;&#25163;&#37096;&#36816;&#21160;&#31995;&#32479;&#65289;&#30340;&#25511;&#21046;&#31574;&#30053;&#35774;&#35745;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20351;&#29992;&#26680;&#21270;SVM&#20998;&#31867;&#22120;&#23545;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#20449;&#21495;&#30340;&#20449;&#24687;&#36827;&#34892;&#35299;&#30721;&#26469;&#25512;&#26029;&#28508;&#22312;&#30340;&#32908;&#32905;&#36816;&#21160;&#65292;&#35774;&#35745;&#19968;&#31181;&#33021;&#37327;&#39640;&#25928;&#30340;&#22522;&#20110;EMG&#30340;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#23454;&#29616;EMG&#25511;&#21046;&#22120;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#31867;&#22120;&#35774;&#35745;&#31574;&#30053;&#26159;&#20943;&#23569;&#25972;&#20010;&#31995;&#32479;&#35823;&#21160;&#20316;&#65288;&#24403;EMG&#25511;&#21046;&#22120;&#22788;&#20110;&#8220;&#20241;&#24687;&#8221;&#20301;&#32622;&#26102;&#65289;&#12290;&#20026;&#27492;&#65292;&#19982;&#20256;&#32479;&#30340;&#36719;&#38388;&#38548;&#26680;&#21270;SVM&#21333;&#20010;&#35757;&#32451;&#30446;&#26631;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#30340;&#35757;&#32451;&#31639;&#27861;&#21046;&#23450;&#20026;&#19968;&#33324;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#37319;&#29992;&#31934;&#33521;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;NSGA-II&#26469;&#25214;&#21040;&#21463;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#65292;&#24182;&#26681;&#25454;&#25903;&#25345;&#21521;&#37327;&#30340;&#25968;&#37327;&#21644;&#20998;&#31867;&#20934;&#30830;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#36873;&#25321;&#26368;&#32456;&#20998;&#31867;&#22120;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;sEMG&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21151;&#29575;&#25928;&#29575;&#21644;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myoelectric pattern recognition is one of the important aspects in the design of the control strategy for various applications including upper-limb prostheses and bio-robotic hand movement systems. The current work has proposed an approach to design an energy-efficient EMG-based controller by considering a supervised learning framework using a kernelized SVM classifier for decoding the information of surface electromyography (sEMG) signals to infer the underlying muscle movements. In order to achieve the optimized performance of the EMG-based controller, our main strategy of classifier design is to reduce the false movements of the overall system (when the EMG-based controller is at the `Rest' position). To this end, unlike the traditional single training objective of soft margin kernelized SVM, we have formulated the training algorithm of the proposed supervised learning system as a general constrained multi-objective optimization problem. An elitist multi-objective evolutionary algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#37325;&#21442;&#25968;&#21270;&#30340;&#27010;&#24565;&#19982;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#36229;&#37325;&#21442;&#25968;&#21270;&#35745;&#31639;WCSP&#26368;&#20248;&#20540;&#19978;&#30028;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.02018</link><description>&lt;p&gt;
&#26435;&#37325;&#32422;&#26463;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#36229;&#37325;&#21442;&#25968;&#21270;&#65306;&#24615;&#36136;&#19982;&#20248;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Super-Reparametrizations of Weighted CSPs: Properties and Optimization Perspective. (arXiv:2201.02018v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#37325;&#21442;&#25968;&#21270;&#30340;&#27010;&#24565;&#19982;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#36229;&#37325;&#21442;&#25968;&#21270;&#35745;&#31639;WCSP&#26368;&#20248;&#20540;&#19978;&#30028;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#26435;&#37325;&#32422;&#26463;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;WCSP&#65289;&#8221;&#30340;&#37325;&#21442;&#25968;&#21270;&#26159;&#19968;&#31181;&#24456;&#24120;&#35265;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#36924;&#36817;&#25110;&#32422;&#26463;&#26368;&#20248;&#20215;&#20540;&#12290;&#30456;&#21453;&#65292;&#36229;&#37325;&#21442;&#25968;&#21270;&#30340;&#27010;&#24565;&#65288;&#21363;&#20351;&#24471;&#27599;&#20010;&#20998;&#37197;&#37117;&#20445;&#25345;&#25110;&#22686;&#21152;WCSP&#30446;&#26631;&#30340;&#26435;&#37325;&#25913;&#21464;&#65289;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#20174;&#26410;&#34987;&#35814;&#32454;&#30740;&#31350;&#36807;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#37325;&#21442;&#25968;&#21270;&#30340;&#20960;&#20010;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#23558;&#20854;&#19982;&#37325;&#21442;&#25968;&#21270;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#37325;&#21442;&#25968;&#21270;&#35745;&#31639;WCSP&#30340;&#26368;&#20248;&#20540;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25216;&#26415;&#26465;&#20214;&#19979;&#65292;&#21407;&#21017;&#19978;&#21487;&#20197;&#37319;&#29992;&#20219;&#24847;&#30340;&#32422;&#26463;&#20256;&#25773;&#35268;&#21017;&#26469;&#25552;&#39640;&#36825;&#31181;&#32422;&#26463;&#35745;&#31639;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#24359;&#19968;&#33268;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#21040;&#24050;&#30693;&#30340;&#34394;&#25311;&#24359;&#19968;&#33268;&#24615;&#31639;&#27861;&#65288;VAC&#65289;&#20013;&#12290;&#25105;&#20204;&#22312;&#21333;&#20363;&#24359;&#19968;&#33268;&#24615;WCSP&#20013;&#23454;&#29616;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#25253;&#21578;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion of reparametrizations of Weighted CSPs (WCSPs) (also known as equivalence-preserving transformations of WCSPs) is well-known and finds its use in many algorithms to approximate or bound the optimal WCSP value. In contrast, the concept of super-reparametrizations (which are changes of the weights that keep or increase the WCSP objective for every assignment) was already proposed but never studied in detail. To fill this gap, we present a number of theoretical properties of super-reparametrizations and compare them to those of reparametrizations. Furthermore, we propose a framework for computing upper bounds on the optimal value of the (maximization version of) WCSP using super-reparametrizations. We show that it is in principle possible to employ arbitrary (under some technical conditions) constraint propagation rules to improve the bound. For arc consistency in particular, the method reduces to the known Virtual AC (VAC) algorithm. We implemented the method for singleton arc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36866;&#37327;&#32423;&#21035;&#30340;&#36882;&#22686;&#35745;&#31639;&#26469;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;Epistemic&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20351;&#24471;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#22823;&#24133;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#22823;&#22411;&#38598;&#25104;&#27169;&#22411;&#65292;&#20026;&#27169;&#22411;&#32852;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2107.08924</link><description>&lt;p&gt;
&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Epistemic Neural Networks. (arXiv:2107.08924v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36866;&#37327;&#32423;&#21035;&#30340;&#36882;&#22686;&#35745;&#31639;&#26469;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;Epistemic&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20351;&#24471;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#22823;&#24133;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#22823;&#22411;&#38598;&#25104;&#27169;&#22411;&#65292;&#20026;&#27169;&#22411;&#32852;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#23545;&#20854;&#19981;&#30693;&#36947;&#30340;&#20107;&#29289;&#30340;&#20102;&#35299;&#12290;&#26234;&#33021;&#20307;&#39044;&#27979;&#22810;&#20010;&#36755;&#20837;&#26631;&#31614;&#30340;&#36136;&#37327;&#21487;&#20197;&#35780;&#20272;&#20854;&#23545;&#36825;&#31181;&#33021;&#21147;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;&#38598;&#25104;&#24335;&#26041;&#27861;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#39044;&#27979;&#65292;&#20294;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#20174;&#32780;&#21487;&#33021;&#20250;&#21464;&#24471;&#31105;&#27490;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Epinet&#65306;&#19968;&#31181;&#21487;&#20197;&#21152;&#24378;&#20219;&#20309;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65288;&#21253;&#25324;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#36866;&#37327;&#32423;&#21035;&#30340;&#36882;&#22686;&#35745;&#31639;&#35757;&#32451;&#26469;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#29992;Epinet&#65292;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#22823;&#24133;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#30001;&#25968;&#30334;&#20010;&#25110;&#26356;&#22810;&#31890;&#23376;&#32452;&#25104;&#30340;&#22823;&#22411;&#38598;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#31526;&#21512;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#32479;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#36229;&#36234;BNN&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;Epinet&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20316;&#20026;&#20135;&#29983;&#32852;&#21512;&#39044;&#27979;&#27169;&#22411;&#30340;&#25509;&#21475;&#30340;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. In principle, ensemble-based approaches produce effective joint predictions, but the computational costs of training large ensembles can become prohibitive. We introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks. To accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as an interface for models that produce joint predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;ODENet&#21644;&#19968;&#31867;ResNet&#65292;&#8220;&#23485;&#24230;&#20026;n+m&#30340;ODENet&#21487;&#20197;&#36924;&#36817;${\rm \mathbb{R}^n}$&#19978;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#8221;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#26576;&#20010;&#35843;&#25972;&#21464;&#37327;&#30340;&#26799;&#24230;&#24182;&#29992;&#20110;&#26500;&#24314;ODENet&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;MNIST&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2101.10229</link><description>&lt;p&gt;
&#19968;&#20010;ODENet&#21644;ResNet&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65306;&#25968;&#23398;&#20998;&#26512;&#19982;&#25968;&#20540;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Properties for an ODENet and a ResNet: Mathematical Analysis and Numerical Experiments. (arXiv:2101.10229v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;ODENet&#21644;&#19968;&#31867;ResNet&#65292;&#8220;&#23485;&#24230;&#20026;n+m&#30340;ODENet&#21487;&#20197;&#36924;&#36817;${\rm \mathbb{R}^n}$&#19978;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#8221;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#26576;&#20010;&#35843;&#25972;&#21464;&#37327;&#30340;&#26799;&#24230;&#24182;&#29992;&#20110;&#26500;&#24314;ODENet&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;MNIST&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;ODENet&#21644;&#19968;&#31867;ResNet&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;(UAP)&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#31616;&#21270;&#25968;&#23398;&#27169;&#22411;&#12290; UAP&#21487;&#20197;&#38472;&#36848;&#22914;&#19979;:&#35774;$n$&#21644;$m$&#20998;&#21035;&#20026;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#24182;&#20551;&#35774;$m\leq n$&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#38750;&#22810;&#39033;&#24335;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#30340;&#23485;&#24230;&#20026;$n+m$&#30340;ODENet&#21487;&#20197;&#36924;&#36817;$\mathbb {R} ^ n$&#19978;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#36235;&#20110;&#26080;&#38480;&#26102;&#65292;ResNet&#20855;&#26377;&#30456;&#21516;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26126;&#30830;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#26576;&#20010;&#35843;&#25972;&#21464;&#37327;&#30340;&#26799;&#24230;&#12290; &#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#26500;&#24314;ODENet&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#23637;&#31034;&#27492;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;MNIST&#19978;&#30340;&#22238;&#24402;&#38382;&#39064;&#12289;&#20108;&#20998;&#31867;&#21644;&#22810;&#39033;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a universal approximation property (UAP) for a class of ODENet and a class of ResNet, which are simplified mathematical models for deep learning systems with skip connections. The UAP can be stated as follows. Let $n$ and $m$ be the dimension of input and output data, and assume $m\leq n$. Then we show that ODENet of width $n+m$ with any non-polynomial continuous activation function can approximate any continuous function on a compact subset on $\mathbb{R}^n$. We also show that ResNet has the same property as the depth tends to infinity. Furthermore, we derive the gradient of a loss function explicitly with respect to a certain tuning variable. We use this to construct a learning algorithm for ODENet. To demonstrate the usefulness of this algorithm, we apply it to a regression problem, a binary classification, and a multinomial classification in MNIST.
&lt;/p&gt;</description></item></channel></rss>