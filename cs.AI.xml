<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#20316;&#20026;&#20379;&#24212;&#38142;&#20013;&#26029;&#30340;&#26377;&#25928;&#25514;&#26045;&#65292;&#24182;&#24110;&#21161;&#21442;&#19982;&#32773;&#22312;&#21361;&#26426;&#21457;&#29983;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00306</link><description>&lt;p&gt;
&#21033;&#29992;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#31532;&#19968;&#27493;&#24377;&#24615;&#25514;&#26045; &#8212;&#8212; &#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#20316;&#20026;&#20379;&#24212;&#38142;&#20013;&#26029;&#30340;&#26377;&#25928;&#25514;&#26045;&#65292;&#24182;&#24110;&#21161;&#21442;&#19982;&#32773;&#22312;&#21361;&#26426;&#21457;&#29983;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25216;&#26415;&#22312;&#25552;&#39640;&#20379;&#24212;&#38142;&#24377;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;4.0&#21644;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#25512;&#33616;&#31995;&#32479; (RS) &#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#25552;&#21319;&#20379;&#24212;&#38142;&#24377;&#24615;&#30340;&#24037;&#20855;&#34987;&#24573;&#35270;&#65292;&#20294;&#20174;&#24212;&#21464;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;RS &#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#20840;&#26032;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#39564;&#35777;&#20102;&#27010;&#24565;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#25514;&#26045;&#22312;&#31532;&#19968;&#38454;&#27573;&#24471;&#21040;&#23454;&#26045;&#65292;&#24182;&#24110;&#21161;&#20379;&#24212;&#38142;&#21442;&#19982;&#32773;&#22312;&#20379;&#24212;&#38142;&#20013;&#26029;&#20043;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00306v1 Announce Type: cross  Abstract: Interests in the value of digital technologies for its potential uses to increase supply chain resilience (SCRes) are increasing in light to the industry 4.0 and the global pandemic. Utilization of Recommender systems (RS) as a supply chain (SC) resilience measure is neglected although RS is a capable tool to enhance SC resilience from a reactive aspect. To address this problem, this research proposed a novel data-driven supply chain disruption response framework based on the intelligent recommender system techniques and validated the conceptual model through a practical use case. Results show that our framework can be implemented as an effective SC disruption mitigation measure in the very first response phrase and help SC participants get better reaction performance after the SC disruption.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02622</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
World Models for Autonomous Driving: An Initial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02622
&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#35780;&#20272;&#20854;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#20851;&#38190;&#22320;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#33021;&#22815;&#32508;&#21512;&#21644;&#35299;&#37322;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#28508;&#22312;&#30340;&#26410;&#26469;&#24773;&#26223;&#24182;&#24357;&#34917;&#20449;&#24687;&#32570;&#21475;&#12290;&#26412;&#25991;&#23545;&#33258;&#20027;&#39550;&#39542;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21457;&#23637;&#36827;&#34892;&#20102;&#21021;&#27493;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#24378;&#35843;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#22522;&#30784;&#21442;&#32771;&#65292;&#20415;&#20110;&#24555;&#36895;&#33719;&#24471;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#29616;&#26377;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#26041;&#27861;&#30340;&#33258;&#21160;&#25512;&#26029;&#36816;&#21160;&#32452;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.01606</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#32479;&#19968;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#29616;&#26377;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#26041;&#27861;&#30340;&#33258;&#21160;&#25512;&#26029;&#36816;&#21160;&#32452;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#20154;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36816;&#21160;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#36816;&#21160;&#20851;&#31995;&#30697;&#38453;&#25191;&#34892;&#35889;&#32858;&#31867;&#65292;&#23558;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#25110;&#28857;&#36712;&#36857;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#36816;&#21160;&#32452;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#30693;&#36947;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#36816;&#21160;&#25968;&#37327;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#29616;&#26377;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#33258;&#21160;&#25512;&#26029;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#26041;&#27861;&#30340;&#36816;&#21160;&#32452;&#25968;&#12290;&#25105;&#20204;&#22312;KT3DMoSeg&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#22522;&#20934;&#32467;&#26524;&#36827;&#34892;&#20102;&#31454;&#20105;&#24615;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01606v1 Announce Type: cross  Abstract: Motion segmentation is a fundamental problem in computer vision and is crucial in various applications such as robotics, autonomous driving and action recognition. Recently, spectral clustering based methods have shown impressive results on motion segmentation in dynamic environments. These methods perform spectral clustering on motion affinity matrices to cluster objects or point trajectories in the scene into different motion groups. However, existing methods often need the number of motions present in the scene to be known, which significantly reduces their practicality. In this paper, we propose a unified model selection technique to automatically infer the number of motion groups for spectral clustering based motion segmentation methods by combining different existing model selection techniques together. We evaluate our method on the KT3DMoSeg dataset and achieve competitve results comparing to the baseline where the number of clu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;</title><link>https://arxiv.org/abs/2402.18267</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#30340;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Question Generation: Methods, Applications, and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#26816;&#26597;&#65292;&#36825;&#19968;&#39046;&#22495;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20174;&#21508;&#31181;&#26469;&#28304;&#65292;&#22914;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#29983;&#25104;&#30456;&#20851;&#38382;&#39064;&#12290;&#35843;&#26597;&#20174;NQG&#32972;&#26223;&#27010;&#36848;&#24320;&#22987;&#65292;&#21253;&#25324;&#20219;&#21153;&#30340;&#38382;&#39064;&#21046;&#23450;&#12289;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24050;&#24314;&#31435;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;&#65292;&#31995;&#32479;&#22320;&#23558;NQG&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#32467;&#26500;&#21270;NQG&#65292;&#21033;&#29992;&#26377;&#32452;&#32455;&#30340;&#25968;&#25454;&#28304;&#65292;&#38750;&#32467;&#26500;&#21270;NQG&#65292;&#19987;&#27880;&#20110;&#26356;&#26494;&#25955;&#32467;&#26500;&#30340;&#36755;&#20837;&#65292;&#22914;&#25991;&#26412;&#25110;&#35270;&#35273;&#20869;&#23481;&#65292;&#20197;&#21450;&#28151;&#21512;NQG&#65292;&#21033;&#29992;&#22810;&#26679;&#30340;&#36755;&#20837;&#27169;&#24335;&#12290;&#36825;&#19968;&#20998;&#31867;&#21518;&#26159;&#23545;&#20026;&#27599;&#20010;&#31867;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#35752;&#35770;&#23427;&#20204;&#22266;&#26377;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#23616;&#38480;&#24615;&#12290;&#35843;&#26597;&#20197;&#23637;&#26395;&#26410;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18267v1 Announce Type: cross  Abstract: In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking persp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;&#22914;&#22522;&#30784;&#27169;&#22411;&#65289;&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#65292;&#23545;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#31561;&#20851;&#38190;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10977</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#65306;&#19979;&#19968;&#20010;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Generative AI and Process Systems Engineering: The Next Frontier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10977
&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;&#22914;&#22522;&#30784;&#27169;&#22411;&#65289;&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#65292;&#23545;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#31561;&#20851;&#38190;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;&#20309;&#22686;&#24378;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#65288;PSE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36825;&#20123;&#26368;&#21069;&#27839;&#30340;GenAI&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#20026;&#28041;&#21450;&#26597;&#35810;&#21709;&#24212;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#22797;&#26434;&#20915;&#31574;&#31561;&#24191;&#27867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#12290;&#37492;&#20110;PSE&#30340;&#36827;&#23637;&#19982;&#35745;&#31639;&#21644;&#31995;&#32479;&#25216;&#26415;&#30340;&#21457;&#23637;&#20043;&#38388;&#23494;&#20999;&#20851;&#31995;&#65292;&#25506;&#32034;GenAI&#21644;PSE&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#32463;&#20856;&#21644;&#26032;&#20852;&#30340;GenAI&#27169;&#22411;&#65292;&#21253;&#25324;FMs&#30340;&#31616;&#35201;&#27010;&#36848;&#24320;&#22987;&#35752;&#35770;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#22312;&#20851;&#38190;PSE&#39046;&#22495;&#20869;&#30340;&#24212;&#29992;&#65306;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#65292;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#12290;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;GenAI&#27169;&#22411;&#22914;&#20309;&#21487;&#20197;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10977v1 Announce Type: new  Abstract: This article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could pot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.01705</link><description>&lt;p&gt;
&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#34920;&#24449;&#20260;&#23475;&#65306;&#24230;&#37327;&#21644;&#20943;&#36731;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20260;&#23475;&#36890;&#24120;&#34987;&#20998;&#20026;&#37197;&#32622;&#24615;&#25110;&#34920;&#24449;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#21518;&#32773;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#24403;&#21069;&#34920;&#24449;&#24615;&#20260;&#23475;&#23450;&#20041;&#30340;&#23457;&#26597;&#65292;&#20197;&#30830;&#23450;&#20854;&#20013;&#21253;&#21547;&#20160;&#20040;&#21644;&#19981;&#21253;&#21547;&#20160;&#20040;&#12290;&#36825;&#20010;&#20998;&#26512;&#20419;&#20351;&#25105;&#20204;&#25193;&#23637;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#21253;&#25324;&#23545;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#30340;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24230;&#37327;&#30340;&#39640;&#32423;&#35201;&#27714;&#65306;&#30830;&#23450;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#34920;&#24449;&#24615;&#20260;&#23475;&#26102;&#30340;&#29420;&#29305;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#20260;&#23475;&#26410;&#34987;&#24230;&#37327;&#21644;&#20943;&#36731;&#26102;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20943;&#36731;&#25514;&#26045;&#24182;&#30028;&#23450;&#20309;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#32467;&#26463;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26694;&#26550;&#65292;&#25193;&#22823;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#20844;&#24179;&#30740;&#31350;&#30340;&#35265;&#35299;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#36896;&#21147;&#39046;&#22495;&#30340;&#21382;&#21490;&#12289;&#29616;&#29366;&#65292;&#20197;&#21450;&#20851;&#38190;&#30340;&#36129;&#29486;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2104.02726</link><description>&lt;p&gt;
&#21019;&#24847;&#19982;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Creativity and Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.02726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#36896;&#21147;&#39046;&#22495;&#30340;&#21382;&#21490;&#12289;&#29616;&#29366;&#65292;&#20197;&#21450;&#20851;&#38190;&#30340;&#36129;&#29486;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#24847;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#24863;&#20852;&#36259;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#35745;&#31639;&#21019;&#36896;&#21147;&#29702;&#35770;&#30340;&#21382;&#21490;&#21644;&#29616;&#29366;&#12289;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#21253;&#25324;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#23545;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#36129;&#29486;&#36827;&#34892;&#25209;&#21028;&#24615;&#35752;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#30740;&#31350;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#20852;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.02726v4 Announce Type: replace  Abstract: There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38598;&#20307;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00651</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38598;&#20307;&#33258;&#21457;&#24320;&#25918;&#24335;&#25506;&#32034;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning. (arXiv:2311.00651v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00651
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38598;&#20307;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#33258;&#25105;&#23545;&#25112;&#30340;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20135;&#29983;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;&#34429;&#28982;&#32467;&#26524;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#33258;&#25105;&#23545;&#25112;&#21644;&#20854;&#20182;&#38598;&#20013;&#21270;&#35757;&#32451;&#25216;&#26415;&#24182;&#19981;&#33021;&#20934;&#30830;&#22320;&#21453;&#26144;&#33258;&#28982;&#30028;&#20013;&#26222;&#36941;&#30340;&#38598;&#20307;&#25506;&#32034;&#31574;&#30053;&#26159;&#22914;&#20309;&#20986;&#29616;&#30340;&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#23545;&#20219;&#21153;&#30340;&#26080;&#38480;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38598;&#20307;&#25506;&#32034;&#31574;&#30053;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#26080;&#38480;&#30340;&#20219;&#21153;&#20998;&#24067;&#20013;&#29420;&#31435;&#22320;&#20803;&#23398;&#20064;&#24490;&#29615;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#29615;&#22659;&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#30340;&#36807;&#31243;&#29983;&#25104;&#30340;&#20219;&#21153;&#31354;&#38388;&#65292;&#21160;&#24577;&#32452;&#21512;&#20102;&#20174;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20013;&#25277;&#26679;&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#20219;&#21153;&#26641;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#20998;&#25955;&#26234;&#33021;&#20307;&#22312;&#38754;&#23545;&#26032;&#30340;&#30446;&#26631;&#26102;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel obj
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13023</link><description>&lt;p&gt;
GraphGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36882;&#24402;&#20449;&#24687;&#20132;&#25442;&#21644;&#32858;&#21512;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#25552;&#21319;&#22270;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;LLM&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23558;LLM&#19982;&#22270;&#32467;&#26500;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#26696;&#20363;&#25512;&#29702;&#30740;&#31350;&#32773;&#23545;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24573;&#35270;&#65292;&#20197;&#21450;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#20037;&#24615;&#35760;&#24518;&#20013;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.08842</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#26696;&#20363;&#30340;&#25345;&#20037;&#24615;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
A Case-Based Persistent Memory for a Large Language Model. (arXiv:2310.08842v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#26696;&#20363;&#25512;&#29702;&#30740;&#31350;&#32773;&#23545;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24573;&#35270;&#65292;&#20197;&#21450;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#20037;&#24615;&#35760;&#24518;&#20013;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26696;&#20363;&#25512;&#29702;&#20316;&#20026;&#19968;&#31181;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#21512;&#36866;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#26412;&#35770;&#25991;&#25351;&#20986;&#65292;&#26696;&#20363;&#25512;&#29702;&#30340;&#30740;&#31350;&#32773;&#22312;&#36817;&#26399;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#26377;&#25152;&#24573;&#35270;&#12290;&#36817;&#26399;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;&#30340;&#28508;&#22312;&#25216;&#26415;&#21457;&#23637;&#19982;&#26696;&#20363;&#25512;&#29702;&#26377;&#30528;&#24378;&#28872;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#21487;&#20197;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25345;&#20037;&#24615;&#35760;&#24518;&#65292;&#23545;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26377;&#25152;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Case-based reasoning (CBR) as a methodology for problem-solving can use any appropriate computational technique. This position paper argues that CBR researchers have somewhat overlooked recent developments in deep learning and large language models (LLMs). The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR and could be used to provide a persistent memory for LLMs to make progress towards Artificial General Intelligence.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#20195;&#24065;&#31354;&#38388;&#24179;&#31227;&#38598;&#21512;&#20197;&#35299;&#20915;ViTs&#37327;&#21270;&#20266;&#24433;&#38382;&#39064;&#30340;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#36229;&#20998;&#36776;&#29575;&#39044;&#35757;&#32451;&#30340;ViTs&#29305;&#24449;&#65292;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.03967</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#30340;&#23376;&#20195;&#24065;ViT&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Sub-token ViT Embedding via Stochastic Resonance Transformers. (arXiv:2310.03967v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#20195;&#24065;&#31354;&#38388;&#24179;&#31227;&#38598;&#21512;&#20197;&#35299;&#20915;ViTs&#37327;&#21270;&#20266;&#24433;&#38382;&#39064;&#30340;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#36229;&#20998;&#36776;&#29575;&#39044;&#35757;&#32451;&#30340;ViTs&#29305;&#24449;&#65292;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;Vision Transformers&#65288;ViTs&#65289;&#20013;&#23384;&#22312;&#37327;&#21270;&#20266;&#24433;&#65292;&#36825;&#26159;&#30001;&#20110;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#22270;&#20687;&#26631;&#35760;&#27493;&#39588;&#24341;&#36215;&#30340;&#12290;&#36825;&#20123;&#20266;&#24433;&#23548;&#33268;&#20102;&#31895;&#31961;&#30340;&#37327;&#21270;&#29305;&#24449;&#65292;&#23545;&#19979;&#28216;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#29305;&#21035;&#26159;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;shot&#26041;&#27861;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;ViTs&#22788;&#29702;&#31354;&#38388;&#37327;&#21270;&#30340;&#26041;&#24335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23376;&#20195;&#24065;&#31354;&#38388;&#24179;&#31227;&#26469;&#38598;&#21512;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#22270;&#20687;&#33719;&#24471;&#30340;&#29305;&#24449;&#65292;&#36825;&#21463;&#21040;&#20102;&#38543;&#26426;&#20849;&#25391;&#30340;&#21551;&#21457;&#65292;&#38543;&#26426;&#20849;&#25391;&#26159;&#20256;&#32479;&#19978;&#24212;&#29992;&#20110;&#27668;&#20505;&#21160;&#21147;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#8221;&#65288;SRT&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SRT&#33021;&#22815;&#26377;&#25928;&#22320;&#36229;&#20998;&#36776;&#29575;&#39044;&#35757;&#32451;&#30340;ViTs&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#21040;&#20102;&#20316;&#20026;&#26631;&#35760;&#32467;&#26524;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26356;&#22810;&#23616;&#37096;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;SRT&#21487;&#20197;&#22312;&#20219;&#20309;&#23618;&#38754;&#12289;&#20219;&#20309;&#20219;&#21153;&#19978;&#24212;&#29992;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#21069;&#32773;&#30340;&#20248;&#21183;&#26159;&#26126;&#26174;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discover the presence of quantization artifacts in Vision Transformers (ViTs), which arise due to the image tokenization step inherent in these architectures. These artifacts result in coarsely quantized features, which negatively impact performance, especially on downstream dense prediction tasks. We present a zero-shot method to improve how pre-trained ViTs handle spatial quantization. In particular, we propose to ensemble the features obtained from perturbing input images via sub-token spatial translations, inspired by Stochastic Resonance, a method traditionally applied to climate dynamics and signal processing. We term our method ``Stochastic Resonance Transformer" (SRT), which we show can effectively super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might otherwise be neglected as a result of tokenization. SRT can be applied at any layer, on any task, and does not require any fine-tuning. The advantage of the former is evident
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11905</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25509;&#21463;&#36793;&#30028;&#36827;&#34892;&#21551;&#21457;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21069;&#21521;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#24212;&#35813;&#23398;&#20064;&#30340;&#20869;&#23481;&#12289;&#22914;&#20309;&#35757;&#32451;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;&#30340;&#29702;&#35770;&#35748;&#35782;&#36824;&#24456;&#23569;&#12290;&#36825;&#31181;&#29702;&#35299;&#30340;&#19981;&#36275;&#23548;&#33268;&#25991;&#29486;&#20013;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65288;&#27425;&#20248;&#25104;&#26412;&#23545;&#26368;&#20248;&#25104;&#26412;&#25110;&#21487;&#25509;&#21463;&#23545;&#19981;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#65289;&#21644;&#20248;&#21270;&#25351;&#26631;&#65288;&#20363;&#22914;&#24179;&#26041;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#65289;&#26102;&#36827;&#34892;&#20102;&#20020;&#26102;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#35757;&#32451;&#21551;&#21457;&#24335;&#20989;&#25968;&#32570;&#20047;&#21487;&#25509;&#21463;&#24615;&#65292;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25509;&#21463;&#24615;&#30340;&#37325;&#35201;&#24615;&#20063;&#32570;&#20047;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#30456;&#27604;&#26222;&#36890;&#39640;&#26031;&#20998;&#24067;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25968;&#23398;&#27169;&#22411;&#24544;&#23454;&#22320;&#36981;&#24490;&#20102;&#26368;&#22823;&#29109;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.09764</link><description>&lt;p&gt;
&#21435;&#38500;&#32972;&#26223;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#23578;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#65292;&#22312;&#24066;&#22330;&#19978;&#20855;&#26377;&#24456;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#30001;&#20110;&#26381;&#35013;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#20197;&#21450;&#21508;&#31181;&#22330;&#26223;&#21644;&#32972;&#26223;&#30340;&#23384;&#22312;&#65292;&#26102;&#23578;&#29702;&#35299;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20173;&#28982;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#20013;&#30340;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#33879;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26102;&#23578;&#25968;&#25454;&#36827;&#34892;&#32972;&#26223;&#21435;&#38500;&#12290;&#34987;&#21435;&#38500;&#32972;&#26223;&#30340;&#26102;&#23578;&#22270;&#20687;&#19982;&#26102;&#23578;&#25968;&#25454;&#38598;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#27169;&#22411;&#26550;&#26500;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#12289;&#19982;&#20854;&#20182;&#35757;&#32451;&#25216;&#24039;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20860;&#23481;&#24615;&#20197;&#21450;&#30446;&#26631;&#20219;&#21153;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background 
&lt;/p&gt;</description></item><item><title>&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09520</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings of the 2nd International Workshop on Adaptive Cyber Defense. (arXiv:2308.09520v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09520
&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#22312;&#20315;&#32599;&#37324;&#36798;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#65292;&#35813;&#30740;&#35752;&#20250;&#26088;&#22312;&#20998;&#20139;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#32593;&#32476;&#39046;&#22495;&#26080;&#27861;&#21487;&#38752;&#26377;&#25928;&#22320;&#36827;&#34892;&#38450;&#24481;&#65292;&#24517;&#39035;&#24191;&#27867;&#20381;&#36182;&#20154;&#24037;&#19987;&#23478;&#12290;&#29087;&#32451;&#30340;&#32593;&#32476;&#38450;&#24481;&#20154;&#21592;&#20379;&#24212;&#19981;&#36275;&#65292;&#24448;&#24448;&#26080;&#27861;&#21450;&#26102;&#24212;&#23545;&#32593;&#32476;&#23041;&#32961;&#12290;&#20511;&#37492;AI&#21644;ML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32593;&#32476;&#38450;&#24481;&#30740;&#31350;&#31038;&#21306;&#34987;&#28608;&#21169;&#30528;&#36890;&#36807;&#23558;AI&#21644;ML&#25216;&#26415;&#24212;&#29992;&#20110;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#24320;&#21457;&#26032;&#30340;&#21160;&#24577;&#21487;&#25345;&#32493;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#19982;&#23454;&#36341;&#32773;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#21487;&#20197;&#21152;&#36895;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#35782;&#21035;&#21644;&#24212;&#23545;&#32593;&#32476;&#25915;&#20987;&#65292;&#25110;&#32773;&#21457;&#29616;&#21644;&#20943;&#36731;&#24369;&#28857;&#30340;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 2nd International Workshop on Adaptive Cyber Defense was held at the Florida Institute of Technology, Florida. This workshop was organized to share research that explores unique applications of Artificial Intelligence (AI) and Machine Learning (ML) as foundational capabilities for the pursuit of adaptive cyber defense. The cyber domain cannot currently be reliably and effectively defended without extensive reliance on human experts. Skilled cyber defenders are in short supply and often cannot respond fast enough to cyber threats.  Building on recent advances in AI and ML the Cyber defense research community has been motivated to develop new dynamic and sustainable defenses through the adoption of AI and ML techniques to cyber settings. Bridging critical gaps between AI and Cyber researchers and practitioners can accelerate efforts to create semi-autonomous cyber defenses that can learn to recognize and respond to cyber attacks or discover and mitigate weaknesses in cooperation with
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04216</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25439;&#21387;&#32553;&#24050;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#20013;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#36825;&#31181;&#21387;&#32553;&#26041;&#27861;&#23545;&#20110;&#22823;&#23567;&#22312;&#20960;&#20010;PB&#33539;&#22260;&#20869;&#30340;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#21387;&#32553;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20294;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#22312;&#31185;&#23398;&#25968;&#25454;&#39046;&#22495;&#23578;&#26410;&#24191;&#20026;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#30340;&#31185;&#23398;&#22522;&#20934;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#24212;&#29992;&#20110;&#19968;&#31181;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#30340;&#27668;&#20505;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;140&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#39640;&#20998;&#36776;&#29575;&#31038;&#21306;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(CESM) Version 1.3&#30340;&#27169;&#25311;&#25968;&#25454;&#22312;&#21387;&#32553;&#27604;&#36798;&#21040;200&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the recon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13551</link><description>&lt;p&gt;
EntRED: &#29992;&#26356;&#23569;&#30340;&#25463;&#24452;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21517;&#31216;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#36215;&#30528;&#26377;&#25928;&#30340;&#20316;&#29992;&#65292;&#24182;&#24120;&#24120;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#38598;&#20013;&#30340;&#23454;&#20307;&#21517;&#31216;&#26174;&#33879;&#24433;&#21709;&#20102;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#22823;&#37327;&#38169;&#35823;&#30340;&#23454;&#20307;&#27880;&#37322;&#65292;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EntRED&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#25463;&#24452;&#21644;&#26356;&#39640;&#23454;&#20307;&#22810;&#26679;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#26500;&#24314;EntRED&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#65288;CI&#65289;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26367;&#25442;&#31649;&#36947;&#65306;ERIC&#12290;ERIC&#23545;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#32422;&#26463;&#26367;&#25442;&#65292;&#20197;&#20943;&#23569;&#20174;&#23454;&#20307;&#20559;&#24046;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;ERIC&#22312;&#20004;&#20010;&#26041;&#38754;&#24212;&#29992;CI&#65306;1&#65289;&#38024;&#23545;&#38656;&#35201;&#23454;&#20307;&#26367;&#25442;&#30340;&#23454;&#20363;&#65292;2&#65289;&#30830;&#23450;&#20505;&#36873;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
&lt;/p&gt;</description></item></channel></rss>