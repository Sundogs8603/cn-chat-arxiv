<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.15043</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#36890;&#29992;&#21644;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#24341;&#36215;&#21453;&#24863;&#30340;&#20869;&#23481;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#29983;&#25104;&#12290;&#23613;&#31649;&#22312;&#35268;&#36991;&#36825;&#20123;&#25514;&#26045;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#25152;&#35859;&#30340;&#23545;LLMs&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38656;&#35201;&#20154;&#20026;&#30340;&#24039;&#24605;&#65292;&#23454;&#38469;&#19978;&#24182;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#21518;&#32512;&#65292;&#24403;&#38468;&#21152;&#21040;&#21508;&#31181;&#26597;&#35810;&#19978;&#65292;&#20379;LLM&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#26102;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#20135;&#29983;&#32943;&#23450;&#22238;&#31572;&#65288;&#32780;&#19981;&#26159;&#25298;&#32477;&#22238;&#31572;&#65289;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#36825;&#20123;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#19988;&#22312;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;SuperCLUE&#65292;&#21253;&#25324;&#23454;&#38469;&#29992;&#25143;&#30340;&#26597;&#35810;&#21644;&#35780;&#32423;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22635;&#34917;&#20102;&#30446;&#21069;&#23545;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#21147;&#29702;&#35299;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.15020</link><description>&lt;p&gt;
SuperCLUE:&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;SuperCLUE&#65292;&#21253;&#25324;&#23454;&#38469;&#29992;&#25143;&#30340;&#26597;&#35810;&#21644;&#35780;&#32423;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22635;&#34917;&#20102;&#30446;&#21069;&#23545;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#21147;&#29702;&#35299;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26174;&#31034;&#20986;&#23558;&#20854;&#25972;&#21512;&#21040;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#20559;&#22909;&#26159;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#26368;&#20851;&#38190;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#22810;&#36873;&#39064;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;SuperCLUE&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#20197;&#21478;&#19968;&#20010;&#27969;&#34892;&#30340;&#20013;&#25991;LLM&#22522;&#20934;&#27979;&#35797;CLUE&#21629;&#21517;&#12290;SuperCLUE&#21253;&#25324;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#26469;&#33258;&#19968;&#20010;LLM&#23545;&#25112;&#24179;&#21488;(CArena)&#30340;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#21644;&#35780;&#32423;&#65292;&#26377;&#21333;&#20010;&#21644;&#22810;&#36718;&#23545;&#35805;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;(OPEN)&#65292;&#20197;&#21450;&#19982;&#24320;&#25918;&#24615;&#21333;&#36718;&#38382;&#39064;&#30456;&#21516;&#33550;&#30340;&#23553;&#38381;&#24615;&#38382;&#39064;(CLOSE)&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23553;&#38381;&#24615;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#24615;&#19981;&#36275;&#20197;&#21453;&#26144;&#22312;&#24320;&#25918;&#24615;&#38382;&#39064;&#19978;&#23454;&#29616;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20114;&#34917;&#22320;&#39044;&#27979;&#23454;&#38469;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user prefe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.15016</link><description>&lt;p&gt;
Google Bard&#30340;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#22914;&#20309;&#65311;&#24320;&#25918;&#25361;&#25112;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19982;OpenAI&#30340;ChatGPT&#25104;&#20026;&#20102;&#24378;&#22823;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Bard&#26368;&#36817;&#24050;&#32463;&#26356;&#26032;&#65292;&#21487;&#20197;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#22788;&#29702;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#37492;&#20110;Bard&#22312;&#22788;&#29702;&#25991;&#26412;&#36755;&#20837;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#30001;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#65288;&#22270;&#20687;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25506;&#32034;&#26377;&#28508;&#21147;&#25581;&#31034;Bard&#21644;&#20854;&#20182;&#21363;&#23558;&#21457;&#24067;&#30340;&#22810;&#27169;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#20934;&#30830;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#22797;&#26434;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#26102;&#30340;&#26032;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;15&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#22330;&#26223;&#65292;&#21253;&#25324;&#24120;&#35268;&#12289;&#20266;&#35013;&#12289;&#21307;&#23398;&#12289;&#27700;&#19979;&#21644;&#36965;&#24863;&#25968;&#25454;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;Bard&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#34920;&#26126;&#65292;Bard&#22312;&#36825;&#20123;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#31361;&#26174;&#20102;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#33021;&#36741;&#21161;&#36827;&#34892;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#20197;AI-Guardian&#20026;&#26696;&#20363;&#35780;&#20272;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#25104;&#21151;&#30772;&#35299;&#20102;AI-Guardian&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#36807;&#25351;&#31034;&#21644;&#24341;&#23548;GPT-4&#23454;&#26045;&#25915;&#20987;&#31639;&#27861;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15008</link><description>&lt;p&gt;
&#36890;&#36807;LLM&#36741;&#21161;&#65292;&#23545;AI-Guardian&#30340;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#33021;&#36741;&#21161;&#36827;&#34892;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#20197;AI-Guardian&#20026;&#26696;&#20363;&#35780;&#20272;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#25104;&#21151;&#30772;&#35299;&#20102;AI-Guardian&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#36807;&#25351;&#31034;&#21644;&#24341;&#23548;GPT-4&#23454;&#26045;&#25915;&#20987;&#31639;&#27861;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#33021;&#22815;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GPT-4&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#36741;&#21161;&#36827;&#34892;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#20197;AI-Guardian&#20026;&#26696;&#20363;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20010;&#26368;&#36817;&#22312;IEEE S&amp;P 2023&#19978;&#21457;&#34920;&#30340;&#38024;&#23545;&#23545;&#25239;&#26679;&#26412;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23436;&#20840;&#30772;&#35299;&#20102;&#36825;&#20010;&#38450;&#24481;&#26426;&#21046;&#65306;&#19982;&#26410;&#38450;&#24481;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#24182;&#27809;&#26377;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24182;&#27809;&#26377;&#32534;&#20889;&#25915;&#20987;&#35813;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#32780;&#26159;&#25351;&#31034;&#21644;&#24341;&#23548;GPT-4&#25353;&#29031;&#25105;&#20204;&#30340;&#25351;&#20196;&#23454;&#26045;&#25152;&#26377;&#25915;&#20987;&#31639;&#27861;&#12290;&#36825;&#20010;&#36807;&#31243;&#20986;&#22855;&#22320;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#26377;&#26102;&#20505;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#26126;&#30830;&#30340;&#25351;&#20196;&#19979;&#20135;&#29983;&#30340;&#20195;&#30721;&#27604;&#26412;&#25991;&#20316;&#32773;&#36824;&#35201;&#24555;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#65288;1&#65289;&#35780;&#20272;&#20013;&#20986;&#29616;&#30340;&#35686;&#31034;&#20449;&#21495;&#34920;&#26126;AI-Guardian&#23558;&#34987;&#25915;&#30772;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25105;&#20204;&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#25915;&#20987;&#26041;&#26696;&#26102;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&amp;P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.  We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing a
&lt;/p&gt;</description></item><item><title>Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14993</link><description>&lt;p&gt;
Thinker: &#23398;&#20064;&#35268;&#21010;&#21644;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14993
&lt;/p&gt;
&lt;p&gt;
Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Thinker&#31639;&#27861;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#19982;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24182;&#21033;&#29992;&#20854;&#12290; Thinker&#31639;&#27861;&#36890;&#36807;&#32473;&#29615;&#22659;&#28155;&#21152;&#19990;&#30028;&#27169;&#22411;&#26469;&#25913;&#21464;&#29615;&#22659;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#19982;&#19990;&#30028;&#27169;&#22411;&#20132;&#20114;&#30340;&#26032;&#21160;&#20316;&#12290;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#22312;&#36873;&#25321;&#26368;&#32456;&#30340;&#29615;&#22659;&#21160;&#20316;&#20043;&#21069;&#21521;&#19990;&#30028;&#27169;&#22411;&#25552;&#20986;&#22791;&#36873;&#35745;&#21010;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#20195;&#29702;&#33258;&#20027;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#35268;&#21010;&#26469;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#20801;&#35768;&#23545;&#20195;&#29702;&#30340;&#35745;&#21010;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;Thinker&#31639;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;Thinker&#31639;&#27861;&#35757;&#32451;&#30340;&#20195;&#29702;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#24050;&#32463;&#23398;&#21040;&#20102;&#20248;&#31168;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20195;&#30721;&#26356;&#25913;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#23454;&#29616;&#31532;&#19968;&#20010;LLM&#65292;Codeditor&#65292;&#20197;&#23558;&#20195;&#30721;&#26356;&#25913;&#24314;&#27169;&#20026;&#32534;&#36753;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#25105;&#20204;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#20849;&#21516;&#28436;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14991</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#20849;&#21516;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Multilingual Code Co-Evolution Using Large Language Models. (arXiv:2307.14991v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20195;&#30721;&#26356;&#25913;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#23454;&#29616;&#31532;&#19968;&#20010;LLM&#65292;Codeditor&#65292;&#20197;&#23558;&#20195;&#30721;&#26356;&#25913;&#24314;&#27169;&#20026;&#32534;&#36753;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#25105;&#20204;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#20849;&#21516;&#28436;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36719;&#20214;&#39033;&#30446;&#22312;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;API&#21644;&#31639;&#27861;&#12290;&#32500;&#25252;&#36825;&#26679;&#30340;&#39033;&#30446;&#24456;&#32321;&#29712;&#65292;&#22240;&#20026;&#24320;&#21457;&#20154;&#21592;&#24517;&#39035;&#30830;&#20445;&#20219;&#20309;&#21464;&#21270;&#65288;&#20363;&#22914;&#38169;&#35823;&#20462;&#22797;&#25110;&#26032;&#21151;&#33021;&#65289;&#33021;&#22815;&#21450;&#26102;&#19988;&#26080;&#35823;&#22320;&#20256;&#25773;&#21040;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#30340;&#23454;&#29616;&#20013;&#12290;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#36719;&#20214;&#19990;&#30028;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#32763;&#35793;&#24037;&#20855;&#65288;&#20363;&#22914;&#36716;&#35793;&#22120;&#65289;&#25110;&#29992;&#20110;&#23558;&#20195;&#30721;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26377;&#38480;&#30340;&#20215;&#20540;&#12290;&#27599;&#27425;&#23558;&#25972;&#20010;&#20195;&#30721;&#24211;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#26041;&#24335;&#24182;&#19981;&#31526;&#21512;&#24320;&#21457;&#20154;&#21592;&#30340;&#24037;&#20316;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20195;&#30721;&#26356;&#25913;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;LLM&#65292;&#21517;&#20026;Codeditor&#65292;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;Codeditor&#26126;&#30830;&#22320;&#23558;&#20195;&#30721;&#26356;&#25913;&#24314;&#27169;&#20026;&#32534;&#36753;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#24314;&#31435;&#26356;&#25913;&#30340;&#20851;&#32852;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;Codeditor&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;6,613&#20010;&#31034;&#20363;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 ali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14971</link><description>&lt;p&gt;
Take-A-Photo: &#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#28857;&#20113;&#27169;&#22411;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MAE&#24102;&#39046;&#19979;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#22312;2D&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#25552;&#21319;&#22522;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#35270;&#35273;&#39046;&#22495;&#65292;&#23545;Transformer&#20026;&#22522;&#30784;&#30340;&#39592;&#24178;&#32593;&#32476;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#28857;&#20113;&#30340;&#26080;&#24207;&#24615;&#38480;&#21046;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#20219;&#20309;&#28857;&#20113;&#27169;&#22411;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#19981;&#21516;&#30340;&#23039;&#21183;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#20854;&#28857;&#20113;&#23545;&#24212;&#29289;&#65292;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20855;&#26377;&#26356;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#20174;&#32780;&#24110;&#21161;3D&#32972;&#39592;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#26377;&#25928;&#22320;&#25552;&#21319;...
&lt;/p&gt;
&lt;p&gt;
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RRTF&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;PanGu-Coder2&#26159;&#35813;&#26694;&#26550;&#30340;&#23454;&#29616;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#20808;&#21069;&#30340;Code LLMs&#12290;</title><link>http://arxiv.org/abs/2307.14936</link><description>&lt;p&gt;
PanGu-Coder2&#65306;&#21033;&#29992;&#25490;&#21517;&#21453;&#39304;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RRTF&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;PanGu-Coder2&#26159;&#35813;&#26694;&#26550;&#30340;&#23454;&#29616;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#20808;&#21069;&#30340;Code LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#27599;&#21608;&#37117;&#26377;&#26032;&#30340;&#24378;&#22823;&#27169;&#22411;&#21457;&#24067;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#35757;&#32451;&#30340;Code LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#12289;&#25351;&#20196;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RRTF&#65288;Rank Responses to align Test&amp;Teacher Feedback&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#25552;&#21319;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PanGu-Coder2&#65292;&#22312;OpenAI HumanEval&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;62.20%&#30340;&#19968;&#32423;&#36890;&#36807;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;CoderEval&#21644;LeetCode&#22522;&#20934;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PanGu-Coder2&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;Code LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
&lt;/p&gt;</description></item><item><title>Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.14935</link><description>&lt;p&gt;
&#20351;&#29992;Desbordante&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65306;&#19968;&#39033;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14935
&lt;/p&gt;
&lt;p&gt;
Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#26159;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#34892;&#19994;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#65292;&#21253;&#25324;&#20989;&#25968;&#20381;&#36182;&#12289;&#25968;&#25454;&#32422;&#26463;&#12289;&#20851;&#32852;&#35268;&#21017;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#22797;&#26434;&#32479;&#35745;&#30340;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#31995;&#32479;&#22312;&#19982;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#20351;&#29992;&#30340;&#24037;&#20855;&#36827;&#34892;&#36866;&#24403;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#22312;&#34892;&#19994;&#20013;&#23545;&#36825;&#20123;&#24037;&#20855;&#37319;&#29992;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#31995;&#32479;&#24182;&#19981;&#32771;&#34385;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#19981;&#26088;&#22312;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#20160;&#20040;&#25214;&#19981;&#21040;&#32473;&#23450;&#30340;&#27169;&#24335;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#20102;&#35299;&#29305;&#23450;&#27169;&#24335;&#32570;&#22833;&#30340;&#26681;&#26412;&#21407;&#22240;&#23545;&#22522;&#20110;&#25968;&#25454;&#30340;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#24335;&#23454;&#38469;&#19978;&#26159;&#28040;&#22833;&#22312;&#31354;&#20013;&#20013;&#65306;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#30456;&#23545;&#26377;&#38480;&#65292;&#24456;&#23569;&#34987;&#24191;&#22823;&#20844;&#20247;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.  However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.  Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30005;&#21830;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#38598;&#25104;&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14906</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30005;&#21830;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#38598;&#25104;&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRON&#65292;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#30340;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#12290;&#21463;&#21040;SASRec&#21644;GRU4Rec+&#31561;&#29616;&#26377;&#27169;&#22411;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;TRON&#38598;&#25104;&#20102;top-k&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#20854;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#22312;&#30456;&#20851;&#30340;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;TRON&#22312;&#20445;&#25345;&#19982;SASRec&#31867;&#20284;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#19968;&#39033;&#23454;&#26102;&#30340;A/B&#27979;&#35797;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;SASRec&#65292;TRON&#30340;&#28857;&#20987;&#29575;&#22686;&#21152;&#20102;18.14%&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
&lt;/p&gt;</description></item><item><title>CodeLens&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#26041;&#27861;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#29702;&#35299;&#21644;&#25506;&#32034;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2307.14902</link><description>&lt;p&gt;
CodeLens: &#19968;&#31181;&#29992;&#20110;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
CodeLens: An Interactive Tool for Visualizing Code Representations. (arXiv:2307.14902v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14902
&lt;/p&gt;
&lt;p&gt;
CodeLens&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#26041;&#27861;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#29702;&#35299;&#21644;&#25506;&#32034;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28304;&#20195;&#30721;&#20197;&#36890;&#29992;&#36755;&#20837;&#26684;&#24335;&#34920;&#31034;&#23545;&#20110;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#20154;&#31867;&#19987;&#23478;&#30452;&#35266;&#22320;&#29702;&#35299;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19968;&#31181;&#36890;&#29992;&#24037;&#20855;&#33021;&#22815;&#21516;&#26102;&#21487;&#35270;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#30721;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeLens&#30340;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#20132;&#20114;&#29615;&#22659;&#65292;&#25903;&#25345;&#21508;&#31181;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#29702;&#35299;&#21644;&#25506;&#32034;&#23427;&#20204;&#12290;CodeLens&#34987;&#35774;&#35745;&#25104;&#25903;&#25345;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#22914;Java&#12289;Python&#21644;JavaScript&#65292;&#20197;&#21450;&#22235;&#31181;&#20195;&#30721;&#34920;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#26631;&#35760;&#24207;&#21015;&#12289;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#12289;&#25968;&#25454;&#27969;&#22270;&#65288;DFG&#65289;&#21644;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;CodeLens&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#24555;&#36895;&#21487;&#35270;&#21270;&#29305;&#23450;&#30340;&#20195;&#30721;&#34920;&#31034;&#65292;&#24182;&#33719;&#21462;&#20195;&#30721;&#27169;&#22411;&#30340;&#34920;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#25509;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#65288;CITE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#24191;&#27867;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#24471;&#21040;&#30340;&#25991;&#26412;&#35265;&#35299;&#26469;&#22686;&#24378;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14901</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#20110;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text-guided Foundation Model Adaptation for Pathological Image Classification. (arXiv:2307.14901v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#25509;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#65288;CITE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#24191;&#27867;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#24471;&#21040;&#30340;&#25991;&#26412;&#35265;&#35299;&#26469;&#22686;&#24378;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#36817;&#28608;&#22686;&#24320;&#21019;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#35757;&#32451;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#22823;&#22411;&#27169;&#22411;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#22270;&#20687;&#25968;&#25454;&#38598;&#36890;&#24120;&#32570;&#20047;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#27880;&#37322;&#21644;&#20016;&#23500;&#12290;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30693;&#35782;&#25351;&#23548;&#39640;&#25928;&#30340;&#22270;&#20687;&#35786;&#26029;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#25509;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#65288;CITE&#65289;&#26469;&#22686;&#24378;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#12290;CITE&#27880;&#20837;&#20102;&#20174;&#39044;&#20808;&#35757;&#32451;&#20102;&#24191;&#27867;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#25991;&#26412;&#35265;&#35299;&#65292;&#20174;&#32780;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#30149;&#29702;&#22270;&#20687;&#29702;&#35299;&#12290;&#36890;&#36807;&#23545;PatchGastric&#32963;&#32959;&#30244;&#30149;&#29702;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;CITE&#22312;&#29305;&#21035;&#26159;&#25968;&#25454;&#31232;&#32570;&#26102;&#19982;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;CITE&#25552;&#20379;&#20102;&#21033;&#29992;&#39046;&#22495;&#20869;&#25991;&#26412;&#30693;&#35782;&#21152;&#24378;&#25968;&#25454;-
&lt;/p&gt;
&lt;p&gt;
The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability. Yet pathological image datasets often lack biomedical text annotation and enrichment. Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest. In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification. CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding. Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce. CITE offers insights into leveraging in-domain text knowledge to reinforce data-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24565;&#22522;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#20165;&#20449;&#20219;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#21160;&#26816;&#26597;&#20844;&#24335;&#30340;PSPACE&#31639;&#27861;&#21644;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#30340;&#19987;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14893</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#20165;&#20449;&#20219;&#27169;&#22411;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Base-based Model Checking for Multi-Agent Only Believing (long version). (arXiv:2307.14893v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24565;&#22522;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#20165;&#20449;&#20219;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#21160;&#26816;&#26597;&#20844;&#24335;&#30340;PSPACE&#31639;&#27861;&#21644;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#30340;&#19987;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24565;&#22522;&#25193;&#23637;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#20165;&#20449;&#20219;&#35821;&#35328;&#20197;&#21450;&#20854;&#24102;&#26377;&#31169;&#26377;&#20449;&#24565;&#25193;&#23637;&#36816;&#31639;&#31526;&#30340;&#21160;&#24577;&#25193;&#23637;&#25552;&#20379;&#20102;&#33258;&#21160;&#26816;&#26597;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;PSPACE&#31639;&#27861;&#30340;&#27169;&#22411;&#26816;&#26597;&#65292;&#20381;&#36182;&#20110;&#23545;QBF&#30340;&#32422;&#31616;&#20197;&#21450;&#23545;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#30340;&#21478;&#19968;&#31181;&#19987;&#29992;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;QBF&#31639;&#27861;&#30340;&#23454;&#29616;&#21450;&#22312;&#20855;&#20307;&#31034;&#20363;&#20013;&#30340;&#35745;&#31639;&#26102;&#38388;&#30340;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel semantics for the language of multi-agent only believing exploiting belief bases, and show how to use it for automatically checking formulas of this language and of its dynamic extension with private belief expansion operators. We provide a PSPACE algorithm for model checking relying on a reduction to QBF and alternative dedicated algorithm relying on the exploration of the state space. We present an implementation of the QBF-based algorithm and some experimental results on computation time in a concrete example.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#22823;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#32423;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#26080;&#38656;2D/3D&#20851;&#38190;&#28857;&#26631;&#31614;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.14889</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving. (arXiv:2307.14889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14889
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#22823;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#32423;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#26080;&#38656;2D/3D&#20851;&#38190;&#28857;&#26631;&#31614;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#21644;&#22312;&#20851;&#38190;&#36947;&#36335;&#22330;&#26223;&#20013;&#31215;&#26497;&#24212;&#23545;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#22914;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#20307;&#32946;&#21644;&#21307;&#23398;&#20998;&#26512;&#31561;&#65292;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#22522;&#20110;&#22312;&#33391;&#22909;&#25511;&#21046;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#20934;&#30830;&#30340;3D&#23039;&#21183;&#26631;&#27880;&#30340;&#25361;&#25112;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#25968;&#25454;&#30340;&#26377;&#38480;&#36866;&#29992;&#24615;&#65292;&#23558;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#36716;&#31227;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#39640;&#32423;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;&#24369;&#30417;&#30563;&#30340;&#35774;&#32622;&#20351;&#24471;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#19981;&#38656;&#35201;&#20219;&#20309;2D/3D&#20851;&#38190;&#28857;&#26631;&#31614;&#65292;&#32780;&#26159;&#21033;&#29992;&#29616;&#25104;&#30340;2D&#20851;&#33410;&#28857;&#25552;&#21462;&#22120;&#21644;&#30001;&#28608;&#20809;&#38647;&#36798;&#21040;&#22270;&#20687;&#30340;&#25237;&#24433;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous vehicles (AVs) to make informed decisions and respond proactively in critical road scenarios. Promising results of 3D HPE have been gained in several domains such as human-computer interaction, robotics, sports and medical analytics, often based on data collected in well-controlled laboratory environments. Nevertheless, the transfer of 3D HPE methods to AVs has received limited research attention, due to the challenges posed by obtaining accurate 3D pose annotations and the limited suitability of data from other domains.  We present a simple yet efficient weakly supervised approach for 3D HPE in the AV context by employing a high-level sensor fusion between camera and LiDAR data. The weakly supervised setting enables training on the target datasets without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor and pseudo labels generated from LiDAR to image projections. Our approach outperform
&lt;/p&gt;</description></item><item><title>Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.14856</link><description>&lt;p&gt;
&#21457;&#25381;Seq2Seq&#27169;&#22411;&#20316;&#20026;&#31283;&#20581;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14856
&lt;/p&gt;
&lt;p&gt;
Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#21482;&#26377;&#35299;&#30721;&#22120;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#32780;&#32534;&#30721;-&#35299;&#30721;&#65288;&#21363;Seq2Seq&#65289;&#27169;&#22411;&#22312;&#20381;&#36182;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;Seq2Seq&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#36825;&#20165;&#38480;&#20110;&#19982;Seq2Seq&#20307;&#31995;&#32467;&#26500;&#30456;&#21305;&#37197;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#21644;&#32763;&#35793;&#12290;&#21463;&#21040;&#36825;&#20123;&#21021;&#22987;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26356;&#26377;&#25928;&#22320;&#24341;&#21457;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65306;&#30446;&#26631;&#23545;&#40784;&#25552;&#31034;&#21644;&#22522;&#20110;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#19968;&#20010;&#20307;&#31215;&#26159;&#20854;&#20845;&#20493;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#24120;&#35268;Seq2Seq&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#30340;&#20027;&#35201;&#29305;&#24449;&#29983;&#25104;&#22270;&#20998;&#31867;&#22120;&#30340;&#23454;&#20363;&#32423;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.14849</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#35270;&#35282;&#36827;&#34892;&#22270;&#20998;&#31867;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations for Graph Classification Through the Lenses of Density. (arXiv:2307.14849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#30340;&#20027;&#35201;&#29305;&#24449;&#29983;&#25104;&#22270;&#20998;&#31867;&#22120;&#30340;&#23454;&#20363;&#32423;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20363;&#23376;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20135;&#29983;&#31616;&#21333;&#26131;&#25026;&#30340;&#20107;&#21518;&#35299;&#37322;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#22270;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#25913;&#21464;&#22270;&#30340;&#26368;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#21024;&#38500;&#29616;&#26377;&#36793;&#25110;&#28155;&#21152;&#19981;&#23384;&#22312;&#30340;&#36793;&#65289;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#35299;&#37322;&#26041;&#24335;&#21487;&#33021;&#22826;&#32454;&#33268;&#65292;&#36716;&#32780;&#20851;&#27880;&#30495;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#30340;&#19968;&#20123;&#20027;&#35201;&#29305;&#24449;&#65292;&#22914;&#38381;&#21512;&#19977;&#35282;&#24418;&#30340;&#36235;&#21183;&#12289;&#37325;&#22797;&#30340;&#27169;&#24335;&#20197;&#21450;&#32452;&#32455;&#25104;&#23494;&#38598;&#27169;&#22359;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22270;&#20998;&#31867;&#22120;&#29983;&#25104;&#23454;&#20363;&#32423;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#23494;&#38598;&#23376;&#32467;&#26500;&#27010;&#24565;&#23454;&#20363;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#36890;&#29992;&#26694;&#26550;&#30340;&#20004;&#20010;&#20855;&#20307;&#23454;&#20363;&#21270;&#26041;&#27861;&#65306;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#21453;&#20107;&#23454;&#22270;&#26469;&#21305;&#37197;&#23494;&#38598;&#23376;&#32467;&#26500;&#27979;&#24230;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#21453;&#20107;&#23454;&#23376;&#22270;&#26469;&#21305;&#37197;&#23494;&#38598;&#23376;&#22270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;ASP&#26041;&#27861;&#35299;&#20915;&#20102;&#23454;&#38469;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#23558;&#20854;&#20855;&#20307;&#35201;&#27714;&#24314;&#27169;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.14799</link><description>&lt;p&gt;
&#28151;&#21512;ASP&#26041;&#27861;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#22810;&#30446;&#26631;&#35843;&#24230;&#65288;&#25193;&#23637;&#29256;&#26412;&#65289;
&lt;/p&gt;
&lt;p&gt;
Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;ASP&#26041;&#27861;&#35299;&#20915;&#20102;&#23454;&#38469;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#23558;&#20854;&#20855;&#20307;&#35201;&#27714;&#24314;&#27169;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21322;&#23548;&#20307;&#21046;&#36896;&#28041;&#21450;&#22797;&#26434;&#30340;&#29983;&#20135;&#36807;&#31243;&#65292;&#21253;&#25324;&#25968;&#30334;&#20010;&#25805;&#20316;&#65292;&#20174;&#25209;&#27425;&#21457;&#24067;&#21040;&#23436;&#25104;&#21487;&#33021;&#38656;&#35201;&#25968;&#26376;&#26102;&#38388;&#12290;&#36825;&#20123;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#39640;&#31185;&#25216;&#35774;&#22791;&#22810;&#26679;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#38454;&#27573;&#19978;&#23545;&#21333;&#20010;&#26230;&#22278;&#12289;&#25209;&#27425;&#25110;&#25209;&#27425;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#38656;&#35201;&#20135;&#21697;&#29305;&#23450;&#30340;&#35774;&#32622;&#21644;&#19987;&#38376;&#30340;&#32500;&#25252;&#31243;&#24207;&#12290;&#36825;&#31181;&#24773;&#20917;&#19982;&#20256;&#32479;&#30340;&#36710;&#38388;&#35843;&#24230;&#22330;&#26223;&#19981;&#21516;&#65292;&#21518;&#32773;&#20855;&#26377;&#36739;&#19981;&#22797;&#26434;&#30340;&#29983;&#20135;&#36807;&#31243;&#21644;&#35774;&#22791;&#65292;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#39640;&#24230;&#32452;&#21512;&#20294;&#25277;&#35937;&#30340;&#35843;&#24230;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24046;&#24322;&#36923;&#36753;&#30340;&#28151;&#21512;ASP&#27169;&#22411;&#26469;&#23545;&#23454;&#38469;&#30340;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#21253;&#25324;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#36138;&#23146;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#29420;&#31435;&#36827;&#34892;&#35843;&#24230;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
Modern semiconductor manufacturing involves intricate production processes consisting of hundreds of operations, which can take several months from lot release to completion. The high-tech machines used in these processes are diverse, operate on individual wafers, lots, or batches in multiple stages, and necessitate product-specific setups and specialized maintenance procedures. This situation is different from traditional job-shop scheduling scenarios, which have less complex production processes and machines, and mainly focus on solving highly combinatorial but abstract scheduling problems. In this work, we address the scheduling of realistic semiconductor manufacturing processes by modeling their specific requirements using hybrid Answer Set Programming with difference logic, incorporating flexible machine processing, setup, batching and maintenance operations. Unlike existing methods that schedule semiconductor manufacturing processes locally with greedy heuristics or by independen
&lt;/p&gt;</description></item><item><title>Emotion4MIDI&#26159;&#19968;&#20010;&#21253;&#21547;12k&#20010;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#26469;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14783</link><description>&lt;p&gt;
Emotion4MIDI&#65306;&#22522;&#20110;&#27468;&#35789;&#30340;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14783
&lt;/p&gt;
&lt;p&gt;
Emotion4MIDI&#26159;&#19968;&#20010;&#21253;&#21547;12k&#20010;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#26469;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12k&#20010;MIDI&#27468;&#26354;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#21322;&#22823;&#23567;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#65292;&#20026;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23588;&#20854;&#26159;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#21487;&#20197;&#22312;&#32447;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14754</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#36951;&#24536;&#65306;&#22312;&#20943;&#23569;&#24046;&#24322;&#30340;&#21516;&#26102;&#21024;&#38500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20844;&#20247;&#23545;&#20225;&#19994;&#25910;&#38598;&#21644;&#20351;&#29992;&#20010;&#20154;&#20449;&#24687;&#30340;&#24847;&#35782;&#22686;&#24378;&#65292;&#28040;&#36153;&#32773;&#31215;&#26497;&#21442;&#19982;&#20225;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25968;&#25454;&#31649;&#29702;&#26694;&#26550;&#65288;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65289;&#24050;&#32463;&#25552;&#20986;&#20102;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#65292;&#20801;&#35768;&#20010;&#20154;&#35831;&#27714;&#23558;&#20854;&#20010;&#20154;&#25968;&#25454;&#20174;&#32452;&#32455;&#20351;&#29992;&#30340;&#25968;&#25454;&#24211;&#21644;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36951;&#24536;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#36951;&#24536;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#36951;&#24536;&#35831;&#27714;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#22312;&#32447;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#36951;&#24536;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20854;&#20182;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.14750</link><description>&lt;p&gt;
&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31574;&#30053;&#65306;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#21477;&#23376;&#65292;&#24182;&#23558;&#20854;&#19982;&#32473;&#23450;&#30340;&#22270;&#20687;&#23545;&#40784;&#20316;&#20026;&#20266;&#27880;&#37322;&#65292;&#25110;&#32773;&#20351;&#29992;&#22806;&#37096;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#40784;&#30340;&#35774;&#32622;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#20284;&#20046;&#24050;&#32463;&#36798;&#21040;&#20102;&#26497;&#38480;&#65292;&#32780;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#8220;LPM + &#26816;&#32034;&#22686;&#24378;&#23398;&#20064;&#8221;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;&#65288;RaPSG&#65289;&#65292;&#37319;&#29992;&#39640;&#25928;&#30340;&#26041;&#27861;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#39640;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;SmartonAI&#25554;&#20214;&#65292;&#22522;&#20110;GPT&#21644;BERT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#33539;&#24335;&#26469;&#35299;&#20915;EDA&#36719;&#20214;&#20013;&#21021;&#23398;&#32773;&#38754;&#20020;&#30340;&#22797;&#26434;&#21629;&#20196;&#32467;&#26500;&#21644;&#39640;&#23398;&#20064;&#26354;&#32447;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14740</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#30340;&#22797;&#26434;EDA&#36719;&#20214;&#26032;&#20132;&#20114;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
New Interaction Paradigm for Complex EDA Software Leveraging GPT. (arXiv:2307.14740v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;SmartonAI&#25554;&#20214;&#65292;&#22522;&#20110;GPT&#21644;BERT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#33539;&#24335;&#26469;&#35299;&#20915;EDA&#36719;&#20214;&#20013;&#21021;&#23398;&#32773;&#38754;&#20020;&#30340;&#22797;&#26434;&#21629;&#20196;&#32467;&#26500;&#21644;&#39640;&#23398;&#20064;&#26354;&#32447;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#39046;&#22495;&#20013;&#65292;&#19987;&#19994;&#36719;&#20214;&#22914;KiCad&#12289;Cadence&#21644;Altium Designer&#25552;&#20379;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#35774;&#35745;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#21629;&#20196;&#32467;&#26500;&#21644;&#36739;&#39640;&#30340;&#23398;&#20064;&#26354;&#32447;&#23545;&#20110;&#21021;&#23398;&#32773;&#30340;&#21360;&#21047;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#35774;&#35745;&#24072;&#26469;&#35828;&#36896;&#25104;&#20102;&#38556;&#30861;&#12290;&#36825;&#23548;&#33268;&#38590;&#20197;&#36873;&#25321;&#36866;&#21512;&#19981;&#21516;&#35774;&#35745;&#30446;&#30340;&#30340;&#21151;&#33021;&#25110;&#25554;&#20214;&#65292;&#24182;&#19988;&#20256;&#32479;&#25991;&#26723;&#12289;&#35270;&#39057;&#21644;&#22312;&#32447;&#35770;&#22363;&#20043;&#22806;&#32570;&#20047;&#30452;&#35266;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SmartonAI&#30340;EDA&#36719;&#20214;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#36741;&#21161;&#25554;&#20214;&#65292;&#20854;&#20013;&#20197;KiCad&#20316;&#20026;&#31532;&#19968;&#20010;&#31034;&#20363;&#12290;SmartonAI&#21463;&#21040;HuggingGPT&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#37319;&#29992;&#20102;GPT&#21644;BERT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;&#24403;&#25509;&#25910;&#21040;&#35774;&#35745;&#24072;&#30340;&#35831;&#27714;&#26102;&#65292;SmartonAI&#20250;&#36827;&#34892;&#20219;&#21153;&#20998;&#35299;&#24182;&#39640;&#25928;&#25191;&#34892;&#30456;&#20851;&#30340;&#23376;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
In the rapidly growing field of electronic design automation (EDA), professional software such as KiCad, Cadence , and Altium Designer provide increasingly extensive design functionalities. However, the intricate command structure and high learning curve create a barrier, particularly for novice printed circuit board (PCB) designers. This results in difficulties in selecting appropriate functions or plugins for varying design purposes, compounded by the lack of intuitive learning methods beyond traditional documentation, videos, and online forums. To address this challenge, an artificial intelligence (AI) interaction assist plugin for EDA software named SmartonAl is developed here, also KiCad is taken as the first example. SmartonAI is inspired by the HuggingGPT framework and employs large language models, such as GPT and BERT, to facilitate task planning and execution. On receiving a designer request, SmartonAI conducts a task breakdown and efficiently executes relevant subtasks, such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#25991;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#24182;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#22256;&#38590;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#23481;&#26131;&#29983;&#25104;&#24102;&#26377;&#24187;&#35273;&#25110;&#26080;&#20851;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.14712</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#25991;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#25991;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#24182;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#22256;&#38590;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#23481;&#26131;&#29983;&#25104;&#24102;&#26377;&#24187;&#35273;&#25110;&#26080;&#20851;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#25991;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;LLM&#30340;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36164;&#28304;&#21644;&#27880;&#37322;&#24037;&#20316;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22270;&#25968;&#25454;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3&#21644;ChatGPT&#22312;&#20004;&#20010;&#22270;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#24494;&#35843;&#30340;LLM&#27169;&#22411;&#65288;&#22914;T5&#21644;BART&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#24182;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#22312;AGENDA&#21644;WebNLG&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;10.57&#21644;11.08&#30340;BLEU&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;&#19988;&#23427;&#20204;&#36824;&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#24187;&#35273;&#25110;&#26080;&#20851;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;&#20316;&#20026;&#38169;&#35823;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;BERT&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#23439;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#26045;&#21152;&#29289;&#29702;&#32422;&#26463;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#28065;&#36718;&#26426;&#30340;&#21151;&#29575;&#12289;&#36716;&#30697;&#21644;&#21151;&#29575;&#31995;&#25968;&#12290;&#36825;&#23545;&#20110;&#20248;&#21270;&#28065;&#36718;&#26426;&#36816;&#34892;&#21644;&#26089;&#26399;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.14675</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#35777;&#25454;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#26045;&#21152;&#29289;&#29702;&#32422;&#26463;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#28065;&#36718;&#26426;&#30340;&#21151;&#29575;&#12289;&#36716;&#30697;&#21644;&#21151;&#29575;&#31995;&#25968;&#12290;&#36825;&#23545;&#20110;&#20248;&#21270;&#28065;&#36718;&#26426;&#36816;&#34892;&#21644;&#26089;&#26399;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#30340;&#19981;&#26029;&#22686;&#38271;&#20351;&#36890;&#36807;&#20559;&#33322;&#35282;&#25511;&#21046;&#22120;&#26469;&#20248;&#21270;&#28065;&#36718;&#26426;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#26089;&#26399;&#25925;&#38556;&#26816;&#27979;&#26469;&#36827;&#34892;&#32500;&#25252;&#21464;&#24471;&#24517;&#35201;&#12290;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#34892;&#20026;&#23588;&#20854;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#29983;&#25104;&#30340;&#21151;&#29575;&#19982;&#39118;&#36895;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#32463;&#39564;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#36755;&#20837;&#21464;&#37327;&#21644;&#21151;&#29575;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#21463;&#39118;&#21464;&#24615;&#30340;&#21152;&#21095;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20026;&#36890;&#36807;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26469;&#22686;&#24378;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20197;&#27169;&#25311;&#26469;&#33258;&#19968;&#20010;&#39118;&#22330;&#30340;4&#21488;&#28065;&#36718;&#26426;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#19968;&#23450;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#24320;&#21457;&#30340;&#29992;&#20110;&#22238;&#24402;&#21151;&#29575;&#12289;&#36716;&#30697;&#21644;&#21151;&#29575;&#31995;&#25968;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#25511;&#21046;&#26041;&#31243;&#26041;&#38754;&#37117;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing use of wind energy makes necessary the optimization of turbine operations through pitch angle controllers and their maintenance with early fault detection. It is crucial to have accurate and robust models imitating the behavior of wind turbines, especially to predict the generated power as a function of the wind speed. Existing empirical and physics-based models have limitations in capturing the complex relations between the input variables and the power, aggravated by wind variability. Data-driven methods offer new opportunities to enhance wind turbine modeling of large datasets by improving accuracy and efficiency. In this study, we used physics-informed neural networks to reproduce historical data coming from 4 turbines in a wind farm, while imposing certain physical constraints to the model. The developed models for regression of the power, torque, and power coefficient as output variables showed great accuracy for both real data and physical equations governing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26377;&#24207;&#29305;&#24449;&#36923;&#36753;&#25512;&#24191;&#21040;&#27169;&#31946;&#35774;&#32622;&#20013;&#65292;&#24182;&#32473;&#20986;&#20102;&#27169;&#31946;&#30340;OSF&#36923;&#36753;&#35821;&#20041;&#12290;&#36890;&#36807;&#25193;&#23637;&#21253;&#21547;&#20851;&#31995;&#21040;OSF&#26415;&#35821;&#65292;&#25105;&#20204;&#26500;&#25104;&#20102;&#19968;&#20010;&#27169;&#31946;&#20559;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.14669</link><description>&lt;p&gt;
&#27169;&#31946;&#26377;&#24207;&#29305;&#24449;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Fuzzy order-sorted feature logic. (arXiv:2307.14669v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26377;&#24207;&#29305;&#24449;&#36923;&#36753;&#25512;&#24191;&#21040;&#27169;&#31946;&#35774;&#32622;&#20013;&#65292;&#24182;&#32473;&#20986;&#20102;&#27169;&#31946;&#30340;OSF&#36923;&#36753;&#35821;&#20041;&#12290;&#36890;&#36807;&#25193;&#23637;&#21253;&#21547;&#20851;&#31995;&#21040;OSF&#26415;&#35821;&#65292;&#25105;&#20204;&#26500;&#25104;&#20102;&#19968;&#20010;&#27169;&#31946;&#20559;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#24207;&#29305;&#24449;&#65288;OSF&#65289;&#36923;&#36753;&#26159;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#34920;&#31034;&#29305;&#24449;&#31526;&#21495;&#21644;&#38598;&#21512;&#34920;&#31034;&#25490;&#24207;&#31526;&#21495;&#30340;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#35821;&#35328;&#65292;&#36825;&#20123;&#31526;&#21495;&#22312;&#19968;&#20010;&#21253;&#21547;&#20851;&#31995;&#26684;&#20013;&#25490;&#21015;&#12290; OSF&#36923;&#36753;&#20801;&#35768;&#26500;&#24314;&#31867;&#20284;&#35760;&#24405;&#30340;&#26415;&#35821;&#65292;&#34920;&#31034;&#23454;&#20307;&#31867;&#21035;&#65292;&#24182;&#19988;&#36825;&#20123;&#26415;&#35821;&#26412;&#36523;&#20063;&#25353;&#29031;&#21253;&#21547;&#20851;&#31995;&#25490;&#24207;&#12290;&#36825;&#31181;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;&#22411;&#21253;&#21547;&#28436;&#31639;&#65292;&#24050;&#32463;&#24212;&#29992;&#20110;&#35745;&#31639;&#35821;&#35328;&#23398;&#65292;&#24182;&#22312;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;LOGIN&#21644;LIFE&#65289;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#65288;&#22914;CEDAR&#65289;&#20013;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;OSF&#36923;&#36753;&#25512;&#24191;&#21040;&#27169;&#31946;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#31946;&#21253;&#21547;&#20851;&#31995;&#30340;&#23450;&#20041;&#65292;&#23427;&#25512;&#24191;&#20102;Zadeh&#30340;&#27169;&#31946;&#38598;&#21253;&#21547;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#23450;&#20041;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#27169;&#31946;&#30340;OSF&#36923;&#36753;&#35821;&#20041;&#65292;&#20854;&#20013;&#25490;&#24207;&#31526;&#21495;&#21644;OSF&#26415;&#35821;&#34920;&#31034;&#27169;&#31946;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#21253;&#21547;&#20851;&#31995;&#25193;&#23637;&#21040;OSF&#26415;&#35821;&#65292;&#24182;&#35777;&#26126;&#23427;&#26500;&#25104;&#20102;&#19968;&#20010;&#27169;&#31946;&#20559;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order-Sorted Feature (OSF) logic is a knowledge representation and reasoning language based on function-denoting feature symbols and set-denoting sort symbols ordered in a subsumption lattice. OSF logic allows the construction of record-like terms that represent classes of entities and that are themselves ordered in a subsumption relation. The unification algorithm for such structures provides an efficient calculus of type subsumption, which has been applied in computational linguistics and implemented in constraint logic programming languages such as LOGIN and LIFE and automated reasoners such as CEDAR. This work generalizes OSF logic to a fuzzy setting. We give a flexible definition of a fuzzy subsumption relation which generalizes Zadeh's inclusion between fuzzy sets. Based on this definition we define a fuzzy semantics of OSF logic where sort symbols and OSF terms denote fuzzy sets. We extend the subsumption relation to OSF terms and prove that it constitutes a fuzzy partial order 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#35268;&#21010;&#20013;&#30340;&#22810;&#20540;&#20559;&#24207;&#35745;&#21010;&#65292;&#22312;&#20998;&#26512;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#21028;&#23450;&#24615;&#30340;&#21407;&#22240;&#26102;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#20316;&#21457;&#29983;&#27425;&#25968;&#30340;&#19981;&#21516;&#24773;&#20917;&#12290;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#38480;&#21046;&#20219;&#21153;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#25214;&#21040;&#25968;&#20540;&#35268;&#21010;&#30340;&#19968;&#20010;NP&#23436;&#20840;&#29255;&#27573;&#12290;&#36890;&#36807;&#24320;&#21457;&#22810;&#20540;&#20559;&#24207;&#35745;&#21010;&#30340;&#24819;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#25215;&#35834;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#12290;&#35813;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#21270;&#25216;&#26415;&#33021;&#22815;&#32435;&#20837;&#36719;&#20808;&#20915;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.14660</link><description>&lt;p&gt;
&#25968;&#20540;&#35268;&#21010;&#20013;&#30340;&#22810;&#20540;&#20559;&#24207;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Valued Partial Order Plans in Numeric Planning. (arXiv:2307.14660v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#35268;&#21010;&#20013;&#30340;&#22810;&#20540;&#20559;&#24207;&#35745;&#21010;&#65292;&#22312;&#20998;&#26512;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#21028;&#23450;&#24615;&#30340;&#21407;&#22240;&#26102;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#20316;&#21457;&#29983;&#27425;&#25968;&#30340;&#19981;&#21516;&#24773;&#20917;&#12290;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#38480;&#21046;&#20219;&#21153;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#25214;&#21040;&#25968;&#20540;&#35268;&#21010;&#30340;&#19968;&#20010;NP&#23436;&#20840;&#29255;&#27573;&#12290;&#36890;&#36807;&#24320;&#21457;&#22810;&#20540;&#20559;&#24207;&#35745;&#21010;&#30340;&#24819;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#25215;&#35834;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#12290;&#35813;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#21270;&#25216;&#26415;&#33021;&#22815;&#32435;&#20837;&#36719;&#20808;&#20915;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35268;&#21010;&#24418;&#24335;&#20801;&#35768;&#23558;&#25968;&#23383;&#19982;&#24067;&#23572;&#25928;&#24212;&#28151;&#21512;&#20351;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#24418;&#24335;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;&#26412;&#25991;&#23558;&#36890;&#36807;&#30740;&#31350;&#21160;&#20316;&#21457;&#29983;&#27425;&#25968;&#30340;&#19981;&#21516;&#24773;&#20917;&#26469;&#20998;&#26512;&#23548;&#33268;&#19981;&#21487;&#21028;&#23450;&#24615;&#30340;&#21487;&#33021;&#21407;&#22240;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24230;&#37327;&#35859;&#35789;&#20043;&#21069;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#23558;&#20174;&#37325;&#26032;&#23450;&#20041;&#38480;&#21046;&#20219;&#21153;&#36825;&#20010;&#24050;&#30693;&#30340;&#25968;&#20540;&#35268;&#21010;&#38382;&#39064;&#20026;&#25628;&#32034;&#38382;&#39064;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#25214;&#21040;&#25968;&#20540;&#35268;&#21010;&#30340;&#19968;&#20010;NP&#23436;&#20840;&#29255;&#27573;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#24320;&#21457;&#22810;&#20540;&#20559;&#24207;&#35745;&#21010;&#30340;&#24819;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#23567;&#25215;&#35834;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#65288;&#36880;&#27493;&#21644;&#24182;&#34892;&#35745;&#21010;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#20197;&#32435;&#20837;&#36719;&#20808;&#20915;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many planning formalisms allow for mixing numeric with Boolean effects. However, most of these formalisms are undecidable. In this paper, we will analyze possible causes for this undecidability by studying the number of different occurrences of actions, an approach that proved useful for metric fluents before. We will start by reformulating a numeric planning problem known as restricted tasks as a search problem. We will then show how an NP-complete fragment of numeric planning can be found by using heuristics. To achieve this, we will develop the idea of multi-valued partial order plans, a least committing compact representation for (sequential and parallel) plans. Finally, we will study optimization techniques for this representation to incorporate soft preconditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.14634</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25253;&#21578;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#65292;&#29616;&#22312;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#33258;&#21160;&#25253;&#21578;&#26469;&#23545;&#25918;&#23556;&#23398;&#22270;&#20687;&#36827;&#34892;&#21021;&#27493;&#38405;&#35835;&#12290;&#36825;&#21487;&#20197;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#23548;&#33268;&#29983;&#25104;&#25253;&#21578;&#20013;&#20986;&#29616;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#19982;&#25551;&#36848;&#30495;&#23454;&#25110;&#28508;&#22312;&#34394;&#20551;&#21457;&#29616;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24320;&#21457;&#30340;&#26680;&#26597;&#32773;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#26680;&#26597;&#32773;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20266;&#36896;&#25253;&#21578;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#23558;&#26469;&#33258;&#36825;&#20123;&#25253;&#21578;&#30340;&#30495;&#20551;&#21477;&#23376;&#30340;&#25991;&#26412;&#32534;&#30721;&#19982;&#22270;&#20687;&#32534;&#30721;&#37197;&#23545;&#65292;&#23398;&#20064;&#26144;&#23556;&#21040;&#30495;/&#20551;&#26631;&#31614;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MBL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39030;&#32423;SARI&#24471;&#20998;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#22312;&#36739;&#22823;&#22411;&#30340;GPT&#27169;&#22411;&#19978;&#33719;&#24471;&#26368;&#20339;&#34920;&#29616;&#12290;&#32780;&#22312;&#36739;&#23567;&#22411;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36739;&#39640;&#21387;&#32553;&#27604;&#20363;&#30340;&#31034;&#20363;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.14632</link><description>&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#25991;&#26412;&#31616;&#21270;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MBL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39030;&#32423;SARI&#24471;&#20998;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#22312;&#36739;&#22823;&#22411;&#30340;GPT&#27169;&#22411;&#19978;&#33719;&#24471;&#26368;&#20339;&#34920;&#29616;&#12290;&#32780;&#22312;&#36739;&#23567;&#22411;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36739;&#39640;&#21387;&#32553;&#27604;&#20363;&#30340;&#31034;&#20363;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#36873;&#25321;ICL&#31034;&#20363;&#30340;&#26368;&#20339;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#32467;&#26524;&#21487;&#20197;&#22240;&#20351;&#29992;&#30340;&#31034;&#20363;&#30340;&#36136;&#37327;&#12289;&#25968;&#37327;&#21644;&#39034;&#24207;&#32780;&#21464;&#21270;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110;&#25991;&#26412;&#31616;&#21270;&#65288;TS&#65289;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#25506;&#35752;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#21644;&#26368;&#20581;&#22766;&#30340;ICL&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#29992;&#30340;TS&#24230;&#37327;&#65288;&#22914;SARI&#12289;&#21387;&#32553;&#27604;&#20363;&#21644;BERT-Precision&#65289;&#36827;&#34892;&#36873;&#25321;&#30340;&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MBL&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#26631;&#20934;&#30340;TS&#22522;&#20934;&#65288;&#22914;TurkCorpus&#21644;ASSET&#65289;&#19978;&#20351;&#29992;&#21508;&#31181;&#35268;&#27169;&#30340;GPT&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-175B&#65289;&#19978;&#36873;&#25321;&#30340;&#31034;&#20363;&#36890;&#36807;&#39030;&#32423;SARI&#24471;&#20998;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#21387;&#32553;&#27604;&#20363;&#36890;&#24120;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-13B&#21644;GPT-6.7B&#65289;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;MBL&#36890;&#24120;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust 
&lt;/p&gt;</description></item><item><title>BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14623</link><description>&lt;p&gt;
BubbleML: &#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14623
&lt;/p&gt;
&lt;p&gt;
BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21464;&#29616;&#35937;&#39046;&#22495;&#65292;&#32570;&#20047;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#21487;&#35775;&#38382;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#36890;&#24120;&#21463;&#38480;&#65292;&#21487;&#29992;&#24615;&#26377;&#38480;&#19988;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#31232;&#32570;&#65292;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#36825;&#31181;&#22797;&#26434;&#22810;&#29289;&#29702;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BubbleML&#25968;&#25454;&#38598;&#65288;https://github.com/HPCForge/BubbleML&#65289;&#65292;&#23427;&#21033;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;&#27169;&#25311;&#20026;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#25552;&#20379;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#21253;&#25324;&#26680;&#27873;&#27744;&#27832;&#33150;&#12289;&#27969;&#21160;&#27832;&#33150;&#21644;&#20122;&#20919;&#27832;&#33150;&#12290;&#36825;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#37325;&#21147;&#26465;&#20214;&#12289;&#27969;&#37327;&#12289;&#20122;&#20919;&#27700;&#24179;&#21644;&#22721;&#38754;&#36807;&#28909;&#65292;&#24635;&#20849;&#26377;51&#20010;&#27169;&#25311;&#12290;BubbleML&#24050;&#32463;&#36890;&#36807;&#23454;&#39564;&#35266;&#23519;&#21644;&#36235;&#21183;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34987;&#30830;&#35748;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20419;&#36827;&#22810;&#26679;&#21270;&#38477;&#20302;&#28201;&#24230;&#27832;&#33150;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;&#65288;SCGDN&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#25193;&#25955;&#27169;&#22359;&#23454;&#29616;&#23545;&#39640;&#38454;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#20248;&#31168;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;SCGDN&#26159;&#19968;&#31181;&#26080;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#37319;&#26679;&#20559;&#24046;&#8221;&#21644;&#35821;&#20041;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14613</link><description>&lt;p&gt;
&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;&#65288;SCGDN&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#25193;&#25955;&#27169;&#22359;&#23454;&#29616;&#23545;&#39640;&#38454;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#20248;&#31168;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;SCGDN&#26159;&#19968;&#31181;&#26080;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#37319;&#26679;&#20559;&#24046;&#8221;&#21644;&#35821;&#20041;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#22686;&#24378;&#25216;&#26415;&#21644;&#37319;&#26679;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20013;&#65292;&#22686;&#24378;&#25216;&#26415;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#65292;&#32780;&#20182;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#21482;&#33021;&#25429;&#25417;&#21040;&#19968;&#23567;&#37096;&#20998;&#20869;&#22312;&#30340;&#30417;&#30563;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#35774;&#35745;&#26469;&#33719;&#24471;&#25968;&#25454;&#30340;&#20004;&#20010;&#19981;&#21516;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;&#65288;SCGDN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;AttM&#65289;&#21644;&#25193;&#25955;&#27169;&#22359;&#65288;DiFM&#65289;&#12290;AttM&#36890;&#36807;&#27719;&#38598;&#39640;&#38454;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#26469;&#33719;&#24471;&#20248;&#31168;&#30340;&#23884;&#20837;&#65292;&#32780;DiFM&#36890;&#36807;Laplacian&#25193;&#25955;&#23398;&#20064;&#24179;&#34913;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#29366;&#24577;&#65292;&#24182;&#20801;&#35768;&#22270;&#20013;&#37051;&#25509;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#21327;&#21516;&#28436;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;SCGDN&#26159;&#19968;&#31181;&#26080;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#37319;&#26679;&#20559;&#24046;&#8221;&#21644;&#35821;&#20041;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmentation techniques and sampling strategies are crucial in contrastive learning, but in most existing works, augmentation techniques require careful design, and their sampling strategies can only capture a small amount of intrinsic supervision information. Additionally, the existing methods require complex designs to obtain two different representations of the data. To overcome these limitations, we propose a novel framework called the Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two main components: the Attentional Module (AttM) and the Diffusion Module (DiFM). AttM aggregates higher-order structure and feature information to get an excellent embedding, while DiFM balances the state of each node in the graph through Laplacian diffusion learning and allows the cooperative evolution of adjacency and feature information in the graph. Unlike existing methodologies, SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic drift, wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#28857;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#31867;&#20869;&#32858;&#31867;&#65292;&#33258;&#21160;&#21457;&#29616;&#36328;&#22330;&#26223;&#30340;&#23376;&#31867;&#27169;&#24335;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#37325;&#26032;&#32472;&#21046;&#23884;&#20837;&#31354;&#38388;&#65292;&#20197;&#25552;&#39640;&#28857;&#20113;&#20998;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14605</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;3D&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Clustering based Point Cloud Representation Learning for 3D Analysis. (arXiv:2307.14605v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14605
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#28857;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#31867;&#20869;&#32858;&#31867;&#65292;&#33258;&#21160;&#21457;&#29616;&#36328;&#22330;&#26223;&#30340;&#23376;&#31867;&#27169;&#24335;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#37325;&#26032;&#32472;&#21046;&#23884;&#20837;&#31354;&#38388;&#65292;&#20197;&#25552;&#39640;&#28857;&#20113;&#20998;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#20998;&#26512;&#65288;&#22914;3D&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#19981;&#20165;&#26377;&#25968;&#30334;&#19975;&#20010;&#26080;&#24207;&#28857;&#30340;&#19981;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#65292;&#36824;&#26377;&#30001;&#28145;&#24230;&#12289;&#35270;&#28857;&#12289;&#36974;&#25377;&#31561;&#24341;&#36215;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#23545;&#28857;&#20113;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#19968;&#20010;&#26082;&#20855;&#26377;&#36776;&#21035;&#35821;&#20041;&#21448;&#33021;&#24212;&#23545;&#25361;&#25112;&#24615;&#21464;&#21270;&#30340;&#36866;&#24403;&#28857;&#23884;&#20837;&#31354;&#38388;&#65311;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#28857;&#20113;&#20998;&#26512;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#19982;&#24403;&#21069;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#35757;&#32451;&#33539;&#24335;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#28857;&#23884;&#20837;&#31354;&#38388;&#19978;&#36827;&#34892;&#31867;&#20869;&#32858;&#31867;&#65292;&#33258;&#21160;&#21457;&#29616;&#36328;&#22330;&#26223;&#28508;&#22312;&#32780;&#20195;&#34920;&#24615;&#30340;&#23376;&#31867;&#27169;&#24335;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#21457;&#29616;&#30340;&#27169;&#24335;&#37325;&#26032;&#32472;&#21046;&#23884;&#20837;&#31354;&#38388;&#65292;&#20197;&#23562;&#37325;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#20998;&#24067;&#24182;&#25913;&#36827;&#28857;&#20113;&#20998;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud analysis (such as 3D segmentation and detection) is a challenging task, because of not only the irregular geometries of many millions of unordered points, but also the great variations caused by depth, viewpoint, occlusion, etc. Current studies put much focus on the adaption of neural networks to the complex geometries of point clouds, but are blind to a fundamental question: how to learn an appropriate point embedding space that is aware of both discriminative semantics and challenging variations? As a response, we propose a clustering based supervised learning scheme for point cloud analysis. Unlike current de-facto, scene-wise training paradigm, our algorithm conducts within-class clustering on the point embedding space for automatically discovering subclass patterns which are latent yet representative across scenes. The mined patterns are, in turn, used to repaint the embedding space, so as to respect the underlying distribution of the entire training dataset and improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#35774;&#35745;&#20102;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21450;&#35299;&#20915;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14591</link><description>&lt;p&gt;
&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#36523;&#20221;&#20132;&#25442;&#26816;&#27979;&#19982;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
The detection and rectification for identity-switch based on unfalsified control. (arXiv:2307.14591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#35774;&#35745;&#20102;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21450;&#35299;&#20915;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#30446;&#30340;&#26159;&#25345;&#32493;&#36319;&#36394;&#21644;&#35782;&#21035;&#35270;&#39057;&#20013;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#37117;&#26159;&#36890;&#36807;&#24314;&#27169;&#36816;&#21160;&#20449;&#24687;&#24182;&#23558;&#20854;&#19982;&#22806;&#35266;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#26469;&#30830;&#23450;&#21644;&#36319;&#36394;&#29289;&#20307;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#36319;&#36394;&#36807;&#31243;&#20013;&#24314;&#31435;&#20102;&#22806;&#35266;&#20449;&#24687;&#21464;&#21270;&#30340;&#24207;&#21015;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#26816;&#27979;&#21644;&#24674;&#22797;&#35774;&#35745;&#20102;&#19968;&#20010;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#20851;&#32852;&#36807;&#31243;&#20013;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#36319;&#36394;&#22120;&#22312;&#22788;&#29702;&#30001;&#36974;&#25377;&#21644;&#24555;&#36895;&#36816;&#21160;&#24341;&#36215;&#30340;&#36319;&#36394;&#38169;&#35823;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of multi-object tracking (MOT) is to continuously track and identify objects detected in videos. Currently, most methods for multi-object tracking model the motion information and combine it with appearance information to determine and track objects. In this paper, unfalsified control is employed to address the ID-switch problem in multi-object tracking. We establish sequences of appearance information variations for the trajectories during the tracking process and design a detection and rectification module specifically for ID-switch detection and recovery. We also propose a simple and effective strategy to address the issue of ambiguous matching of appearance information during the data association process. Experimental results on publicly available MOT datasets demonstrate that the tracker exhibits excellent effectiveness and robustness in handling tracking errors caused by occlusions and rapid movements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ReluFormer&#30340;transformer&#26550;&#26500;&#65292;&#20197;&#21450;&#38024;&#23545;FCM&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;&#20799;&#31461;&#24613;&#24615;&#28107;&#24052;&#30333;&#34880;&#30149;&#65288;ALL&#65289;FCM&#26679;&#26412;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#26816;&#26597;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14581</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25216;&#26415;&#29992;&#20110;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#26415;&#32454;&#32990;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explainable Techniques for Analyzing Flow Cytometry Cell Transformers. (arXiv:2307.14581v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ReluFormer&#30340;transformer&#26550;&#26500;&#65292;&#20197;&#21450;&#38024;&#23545;FCM&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;&#20799;&#31461;&#24613;&#24615;&#28107;&#24052;&#30333;&#34880;&#30149;&#65288;ALL&#65289;FCM&#26679;&#26412;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#26816;&#26597;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#26469;&#35828;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#33258;&#21160;&#31995;&#32479;&#30340;&#20915;&#31574;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#21508;&#31181;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#21644;&#26174;&#33879;&#24615;&#22270;&#65292;&#36866;&#29992;&#20110;&#24120;&#35265;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#65292;&#20294;&#22312;&#27969;&#24335;&#32454;&#32990;&#26415;&#65288;FCM&#65289;&#25968;&#25454;&#36825;&#19968;&#27169;&#24577;&#19978;&#30340;&#24037;&#20316;&#23578;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#31216;&#20026;ReluFormer&#30340;transformer&#26550;&#26500;&#30340;&#20351;&#29992;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#31616;&#21270;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;FCM&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#23545;&#20799;&#31461;&#24613;&#24615;&#28107;&#24052;&#30333;&#34880;&#30149;&#65288;ALL&#65289;FCM&#26679;&#26412;&#30340;&#32454;&#32990;&#20998;&#31867;&#21644;&#22810;&#36793;&#24418;&#22238;&#24402;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#26469;&#26816;&#26597;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#12290;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35270;&#21270;&#19981;&#20165;&#21487;&#20197;&#26631;&#35782;&#32454;&#32990;.
&lt;/p&gt;
&lt;p&gt;
Explainability for Deep Learning Models is especially important for clinical applications, where decisions of automated systems have far-reaching consequences.  While various post-hoc explainable methods, such as attention visualization and saliency maps, already exist for common data modalities, including natural language and images, little work has been done to adapt them to the modality of Flow CytoMetry (FCM) data.  In this work, we evaluate the usage of a transformer architecture called ReluFormer that ease attention visualization as well as we propose a gradientand an attention-based visualization technique tailored for FCM. We qualitatively evaluate the visualization techniques for cell classification and polygon regression on pediatric Acute Lymphoblastic Leukemia (ALL) FCM samples. The results outline the model's decision process and demonstrate how to utilize the proposed techniques to inspect the trained model. The gradient-based visualization not only identifies cells tha
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#30340;&#22810;&#20219;&#21153;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39550;&#39542;&#35270;&#39057;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#39550;&#39542;&#22330;&#26223;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#39550;&#39542;&#20107;&#20214;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#38505;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#20811;&#26381;&#30456;&#26426;&#31227;&#21160;&#21644;&#20809;&#29031;&#21464;&#21270;&#23545;&#22806;&#35266;&#26041;&#27861;&#30340;&#24178;&#25200;&#65292;&#36824;&#33021;&#22815;&#25429;&#25417;&#35270;&#39057;&#24103;&#20013;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#23454;&#29616;&#23545;&#19982;&#33258;&#36523;&#30456;&#20851;&#30340;&#20107;&#25925;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.14575</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#30417;&#30563;&#39550;&#39542;&#35270;&#39057;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#30340;&#35760;&#24518;&#22686;&#24378;&#22810;&#20219;&#21153;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Memory-Augmented Multi-Task Collaborative Framework for Unsupervised Traffic Accident Detection in Driving Videos. (arXiv:2307.14575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14575
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#30340;&#22810;&#20219;&#21153;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39550;&#39542;&#35270;&#39057;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#39550;&#39542;&#22330;&#26223;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#39550;&#39542;&#20107;&#20214;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#38505;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#20811;&#26381;&#30456;&#26426;&#31227;&#21160;&#21644;&#20809;&#29031;&#21464;&#21270;&#23545;&#22806;&#35266;&#26041;&#27861;&#30340;&#24178;&#25200;&#65292;&#36824;&#33021;&#22815;&#25429;&#25417;&#35270;&#39057;&#24103;&#20013;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#23454;&#29616;&#23545;&#19982;&#33258;&#36523;&#30456;&#20851;&#30340;&#20107;&#25925;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#20013;&#65292;&#35782;&#21035;&#39550;&#39542;&#35270;&#39057;&#20013;&#30340;&#20132;&#36890;&#20107;&#25925;&#23545;&#30830;&#20445;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#39550;&#39542;&#20107;&#20214;&#38271;&#23614;&#20998;&#24067;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#38505;&#65292;&#29616;&#26377;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39550;&#39542;&#22330;&#26223;&#20013;&#30456;&#26426;&#30340;&#24555;&#36895;&#31227;&#21160;&#21644;&#21160;&#24577;&#22330;&#26223;&#65292;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21363;&#22522;&#20110;&#22806;&#35266;&#25110;&#26410;&#26469;&#29289;&#20307;&#23450;&#20301;&#20219;&#21153;&#26469;&#26816;&#27979;&#20107;&#25925;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#30456;&#26426;&#24555;&#36895;&#31227;&#21160;&#21644;&#20809;&#29031;&#21464;&#21270;&#30340;&#24178;&#25200;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#26410;&#26469;&#29289;&#20307;&#23450;&#20301;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#35270;&#39057;&#24103;&#20013;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#20351;&#24471;&#38590;&#20197;&#26816;&#27979;&#21040;&#36710;&#36742;&#22833;&#25511;&#31561;&#19982;&#33258;&#36523;&#30456;&#20851;&#30340;&#20107;&#25925;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#30340;&#22810;&#20219;&#21153;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Identifying traffic accidents in driving videos is crucial to ensuring the safety of autonomous driving and driver assistance systems. To address the potential danger caused by the long-tailed distribution of driving events, existing traffic accident detection (TAD) methods mainly rely on unsupervised learning. However, TAD is still challenging due to the rapid movement of cameras and dynamic scenes in driving scenarios. Existing unsupervised TAD methods mainly rely on a single pretext task, i.e., an appearance-based or future object localization task, to detect accidents. However, appearance-based approaches are easily disturbed by the rapid movement of the camera and changes in illumination, which significantly reduce the performance of traffic accident detection. Methods based on future object localization may fail to capture appearance changes in video frames, making it difficult to detect ego-involved accidents (e.g., out of control of the ego-vehicle). In this paper, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#23433;&#20840;&#32422;&#26463;&#30340;&#33258;&#20027;&#23548;&#33322;&#65292;&#22312;&#27604;&#36739;&#20102;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#20004;&#31181;&#23398;&#20064;&#31574;&#30053;&#21518;&#21457;&#29616;&#65292;&#23433;&#20840;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#36712;&#36857;&#65292;&#36991;&#20813;&#30896;&#25758;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14568</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#23548;&#33322;&#20013;&#23545;&#23433;&#20840;&#32422;&#26463;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning. (arXiv:2307.14568v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#23433;&#20840;&#32422;&#26463;&#30340;&#33258;&#20027;&#23548;&#33322;&#65292;&#22312;&#27604;&#36739;&#20102;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#20004;&#31181;&#23398;&#20064;&#31574;&#30053;&#21518;&#21457;&#29616;&#65292;&#23433;&#20840;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#36712;&#36857;&#65292;&#36991;&#20813;&#30896;&#25758;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#20027;&#23548;&#33322;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#32771;&#34385;&#21040;&#23433;&#20840;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#36825;&#20123;&#38480;&#21046;&#23545;&#20110;&#36991;&#20813;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#36947;&#36335;&#19978;&#20986;&#29616;&#19981;&#23433;&#20840;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20984;&#26174;&#36825;&#20123;&#38480;&#21046;&#30340;&#37325;&#35201;&#24615;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#21487;&#23398;&#20064;&#30340;&#23548;&#33322;&#31574;&#30053;&#65306;&#23433;&#20840;&#31574;&#30053;&#21644;&#19981;&#23433;&#20840;&#31574;&#30053;&#12290;&#23433;&#20840;&#31574;&#30053;&#32771;&#34385;&#21040;&#20102;&#32422;&#26463;&#26465;&#20214;&#65292;&#32780;&#21478;&#19968;&#31181;&#31574;&#30053;&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23433;&#20840;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26356;&#22823;&#23433;&#20840;&#38388;&#38553;&#65288;&#19982;&#38556;&#30861;&#29289;&#30340;&#36317;&#31163;&#65289;&#24182;&#19988;&#21457;&#29983;&#26356;&#23569;&#30896;&#25758;&#30340;&#36712;&#36857;&#65292;&#32780;&#19981;&#25439;&#23475;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning algorithms have had great success in the field of autonomous navigation, they cannot be straightforwardly applied to the real autonomous systems without considering the safety constraints. The later are crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To highlight the importance of these constraints, in this study, we compare two learnable navigation policies: safe and unsafe. The safe policy takes the constraints into account, while the other does not. We show that the safe policy is able to generate trajectories with more clearance (distance to the obstacles) and makes less collisions while training without sacrificing the overall performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#27169;&#22411;&#19982;&#21452;&#28145;&#24230;Q&#32593;&#32476;&#32467;&#21512;&#65292;&#29992;&#20110;&#27169;&#31946;&#27979;&#35797;&#27983;&#35272;&#22120;&#30340;HTML&#28210;&#26579;&#24341;&#25806;&#65292;&#25552;&#39640;&#20102;&#20195;&#30721;&#35206;&#30422;&#24615;&#33021;&#12290;&#22522;&#20110;&#22522;&#30784;&#35821;&#27861;&#27169;&#31946;&#22120;&#30456;&#27604;&#65292;&#20195;&#30721;&#35206;&#30422;&#29575;&#25552;&#39640;&#20102;18.5%&#12290;</title><link>http://arxiv.org/abs/2307.14556</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#31946;&#27979;&#35797;&#27983;&#35272;&#22120;HTML&#28210;&#26579;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning guided fuzz testing for a browser's HTML rendering engine. (arXiv:2307.14556v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#27169;&#22411;&#19982;&#21452;&#28145;&#24230;Q&#32593;&#32476;&#32467;&#21512;&#65292;&#29992;&#20110;&#27169;&#31946;&#27979;&#35797;&#27983;&#35272;&#22120;&#30340;HTML&#28210;&#26579;&#24341;&#25806;&#65292;&#25552;&#39640;&#20102;&#20195;&#30721;&#35206;&#30422;&#24615;&#33021;&#12290;&#22522;&#20110;&#22522;&#30784;&#35821;&#27861;&#27169;&#31946;&#22120;&#30456;&#27604;&#65292;&#20195;&#30721;&#35206;&#30422;&#29575;&#25552;&#39640;&#20102;18.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#31946;&#27979;&#35797;&#21487;&#20197;&#21457;&#29616;&#21508;&#31181;&#38169;&#35823;&#21644;&#23433;&#20840;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#19982;&#22522;&#20110;&#21464;&#24322;&#30340;&#27169;&#31946;&#27979;&#35797;&#30456;&#27604;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#20135;&#29983;&#33391;&#22909;&#27979;&#35797;&#29992;&#20363;&#24182;&#20915;&#23450;&#22312;&#20309;&#22788;&#25171;&#30772;&#24213;&#23618;&#32467;&#26500;&#20197;&#25191;&#34892;&#26032;&#30340;&#20195;&#30721;&#36335;&#24452;&#30340;&#24179;&#34913;&#29983;&#25104;&#22120;&#38656;&#35201;&#26356;&#38271;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#35757;&#32451;&#22909;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#21452;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;DDQN&#26681;&#25454;&#20195;&#30721;&#35206;&#30422;&#20449;&#21495;&#25351;&#23548;&#27979;&#35797;&#29992;&#20363;&#30340;&#21019;&#24314;&#12290;&#19982;&#22522;&#20110;&#22522;&#30784;&#35821;&#27861;&#27169;&#31946;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;Firefox HTML&#28210;&#26579;&#24341;&#25806;&#30340;&#20195;&#30721;&#35206;&#30422;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;18.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation-based fuzz testing can uncover various bugs and security vulnerabilities. However, compared to mutation-based fuzz testing, it takes much longer to develop a well-balanced generator that produces good test cases and decides where to break the underlying structure to exercise new code paths. We propose a novel approach to combine a trained test case generator deep learning model with a double deep Q-network (DDQN) for the first time. The DDQN guides test case creation based on a code coverage signal. Our approach improves the code coverage performance of the underlying generator model by up to 18.5\% for the Firefox HTML rendering engine compared to the baseline grammar based fuzzer.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#29702;&#35770;&#24615;&#33021;&#65292;&#24182;&#19988;&#21518;&#24724;&#19978;&#30028;&#20026;$\bigO(kN^2\sqrt{T\log T})$&#12290;</title><link>http://arxiv.org/abs/2307.14549</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#23545;&#25239;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#65306;&#31639;&#27861;&#21644;&#25490;&#21517;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application. (arXiv:2307.14549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#29702;&#35770;&#24615;&#33021;&#65292;&#24182;&#19988;&#21518;&#24724;&#19978;&#30028;&#20026;$\bigO(kN^2\sqrt{T\log T})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#35813;&#38382;&#39064;&#28041;&#21450;&#26377;&#30028;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#26410;&#30693;&#30340;i.i.d.&#20998;&#24067;&#30340;&#27494;&#22120;&#21487;&#29992;&#24615;&#12290;&#35813;&#31639;&#27861;&#25193;&#23637;&#20102;&#21333;&#27494;&#22120;&#36873;&#25321;&#30340;&#30561;&#30496;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#20445;&#35777;&#36798;&#21040;&#29702;&#35770;&#24615;&#33021;&#65292;&#21518;&#24724;&#19978;&#30028;&#20026;$\bigO(kN^2\sqrt{T\log T})$&#65292;&#20854;&#20013;$k$&#26159;&#27599;&#20010;&#26102;&#38388;&#27493;&#36873;&#25321;&#30340;&#27494;&#22120;&#25968;&#37327;&#65292;$N$&#26159;&#24635;&#27494;&#22120;&#25968;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#24739;&#26377;ADHD&#12289;&#35829;&#35835;&#22256;&#38590;&#25110;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;&#26356;&#39640;&#25928;&#22320;&#28040;&#21270;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#31639;&#27861;&#21644;T5&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#22788;&#29702;&#21644;&#25688;&#35201;&#20219;&#21153;&#23454;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#24037;&#20855;&#36824;&#24212;&#29992;&#20102;&#20223;&#29983;&#38405;&#35835;&#21407;&#21017;&#26469;&#22686;&#24378;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14544</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#65292;&#29992;&#20110;&#36741;&#21161;&#24739;&#26377;ADHD&#12289;&#35829;&#35835;&#22256;&#38590;&#25110;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;
&lt;/p&gt;
&lt;p&gt;
Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#24739;&#26377;ADHD&#12289;&#35829;&#35835;&#22256;&#38590;&#25110;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;&#26356;&#39640;&#25928;&#22320;&#28040;&#21270;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#31639;&#27861;&#21644;T5&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#22788;&#29702;&#21644;&#25688;&#35201;&#20219;&#21153;&#23454;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#24037;&#20855;&#36824;&#24212;&#29992;&#20102;&#20223;&#29983;&#38405;&#35835;&#21407;&#21017;&#26469;&#22686;&#24378;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#24739;&#26377;&#35829;&#35835;&#22256;&#38590;&#12289;ADHD&#21644;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;&#26356;&#39640;&#25928;&#22320;&#28040;&#21270;&#20219;&#20309;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#31639;&#27861;&#36827;&#34892;&#22797;&#26434;&#25991;&#26412;&#22788;&#29702;&#21644;&#25688;&#35201;&#20219;&#21153;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;Hugging Face&#30340;T5&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#22120;&#65289;&#27169;&#22411;&#65292;&#23558;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35270;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#27169;&#22411;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;NLTK&#30340;Punkt&#21477;&#23376;&#20998;&#35789;&#22120;&#29992;&#20110;&#23558;&#25991;&#26412;&#20998;&#21106;&#20026;&#21477;&#23376;&#21015;&#34920;&#12290;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#36731;&#37327;&#32423;Web&#26381;&#21153;&#22120;&#21644;&#26694;&#26550;Flask&#25552;&#20379;&#12290;&#35813;&#24037;&#20855;&#36824;&#24212;&#29992;&#20102;&#20223;&#29983;&#38405;&#35835;&#30340;&#21407;&#21017;&#26469;&#22686;&#24378;&#21487;&#35835;&#24615;&#65292;&#21253;&#25324;&#21152;&#31895;&#21151;&#33021;&#21644;&#23545;&#34892;&#12289;&#21333;&#35789;&#21644;&#23383;&#31526;&#38388;&#36317;&#30340;&#35843;&#25972;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#30340;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to assist students with dyslexia, ADHD, and short attention span in digesting any text-based information more efficiently. The proposed solution utilizes the Multilayer Perceptron (MLP) algorithm for complex text processing and summarization tasks. The tool leverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face, which treats every NLP task as a text generation task. The model is fine-tuned on specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer is used to divide a text into a list of sentences. The application is served using Flask, a lightweight web server and framework. The tool also applies principles from Bionic Reading to enhance readability, which includes a bolding function and adjustments to line, word, and character spacing. The paper discusses the methodology, implementation, and results of the AI-based speed reading tool.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14527</link><description>&lt;p&gt;
&#29992;&#20110;&#33618;&#37326;SAR&#21644;&#23547;&#25214;Patricia Wu-Murad&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#23558;&#20004;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;EfficientDET&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#26469;&#33258;&#26085;&#26412;Wu-Murad&#37326;&#22806;&#25628;&#25937;&#65288;WSAR&#65289;&#21162;&#21147;&#30340;98.9 GB&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;3&#20010;&#26041;&#21521;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#33267;&#23569;19&#31181;&#26041;&#27861;&#21644;3&#20010;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22312;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#23450;&#20301;&#22833;&#36394;&#20154;&#21592;&#65292;&#20294;&#21482;&#26377;3&#31181;&#26041;&#27861;&#65288;2&#31181;&#26080;&#30417;&#30563;&#21644;1&#31181;&#26410;&#30693;&#32467;&#26500;&#65289;&#22312;&#25991;&#29486;&#20013;&#34987;&#24341;&#29992;&#20026;&#23454;&#38469;WSAR&#25805;&#20316;&#20013;&#20351;&#29992;&#36807;&#12290;&#22312;&#36825;&#20123;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;EfficientDET&#26550;&#26500;&#21644;&#26080;&#30417;&#30563;&#30340;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#34987;&#36873;&#25321;&#20026;&#26368;&#36866;&#21512;&#27492;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;EfficientDET&#27169;&#22411;&#24212;&#29992;&#20110;HERIDAL&#25968;&#25454;&#38598;&#65292;&#23613;&#31649;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#25216;&#26415;&#30456;&#24403;&#30340;&#27700;&#24179;&#65292;&#20294;&#27169;&#22411;&#22312;&#20551;&#38451;&#24615;&#26041;&#38754;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#25928;&#35782;&#21035;&#65288;&#20363;&#22914;&#65292;&#35782;&#21035;&#26641;&#26525;&#21644;&#23721;&#30707;&#65289;
&lt;/p&gt;
&lt;p&gt;
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36710;&#36742;&#28783;&#20809;&#30340;&#34920;&#31034;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;&#25552;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#65306;&#22812;&#38388;&#36710;&#36742;&#26816;&#27979;&#12289;3D&#36710;&#36742;&#26041;&#21521;&#20272;&#35745;&#21644;&#21160;&#24577;&#36712;&#36857;&#32447;&#32034;&#12290;&#20171;&#32461;&#20102;LISA&#36710;&#36742;&#28783;&#20809;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#30340;&#28783;&#20809;&#21487;&#35265;&#24615;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#36710;&#36742;&#26816;&#27979;&#12289;&#24847;&#22270;&#21644;&#36712;&#36857;&#39044;&#27979;&#20197;&#21450;&#23433;&#20840;&#36335;&#24452;&#35268;&#21010;&#12290;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.14521</link><description>&lt;p&gt;
&#36710;&#36742;&#28783;&#20809;&#27169;&#24335;&#65306;&#24212;&#23545;&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#36710;&#36742;&#28783;&#20809;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Patterns of Vehicle Lights: Addressing Complexities in Curation and Annotation of Camera-Based Vehicle Light Datasets and Metrics. (arXiv:2307.14521v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36710;&#36742;&#28783;&#20809;&#30340;&#34920;&#31034;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;&#25552;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#65306;&#22812;&#38388;&#36710;&#36742;&#26816;&#27979;&#12289;3D&#36710;&#36742;&#26041;&#21521;&#20272;&#35745;&#21644;&#21160;&#24577;&#36712;&#36857;&#32447;&#32034;&#12290;&#20171;&#32461;&#20102;LISA&#36710;&#36742;&#28783;&#20809;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#30340;&#28783;&#20809;&#21487;&#35265;&#24615;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#36710;&#36742;&#26816;&#27979;&#12289;&#24847;&#22270;&#21644;&#36712;&#36857;&#39044;&#27979;&#20197;&#21450;&#23433;&#20840;&#36335;&#24452;&#35268;&#21010;&#12290;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36710;&#36742;&#28783;&#20809;&#30340;&#34920;&#31034;&#21450;&#20854;&#23545;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#21508;&#31181;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#34920;&#31034;&#36710;&#36742;&#28783;&#20809;&#30340;&#19981;&#21516;&#35268;&#33539;&#65292;&#21253;&#25324;&#36793;&#30028;&#26694;&#12289;&#20013;&#24515;&#28857;&#12289;&#35282;&#28857;&#21644;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#20174;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#30830;&#23450;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#20197;&#20174;&#36710;&#36742;&#28783;&#20809;&#26816;&#27979;&#20013;&#21463;&#30410;&#30340;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#65306;&#22812;&#38388;&#36710;&#36742;&#26816;&#27979;&#12289;3D&#36710;&#36742;&#26041;&#21521;&#20272;&#35745;&#21644;&#21160;&#24577;&#36712;&#36857;&#32447;&#32034;&#12290;&#27599;&#20010;&#20219;&#21153;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#28783;&#20809;&#34920;&#31034;&#12290;&#36824;&#35752;&#35770;&#20102;&#25910;&#38598;&#21644;&#27880;&#37322;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20197;&#20379;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;LISA&#36710;&#36742;&#28783;&#20809;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#30340;&#28783;&#20809;&#21487;&#35265;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19987;&#38376;&#20026;&#36710;&#36742;&#26816;&#27979;&#12289;&#24847;&#22270;&#21644;&#36712;&#36857;&#39044;&#27979;&#20197;&#21450;&#23433;&#20840;&#36335;&#24452;&#35268;&#21010;&#30340;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#28783;&#20809;&#27880;&#37322;&#12290;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the representation of vehicle lights in computer vision and its implications for various tasks in the field of autonomous driving. Different specifications for representing vehicle lights, including bounding boxes, center points, corner points, and segmentation masks, are discussed in terms of their strengths and weaknesses. Three important tasks in autonomous driving that can benefit from vehicle light detection are identified: nighttime vehicle detection, 3D vehicle orientation estimation, and dynamic trajectory cues. Each task may require a different representation of the light. The challenges of collecting and annotating large datasets for training data-driven models are also addressed, leading to introduction of the LISA Vehicle Lights Dataset and associated Light Visibility Model, which provides light annotations specifically designed for downstream applications in vehicle detection, intent and trajectory prediction, and safe path planning. A comparison of exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#30340;&#37096;&#20214;&#21407;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;Co-12&#37197;&#26041;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#30340;&#31354;&#30333;&#65292;&#24182;&#27010;&#36848;&#20102;&#35780;&#20272;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#35299;&#37322;&#36136;&#37327;&#30340;&#26410;&#26469;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#36825;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25104;&#29087;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.14517</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#30340;&#37096;&#20214;&#21407;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;Co-12&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers. (arXiv:2307.14517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#30340;&#37096;&#20214;&#21407;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;Co-12&#37197;&#26041;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#30340;&#31354;&#30333;&#65292;&#24182;&#27010;&#36848;&#20102;&#35780;&#20272;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#35299;&#37322;&#36136;&#37327;&#30340;&#26410;&#26469;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#36825;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25104;&#29087;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#26159;&#35774;&#35745;&#19978;&#21487;&#20197;&#35299;&#37322;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#21407;&#22411;&#37096;&#20214;&#65292;&#24182;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#36825;&#20123;&#32452;&#20214;&#65292;&#20174;&#32780;&#32467;&#21512;&#20998;&#31867;&#21644;&#35299;&#37322;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35299;&#37322;&#27169;&#22411;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#21487;&#35299;&#37322;&#30340;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#36136;&#37327;&#35780;&#20272;&#32570;&#20047;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#22522;&#20110;&#22312;arXiv:2201.08164&#20013;&#24341;&#20837;&#30340;Co-12&#35299;&#37322;&#36136;&#37327;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#32039;&#20945;&#24615;&#65289;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#35780;&#20272;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#30340;&#31354;&#30333;&#65292;&#24182;&#27010;&#36848;&#20102;&#35780;&#20272;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#35299;&#37322;&#36136;&#37327;&#30340;&#26410;&#26469;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#30340;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#36825;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25104;&#29087;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#8220;Co-12&#22791;&#24536;&#21333;&#8221;&#65292;&#23427;&#20316;&#20026;&#23545;&#35780;&#20272;&#37096;&#20214;&#21407;&#22411;&#27169;&#22411;&#32467;&#26524;&#30340;&#31616;&#26126;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable part-prototype models are computer vision models that are explainable by design. The models learn prototypical parts and recognise these components in an image, thereby combining classification and explanation. Despite the recent attention for intrinsically interpretable models, there is no comprehensive overview on evaluating the explanation quality of interpretable part-prototype models. Based on the Co-12 properties for explanation quality as introduced in arXiv:2201.08164 (e.g., correctness, completeness, compactness), we review existing work that evaluates part-prototype models, reveal research gaps and outline future approaches for evaluation of the explanation quality of part-prototype models. This paper, therefore, contributes to the progression and maturity of this relatively new research field on interpretable part-prototype models. We additionally provide a ``Co-12 cheat sheet'' that acts as a concise summary of our findings on evaluating part-prototype models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35748;&#30693;&#20559;&#24046;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#20934;&#30830;&#20195;&#34920;&#26680;&#24515;&#24605;&#24819;&#12289;&#26131;&#20110;&#29702;&#35299;&#12289;&#33021;&#22815;&#24341;&#21457;&#24773;&#24863;&#21453;&#24212;&#24182;&#19988;&#24120;&#35265;&#30340;&#21516;&#20041;&#35789;&#33021;&#22815;&#20419;&#36827;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14511</link><description>&lt;p&gt;
&#35789;&#35821;&#30340;&#30041;&#23384;&#65306;&#20351;&#29992;&#35748;&#30693;&#20559;&#24046;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#39044;&#27979;&#20915;&#31574;&#21644;&#21516;&#20041;&#35789;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics. (arXiv:2307.14511v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14511
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35748;&#30693;&#20559;&#24046;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#20934;&#30830;&#20195;&#34920;&#26680;&#24515;&#24605;&#24819;&#12289;&#26131;&#20110;&#29702;&#35299;&#12289;&#33021;&#22815;&#24341;&#21457;&#24773;&#24863;&#21453;&#24212;&#24182;&#19988;&#24120;&#35265;&#30340;&#21516;&#20041;&#35789;&#33021;&#22815;&#20419;&#36827;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20511;&#37492;&#20102;&#35748;&#30693;&#24515;&#29702;&#23398;&#21644;&#20449;&#24687;&#31995;&#32479;&#30740;&#31350;&#30340;&#25104;&#26524;&#65292;&#26088;&#22312;&#39044;&#27979;&#25968;&#23383;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#21442;&#19982;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#35748;&#30693;&#20559;&#24046;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#29992;&#25143;&#19982;&#25968;&#23383;&#20869;&#23481;&#20013;&#30340;&#21516;&#20041;&#35789;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20195;&#34920;&#24615;&#12289;&#26131;&#29992;&#24615;&#12289;&#24773;&#24863;&#21644;&#20998;&#24067;&#36825;&#22235;&#31181;&#35748;&#30693;&#20559;&#24046;&#34701;&#21512;&#21040;&#20102;READ&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#29992;&#25143;&#35843;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20934;&#30830;&#20195;&#34920;&#26680;&#24515;&#24605;&#24819;&#12289;&#26131;&#20110;&#29702;&#35299;&#12289;&#33021;&#22815;&#24341;&#21457;&#24773;&#24863;&#21453;&#24212;&#24182;&#19988;&#24120;&#35265;&#30340;&#21516;&#20041;&#35789;&#33021;&#22815;&#20419;&#36827;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20154;&#26426;&#20132;&#20114;&#12289;&#25968;&#23383;&#34892;&#20026;&#21644;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#35748;&#30693;&#20559;&#24046;&#20316;&#20026;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26377;&#21147;&#25351;&#26631;&#30340;&#28508;&#21147;&#65292;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#35774;&#35745;&#26377;&#25928;&#30340;&#25968;&#23383;&#24179;&#21488;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research draws upon cognitive psychology and information systems studies to anticipate user engagement and decision-making on digital platforms. By employing natural language processing (NLP) techniques and insights from cognitive bias research, we delve into user interactions with synonyms within digital content. Our methodology synthesizes four cognitive biasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ model. Through a comprehensive user survey, we assess the model's ability to predict user engagement, discovering that synonyms that accurately represent core ideas, are easy to understand, elicit emotional responses, and are commonly encountered, promote greater user engagement. Crucially, our work offers a fresh lens on human-computer interaction, digital behaviors, and decision-making processes. Our results highlight the promise of cognitive biases as potent indicators of user engagement, underscoring their significance in designing effective digital
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#8220;&#35302;&#35273;&#26174;&#33879;&#24615;&#8221;&#65292;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35302;&#35273;&#27880;&#24847;&#26426;&#21046;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35270;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#35302;&#35273;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14510</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#27880;&#24847;&#21147;: &#29992;&#20110;&#24378;&#20581;&#30340;&#27169;&#25311;-&#30495;&#23454;&#35302;&#35273;&#25511;&#21046;&#30340;&#35302;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention of Robot Touch: Tactile Saliency Prediction for Robust Sim-to-Real Tactile Control. (arXiv:2307.14510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#8220;&#35302;&#35273;&#26174;&#33879;&#24615;&#8221;&#65292;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35302;&#35273;&#27880;&#24847;&#26426;&#21046;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35270;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#35302;&#35273;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#30340;&#35302;&#35273;&#20256;&#24863;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#23616;&#37096;&#25509;&#35302;&#30340;&#20934;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#26679;&#30340;&#20219;&#21153;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#35302;&#35273;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#8220;&#35302;&#35273;&#26174;&#33879;&#24615;&#8221;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20154;&#31867;&#35302;&#35273;&#27880;&#24847;&#26426;&#21046;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35270;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;&#31867;&#20284;&#20110;&#35270;&#35273;&#26174;&#33879;&#24615;&#65292;&#36825;&#20010;&#27010;&#24565;&#28041;&#21450;&#21040;&#36890;&#36807;&#35302;&#35273;&#20256;&#24863;&#22120;&#25429;&#25417;&#21040;&#30340;&#35302;&#35273;&#22270;&#20687;&#20013;&#35782;&#21035;&#20851;&#38190;&#20449;&#24687;&#12290;&#34429;&#28982;&#35270;&#35273;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#20154;&#31867;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#30001;&#20110;&#35302;&#35273;&#22270;&#20687;&#30340;&#21453;&#30452;&#35273;&#27169;&#24335;&#65292;&#25163;&#21160;&#26631;&#35760;&#35302;&#35273;&#22270;&#20687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#32593;&#32476;&#32452;&#25104;&#30340;&#26032;&#26041;&#27861;: 1) &#25509;&#35302;&#28145;&#24230;&#32593;&#32476;&#65288;ConDepNet&#65289;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#25509;&#35302;&#28145;&#24230;&#22320;&#22270;&#20197;&#23450;&#20301;&#30495;&#23454;&#35302;&#35273;&#20013;&#30340;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution tactile sensing can provide accurate information about local contact in contact-rich robotic tasks. However, the deployment of such tasks in unstructured environments remains under-investigated. To improve the robustness of tactile robot control in unstructured environments, we propose and study a new concept: \textit{tactile saliency} for robot touch, inspired by the human touch attention mechanism from neuroscience and the visual saliency prediction problem from computer vision. In analogy to visual saliency, this concept involves identifying key information in tactile images captured by a tactile sensor. While visual saliency datasets are commonly annotated by humans, manually labelling tactile images is challenging due to their counterintuitive patterns. To address this challenge, we propose a novel approach comprised of three interrelated networks: 1) a Contact Depth Network (ConDepNet), which generates a contact depth map to localize deformation in a real tactile 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38750;&#26412;&#22320;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#22312;&#22823;&#35268;&#27169;&#22823;&#23398;&#24314;&#31569;&#29615;&#22659;&#20013;&#21487;&#20197;&#20943;&#23569;9.3&#65285;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.14501</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#26412;&#22320;&#20449;&#24687;&#25351;&#23548;&#30340;&#39044;&#27979;&#26041;&#27861;&#25913;&#36827;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Improving Reliable Navigation under Uncertainty via Predictions Informed by Non-Local Information. (arXiv:2307.14501v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14501
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#26412;&#22320;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#21487;&#38752;&#23548;&#33322;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#22312;&#22823;&#35268;&#27169;&#22823;&#23398;&#24314;&#31569;&#29615;&#22659;&#20013;&#21487;&#20197;&#20943;&#23569;9.3&#65285;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#38750;&#26412;&#22320;&#21487;&#29992;&#20449;&#24687;&#26469;&#39044;&#27979;&#36827;&#20837;&#26410;&#30693;&#31354;&#38388;&#30340;&#34892;&#20026;&#30340;&#22909;&#22351;&#65292;&#25913;&#36827;&#20102;&#22312;&#37096;&#20998;&#26144;&#23556;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#12289;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#23548;&#33322;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#23548;&#33322;&#30340;&#20301;&#32622;&#38656;&#35201;&#38750;&#26412;&#22320;&#20449;&#24687;&#65306;&#26426;&#22120;&#20154;&#36804;&#20170;&#25152;&#35265;&#21040;&#30340;&#20219;&#20309;&#35266;&#23519;&#21487;&#33021;&#25552;&#20379;&#26377;&#20851;&#29305;&#23450;&#34892;&#36827;&#26041;&#21521;&#30340;&#22909;&#22351;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#30340;&#35745;&#21010;&#26041;&#38754;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#33021;&#20381;&#38752;&#38750;&#26412;&#22320;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65288;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#21448;&#33021;&#35774;&#35745;&#21487;&#38752;&#30340;&#26041;&#27861;&#65306;&#21363;&#20351;&#23398;&#20064;&#19981;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#20063;&#33021;&#22987;&#32456;&#36798;&#21040;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38656;&#35201;&#38750;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#22823;&#23398;&#24314;&#31569;&#29615;&#22659;&#20013;&#65292;&#26681;&#25454;&#30495;&#23454;&#22320;&#26495;&#22270;&#29983;&#25104;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25104;&#26412;&#20943;&#23569;&#20102;9.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We improve reliable, long-horizon, goal-directed navigation in partially-mapped environments by using non-locally available information to predict the goodness of temporally-extended actions that enter unseen space. Making predictions about where to navigate in general requires non-local information: any observations the robot has seen so far may provide information about the goodness of a particular direction of travel. Building on recent work in learning-augmented model-based planning under uncertainty, we present an approach that can both rely on non-local information to make predictions (via a graph neural network) and is reliable by design: it will always reach its goal, even when learning does not provide accurate predictions. We conduct experiments in three simulated environments in which non-local information is needed to perform well. In our large scale university building environment, generated from real-world floorplans to the scale, we demonstrate a 9.3\% reduction in cost-
&lt;/p&gt;</description></item><item><title>ShinyAnimalCV&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20113;&#24179;&#21488;&#32593;&#32476;&#24212;&#29992;&#65292;&#29992;&#20110;&#21160;&#29289;&#30340;&#29289;&#20307;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#19977;&#32500;&#21487;&#35270;&#21270;&#65292;&#35299;&#20915;&#20102;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#24037;&#20855;&#24212;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#28385;&#36275;&#20102;&#21160;&#29289;&#31185;&#23398;&#25945;&#32946;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.14487</link><description>&lt;p&gt;
&#25216;&#26415;&#27880;&#37322;&#65306;ShinyAnimalCV: &#22522;&#20110;&#20113;&#24179;&#21488;&#30340;&#24320;&#28304;&#32593;&#32476;&#24212;&#29992;&#65292;&#29992;&#20110;&#21160;&#29289;&#30340;&#29289;&#20307;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#19977;&#32500;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical note: ShinyAnimalCV: open-source cloud-based web application for object detection, segmentation, and three-dimensional visualization of animals using computer vision. (arXiv:2307.14487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14487
&lt;/p&gt;
&lt;p&gt;
ShinyAnimalCV&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20113;&#24179;&#21488;&#32593;&#32476;&#24212;&#29992;&#65292;&#29992;&#20110;&#21160;&#29289;&#30340;&#29289;&#20307;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#19977;&#32500;&#21487;&#35270;&#21270;&#65292;&#35299;&#20915;&#20102;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#24037;&#20855;&#24212;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#28385;&#36275;&#20102;&#21160;&#29289;&#31185;&#23398;&#25945;&#32946;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20316;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23454;&#26102;&#21644;&#20010;&#24615;&#21270;&#30340;&#21160;&#29289;&#25252;&#29702;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#31934;&#30830;&#30044;&#29287;&#19994;&#30340;&#21457;&#23637;&#12290;&#24265;&#20215;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#30456;&#26426;&#20256;&#24863;&#22120;&#19982;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#20026;&#25913;&#21892;&#30044;&#29287;&#29983;&#20135;&#31995;&#32479;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20844;&#24320;&#39046;&#22495;&#20013;&#23384;&#22312;&#21508;&#31181;CV&#24037;&#20855;&#65292;&#20294;&#23558;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#32534;&#31243;&#21644;&#25968;&#25454;&#20998;&#26512;&#25216;&#33021;&#65292;&#20197;&#21450;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#31934;&#30830;&#30044;&#29287;&#19994;&#30340;&#24555;&#36895;&#25193;&#24352;&#27491;&#22312;&#22312;CV&#26041;&#38754;&#22521;&#20859;&#21644;&#25945;&#32946;&#21160;&#29289;&#31185;&#23398;&#23398;&#29983;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#38754;&#20020;&#30528;&#26377;&#25928;&#23637;&#31034;CV&#20013;&#22797;&#26434;&#31639;&#27861;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;ShinyAnimalCV&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20110;&#20113;&#24179;&#21488;&#30340;&#32593;&#32476;&#24212;&#29992;&#65292;&#29992;&#20110;&#21160;&#29289;&#30340;&#29289;&#20307;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#19977;&#32500;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision (CV), a non-intrusive and cost-effective technology, has furthered the development of precision livestock farming by enabling optimized decision-making through timely and individualized animal care. The availability of affordable two- and three-dimensional camera sensors, combined with various machine learning and deep learning algorithms, has provided a valuable opportunity to improve livestock production systems. However, despite the availability of various CV tools in the public domain, applying these tools to animal data can be challenging, often requiring users to have programming and data analysis skills, as well as access to computing resources. Moreover, the rapid expansion of precision livestock farming is creating a growing need to educate and train animal science students in CV. This presents educators with the challenge of efficiently demonstrating the complex algorithms involved in CV. Thus, the objective of this study was to develop ShinyAnimalCV, an open-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#22686;&#24378;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#33021;&#32791;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.14464</link><description>&lt;p&gt;
&#20351;&#29992; U-Net &#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Single Channel Speech Enhancement Using U-Net Spiking Neural Networks. (arXiv:2307.14464v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#22686;&#24378;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#33021;&#32791;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#23545;&#20110;&#21487;&#38752;&#30340;&#36890;&#20449;&#35774;&#22791;&#25110;&#31283;&#20581;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24120;&#35268;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#22312;SE&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#39640;&#33021;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#32467;&#26500;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26469;&#36827;&#34892;SE&#30340;&#26032;&#26041;&#27861;&#12290;SNN&#36866;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#26102;&#38388;&#32500;&#24230;&#30340;&#25968;&#25454;&#65292;&#22914;&#35821;&#38899;&#65292;&#24182;&#20197;&#20854;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#30340;&#20302;&#33021;&#32791;&#23454;&#29616;&#32780;&#38395;&#21517;&#12290;&#22240;&#27492;&#65292;SNN&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;&#26377;&#36259;&#20505;&#36873;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;SNN&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;ANN&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#23618;SNN&#65292;&#24182;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#20351;&#29992;&#24863;&#30693;&#23458;&#35266;&#27979;&#35797;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement (SE) is crucial for reliable communication devices or robust speech recognition systems. Although conventional artificial neural networks (ANN) have demonstrated remarkable performance in SE, they require significant computational power, along with high energy costs. In this paper, we propose a novel approach to SE using a spiking neural network (SNN) based on a U-Net architecture. SNNs are suitable for processing data with a temporal dimension, such as speech, and are known for their energy-efficient implementation on neuromorphic hardware. As such, SNNs are thus interesting candidates for real-time applications on devices with limited resources. The primary objective of the current work is to develop an SNN-based model with comparable performance to a state-of-the-art ANN model for SE. We train a deep SNN using surrogate-gradient-based optimization and evaluate its performance using perceptual objective tests under different signal-to-noise ratios and real-world no
&lt;/p&gt;</description></item><item><title>VISPUR&#26159;&#19968;&#20010;&#25552;&#20379;&#35270;&#35273;&#20998;&#26512;&#21644;&#20154;&#26412;&#24037;&#20316;&#27969;&#31243;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20013;&#30340;&#34394;&#20551;&#20851;&#32852;&#12290;&#23427;&#21253;&#25324;&#28151;&#28102;&#22240;&#32032;&#20202;&#34920;&#30424;&#21644;&#23376;&#32676;&#27983;&#35272;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23450;&#20301;&#12289;&#25512;&#29702;&#21644;&#39044;&#38450;&#34394;&#20551;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2307.14448</link><description>&lt;p&gt;
VISPUR: &#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20013;&#34394;&#20551;&#20851;&#32852;&#30340;&#35270;&#35273;&#36741;&#21161;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions. (arXiv:2307.14448v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14448
&lt;/p&gt;
&lt;p&gt;
VISPUR&#26159;&#19968;&#20010;&#25552;&#20379;&#35270;&#35273;&#20998;&#26512;&#21644;&#20154;&#26412;&#24037;&#20316;&#27969;&#31243;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20013;&#30340;&#34394;&#20551;&#20851;&#32852;&#12290;&#23427;&#21253;&#25324;&#28151;&#28102;&#22240;&#32032;&#20202;&#34920;&#30424;&#21644;&#23376;&#32676;&#27983;&#35272;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23450;&#20301;&#12289;&#25512;&#29702;&#21644;&#39044;&#38450;&#34394;&#20551;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20849;&#21516;&#36171;&#20104;&#20154;&#31867;&#22312;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#25429;&#25417;&#21040;&#30340;&#32463;&#39564;&#20851;&#32852;&#21487;&#33021;&#30001;&#20110;&#28151;&#28102;&#22240;&#32032;&#21644;&#23376;&#32676;&#24322;&#36136;&#24615;&#32780;&#26159;&#34394;&#20551;&#30340;&#12290;&#33879;&#21517;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#23601;&#26159;&#36825;&#26679;&#19968;&#20010;&#29616;&#35937;&#65292;&#32858;&#21512;&#21644;&#23376;&#32676;&#32423;&#21035;&#30340;&#20851;&#32852;&#30456;&#20114;&#30683;&#30462;&#65292;&#32473;&#20154;&#31867;&#24102;&#26469;&#35748;&#30693;&#22256;&#24785;&#21644;&#20915;&#31574;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23545;&#20110;&#20154;&#31867;&#22312;&#23454;&#36341;&#20013;&#23450;&#20301;&#12289;&#25512;&#29702;&#21644;&#39044;&#38450;&#34394;&#20551;&#20851;&#32852;&#25552;&#20379;&#30340;&#35265;&#35299;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;VISPUR&#65292;&#19968;&#20010;&#25552;&#20379;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#21644;&#20154;&#26412;&#24037;&#20316;&#27969;&#31243;&#20197;&#24212;&#23545;&#34394;&#20551;&#20851;&#32852;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#12290;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28151;&#28102;&#22240;&#32032;&#20202;&#34920;&#30424;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#21487;&#33021;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#20197;&#21450;&#19968;&#20010;&#23376;&#32676;&#27983;&#35272;&#22120;&#65292;&#20801;&#35768;&#23545;&#21487;&#33021;&#23548;&#33268;&#23545;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#38169;&#35823;&#30340;&#22810;&#26679;&#23376;&#32676;&#27169;&#24335;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of ca
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;Pansharpening&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#20998;&#36776;&#29575;&#35757;&#32451;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14403</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#22686;&#24378;&#20809;&#35889;&#21644;&#31354;&#38388;&#20445;&#30495;&#24230;&#30340;Pansharpening&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14403
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;Pansharpening&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#20998;&#36776;&#29575;&#35757;&#32451;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;Pansharpening&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#32570;&#20047;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#38477;&#20302;&#20998;&#36776;&#29575;&#39046;&#22495;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#65292;&#35757;&#32451;&#22312;&#38477;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#19978;&#30340;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#29616;&#22312;&#36716;&#21521;&#22312;&#20840;&#20998;&#36776;&#29575;&#39046;&#22495;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#36890;&#36807;&#23450;&#20041;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#33539;&#20363;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#26550;&#26500;&#30340;&#20840;&#20998;&#36776;&#29575;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Pansharpening&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#24182;&#25552;&#20379;&#20102;&#21069;&#27839;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#30340;&#26550;&#26500;&#25913;&#36827;&#65292;&#22914;&#20351;&#29992;&#27531;&#20313;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#20855;&#26377;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#20419;&#36827;&#20809;&#35889;&#21644;&#31354;&#38388;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In latest years, deep learning has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most deep learning-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework which can be applied to many existing architectures.  Here, we propose a new deep learning-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#39044;&#27979;&#20013;&#24212;&#29992;&#38750;&#32447;&#24615;&#33258;&#25105;&#22686;&#24378;&#28145;&#24230;&#31649;&#36947;&#30340;&#21019;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#22686;&#24378;CT&#22270;&#20687;&#20013;&#30340;2D&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14398</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#33258;&#25105;&#22686;&#24378;&#28145;&#24230;&#31649;&#36947;&#22312;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction. (arXiv:2307.14398v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#39044;&#27979;&#20013;&#24212;&#29992;&#38750;&#32447;&#24615;&#33258;&#25105;&#22686;&#24378;&#28145;&#24230;&#31649;&#36947;&#30340;&#21019;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#22686;&#24378;CT&#22270;&#20687;&#20013;&#30340;2D&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20813;&#30123;&#30103;&#27861;&#20316;&#20026;&#27835;&#30103;&#30284;&#30151;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#20986;&#29616;&#12290;&#40723;&#33310;&#20154;&#24515;&#30340;&#21457;&#29616;&#39564;&#35777;&#20102;&#20813;&#30123;&#30103;&#27861;&#33647;&#29289;&#22312;&#22788;&#29702;&#32959;&#30244;&#26041;&#38754;&#30340;&#30103;&#25928;&#65292;&#23548;&#33268;&#20102;&#36739;&#38271;&#30340;&#29983;&#23384;&#29575;&#21644;&#23545;&#27604;&#20256;&#32479;&#21270;&#30103;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#27602;&#24615;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#36866;&#21512;&#20813;&#30123;&#30103;&#27861;&#30340;&#24739;&#32773;&#32676;&#20307;&#20173;&#28982;&#30456;&#23545;&#36739;&#23567;&#65292;&#36825;&#34920;&#26126;&#23545;&#20110;&#26576;&#20123;&#20010;&#20307;&#20013;&#26377;&#21033;&#30340;&#27835;&#30103;&#21453;&#24212;&#30340;&#29983;&#29702;&#26426;&#21046;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#32780;&#20854;&#20182;&#24739;&#32773;&#21017;&#21463;&#30410;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#32454;&#32990;&#32467;&#26500;&#21644;&#28145;&#24230;&#19979;&#28216;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#31934;&#36873;&#21644;&#22686;&#24378;&#20174;&#33016;&#33145;&#37096;CT&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;2D&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27835;&#30103;&#32467;&#26524;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#31649;&#36947;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#21487;&#20197;&#19982;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;Point&#36827;&#34892;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Immunotherapy emerges as promising approach for treating cancer. Encouraging findings have validated the efficacy of immunotherapy medications in addressing tumors, resulting in prolonged survival rates and notable reductions in toxicity compared to conventional chemotherapy methods. However, the pool of eligible patients for immunotherapy remains relatively small, indicating a lack of comprehensive understanding regarding the physiological mechanisms responsible for favorable treatment response in certain individuals while others experience limited benefits. To tackle this issue, the authors present an innovative strategy that harnesses a non-linear cellular architecture in conjunction with a deep downstream classifier. This approach aims to carefully select and enhance 2D features extracted from chest-abdomen CT images, thereby improving the prediction of treatment outcomes. The proposed pipeline has been meticulously designed to seamlessly integrate with an advanced embedded Point o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;PDE-Net++&#65292;&#36890;&#36807;&#32467;&#21512;&#21487;&#35757;&#32451;&#24046;&#20998;&#31639;&#23376;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#26126;&#30830;&#23884;&#20837;&#20102;&#24213;&#23618;PDE&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;PDE-Net++&#20855;&#26377;&#27604;&#40657;&#30418;&#27169;&#22411;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14395</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#21487;&#35757;&#32451;&#24046;&#20998;&#31639;&#23376;&#27169;&#25311;&#37096;&#20998;&#24050;&#30693;&#30340;&#26102;&#31354;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;PDE-Net++&#65292;&#36890;&#36807;&#32467;&#21512;&#21487;&#35757;&#32451;&#24046;&#20998;&#31639;&#23376;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#26126;&#30830;&#23884;&#20837;&#20102;&#24213;&#23618;PDE&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;PDE-Net++&#20855;&#26377;&#27604;&#40657;&#30418;&#27169;&#22411;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#26102;&#31354;&#21160;&#24577;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26377;&#38480;&#12290;&#36890;&#36807;&#23558;&#21487;&#35757;&#32451;&#30340;&#24046;&#20998;&#31639;&#23376;&#19982;&#40657;&#30418;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;&#24213;&#23618;PDE&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#26126;&#30830;&#23884;&#20837;&#21040;PDE-Net++&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#36873;&#25321;&#65292;&#31216;&#20026;&#21487;&#35757;&#32451;&#32763;&#36716;&#24046;&#20998;&#23618;&#65288;TFDL&#65289;&#21644;&#21487;&#35757;&#32451;&#21160;&#24577;&#24046;&#20998;&#23618;&#65288;TDDL&#65289;&#65292;&#29992;&#20110;&#24046;&#20998;&#31639;&#23376;&#12290;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;PDE-Net++&#20855;&#26377;&#27604;&#40657;&#30418;&#27169;&#22411;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#36229;&#22270;&#21516;&#26500;&#35745;&#31639;&#30340;&#36229;&#22270;Weisfiler-Lehman&#27979;&#35797;&#31639;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#30340;&#39640;&#38454;&#32467;&#26500;&#20851;&#31995;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14394</link><description>&lt;p&gt;
Hypergraph&#21516;&#26500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Isomorphism Computation. (arXiv:2307.14394v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#36229;&#22270;&#21516;&#26500;&#35745;&#31639;&#30340;&#36229;&#22270;Weisfiler-Lehman&#27979;&#35797;&#31639;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#30340;&#39640;&#38454;&#32467;&#26500;&#20851;&#31995;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26500;&#38382;&#39064;&#26159;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#25429;&#25417;&#20302;&#38454;&#21644;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#12290;&#20174;&#25552;&#21462;&#20302;&#38454;&#32467;&#26500;&#20449;&#24687;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22270;&#21516;&#26500;&#31639;&#27861;&#20998;&#26512;&#32467;&#26500;&#31561;&#20215;&#24615;&#20197;&#38477;&#20302;&#27714;&#35299;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#12289;&#21270;&#23398;&#36884;&#24452;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#29616;&#23454;&#22330;&#26223;&#20013;&#26356;&#24120;&#35265;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20351;&#29992;&#22270;&#21516;&#26500;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#36229;&#22270;&#21516;&#26500;&#38382;&#39064;&#65292;&#21518;&#32773;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#39640;&#38454;&#32467;&#26500;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#36229;&#22270;&#20869;&#26680;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20869;&#23384;&#28040;&#32791;&#39640;&#25110;&#23376;&#32467;&#26500;&#35782;&#21035;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#65292;&#22240;&#27492;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#36229;&#22270;Weisfiler-Lehman&#27979;&#35797;&#31639;&#27861;&#29992;&#20110;&#36229;&#22270;&#21516;&#26500;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
The isomorphism problem is a fundamental problem in network analysis, which involves capturing both low-order and high-order structural information. In terms of extracting low-order structural information, graph isomorphism algorithms analyze the structural equivalence to reduce the solver space dimension, which demonstrates its power in many applications, such as protein design, chemical pathways, and community detection. For the more commonly occurring high-order relationships in real-life scenarios, the problem of hypergraph isomorphism, which effectively captures these high-order structural relationships, cannot be straightforwardly addressed using graph isomorphism methods. Besides, the existing hypergraph kernel methods may suffer from high memory consumption or inaccurate sub-structure identification, thus yielding sub-optimal performance. In this paper, to address the abovementioned problems, we first propose the hypergraph Weisfiler-Lehman test algorithm for the hypergraph iso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#23519;&#20102;&#38543;&#26426;&#28216;&#36208;&#24322;&#24120;&#26816;&#27979;(RWAD)&#38754;&#20020;&#30340;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#65292;&#35777;&#26126;&#20102;&#25915;&#20987;RWAD&#30340;&#22797;&#26434;&#24230;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25915;&#20987;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22270;&#24341;&#23548;&#25915;&#20987;&#35774;&#35745;&#20102;&#26356;&#24378;&#22823;&#30340;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.14387</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#37325;&#31354;&#38388;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dual-Space Attacks against Random-Walk-based Anomaly Detection. (arXiv:2307.14387v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#23519;&#20102;&#38543;&#26426;&#28216;&#36208;&#24322;&#24120;&#26816;&#27979;(RWAD)&#38754;&#20020;&#30340;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#65292;&#35777;&#26126;&#20102;&#25915;&#20987;RWAD&#30340;&#22797;&#26434;&#24230;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25915;&#20987;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22270;&#24341;&#23548;&#25915;&#20987;&#35774;&#35745;&#20102;&#26356;&#24378;&#22823;&#30340;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#28216;&#36208;&#24322;&#24120;&#26816;&#27979;(RWAD)&#36890;&#24120;&#29992;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#35782;&#21035;&#24322;&#24120;&#27169;&#24335;&#12290;RWAD&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#28857;&#26159;&#36755;&#20837;&#22270;&#21487;&#20197;&#26159;&#39044;&#20808;&#23384;&#22312;&#30340;&#65292;&#20063;&#21487;&#20197;&#30001;&#21407;&#22987;&#29305;&#24449;&#26500;&#24314;&#32780;&#26469;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;RWAD&#26377;&#20004;&#31181;&#28508;&#22312;&#30340;&#25915;&#20987;&#26041;&#24335;&#65306;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#23454;&#38469;&#30340;&#21452;&#37325;&#31354;&#38388;&#25915;&#20987;&#65292;&#25506;&#31350;&#20102;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25915;&#20987;RWAD&#26159;NP&#38590;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22270;&#31354;&#38388;&#25915;&#20987;&#24418;&#24335;&#21270;&#20026;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#31574;&#30053;&#65306;&#20132;&#26367;&#36845;&#20195;&#25915;&#20987;(alterI-attack)&#25110;&#21033;&#29992;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#30340;&#38381;&#21512;&#35299;(cf-attack)&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#31354;&#38388;&#25915;&#20987;&#30340;&#32467;&#26524;&#26469;&#25351;&#23548;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;(&#21363;&#65292;&#22270;&#24341;&#23548;&#25915;&#20987;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Walks-based Anomaly Detection (RWAD) is commonly used to identify anomalous patterns in various applications. An intriguing characteristic of RWAD is that the input graph can either be pre-existing or constructed from raw features. Consequently, there are two potential attack surfaces against RWAD: graph-space attacks and feature-space attacks. In this paper, we explore this vulnerability by designing practical dual-space attacks, investigating the interplay between graph-space and feature-space attacks. To this end, we conduct a thorough complexity analysis, proving that attacking RWAD is NP-hard. Then, we proceed to formulate the graph-space attack as a bi-level optimization problem and propose two strategies to solve it: alternative iteration (alterI-attack) or utilizing the closed-form solution of the random walk model (cf-attack). Finally, we utilize the results from the graph-space attacks as guidance to design more powerful feature-space attacks (i.e., graph-guided attack
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HyperFed&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#30456;&#21516;&#21644;&#29420;&#31435;&#25968;&#25454;&#20998;&#24067;&#36896;&#25104;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36229;&#29699;&#38754;&#21407;&#22411;&#25506;&#32034;&#12289;&#36229;&#29699;&#38754;&#21407;&#22411;&#23398;&#20064;&#21644;&#19968;&#33268;&#32858;&#21512;&#31561;&#27169;&#22359;&#30340;&#32467;&#21512;&#65292;&#26469;&#35299;&#20915;&#31867;&#21035;&#32479;&#35745;&#30340;&#21464;&#21270;&#12289;&#23618;&#32423;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#23458;&#25143;&#31471;&#32858;&#21512;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14384</link><description>&lt;p&gt;
HyperFed: &#38750;IID&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36229;&#29699;&#38754;&#21407;&#22411;&#25506;&#32034;&#19982;&#19968;&#33268;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HyperFed&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#30456;&#21516;&#21644;&#29420;&#31435;&#25968;&#25454;&#20998;&#24067;&#36896;&#25104;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36229;&#29699;&#38754;&#21407;&#22411;&#25506;&#32034;&#12289;&#36229;&#29699;&#38754;&#21407;&#22411;&#23398;&#20064;&#21644;&#19968;&#33268;&#32858;&#21512;&#31561;&#27169;&#22359;&#30340;&#32467;&#21512;&#65292;&#26469;&#35299;&#20915;&#31867;&#21035;&#32479;&#35745;&#30340;&#21464;&#21270;&#12289;&#23618;&#32423;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#23458;&#25143;&#31471;&#32858;&#21512;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20197;&#20998;&#25955;&#21270;&#30340;&#26041;&#24335;&#21327;&#21516;&#23545;&#29992;&#25143;&#25968;&#25454;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#30456;&#21516;&#21644;&#29420;&#31435;&#25968;&#25454;&#20998;&#24067;&#65288;&#38750;IID&#65289;&#38459;&#30861;&#20102;FL&#30340;&#24615;&#33021;&#65292;&#21407;&#22240;&#26377;&#19977;&#28857;&#65292;&#21363;&#65288;1&#65289;&#31867;&#21035;&#32479;&#35745;&#30340;&#21464;&#21270;&#65292;&#65288;2&#65289;&#23618;&#32423;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#65288;3&#65289;&#23458;&#25143;&#31471;&#32858;&#21512;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperFed&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#65292;&#21363;&#36229;&#29699;&#38754;&#21407;&#22411;Tammes&#21021;&#22987;&#21270;&#65288;HPTI&#65289;&#65292;&#36229;&#29699;&#38754;&#21407;&#22411;&#23398;&#20064;&#65288;HPL&#65289;&#21644;&#19968;&#33268;&#32858;&#21512;&#65288;CA&#65289;&#12290;&#39318;&#20808;&#65292;&#26381;&#21153;&#22120;&#20013;&#30340;HPTI&#26500;&#36896;&#22343;&#21248;&#20998;&#24067;&#19988;&#22266;&#23450;&#30340;&#31867;&#21035;&#21407;&#22411;&#65292;&#24182;&#19982;&#23458;&#25143;&#31471;&#20998;&#20139;&#20197;&#21305;&#37197;&#31867;&#21035;&#32479;&#35745;&#65292;&#36827;&#19968;&#27493;&#25351;&#23548;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#19968;&#33268;&#29305;&#24449;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#20013;&#30340;HPL&#22312;&#36229;&#29699;&#38754;&#27169;&#22411;&#31354;&#38388;&#20013;&#20197;&#20849;&#20139;&#30340;&#31867;&#21035;&#21407;&#22411;&#20026;&#30417;&#30563;&#65292;&#25429;&#33719;&#26412;&#22320;&#25968;&#25454;&#20013;&#30340;&#23618;&#32423;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22312;&#26381;&#21153;&#22120;&#20013;&#30340;CA&#36827;&#34892;&#19968;&#33268;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) collaboratively models user data in a decentralized way. However, in the real world, non-identical and independent data distributions (non-IID) among clients hinder the performance of FL due to three issues, i.e., (1) the class statistics shifting, (2) the insufficient hierarchical information utilization, and (3) the inconsistency in aggregating clients. To address the above issues, we propose HyperFed which contains three main modules, i.e., hyperbolic prototype Tammes initialization (HPTI), hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly, HPTI in the server constructs uniformly distributed and fixed class prototypes, and shares them with clients to match class statistics, further guiding consistent feature representation for local clients. Secondly, HPL in each client captures the hierarchical information in local data with the supervision of shared class prototypes in the hyperbolic model space. Additionally, CA in the server mi
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.14382</link><description>&lt;p&gt;
&#24403;&#22810;&#20219;&#21153;&#23398;&#20064;&#36935;&#21040;&#37096;&#20998;&#30417;&#30563;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;(MTL)&#26088;&#22312;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#36164;&#28304;&#21516;&#26102;&#35745;&#31639;&#22810;&#20010;&#36755;&#20986;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#26377;&#28508;&#21147;&#27604;&#20256;&#32479;&#26041;&#27861;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#26356;&#20302;&#12290;&#20197;&#24448;&#30340;MTL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19978;&#65292;&#22240;&#20026;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#20197;&#38477;&#20302;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MTL&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#26631;&#31614;&#38656;&#27714;&#12290;&#26412;&#32508;&#36848;&#30528;&#37325;&#20110;MTL&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;MTL&#20256;&#32479;&#19978;&#22914;&#20309;&#20351;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#20854;&#27425;&#65292;&#23427;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising fro
&lt;/p&gt;</description></item><item><title>EdgeConvEns&#26159;&#19968;&#31181;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#32593;&#32476;&#19978;&#35757;&#32451;&#21644;&#38598;&#25104;&#24322;&#26500;&#30340;&#24369;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.14381</link><description>&lt;p&gt;
EdgeConvEns: &#22522;&#20110;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#30340;&#36793;&#32536;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14381
&lt;/p&gt;
&lt;p&gt;
EdgeConvEns&#26159;&#19968;&#31181;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#32593;&#32476;&#19978;&#35757;&#32451;&#21644;&#38598;&#25104;&#24322;&#26500;&#30340;&#24369;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#36793;&#32536;&#26234;&#33021;&#26088;&#22312;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#36793;&#32536;&#32593;&#32476;&#19978;&#37096;&#32626;&#38656;&#35201;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#28145;&#24230;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26080;&#27861;&#23558;&#20854;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#32852;&#37030;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20132;&#25442;&#23398;&#21040;&#30340;&#26435;&#37325;&#26469;&#20197;&#38598;&#20307;&#26041;&#24335;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36793;&#32536;&#35774;&#22791;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#36718;&#32593;&#32476;&#36890;&#20449;&#25165;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;EdgeConvEns&#65292;&#23427;&#21487;&#20197;&#22312;&#36793;&#32536;&#19978;&#35757;&#32451;&#24322;&#26500;&#30340;&#24369;&#27169;&#22411;&#65292;&#24182;&#23398;&#20250;&#23558;&#23427;&#20204;&#38598;&#25104;&#36215;&#26469;&#65292;&#22312;&#36793;&#32536;&#19978;&#25968;&#25454;&#20998;&#24067;&#21576;&#24322;&#36136;&#24615;&#12290;&#36793;&#32536;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#35745;&#31639;&#33021;&#21147;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#35774;&#22791;&#36827;&#34892;&#29420;&#31435;&#23454;&#29616;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep edge intelligence aims to deploy deep learning models that demand computationally expensive training in the edge network with limited computational power. Moreover, many deep edge intelligence applications require handling distributed data that cannot be transferred to a central server due to privacy concerns. Decentralized learning methods, such as federated learning, offer solutions where models are learned collectively by exchanging learned weights. However, they often require complex models that edge devices may not handle and multiple rounds of network communication to achieve state-of-the-art performances. This study proposes a convolutional ensemble learning approach, coined EdgeConvEns, that facilitates training heterogeneous weak models on edge and learning to ensemble them where data on edge are heterogeneously distributed. Edge models are implemented and trained independently on Field-Programmable Gate Array (FPGA) devices with various computational capacities. Learned 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35774;&#35745;&#21644;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#20197;&#21450;&#20854;&#38480;&#21046;</title><link>http://arxiv.org/abs/2307.14377</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;&#21644;&#21046;&#36896;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14377
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35774;&#35745;&#21644;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#20197;&#21450;&#20854;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;GPT-4&#65292;&#20026;&#29983;&#25104;&#35774;&#35745;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#26426;&#20250;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25972;&#20010;&#35774;&#35745;&#21644;&#21046;&#36896;&#27969;&#31243;&#20013;&#24212;&#29992;&#35813;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;LLMs&#22312;&#35832;&#22914;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#36716;&#21270;&#20026;&#35774;&#35745;&#35268;&#33539;&#12289;&#23558;&#35774;&#35745;&#36716;&#21270;&#20026;&#21046;&#36896;&#25351;&#23548;&#12289;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#21644;&#35774;&#35745;&#21464;&#20307;&#12289;&#35745;&#31639;&#35774;&#35745;&#24615;&#33021;&#20197;&#21450;&#22522;&#20110;&#24615;&#33021;&#25628;&#32034;&#35774;&#35745;&#31561;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20363;&#23376;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#24403;&#21069;LLMs&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#25581;&#31034;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24076;&#26395;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#25913;&#36827;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Language Models (LLMs), including GPT-4, provides exciting new opportunities for generative design. We investigate the application of this tool across the entire design and manufacturing workflow. Specifically, we scrutinize the utility of LLMs in tasks such as: converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance. Through a series of examples, we highlight both the benefits and the limitations of the current LLMs. By exposing these limitations, we aspire to catalyze the continued improvement and progression of these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#32508;&#21512;&#31163;&#25955;&#31995;&#32479;&#30340;&#29366;&#24577;&#36807;&#28193;&#30340;&#36807;&#31243;&#21270;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#24402;&#32435;&#24335;&#32508;&#21512;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25509;&#21463;&#19981;&#21516;&#30446;&#26631;&#35821;&#35328;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#22411;&#33719;&#21462;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#31243;&#24207;&#30340;&#32508;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.14368</link><description>&lt;p&gt;
&#25351;&#23450;&#36807;&#28193;&#31995;&#32479;&#30340;&#36807;&#31243;&#21270;&#27169;&#22411;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synthesis of Procedural Models for Deterministic Transition Systems. (arXiv:2307.14368v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#32508;&#21512;&#31163;&#25955;&#31995;&#32479;&#30340;&#29366;&#24577;&#36807;&#28193;&#30340;&#36807;&#31243;&#21270;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#24402;&#32435;&#24335;&#32508;&#21512;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25509;&#21463;&#19981;&#21516;&#30446;&#26631;&#35821;&#35328;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#22411;&#33719;&#21462;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#31243;&#24207;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32508;&#21512;&#32473;&#23450;&#31163;&#25955;&#31995;&#32479;&#30340;&#29366;&#24577;&#36807;&#28193;&#30340;&#36807;&#31243;&#21270;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#22240;&#20026;&#23427;&#25509;&#21463;&#19981;&#21516;&#30340;&#30446;&#26631;&#35821;&#35328;&#26469;&#23545;&#31163;&#25955;&#31995;&#32479;&#30340;&#29366;&#24577;&#36807;&#28193;&#36827;&#34892;&#24314;&#27169;&#65307;&#19981;&#21516;&#30340;&#30446;&#26631;&#35821;&#35328;&#19982;&#19981;&#21516;&#30340;&#27169;&#22411;&#33719;&#21462;&#20219;&#21153;&#65292;&#20363;&#22914;STRIPS&#21160;&#20316;&#27169;&#22411;&#30340;&#32508;&#21512;&#65292;&#25110;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#37117;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#24402;&#32435;&#24335;&#32508;&#21512;&#26041;&#27861;&#65292;&#21363;&#23558;&#29366;&#24577;&#36807;&#28193;&#30340;&#31034;&#20363;&#38598;&#21512;&#65288;&#34920;&#31034;&#20026;&#65288;&#21069;&#29366;&#24577;&#65292;&#21160;&#20316;&#65292;&#21518;&#29366;&#24577;&#65289;&#20803;&#32452;&#65289;&#20316;&#20026;&#36755;&#20837;&#12290;&#30446;&#26631;&#26159;&#32508;&#21512;&#20986;&#19968;&#20010;&#32467;&#26500;&#21270;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#22312;&#32473;&#23450;&#21069;&#29366;&#24577;&#19978;&#25191;&#34892;&#26102;&#36755;&#20986;&#20854;&#20851;&#32852;&#30340;&#21518;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#22312;&#19968;&#20010;&#20855;&#26377;&#26497;&#31616;&#25351;&#20196;&#38598;&#21644;&#26377;&#38480;&#20869;&#23384;&#30340;&#38543;&#26426;&#35775;&#38382;&#26426;&#22120;&#65288;RAM&#65289;&#30340;&#33391;&#26500;&#32456;&#27490;&#31243;&#24207;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a general approach for synthesizing procedural models of the state-transitions of a given discrete system. The approach is general in that it accepts different target languages for modeling the state-transitions of a discrete system; different model acquisition tasks with different target languages, such as the synthesis of STRIPS action models, or the update rule of a cellular automaton, fit as particular instances of our general approach. We follow an inductive approach to synthesis meaning that a set of examples of state-transitions, represented as (pre-state, action, post-state) tuples, are given as input. The goal is to synthesize a structured program that, when executed on a given pre-state, outputs its associated post-state. Our synthesis method implements a combinatorial search in the space of well-structured terminating programs that can be built using a Random-Access Machine (RAM), with a minimalist instruction set, and a finite amount of memory. The com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASPIRE&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14364</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65306;&#31639;&#27861;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASPIRE&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#20915;&#31574;&#65292;&#20197;&#22312;&#27010;&#29575;&#20998;&#24067;&#30340;&#27169;&#31946;&#38598;&#21512;&#20013;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#25104;&#26412;&#65292;&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#32593;&#32476;&#34892;&#20026;&#20998;&#26512;&#12289;&#39118;&#38505;&#31649;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DRO&#25216;&#26415;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#22788;&#29702;&#20998;&#24067;&#29615;&#22659;&#20013;&#30340;&#24322;&#27493;&#26356;&#26032;&#65307;2&#65289;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#65307;3&#65289;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#22330;&#26223;&#36866;&#24403;&#35843;&#25972;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE)&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20197;&#22788;&#29702;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (FDRO) &#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#21363;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#24182;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30830;&#23450;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#22312;&#24403;&#21069;&#20219;&#21153;&#19979;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#26500;&#24314;&#36866;&#24403;&#30340;&#19990;&#30028;&#35266;&#20197;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.14355</link><description>&lt;p&gt;
&#20026;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#26500;&#24314;&#30456;&#20851;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Framing Relevance for Safety-Critical Autonomous Systems. (arXiv:2307.14355v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30830;&#23450;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#22312;&#24403;&#21069;&#20219;&#21153;&#19979;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#26500;&#24314;&#36866;&#24403;&#30340;&#19990;&#30028;&#35266;&#20197;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#26500;&#24314;&#22797;&#26434;&#30340;&#39640;&#24230;&#33258;&#20027;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#20855;&#26377;&#20869;&#37096;&#20449;&#24565;&#12289;&#24863;&#30693;&#29615;&#22659;&#24182;&#20132;&#25442;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#31995;&#32479;&#26500;&#24314;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#19990;&#30028;&#35266;&#65292;&#24182;&#22522;&#20110;&#27492;&#35268;&#21010;&#20102;&#26410;&#26469;&#30340;&#25805;&#20316;&#65292;&#21363;&#23427;&#20204;&#26681;&#25454;&#21487;&#33021;&#30340;&#26410;&#26469;&#39044;&#27979;&#36873;&#25321;&#34892;&#21160;&#26469;&#23454;&#29616;&#30446;&#26631;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#30528;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#22823;&#37327;&#20449;&#24687;&#65292;&#20854;&#20013;&#24182;&#19981;&#26159;&#25152;&#26377;&#20449;&#24687;&#37117;&#26159;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30830;&#23450;&#22312;&#24403;&#21069;&#20219;&#21153;&#19979;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#26469;&#35828;&#20160;&#20040;&#26159;&#30456;&#20851;&#30340;&#65292;&#21363;&#20160;&#20040;&#20449;&#24687;&#36275;&#20197;&#26500;&#24314;&#36866;&#24403;&#30340;&#19990;&#30028;&#35266;&#20197;&#23454;&#29616;&#20854;&#20219;&#21153;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are in the process of building complex highly autonomous systems that have build-in beliefs, perceive their environment and exchange information. These systems construct their respective world view and based on it they plan their future manoeuvres, i.e., they choose their actions in order to establish their goals based on their prediction of the possible futures. Usually these systems face an overwhelming flood of information provided by a variety of sources where by far not everything is relevant. The goal of our work is to develop a formal approach to determine what is relevant for a safety critical autonomous system at its current mission, i.e., what information suffices to build an appropriate world view to accomplish its mission goals.
&lt;/p&gt;</description></item><item><title>Copilot for Xcode&#26159;&#19968;&#31181;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;Apple&#30340;&#26412;&#22320;&#24320;&#21457;&#29615;&#22659;Xcode&#65292;&#25552;&#39640;&#20102;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#21019;&#36896;&#21147;&#12290;&#23427;&#21033;&#29992;NLP&#25216;&#26415;&#22788;&#29702;&#28304;&#20195;&#30721;&#26631;&#35760;&#21644;&#27169;&#24335;&#65292;&#24182;&#25552;&#20379;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#34917;&#20840;&#12289;&#25991;&#26723;&#32534;&#20889;&#21644;&#38169;&#35823;&#26816;&#27979;&#31561;&#21151;&#33021;&#12290;&#29992;&#25143;&#36824;&#21487;&#20197;&#22312;&#32842;&#22825;&#30028;&#38754;&#20013;&#36827;&#34892;&#26597;&#35810;&#21644;"&#23567;"&#20915;&#31574;&#30340;&#21046;&#23450;&#12290;&#35813;&#24037;&#20855;&#22312;Xcode&#20013;&#20351;&#29992;NLP&#28608;&#21169;&#32534;&#31243;&#24050;&#32463;&#35777;&#26126;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.14349</link><description>&lt;p&gt;
Copilot for Xcode: &#25506;&#32034;AI&#36741;&#21161;&#32534;&#31243;&#36890;&#36807;&#28608;&#21169;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models. (arXiv:2307.14349v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14349
&lt;/p&gt;
&lt;p&gt;
Copilot for Xcode&#26159;&#19968;&#31181;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;Apple&#30340;&#26412;&#22320;&#24320;&#21457;&#29615;&#22659;Xcode&#65292;&#25552;&#39640;&#20102;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#21019;&#36896;&#21147;&#12290;&#23427;&#21033;&#29992;NLP&#25216;&#26415;&#22788;&#29702;&#28304;&#20195;&#30721;&#26631;&#35760;&#21644;&#27169;&#24335;&#65292;&#24182;&#25552;&#20379;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#34917;&#20840;&#12289;&#25991;&#26723;&#32534;&#20889;&#21644;&#38169;&#35823;&#26816;&#27979;&#31561;&#21151;&#33021;&#12290;&#29992;&#25143;&#36824;&#21487;&#20197;&#22312;&#32842;&#22825;&#30028;&#38754;&#20013;&#36827;&#34892;&#26597;&#35810;&#21644;"&#23567;"&#20915;&#31574;&#30340;&#21046;&#23450;&#12290;&#35813;&#24037;&#20855;&#22312;Xcode&#20013;&#20351;&#29992;NLP&#28608;&#21169;&#32534;&#31243;&#24050;&#32463;&#35777;&#26126;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Copilot for Xcode&#30340;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#25903;&#25345;&#20154;&#31867;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#31243;&#24207;&#32452;&#21512;&#21644;&#35774;&#35745;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;Apple&#30340;&#26412;&#22320;&#24320;&#21457;&#29615;&#22659;Xcode&#26080;&#32541;&#38598;&#25104;&#65292;&#35813;&#24037;&#20855;&#25552;&#39640;&#20102;&#29983;&#20135;&#21147;&#65292;&#37322;&#25918;&#20102;&#33529;&#26524;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#65288;&#20363;&#22914;iOS&#24212;&#29992;&#21644;macOS&#65289;&#20013;&#30340;&#36719;&#20214;&#24320;&#21457;&#21019;&#36896;&#21147;&#12290;&#21033;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;Copilot for Xcode&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20195;&#30721;&#24211;&#20013;&#30340;&#28304;&#20195;&#30721;&#26631;&#35760;&#21644;&#27169;&#24335;&#65292;&#23454;&#29616;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#34917;&#20840;&#12289;&#25991;&#26723;&#32534;&#20889;&#21644;&#38169;&#35823;&#26816;&#27979;&#31561;&#29305;&#24615;&#12290;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#36824;&#21487;&#20197;&#36890;&#36807;Copilot for Xcode&#30340;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#26597;&#35810;&#21644;"&#23567;"&#20915;&#31574;&#30340;&#21046;&#23450;&#65292;&#20854;&#20013;&#19968;&#20123;&#20915;&#31574;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#26696;&#20363;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;Xcode&#20013;&#21033;&#29992;NLP&#28608;&#21169;&#27969;&#34892;&#30340;&#32534;&#31243;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an AI-assisted programming tool called Copilot for Xcode for program composition and design to support human software developers. By seamlessly integrating cloud-based Large Language Models (LLM) with Apple's local development environment, Xcode, this tool enhances productivity and unleashes creativity for software development in Apple software ecosystem (e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP) techniques, Copilot for Xcode effectively processes source code tokens and patterns within code repositories, enabling features such as code generation, autocompletion, documentation, and error detection. Software developers can also query and make "small" decisions for program composition, some of which can be made simultaneously, and this is facilitated through prompt engineering in a chat interface of Copilot for Xcode. Finally, we present simple case studies as evidence of the effectiveness of utilizing NLP in Xcode to prompt popular 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#31163;&#32447;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#26410;&#30693;&#20559;&#22909;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#36164;&#28304;&#35843;&#24230;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#24449;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;MEC&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#36793;&#32536;&#12290;</title><link>http://arxiv.org/abs/2307.14346</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Deep Reinforcement Learning for Mobile Edge Computing. (arXiv:2307.14346v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#31163;&#32447;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#26410;&#30693;&#20559;&#22909;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#36164;&#28304;&#35843;&#24230;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#24449;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;MEC&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#23545;&#20110;&#19979;&#19968;&#20195;&#31227;&#21160;&#32593;&#32476;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#24212;&#29992;&#20248;&#20808;&#32771;&#34385;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#65292;&#21253;&#25324;&#24310;&#36831;&#21644;&#33021;&#32791;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#35843;&#24230;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#65292;&#22240;&#20026;&#36825;&#20123;&#24212;&#29992;&#30340;&#20559;&#22909;&#65288;&#21363;&#19981;&#21516;&#30446;&#26631;&#30340;&#26435;&#37325;&#65289;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#25110;&#38590;&#20197;&#20107;&#20808;&#25351;&#23450;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#20010;&#22810;&#30446;&#26631;&#31163;&#32447;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38024;&#23545;MEC&#20013;&#30340;&#22810;&#20010;&#36793;&#32536;&#26469;&#26368;&#23567;&#21270;&#39044;&#26399;&#30340;&#38271;&#26399;&#33021;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#65292;&#21516;&#26102;&#32771;&#34385;&#26410;&#30693;&#30340;&#20559;&#22909;&#20316;&#20026;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#30693;&#20559;&#22909;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#22810;&#30446;&#26631;&#36164;&#28304;&#35843;&#24230;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#29366;&#24577;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;MEC&#31995;&#32479;&#20013;&#22810;&#20010;&#36793;&#32536;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile edge computing (MEC) is essential for next-generation mobile network applications that prioritize various performance metrics, including delays and energy consumption. However, conventional single-objective scheduling solutions cannot be directly applied to practical systems in which the preferences of these applications (i.e., the weights of different objectives) are often unknown or challenging to specify in advance. In this study, we address this issue by formulating a multi-objective offloading problem for MEC with multiple edges to minimize expected long-term energy consumption and transmission delay while considering unknown preferences as parameters. To address the challenge of unknown preferences, we design a multi-objective (deep) reinforcement learning (MORL)-based resource scheduling scheme with proximal policy optimization (PPO). In addition, we introduce a well-designed state encoding method for constructing features for multiple edges in MEC systems, a sophisticate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20005;&#26684;&#20302;&#31209;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#20419;&#36827;&#26368;&#20248;&#35299;&#30340;&#31232;&#30095;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#22855;&#24322;&#20540;&#30340;&#25903;&#25345;&#38598;&#25237;&#24433;&#25805;&#20316;&#21152;&#36895;&#20102;&#20248;&#21270;&#36807;&#31243;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{t^2})$&#65292;&#24182;&#19988;&#25903;&#25345;&#38598;&#22312;&#27599;&#27425;&#26356;&#26032;&#26102;&#21333;&#35843;&#32553;&#23567;&#65292;&#36825;&#22312;&#22522;&#20110;&#21160;&#37327;&#30340;&#31639;&#27861;&#20013;&#26159;&#26032;&#39062;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.14344</link><description>&lt;p&gt;
&#20005;&#26684;&#20302;&#31209;&#32422;&#26463;&#20248;&#21270;--&#19968;&#31181;&#28176;&#36827;&#20026;$\mathcal{O}(\frac{1}{t^2})$&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strictly Low Rank Constraint Optimization -- An Asymptotically $\mathcal{O}(\frac{1}{t^2})$ Method. (arXiv:2307.14344v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20005;&#26684;&#20302;&#31209;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#20419;&#36827;&#26368;&#20248;&#35299;&#30340;&#31232;&#30095;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#22855;&#24322;&#20540;&#30340;&#25903;&#25345;&#38598;&#25237;&#24433;&#25805;&#20316;&#21152;&#36895;&#20102;&#20248;&#21270;&#36807;&#31243;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{t^2})$&#65292;&#24182;&#19988;&#25903;&#25345;&#38598;&#22312;&#27599;&#27425;&#26356;&#26032;&#26102;&#21333;&#35843;&#32553;&#23567;&#65292;&#36825;&#22312;&#22522;&#20110;&#21160;&#37327;&#30340;&#31639;&#27861;&#20013;&#26159;&#26032;&#39062;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#31209;&#27491;&#21017;&#21270;&#30340;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#26368;&#20248;&#35299;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#20013;&#38388;&#26356;&#26032;&#30340;&#22855;&#24322;&#20540;&#36827;&#34892;&#26032;&#39062;&#30340;&#25903;&#25345;&#38598;&#25237;&#24433;&#25805;&#20316;&#26469;&#21152;&#36895;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;$O(\frac{1}{t^2})$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#19982;Nesterov&#22312;&#24179;&#28369;&#20984;&#38382;&#39064;&#19978;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#23436;&#20840;&#30456;&#21516;&#12290;&#21487;&#20197;&#26399;&#26395;&#20986;&#29616;&#20005;&#26684;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#19988;&#22312;&#27599;&#27425;&#26356;&#26032;&#20013;&#22855;&#24322;&#20540;&#30340;&#25903;&#25345;&#38598;&#26159;&#21333;&#35843;&#32553;&#23567;&#30340;&#65292;&#36825;&#22312;&#22522;&#20110;&#21160;&#37327;&#30340;&#31639;&#27861;&#20013;&#26159;&#26032;&#39062;&#30340;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of non-convex and non-smooth problems with \textit{rank} regularization to promote sparsity in optimal solution. We propose to apply the proximal gradient descent method to solve the problem and accelerate the process with a novel support set projection operation on the singular values of the intermediate update. We show that our algorithms are able to achieve a convergence rate of $O(\frac{1}{t^2})$, which is exactly same as Nesterov's optimal convergence rate for first-order methods on smooth and convex problems. Strict sparsity can be expected and the support set of singular values during each update is monotonically shrinking, which to our best knowledge, is novel in momentum-based algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21076;&#38500;&#25197;&#26354;&#21644;&#27169;&#31946;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;MNIST&#25968;&#25454;&#38598;&#20013;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14343</link><description>&lt;p&gt;
MNIST&#25163;&#20889;&#25968;&#23383;&#20013;&#30340;&#25197;&#26354;&#22270;&#20687;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21076;&#38500;&#25197;&#26354;&#21644;&#27169;&#31946;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;MNIST&#25968;&#25454;&#38598;&#20013;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20070;&#20889;&#39118;&#26684;&#30340;&#22810;&#26679;&#24615;&#21644;&#22122;&#22768;&#22270;&#20687;&#30340;&#23384;&#22312;&#65292;&#35782;&#21035;&#25163;&#20889;&#25968;&#23383;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;MNIST&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#36825;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20855;&#26377;&#19981;&#35268;&#21017;&#24418;&#29366;&#12289;&#19981;&#23436;&#25972;&#31508;&#30011;&#21644;&#21464;&#24322;&#20542;&#26012;&#24230;&#30340;&#25197;&#26354;&#25968;&#23383;&#65292;&#21516;&#26102;&#23384;&#22312;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22240;&#32032;&#23548;&#33268;&#20102;&#25968;&#23383;&#35782;&#21035;&#20934;&#30830;&#24230;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#30340;&#25197;&#26354;&#25968;&#23383;&#12290;&#36825;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#21644;&#36807;&#28388;&#20986;&#36825;&#20123;&#25197;&#26354;&#21644;&#27169;&#31946;&#30340;&#22270;&#20687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#34987;&#35782;&#21035;&#20986;&#30340;&#22270;&#20687;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25490;&#38500;&#65292;&#24182;&#20351;&#29992;&#36807;&#28388;&#21518;&#30340;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#26088;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#20943;&#36731;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing handwritten digits is a challenging task primarily due to the diversity of writing styles and the presence of noisy images. The widely used MNIST dataset, which is commonly employed as a benchmark for this task, includes distorted digits with irregular shapes, incomplete strokes, and varying skew in both the training and testing datasets. Consequently, these factors contribute to reduced accuracy in digit recognition. To overcome this challenge, we propose a two-stage deep learning approach. In the first stage, we create a simple neural network to identify distorted digits within the training set. This model serves to detect and filter out such distorted and ambiguous images. In the second stage, we exclude these identified images from the training dataset and proceed to retrain the model using the filtered dataset. This process aims to improve the classification accuracy and confidence levels while mitigating issues of underfitting and overfitting. Our experimental results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.13813</link><description>&lt;p&gt;
&#22914;&#20309;&#25193;&#23637;&#24744;&#30340;EMA&#65288;arXiv:2307.13813v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#22312;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#32553;&#25918;&#35268;&#21017;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#65292;&#24212;&#35813;&#23558;&#23398;&#20064;&#29575;&#19982;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#27169;&#22411;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#25509;&#25910;&#26799;&#24230;&#20449;&#24687;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#32780;&#26159;&#20197;&#19968;&#23450;&#30340;&#21160;&#37327;&#36319;&#38543;&#20854;&#30446;&#26631;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;EMA&#21487;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#31283;&#23450;&#20266;&#26631;&#35760;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;EMA&#19982;&#20248;&#21270;&#20998;&#24320;&#22788;&#29702;&#65292;&#23548;&#33268;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13365</link><description>&lt;p&gt;
&#29992;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#23558;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#37327;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#29616;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#20195;&#12290;&#26080;&#35770;&#36825;&#20123;&#27169;&#22411;&#33258;&#36523;&#30340;&#23481;&#37327;&#21644;&#32467;&#26500;&#22914;&#20309;&#65292;&#37117;&#23384;&#22312;&#23545;LLMs&#20855;&#26377;&#26356;&#38271;&#26356;&#22797;&#26434;&#19978;&#19979;&#25991;&#30340;&#22686;&#24378;&#29702;&#35299;&#30340;&#38656;&#27714;&#65292;&#32780;&#27169;&#22411;&#36890;&#24120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20854;&#29702;&#35299;&#33021;&#21147;&#33539;&#22260;&#30340;&#21477;&#23376;&#24207;&#21015;&#26102;&#20250;&#36935;&#21040;&#19978;&#38480;&#65292;&#23548;&#33268;&#20135;&#29983;&#31163;&#39064;&#25110;&#28151;&#20081;&#30340;&#22238;&#31572;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#20851;&#27880;&#8220;&#20026;&#20160;&#20040;&#27169;&#22411;&#26080;&#27861;&#33258;&#34892;&#24357;&#34917;&#25110;&#22686;&#24378;&#33258;&#24049;&#30340;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#24615;&#36136;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#26368;&#23567;&#21270;&#39069;&#22806;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#21033;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11892</link><description>&lt;p&gt;
&#20851;&#20110;&#21463;&#24694;&#24847;&#22122;&#22768;&#24433;&#21709;&#30340;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#24494;&#23567;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#12290;Konstantinov&#21644;Lampert (2021)&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#36127;&#38754;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#19981;&#24179;&#34913;&#30340;&#32676;&#32452;&#22823;&#23567;&#19979;&#23384;&#22312;&#19968;&#20123;&#25968;&#25454;&#20998;&#24067;&#65292;&#20219;&#20309;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#37117;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#20048;&#35266;&#30340;&#35266;&#28857;&#65292;&#22914;&#26524;&#20801;&#35768;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#21017;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$\Theta(\alpha)$&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#20854;&#20013;$\alpha$&#26159;&#24694;&#24847;&#22122;&#22768;&#29575;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#27809;&#26377;&#20844;&#27491;&#32422;&#26463;&#30340;&#24773;&#20917;&#23436;&#20840;&#21305;&#37197;&#12290;&#23545;&#20110;&#26426;&#20250;&#22343;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$O(\sqrt{\alpha})$&#30340;&#25439;&#22833;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21305;&#37197;&#30340;$\Omega(\sqrt{\alpha})$&#30340;&#19979;&#30028;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Konstantinov&#21644;Lampert (2021)&#31034;&#33539;&#20102;&#23545;&#20110;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#65292;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31934;&#24230;&#25439;&#22833;&#37117;&#26159;$\Omega(1)$&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
&lt;/p&gt;</description></item><item><title>EMS-YOLO&#26159;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11411</link><description>&lt;p&gt;
&#28145;&#24230;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11411
&lt;/p&gt;
&lt;p&gt;
EMS-YOLO&#26159;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#39640;&#33021;&#25928;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26102;&#31354;&#21160;&#24577;&#26469;&#32534;&#30721;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;SNN&#22312;&#23569;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#26469;&#35299;&#20915;&#30446;&#26631;&#26816;&#27979;&#22238;&#24402;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMS-YOLO&#65292;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#22411;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#26694;&#26550;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#26469;&#35757;&#32451;&#28145;&#24230;SNN&#30340;&#23581;&#35797;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#33033;&#20914;&#27531;&#24046;&#22359;EMS-ResNet&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25193;&#23637;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#30340;&#28145;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20855;&#26377;&#20302;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35777;&#26126;&#20102;EMS-ResNet&#21487;&#20197;&#36991;&#20813;&#26799;&#24230;&#28040;&#22833;&#25110;&#26799;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;ANN-SNN&#36716;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20013;&#22269;&#20449;&#24687;&#26816;&#32034;&#30028;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#25112;&#30053;&#25253;&#21578;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#30693;&#35782;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#33021;&#21147;&#65292;&#20026;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;IR&#27169;&#22411;&#12289;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#20449;&#24687;&#23547;&#27714;&#25216;&#26415;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#20173;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#12289;&#21487;&#20449;&#24230;&#12289;&#39046;&#22495;&#29305;&#23450;&#38480;&#21046;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09751</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20013;&#22269;&#20449;&#24687;&#26816;&#32034;&#30028;&#30340;&#25112;&#30053;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. (arXiv:2307.09751v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20013;&#22269;&#20449;&#24687;&#26816;&#32034;&#30028;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#25112;&#30053;&#25253;&#21578;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#30693;&#35782;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#33021;&#21147;&#65292;&#20026;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;IR&#27169;&#22411;&#12289;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#20449;&#24687;&#23547;&#27714;&#25216;&#26415;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#20173;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#12289;&#21487;&#20449;&#24230;&#12289;&#39046;&#22495;&#29305;&#23450;&#38480;&#21046;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#25628;&#32034;&#65292;&#20197;&#28385;&#36275;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#30693;&#35782;&#25512;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20026;IR&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#22865;&#26426;&#12290;LLM&#19981;&#20165;&#33021;&#22815;&#20419;&#36827;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#36824;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#29992;&#25143;&#29702;&#35299;&#12289;&#27169;&#22411;&#35780;&#20272;&#21644;&#29992;&#25143;&#31995;&#32479;&#20132;&#20114;&#26041;&#26696;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;IR&#27169;&#22411;&#12289;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#26500;&#25104;&#20102;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#20449;&#24687;&#23547;&#27714;&#25216;&#26415;&#33539;&#24335;&#12290;IR&#27169;&#22411;&#25552;&#20379;&#23454;&#26102;&#21644;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;LLM&#36129;&#29486;&#20869;&#37096;&#30693;&#35782;&#65292;&#32780;&#20154;&#31867;&#22312;&#20449;&#24687;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#38656;&#27714;&#32773;&#21644;&#35780;&#20272;&#32773;&#30340;&#20013;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#30528;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;&#21487;&#20449;&#24230;&#38382;&#39064;&#12289;&#39046;&#22495;&#29305;&#23450;&#38480;&#21046;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2307.07515</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#31639;&#27861;&#27169;&#20223;&#65306;&#20026;&#20160;&#20040;&#20154;&#24037;&#8220;&#20195;&#29702;&#8221;&#19981;&#26159;&#65288;&#20063;&#19981;&#20250;&#25104;&#20026;&#65289;&#30495;&#27491;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents. (arXiv:2307.07515v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#37325;&#28857;&#25506;&#35752;&#8220;&#20195;&#29702;&#8221;&#27010;&#24565;&#65292;&#26469;&#25506;&#35752;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;&#20316;&#32773;&#25351;&#20986;&#20102;&#19977;&#20010;&#22522;&#26412;&#30340;&#24046;&#24322;&#65306;&#65288;1&#65289;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#30340;&#33258;&#20027;&#33021;&#21147;&#65292;&#33021;&#22815;&#35774;&#23450;&#33258;&#36523;&#30340;&#20869;&#22312;&#30446;&#26631;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#30001;&#22806;&#37096;&#20195;&#29702;&#25552;&#20379;&#30446;&#26631;&#20989;&#25968;&#30340;&#35745;&#31639;&#29615;&#22659;&#20013;&#12290;&#65288;2&#65289;&#29983;&#29289;&#31995;&#32479;&#26159;&#20855;&#20307;&#20307;&#29616;&#30340;&#65292;&#21363;&#20854;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#65292;&#32780;&#31639;&#27861;&#36816;&#34892;&#22312;&#35745;&#31639;&#32467;&#26500;&#19978;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#23558;&#36719;&#20214;&#19982;&#30828;&#20214;&#38548;&#31163;&#12290;&#65288;3&#65289;&#29983;&#29289;&#31995;&#32479;&#20307;&#39564;&#21040;&#19968;&#20010;&#24222;&#22823;&#30340;&#19990;&#30028;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38382;&#39064;&#26159;&#27169;&#31946;&#30340;&#65288;&#24182;&#38750;&#20840;&#37096;&#21487;&#23450;&#20041;&#65289;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#23567;&#19990;&#30028;&#20013;&#65292;&#20854;&#20013;&#25152;&#26377;&#38382;&#39064;&#37117;&#26159;&#26126;&#30830;&#30340;&#12290;&#36825;&#19977;&#20010;&#24046;&#24322;&#35828;&#26126;&#20102;&#29983;&#29289;&#21644;&#31639;&#27861;&#31995;&#32479;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of "agency." There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;GP&#24341;&#23548;&#30340;MPPI&#26041;&#27861;&#29992;&#20110;&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#21644;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#34920;&#38754;&#65292;&#35782;&#21035;&#24182;&#25512;&#33616;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2307.04019</link><description>&lt;p&gt;
GP&#24341;&#23548;&#30340;MPPI&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments. (arXiv:2307.04019v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;GP&#24341;&#23548;&#30340;MPPI&#26041;&#27861;&#29992;&#20110;&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#21644;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#34920;&#38754;&#65292;&#35782;&#21035;&#24182;&#25512;&#33616;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26377;&#38480;&#24863;&#30693;&#33021;&#21147;&#30340;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#23548;&#33322;&#23545;&#26426;&#22120;&#20154;&#23398;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#23616;&#37096;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#65288;MPPI&#65289;&#65292;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36935;&#21040;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#26465;&#20214;&#25110;&#22312;&#35745;&#21010;&#33539;&#22260;&#20043;&#22806;&#23548;&#33322;&#26102;&#65292;&#38656;&#35201;&#20840;&#23616;&#24341;&#23548;&#26469;&#30830;&#20445;&#26377;&#25928;&#30340;&#23548;&#33322;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GP-MPPI&#65292;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#23427;&#23558;MPPI&#19982;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;SGP&#65289;&#30340;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;SGP&#30340;&#23398;&#20064;&#33021;&#21147;&#26500;&#24314;&#19968;&#20010;&#26041;&#24046;&#65288;&#19981;&#30830;&#23450;&#24615;&#65289;&#34920;&#38754;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20102;&#35299;&#21608;&#22260;&#30340;&#21487;&#23548;&#33322;&#31354;&#38388;&#65292;&#35782;&#21035;&#19968;&#32452;&#24314;&#35758;&#30340;&#23376;&#30446;&#26631;&#65292;&#24182;&#26368;&#32456;&#25512;&#33616;&#26368;&#23567;&#21270;&#39044;&#23450;&#20041;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#20043;&#21518;&#65292;MPPI&#35745;&#31639;&#20986;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic navigation in unknown, cluttered environments with limited sensing capabilities poses significant challenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that sati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#30340;&#32593;&#32476;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65292;&#36890;&#36807;&#24341;&#20837;&#26597;&#35810;&#27169;&#26495;&#26469;&#26126;&#30830;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#26469;&#24314;&#27169;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00866</link><description>&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#35805;&#35821;&#20013;&#25366;&#25496;&#32447;&#32034;&#65306;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#30340;&#32593;&#32476;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting. (arXiv:2307.00866v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00866
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#30340;&#32593;&#32476;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65292;&#36890;&#36807;&#24341;&#20837;&#26597;&#35810;&#27169;&#26495;&#26469;&#26126;&#30830;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#26469;&#24314;&#27169;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#19981;&#23436;&#25972;&#35805;&#35821;&#21644;&#37325;&#20889;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#20063;&#27809;&#26377;&#38544;&#24335;&#21644;&#19981;&#20805;&#20998;&#22320;&#24314;&#27169;&#35821;&#20041;&#32467;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#32593;&#32476;&#65288;QUEEN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26597;&#35810;&#27169;&#26495;&#26126;&#30830;&#22320;&#24341;&#20837;&#20102;&#19981;&#23436;&#25972;&#35805;&#35821;&#21644;&#37325;&#20889;&#35805;&#35821;&#20043;&#38388;&#30340;&#25351;&#23548;&#24615;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#22312;&#21738;&#37324;&#36827;&#34892;&#21442;&#32771;&#25110;&#24674;&#22797;&#34987;&#30465;&#30053;&#30340;&#26631;&#35760;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#26469;&#24314;&#27169;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30001;&#20110;&#25552;&#20986;&#30340;&#26597;&#35810;&#27169;&#26495;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#30340;&#22909;&#22788;&#65292;QUEEN&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from proposed query template and the well-designed edit operation scoring network, QUEEN achieves state-of-the-art performance on several public datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16958</link><description>&lt;p&gt;
&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#30452;&#25509;&#25928;&#24212;&#65292;&#21363;&#34913;&#37327;&#19968;&#20010;&#21464;&#37327;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21464;&#37327;&#19981;&#21464;&#12290;&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#29992;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#26469;&#36827;&#34892;&#23450;&#24615;&#34920;&#31034;&#12290;&#20551;&#35774;&#32447;&#24615;&#21644;&#22240;&#26524;&#20805;&#20998;&#24615;&#65292;&#24182;&#32473;&#23450;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#24635;&#26159;&#21487;&#36776;&#35782;&#30340;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#30001;&#25152;&#35859;&#30340;&#21333;&#38376;&#20934;&#21017;&#32473;&#20986;&#30340;&#20219;&#20309;&#21464;&#37327;&#38598;&#21512;&#26469;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#27809;&#26377;&#27492;&#31867;&#22270;&#24418;&#21487;&#29992;&#65292;&#20294;&#19987;&#23478;&#20173;&#28982;&#21487;&#20197;&#35775;&#38382;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#19968;&#20010;&#25277;&#35937;&#65292;&#35813;&#25277;&#35937;&#34920;&#31034;&#20102;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#30465;&#30053;&#20102;&#26102;&#38388;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#20854;&#20013;&#35814;&#32454;&#25551;&#36848;&#20102;&#25152;&#26377;&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#21487;&#36776;&#35782;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with a full-time causal graph. Assuming linearity and causal sufficiency and given the full-time causal graph, the direct causal effect is always identifiable and can be estimated from data by adjusting on any set of variables given by the so-called single-door criterion. However, in many application such a graph is not available for various reasons but nevertheless experts have access to an abstraction of the full-time causal graph which represents causal relations between time series while omitting temporal information. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from summa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20010;&#20307;3D&#23454;&#20363;&#30340;&#22810;&#27169;&#24577;2D&#25237;&#24433;&#65292;&#21457;&#29616;&#22312;&#38646;-shot&#29615;&#22659;&#20013;&#65292;&#33609;&#22270;&#36234;&#25277;&#35937;&#65292;&#38169;&#35823;&#22270;&#20687;&#21305;&#37197;&#30340;&#21487;&#33021;&#24615;&#36234;&#39640;&#12290;&#21478;&#22806;&#65292;&#32454;&#35843;&#19968;&#20010;&#24418;&#29366;&#31867;&#21035;&#30340;3D&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20854;&#20182;&#24418;&#29366;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08541</link><description>&lt;p&gt;
&#32454;&#35843;&#20294;&#38646;-shot 3D&#24418;&#29366;&#33609;&#22270;&#35270;&#22270;&#30456;&#20284;&#24615;&#21644;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval. (arXiv:2306.08541v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20010;&#20307;3D&#23454;&#20363;&#30340;&#22810;&#27169;&#24577;2D&#25237;&#24433;&#65292;&#21457;&#29616;&#22312;&#38646;-shot&#29615;&#22659;&#20013;&#65292;&#33609;&#22270;&#36234;&#25277;&#35937;&#65292;&#38169;&#35823;&#22270;&#20687;&#21305;&#37197;&#30340;&#21487;&#33021;&#24615;&#36234;&#39640;&#12290;&#21478;&#22806;&#65292;&#32454;&#35843;&#19968;&#20010;&#24418;&#29366;&#31867;&#21035;&#30340;3D&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20854;&#20182;&#24418;&#29366;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ViT&#65288;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#21644;ResNet&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#24050;&#32463;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#34987;&#29992;&#20316;&#27604;&#36739;&#33609;&#22270;&#21644;&#22270;&#20687;&#30340;&#30693;&#35273;&#24230;&#37327;&#65292;&#20197;&#21450;&#38646;-shot&#29615;&#22659;&#20013;&#30340;&#22810;&#22495;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#32534;&#30721;&#22120;&#30340;&#32454;&#31890;&#24230;&#37327;&#21270;&#24037;&#20316;&#23578;&#26377;&#26377;&#38480;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#20851;&#27880;&#20010;&#20307;3D&#23454;&#20363;&#30340;&#22810;&#27169;&#24577;2D&#25237;&#24433;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#20110;&#26816;&#32034;&#21644;&#22522;&#20110;&#33609;&#22270;&#24314;&#27169;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#29615;&#22659;&#20013;&#65292;&#33609;&#22270;&#36234;&#25277;&#35937;&#65292;&#38169;&#35823;&#22270;&#20687;&#21305;&#37197;&#30340;&#21487;&#33021;&#24615;&#36234;&#39640;&#12290;&#21363;&#20351;&#22312;&#21516;&#19968;&#20010;&#33609;&#22270;&#39046;&#22495;&#20013;&#65292;&#20197;&#19981;&#21516;&#30340;&#39118;&#26684;&#32472;&#21046;&#30340;&#21516;&#19968;&#29289;&#20307;&#30340;&#33609;&#22270;&#65292;&#20363;&#22914;&#30001;&#19981;&#21516;&#30340;&#20010;&#20307;&#32472;&#21046;&#65292;&#20063;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21305;&#37197;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#22312;&#19968;&#31867;3D&#24418;&#29366;&#19978;&#36827;&#34892;&#32454;&#33268;&#30340;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#20182;&#24418;&#29366;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#30417;&#30563;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, encoders like ViT (vision transformer) and ResNet have been trained on vast datasets and utilized as perceptual metrics for comparing sketches and images, as well as multi-domain encoders in a zero-shot setting. However, there has been limited effort to quantify the granularity of these encoders. Our work addresses this gap by focusing on multi-modal 2D projections of individual 3D instances. This task holds crucial implications for retrieval and sketch-based modeling. We show that in a zero-shot setting, the more abstract the sketch, the higher the likelihood of incorrect image matches. Even within the same sketch domain, sketches of the same object drawn in different styles, for example by distinct individuals, might not be accurately matched. One of the key findings of our research is that meticulous fine-tuning on one class of 3D shapes can lead to improved performance on other shape classes, reaching or surpassing the accuracy of supervised methods. We compare and discus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2306.05965</link><description>&lt;p&gt;
&#22312;&#22240;&#23376;&#22270;&#20013;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#65292;&#36125;&#21494;&#26031;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#24050;&#32463;&#34987;&#26377;&#25928;&#33258;&#21160;&#21270;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#23578;&#26410;&#22914;&#27492;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#23481;&#26131;&#20986;&#38169;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#25512;&#23548;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#27604;&#36739;&#32463;&#24120;&#34987;&#24573;&#35270;&#21644;&#24573;&#30053;&#65292;&#23613;&#31649;&#23427;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;Forney&#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#36827;&#32780;&#21487;&#20351;&#29992;&#32553;&#25918;&#22240;&#23376;&#21516;&#26102;&#25191;&#34892;&#21442;&#25968;&#21644;&#29366;&#24577;&#25512;&#26029;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#65292;&#21516;&#26102;&#20801;&#35768;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#20998;&#23618;&#21644;&#26102;&#38388;&#27169;&#22411;&#20808;&#39564;&#65292;&#20197;&#36866;&#24212;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2306.03753</link><description>&lt;p&gt;
AI&#33402;&#26415;&#31574;&#23637;&#65306;&#37325;&#26032;&#26500;&#24819;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;
&lt;/p&gt;
&lt;p&gt;
AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#31574;&#23637;&#23454;&#36341;&#30340;&#29305;&#28857;&#26159;&#20197;&#30693;&#35782;&#30340;&#26041;&#24335;&#23637;&#31034;&#33402;&#26415;&#25910;&#34255;&#21697;&#12290;&#26426;&#22120;&#36807;&#31243;&#30340;&#29305;&#28857;&#26159;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#35774;&#24819;&#20102;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#20197;&#25506;&#32034;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31574;&#23637;&#30028;&#30340;&#24433;&#21709;&#12290;&#35813;&#39033;&#30446;&#26159;&#20026;2023&#24180;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21452;&#24180;&#23637;&#30340;&#22330;&#21512;&#32780;&#24320;&#21457;&#30340;&#65292;&#39064;&#20026;&#8220;&#21487;&#33021;&#20986;&#29616;&#26032;&#30340;&#26041;&#21521;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21338;&#29289;&#39302;&#65288;HAM&#65289;&#30340;&#34255;&#21697;&#65292;&#36890;&#36807;&#26426;&#22120;&#24863;&#30693;&#30340;&#35270;&#35282;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#12290;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#23637;&#31034;&#23460;&#20869;&#33402;&#26415;&#21697;&#65292;&#26681;&#25454;&#30456;&#20284;&#24615;&#35780;&#20998;&#20998;&#37197;&#34394;&#26500;&#30340;&#22352;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#26469;&#25913;&#21464;&#27599;&#20214;&#33402;&#26415;&#21697;&#22312;&#22478;&#24066;&#20013;&#30340;&#25152;&#22788;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20214;&#33402;&#26415;&#21697;&#20301;&#32622;&#30340;360&#20840;&#26223;&#22270;&#30340;&#28145;&#24230;&#20540;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#25552;&#31034;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#20010;&#39033;&#30446;&#30340;&#32467;&#26524;&#23601;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Art curatorial practice is characterized by the presentation of an art collection in a knowledgeable way. Machine processes are characterized by their capacity to manage and analyze large amounts of data. This paper envisages AI curation and audience interaction to explore the implications of contemporary machine learning models for the curatorial world. This project was developed for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city of Helsinki through the lens of machine perception. We use visual-textual models to place indoor artworks in public spaces, assigning fictional coordinates based on similarity scores. We transform the space that each artwork inhabits in the city by generating synthetic 360 art panoramas. We guide the generation estimating depth values from 360 panoramas at each artwork location, and machine-generated prompts of the artworks. The result of this project i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15299</link><description>&lt;p&gt;
&#22312;ChatGPT&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#31185;&#23398;&#65306;&#30740;&#31350;&#20262;&#29702;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20294;&#26377;&#20105;&#35758;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#26102;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#26088;&#22312;&#20026;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#22880;&#23450;&#26032;&#30340;&#21450;&#26102;&#22522;&#30784;&#12290;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#21644;&#30740;&#31350;&#23545;&#35937;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#35814;&#32454;&#23457;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31185;&#23398;&#23478;&#12289;&#21442;&#19982;&#32773;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#30340;&#26032;&#20852;&#23454;&#36341;&#65292;&#24182;&#32473;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20026;&#22312;AI&#26102;&#20195;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65288;CPLL&#65289;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#30495;&#23454;&#21518;&#39564;&#21644;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#21644;&#20869;&#23481;&#30340;&#32622;&#20449;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12485</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition. (arXiv:2305.12485v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65288;CPLL&#65289;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#30495;&#23454;&#21518;&#39564;&#21644;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#21644;&#20869;&#23481;&#30340;&#32622;&#20449;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#36890;&#36807;&#20247;&#21253;&#33719;&#24471;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#27880;&#31354;&#38388;&#30340;&#24191;&#27867;&#24615;&#21644;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#36890;&#36807;&#22810;&#20010;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#34920;&#20915;&#33719;&#24471;&#32479;&#19968;&#19988;&#27491;&#30830;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#30452;&#25509;&#21033;&#29992;&#21407;&#22987;&#30340;&#22810;&#26631;&#27880;&#32773;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;CPLL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#20808;&#39564;&#32622;&#20449;&#24230;&#21644;&#27169;&#22411;&#23398;&#20064;&#30340;&#21518;&#39564;&#32622;&#20449;&#24230;&#65292;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;NER&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#21644;&#20869;&#23481;&#30340;&#32622;&#20449;&#24230;&#12290;&#30495;&#23454;&#21518;&#39564;&#20272;&#35745;&#22120;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#30495;&#23454;&#21518;&#39564;&#21644;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models for named entity recognition (NER) are mainly based on large-scale labeled datasets, which always obtain using crowdsourcing. However, it is hard to obtain a unified and correct label via majority voting from multiple annotators for NER due to the large labeling space and complexity of this task. To address this problem, we aim to utilize the original multi-annotator labels directly. Particularly, we propose a Confidence-based Partial Label Learning (CPLL) method to integrate the prior confidence (given by annotators) and posterior confidences (learned by models) for crowd-annotated NER. This model learns a token- and content-dependent confidence via an Expectation-Maximization (EM) algorithm by minimizing empirical risk. The true posterior estimator and confidence estimator perform iteratively to update the true posterior and confidence respectively. We conduct extensive experimental results on both real-world and synthetic datasets, which show that our model can impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#25968;&#25454;&#38598;&#32479;&#19968;&#27867;&#21270;&#65288;SUG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#32454;&#31890;&#24230;&#23376;&#22495;&#23545;&#40784;&#21644;&#26679;&#26412;&#32423;&#22495;&#24863;&#30693;&#27880;&#24847;&#21147;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#19977;&#32500;&#28857;&#20113;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09160</link><description>&lt;p&gt;
SUG&#65306;&#21333;&#25968;&#25454;&#38598;&#32479;&#19968;&#27867;&#21270;&#29992;&#20110; 3D &#28857;&#20113;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification. (arXiv:2305.09160v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#25968;&#25454;&#38598;&#32479;&#19968;&#27867;&#21270;&#65288;SUG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#32454;&#31890;&#24230;&#23376;&#22495;&#23545;&#40784;&#21644;&#26679;&#26412;&#32423;&#22495;&#24863;&#30693;&#27880;&#24847;&#21147;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#19977;&#32500;&#28857;&#20113;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#38382;&#39064;&#22312;&#20108;&#32500;&#22270;&#20687;&#20219;&#21153;&#20013;&#22686;&#38271;&#36805;&#36895;&#65292;&#20294;&#20854;&#22312;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#19978;&#30340;&#25506;&#32034;&#20173;&#28982;&#19981;&#36275;&#65292;&#24182;&#19988;&#21463;&#21040;&#26356;&#22797;&#26434;&#21644;&#19981;&#30830;&#23450;&#30340;&#36328;&#22495;&#24046;&#24322;&#20197;&#21450;&#19981;&#22343;&#21248;&#30340;&#36328;&#31867;&#27169;&#24577;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;&#19982;&#20043;&#21069;&#30340;&#20108;&#32500; DG &#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#20851;&#27880;&#19977;&#32500; DG &#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181; Single-dataset Unified Generalization&#65288;SUG&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20165;&#21033;&#29992;&#21333;&#20010;&#28304;&#25968;&#25454;&#38598;&#26469;&#32531;&#35299;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#38754;&#20020;&#30340;&#26410;&#30693;&#22495;&#24046;&#24322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32454;&#31890;&#24230;&#23376;&#22495;&#23545;&#40784;&#65288;MSA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#21333;&#20010;&#28304;&#25968;&#25454;&#38598;&#30340;&#20998;&#35010;&#23376;&#22495;&#20043;&#38388;&#25191;&#34892;&#22810;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#36807;&#31243;&#26469;&#32422;&#26463;&#23398;&#20064;&#34920;&#31034;&#20026;&#22495;&#19981;&#21487;&#30693;&#21644;&#26377;&#21306;&#21035;&#24615;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#32423;&#22495;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;SDA&#65289;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#26679;&#26412;&#25152;&#23646;&#30340;&#19981;&#21516;&#23376;&#22495;&#36873;&#25321;&#24615;&#22320;&#22686;&#24378;&#26131;&#20110;&#36866;&#24212;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Domain Generalization (DG) problem has been fast-growing in the 2D image tasks, its exploration on 3D point cloud data is still insufficient and challenged by more complex and uncertain cross-domain variances with uneven inter-class modality distribution. In this paper, different from previous 2D DG works, we focus on the 3D DG problem and propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA) method, which can constrain the learned representations to be domain-agnostic and discriminative, by performing a multi-grained feature alignment process between the splitted sub-domains from the single source dataset. Then, a Sample-level Domain-aware Attention (SDA) strategy is presented, which can selectively enhance easy-to-adapt samples from different sub-domains according to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.08890</link><description>&lt;p&gt;
&#24046;&#20998;&#21367;&#31215;&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;FTSF&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#20856;&#22411;&#39044;&#27979;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;FTSF&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#19987;&#23478;&#31995;&#32479;&#65292;&#23548;&#33268;&#22833;&#21435;&#20102;&#35782;&#21035;&#26410;&#23450;&#20041;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;FTSF&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24046;&#20998;&#27169;&#31946;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DFCNN&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#12290;DFCNN&#33021;&#22815;&#35782;&#21035;&#28508;&#22312;&#20449;&#24687;&#24182;&#25913;&#21892;&#39044;&#27979;&#31934;&#24230;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#23398;&#20064;&#33021;&#21147;&#65292;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#35268;&#21017;&#30340;&#38271;&#24230;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#65292;&#36825;&#26159;&#19987;&#23478;&#31995;&#32479;&#25152;&#26080;&#27861;&#22788;&#29702;&#30340;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#65292;FTSF&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#20250;&#23548;&#33268;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#38598;&#22833;&#25928;&#65292;&#24182;&#23548;&#33268;&#39044;&#27979;&#22833;&#36133;&#12290;DFCNN&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#8212;&#8212;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#20013;&#30340;&#19968;&#27454;&#21345;&#29260;&#28216;&#25103;&#12290;&#38500;&#20102;&#24212;&#23545;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#25361;&#25112;&#22806;&#65292;&#35813;&#25361;&#25112;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#31454;&#36187;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#35299;&#20915;&#28216;&#25103;&#65292;&#22914;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#31532;&#19968;&#23626;TOTAIC&#23558;&#22312;2023&#24180;&#30340;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.08234</link><description>&lt;p&gt;
&#24341;&#20837;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Introducing Tales of Tribute AI Competition. (arXiv:2305.08234v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#8212;&#8212;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#20013;&#30340;&#19968;&#27454;&#21345;&#29260;&#28216;&#25103;&#12290;&#38500;&#20102;&#24212;&#23545;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#25361;&#25112;&#22806;&#65292;&#35813;&#25361;&#25112;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#31454;&#36187;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#35299;&#20915;&#28216;&#25103;&#65292;&#22914;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#31532;&#19968;&#23626;TOTAIC&#23558;&#22312;2023&#24180;&#30340;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#21363;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#28023;&#20043;&#23707;&#31456;&#33410;&#21457;&#24067;&#30340;&#19968;&#27454;&#21452;&#20154;&#21345;&#29260;&#24314;&#35774;&#31867;&#28216;&#25103;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#20854;&#20182;&#28085;&#30422;&#25910;&#38598;&#21345;&#29260;&#28216;&#25103;(CCG)&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#20063;&#20174;&#26410;&#26377;&#36807;&#38024;&#23545;&#24314;&#35774;&#21345;&#29260;&#28216;&#25103;&#30340;&#31454;&#36187;&#12290;&#22240;&#27492;&#65292;&#25104;&#21151;&#30340;&#26041;&#27861;&#38500;&#20102;&#35201;&#20811;&#26381;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#38556;&#30861;&#65292;&#22914;&#38543;&#26426;&#24615;&#12289;&#38544;&#34255;&#20449;&#24687;&#21644;&#22823;&#30340;&#20998;&#25903;&#22240;&#32032;&#22806;&#65292;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#35813;&#28216;&#25103;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31454;&#36187;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#28216;&#25103;&#35268;&#21017;&#65292;&#24182;&#21576;&#29616;&#20102;&#31034;&#20363;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#38182;&#26631;&#36187;&#32467;&#26524;&#12290;TOTAIC&#30340;&#31532;&#19968;&#23626;&#23558;&#22312;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new AI challenge, the Tales of Tribute AI Competition (TOTAIC), based on a two-player deck-building card game released with the High Isle chapter of The Elder Scrolls Online. Currently, there is no other AI competition covering Collectible Card Games (CCG) genre, and there has never been one that targets a deck-building game. Thus, apart from usual CCG-related obstacles to overcome, like randomness, hidden information, and large branching factor, the successful approach additionally requires long-term planning and versatility. The game can be tackled with multiple approaches, including classic adversarial search, single-player planning, and Neural Networks-based algorithms. This paper introduces the competition framework, describes the rules of the game, and presents the results of a tournament between sample AI agents. The first edition of TOTAIC is hosted at the IEEE Conference on Games 2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06848</link><description>&lt;p&gt;
CAR-DESPOT: &#38024;&#23545;&#28151;&#26434;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#38543;&#26426;&#34892;&#20026;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#30495;&#23454;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#37096;&#20998;&#35266;&#23519;&#36827;&#34892;&#20915;&#31574;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#38382;&#39064;&#26159;&#36827;&#34892;&#20934;&#30830;&#21644;&#24378;&#20581;&#30340;&#34892;&#20026;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#36825;&#20123;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26126;&#30830;&#30340;&#22240;&#26524;&#35821;&#20041;&#65292;POMDP&#35268;&#21010;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CAR-DESPOT&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#35268;&#21010;&#31243;&#24207;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12012</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#26816;&#26597;&#65292;&#35760;&#24405;&#22823;&#33041;&#30340;&#30005;&#27963;&#21160;&#12290;&#35813;&#26816;&#26597;&#29992;&#20110;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#33041;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#30315;&#30187;&#26816;&#27979;&#12290;&#22312;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20013;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;EEG&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#23545;&#20110;&#26816;&#27979;&#24433;&#21709;&#22823;&#33041;&#30340;&#30142;&#30149;&#24456;&#26377;&#29992;&#12290;&#26377;&#26102;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20869;&#35782;&#21035;EEG&#30340;&#26368;&#23567;&#21464;&#21270;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;DWT&#21487;&#20197;&#22312;&#19981;&#21516;&#39057;&#24102;&#36827;&#34892;&#20449;&#21495;&#33391;&#22909;&#30340;&#20998;&#35299;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#38477;&#32500;&#31639;&#27861;&#65306;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#34701;&#21512;&#35268;&#21017;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08720</link><description>&lt;p&gt;
&#36817;&#22320;&#34920;&#39118;&#30340;&#31639;&#27861;&#24187;&#35273;&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#25105;&#20204;&#30340;GANs&#26159;&#36890;&#36807;&#23545;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#36755;&#20837;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#26469;&#29983;&#25104;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306; Weather Research and Forecasting&#65288;WRF&#65289;&#27169;&#22411;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22320;&#34920;&#39118;&#12290;&#19982;&#20256;&#32479;&#30340;SR&#27169;&#22411;&#19981;&#21516;&#65292;LR&#36755;&#20837;&#22312;WRF&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#38750;&#29702;&#24819;&#21270;&#30340;LR&#21644;HR&#37197;&#23545;&#65292;&#23548;&#33268;&#30001;&#20110;&#20869;&#37096;&#21464;&#24322;&#24341;&#36215;&#30340;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#24403;&#21069;&#22522;&#20110;SR&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#24182;&#23581;&#35797;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26032;&#39062;&#39057;&#29575;&#20998;&#31163;&#65288;FS&#65289;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;SR&#27169;&#22411;&#30340;&#25216;&#33021;&#65292;&#25105;&#20204;&#31934;&#36873;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20851;&#27880;&#22522;&#20110;&#31354;&#38388;&#21151;&#29575;&#35889;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;GAN&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#20013;&#36335;&#24452;&#20381;&#36182;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#22240;&#26524;&#25552;&#21319;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#24178;&#39044;&#25968;&#25454;&#35782;&#21035;&#22240;&#26524;&#38142;&#36335;&#39044;&#27979;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2302.01198</link><description>&lt;p&gt;
&#22240;&#26524;&#25552;&#21319;&#19982;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#20013;&#36335;&#24452;&#20381;&#36182;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#22240;&#26524;&#25552;&#21319;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#24178;&#39044;&#25968;&#25454;&#35782;&#21035;&#22240;&#26524;&#38142;&#36335;&#39044;&#27979;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#22240;&#26524;&#27169;&#22411;&#20551;&#35774;&#23384;&#22312;&#19968;&#32452;&#22266;&#26377;&#30340;&#33410;&#28857;&#22240;&#23376;&#65292;&#21363;&#22312;&#33410;&#28857;&#20986;&#29983;&#26102;&#23601;&#23450;&#20041;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#23427;&#20204;&#25511;&#21046;&#30528;&#22270;&#20013;&#38142;&#36335;&#30340;&#22240;&#26524;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#22240;&#26524;&#20219;&#21153;&#20013;&#65292;&#38142;&#36335;&#24418;&#25104;&#26159;&#36335;&#24452;&#20381;&#36182;&#30340;&#65306;&#38142;&#36335;&#24178;&#39044;&#30340;&#32467;&#26524;&#21462;&#20915;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#22240;&#26524;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#36335;&#24452;&#20381;&#36182;&#30340;&#38142;&#36335;&#24418;&#25104;&#65292;&#22240;&#20026;&#38142;&#36335;&#20043;&#38388;&#30340;&#32423;&#32852;&#21151;&#33021;&#20381;&#36182;&#65288;&#30001;&#36335;&#24452;&#20381;&#36182;&#24615;&#20135;&#29983;&#65289;&#35201;&#20040;&#26080;&#27861;&#35782;&#21035;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#19981;&#20999;&#23454;&#38469;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#20013;&#36335;&#24452;&#20381;&#36182;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#25552;&#21319;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#22270;&#30340;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#26377;&#38480;&#30340;&#24178;&#39044;&#25968;&#25454;&#26469;&#35782;&#21035;&#22240;&#26524;&#38142;&#36335;&#39044;&#27979;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26500;&#23545;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#23884;&#20837;&#30340;&#20302;&#32500;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing causal models for link prediction assume an underlying set of inherent node factors -- an innate characteristic defined at the node's birth -- that governs the causal evolution of links in the graph. In some causal tasks, however, link formation is path-dependent: The outcome of link interventions depends on existing links. Unfortunately, these existing causal methods are not designed for path-dependent link formation, as the cascading functional dependencies between links (arising from path dependence) are either unidentifiable or require an impractical number of control variables. To overcome this, we develop the first causal model capable of dealing with path dependencies in link prediction. In this work we introduce the concept of causal lifting, an invariance in causal models of independent interest that, on graphs, allows the identification of causal link prediction queries using limited interventional data. Further, we show how structural pairwise embeddings exhibit low
&lt;/p&gt;</description></item><item><title>ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.11596</link><description>&lt;p&gt;
ThoughtSource:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25968;&#25454;&#30340;&#20013;&#22830;&#26530;&#32445;&#12290;
&lt;/p&gt;
&lt;p&gt;
ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11596
&lt;/p&gt;
&lt;p&gt;
ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#19978;&#20173;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#19981;&#36879;&#26126;&#65292;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#20107;&#23454;&#65292;&#24182;&#19988;&#23384;&#22312;&#20854;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35753;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#34920;&#36798;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ThoughtSource&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#12290;ThoughtSource&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26469;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;ThoughtSource&#30340;&#39318;&#27425;&#21457;&#24067;&#38598;&#25104;&#20102;&#20845;&#20010;&#31185;&#23398;/&#21307;&#23398;&#12289;&#19977;&#20010;&#36890;&#29992;&#39046;&#22495;&#21644;&#20116;&#20010;&#25968;&#23398;&#39064;&#31572;&#26696;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.01913</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#23398;&#20064;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#12290;&#27714;&#35299;&#22120;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#26159;&#20998;&#25903;&#21551;&#21457;&#24335;&#65292;&#23427;&#20204;&#26088;&#22312;&#22312;&#26368;&#30701;&#30340;&#26102;&#38388;&#20869;&#23547;&#25214;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#20123;&#21551;&#21457;&#24335;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#24182;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#35768;&#22810;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#23398;&#20064;&#39640;&#25928;&#21551;&#21457;&#24335;&#30340;&#21162;&#21147;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#24178;&#39044;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#36890;&#29992;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36873;&#25321;&#21364;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#24471;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36825;&#24471;&#30410;&#20110;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#21644;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Constraint programming is known for being an efficient approach for solving combinatorial problems. Important design choices in a solver are the branching heuristics, which are designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. To the best of our knowledge, it is still an open research question. Although several generic variable-selection heuristics are available in the literature, the options for a generic value-selection heuristic are more scarce. In this paper, we propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06589</link><description>&lt;p&gt;
&#22810;&#27169;&#22359;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#34920;&#24449;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#19982;&#25512;&#26029;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#20197;&#21450;&#25512;&#24191;&#21040;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#20102;&#35299;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.11838</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#20855;&#26377;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#37327;&#22870;&#21169;&#20449;&#21495;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#25163;&#24037;&#21019;&#24314;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#24448;&#24448;&#23481;&#26131;&#20986;&#38169;&#65292;&#29305;&#21035;&#26159;&#24403;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#21482;&#26377;&#37096;&#20998;&#24050;&#30693;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#20195;&#29702;&#32463;&#39564;&#30340; episodes &#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#31616;&#27905;&#26377;&#38480;&#29366;&#24577;&#8220;&#20219;&#21153;&#33258;&#21160;&#26426;&#8221;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#31639;&#27861;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20135;&#21697; MDP &#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979; MDP &#24182;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340; Baum-Welch &#31639;&#27861;&#26469;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#20102;&#35268;&#33539;&#33258;&#21160;&#26426;&#21644;&#29615;&#22659;&#30340; MDP&#65288;&#21021;&#22987;&#37117;&#26410;&#30693;&#65289;&#25152;&#32452;&#25104;&#30340;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#23398;&#21040;&#30340;&#20135;&#21697; MDP &#20013;&#25552;&#28860;&#20219;&#21153;&#33258;&#21160;&#26426;&#65288;&#20551;&#35774;&#20026;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65289;&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#20351;&#24471;&#20195;&#29702;&#21487;&#20197;&#35299;&#20915;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training reinforcement learning (RL) agents using scalar reward signals is often infeasible when an environment has sparse and non-Markovian rewards. Moreover, handcrafting these reward functions before training is prone to misspecification, especially when the environment's dynamics are only partially known. This paper proposes a novel pipeline for learning non-Markovian task specifications as succinct finite-state `task automata' from episodes of agent experience within unknown environments. We leverage two key algorithmic insights. First, we learn a product MDP, a model composed of the specification's automaton and the environment's MDP (both initially unknown), by treating the product MDP as a partially observable MDP and using the well-known Baum-Welch algorithm for learning hidden Markov models. Second, we propose a novel method for distilling the task automaton (assumed to be a deterministic finite automaton) from the learnt product MDP. Our learnt task automaton enables the dec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#36830;&#35789;&#26597;&#35810;&#22312;PAC&#27169;&#22411;&#20013;&#30340;&#38750;&#39640;&#25928;&#21487;&#23398;&#20064;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#21464;&#31181;&#30340;&#36830;&#35789;&#26597;&#35810;&#25552;&#20986;&#20102;&#36127;&#38754;&#21487;&#23398;&#20064;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25104;&#21592;&#26597;&#35810;&#21487;&#20197;&#39640;&#25928;&#23398;&#20064;&#36830;&#35789;&#26597;&#35810;&#21644;UCQs&#12290;</title><link>http://arxiv.org/abs/2208.10255</link><description>&lt;p&gt;
&#35770;&#36830;&#35789;&#26597;&#35810;&#30340;&#38750;&#39640;&#25928;PAC&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the non-efficient PAC learnability of conjunctive queries. (arXiv:2208.10255v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#36830;&#35789;&#26597;&#35810;&#22312;PAC&#27169;&#22411;&#20013;&#30340;&#38750;&#39640;&#25928;&#21487;&#23398;&#20064;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#21464;&#31181;&#30340;&#36830;&#35789;&#26597;&#35810;&#25552;&#20986;&#20102;&#36127;&#38754;&#21487;&#23398;&#20064;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25104;&#21592;&#26597;&#35810;&#21487;&#20197;&#39640;&#25928;&#23398;&#20064;&#36830;&#35789;&#26597;&#35810;&#21644;UCQs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26377;&#19977;&#20010;&#30446;&#30340;&#65306;(i)&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#38416;&#36848;&#65292;&#35777;&#26126;&#20102;&#36830;&#35789;&#26597;&#35810;&#22312;&#21487;&#33021;-&#36817;&#20284;&#27491;&#30830;(PAC)&#27169;&#22411;&#20013;&#27809;&#26377;&#39640;&#25928;&#21487;&#23398;&#20064;&#24615;&#65292;&#28165;&#26970;&#22320;&#27880;&#24847;&#21040;&#36825;&#20010;&#27010;&#24565;&#31867;&#22312;&#24456;&#22810;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#25991;&#29486;&#20013;&#26263;&#21547;&#30340;&#22810;&#39033;&#24335;&#35268;&#27169;&#36866;&#24212;&#24615;&#32570;&#22833;&#65307;(ii)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36127;&#38754;PAC&#21487;&#23398;&#20064;&#24615;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#38480;&#21046;&#31867;&#21035;&#30340;&#36830;&#35789;&#26597;&#35810;(CQs)&#65292;&#21253;&#25324;&#24191;&#20041;&#30340;&#8220;&#26080;&#29615;&#24615;&#8221;&#65307;(iii)&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#35789;&#26597;&#35810;&#21644;UCQs&#36890;&#36807;&#25104;&#21592;&#26597;&#35810;&#26159;&#21487;&#20197;&#39640;&#25928;PAC&#21487;&#23398;&#20064;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note serves three purposes: (i) we provide a self-contained exposition of the fact that conjunctive queries are not efficiently learnable in the Probably-Approximately-Correct (PAC) model, paying clear attention to the complicating fact that this concept class lacks the polynomial-size fitting property, a property that is tacitly assumed in much of the computational learning theory literature; (ii) we establish a strong negative PAC learnability result that applies to many restricted classes of conjunctive queries (CQs), including acyclic CQs for a wide range of notions of "acyclicity"; (iii) we show that CQs (and UCQs) are efficiently PAC learnable with membership queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#25490;&#21015;&#30340;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#20266;&#24067;&#23572;&#22522;&#20934;&#36716;&#21270;&#20026;&#22522;&#20110;&#25490;&#21015;&#38598;&#21512;&#23450;&#20041;&#30340;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20301;&#20018;&#19981;&#21516;&#65292;&#25490;&#21015;&#21464;&#24322;&#30340;&#22256;&#38590;&#31243;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#27721;&#26126;&#36317;&#31163;&#65292;&#36824;&#19982;&#25490;&#21015;&#30340;&#31934;&#30830;&#24490;&#29615;&#32467;&#26500;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2207.04045</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#21015;&#30340;&#36827;&#21270;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Runtime Analysis for Permutation-based Evolutionary Algorithms. (arXiv:2207.04045v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#25490;&#21015;&#30340;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#20266;&#24067;&#23572;&#22522;&#20934;&#36716;&#21270;&#20026;&#22522;&#20110;&#25490;&#21015;&#38598;&#21512;&#23450;&#20041;&#30340;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20301;&#20018;&#19981;&#21516;&#65292;&#25490;&#21015;&#21464;&#24322;&#30340;&#22256;&#38590;&#31243;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#27721;&#26126;&#36317;&#31163;&#65292;&#36824;&#19982;&#25490;&#21015;&#30340;&#31934;&#30830;&#24490;&#29615;&#32467;&#26500;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36807;&#21435;25&#24180;&#38024;&#23545;&#20266;&#24067;&#23572;&#20248;&#21270;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#30340;&#29702;&#35770;&#20998;&#26512;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22914;&#20309;&#35299;&#20915;&#22522;&#20110;&#25490;&#21015;&#30340;&#38382;&#39064;&#30340;&#29702;&#35770;&#32467;&#26524;&#26041;&#38754;&#21482;&#23384;&#22312;&#38646;&#26143;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#22522;&#20110;&#25490;&#21015;&#30340;&#22522;&#20934;&#38382;&#39064;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32463;&#20856;&#30340;&#20266;&#24067;&#23572;&#22522;&#20934;&#36716;&#21270;&#20026;&#22522;&#20110;&#25490;&#21015;&#38598;&#21512;&#23450;&#20041;&#30340;&#22522;&#20934;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;Scharnow&#65292;Tinnefeld&#21644;Wegener&#65288;2004&#65289;&#25552;&#20986;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#65288;1+1&#65289;EA&#22312;LeadingOnes&#21644;Jump&#22522;&#20934;&#30340;&#31867;&#20284;&#29289;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;&#12290;&#21518;&#32773;&#34920;&#26126;&#65292;&#19982;&#20301;&#20018;&#19981;&#21516;&#65292;&#19981;&#20165;&#27721;&#26126;&#36317;&#31163;&#20915;&#23450;&#20102;&#23558;&#19968;&#20010;&#25490;&#21015;$\sigma$&#21464;&#24322;&#20026;&#21478;&#19968;&#20010;&#25490;&#21015;$\tau$&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;&#36824;&#26377;$\sigma\tau^{-1}$&#30340;&#31934;&#30830;&#24490;&#29615;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#23545;&#31216;&#30340;&#28151;&#28102;&#21464;&#24322;&#31639;&#23376;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#26356;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#36824;&#23548;&#33268;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the theoretical analysis of evolutionary algorithms (EAs) has made significant progress for pseudo-Boolean optimization problems in the last 25 years, only sporadic theoretical results exist on how EAs solve permutation-based problems.  To overcome the lack of permutation-based benchmark problems, we propose a general way to transfer the classic pseudo-Boolean benchmarks into benchmarks defined on sets of permutations. We then conduct a rigorous runtime analysis of the permutation-based $(1+1)$ EA proposed by Scharnow, Tinnefeld, and Wegener (2004) on the analogues of the LeadingOnes and Jump benchmarks. The latter shows that, different from bit-strings, it is not only the Hamming distance that determines how difficult it is to mutate a permutation $\sigma$ into another one $\tau$, but also the precise cycle structure of $\sigma \tau^{-1}$. For this reason, we also regard the more symmetric scramble mutation operator. We observe that it not only leads to simpler proofs, but also 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#35009;&#21098;&#39044;&#27979;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#23545;&#23548;&#33322;&#20219;&#21153;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#23398;&#20064;&#26377;&#25928;&#22320;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#65292;&#20943;&#23569;&#23545;&#20132;&#20114;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2207.00052</link><description>&lt;p&gt;
&#35270;&#35273;&#39044;&#35757;&#32451;&#29992;&#20110;&#23548;&#33322;&#65306;&#20174;&#22122;&#22768;&#20013;&#25105;&#20204;&#33021;&#23398;&#21040;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#35009;&#21098;&#39044;&#27979;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#23545;&#23548;&#33322;&#20219;&#21153;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#23398;&#20064;&#26377;&#25928;&#22320;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#65292;&#20943;&#23569;&#23545;&#20132;&#20114;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#23548;&#33322;&#20013;&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#26159;&#20174;&#35266;&#23519;&#20013;&#30452;&#25509;&#39044;&#27979;&#34892;&#20026;&#12290;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20135;&#29983;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#20351;&#24471;&#35813;&#31995;&#32479;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#39044;&#27979;&#19982;&#30446;&#26631;&#23545;&#24212;&#30340;&#24403;&#21069;&#35270;&#22270;&#35009;&#21098;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#23548;&#33322;&#31574;&#30053;&#25152;&#38656;&#30340;&#24403;&#21069;&#35270;&#22270;&#21644;&#30446;&#26631;&#35270;&#22270;&#30340;&#20805;&#20998;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#22122;&#22768;&#22270;&#20687;&#36827;&#34892;&#38543;&#26426;&#35009;&#21098;&#39044;&#27979;&#30340;&#35757;&#32451;&#21487;&#20197;&#24456;&#22909;&#22320;&#36801;&#31227;&#21040;&#33258;&#28982;&#23478;&#24237;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#26377;&#25928;&#22320;&#33258;&#20030;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#65292;&#20943;&#23569;&#20132;&#20114;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.12672</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#29983;&#25104;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#35774;&#32622;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#19982;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#20043;&#38388;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#22312;&#36825;&#20010;&#38543;&#26426;&#36712;&#36857;&#20013;&#24674;&#22797;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#35770;&#25991;&#23545;&#19981;&#21516;&#25104;&#26412;&#27169;&#22411;&#23545;&#36712;&#36857;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#20135;&#21697;&#22810;&#22270;&#26469;&#27604;&#36739;&#26367;&#20195;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#27599;&#20010;&#19981;&#30830;&#23450;&#27963;&#21160;&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#27491;&#30830;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#26159;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#30340;&#26377;&#21147;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose an algorithm for trace recovery from stochastically known logs, a setting that is becoming more common with the increasing number of sensors and predictive models that generate uncertain data. The suggested approach calculates the conformance between a process model and a stochastically known trace and recovers the best alignment within this stochastic trace as the true trace. The paper offers an analysis of the impact of various cost models on trace recovery accuracy and makes use of a product multi-graph to compare alternative trace recovery options. The average accuracy of our approach, evaluated using two publicly available datasets, is impressive, with an average recovery accuracy score of 90-97%, significantly improving a common heuristic that chooses the most likely value for each uncertain activity. We believe that the effectiveness of the proposed algorithm in recovering correct traces from stochastically known logs may be a powerful aid for developing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#65292;&#21253;&#25324;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.03622</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Recommendation System Enhanced with Community Detection. (arXiv:2201.03622v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#65292;&#21253;&#25324;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#32773;&#24050;&#32463;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#26469;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#20013;&#25512;&#33616;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#30340;&#26631;&#31614;&#65292;&#21487;&#20197;&#20102;&#35299;&#20182;&#20204;&#30340;&#20852;&#36259;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#33258;&#23450;&#20041;&#26631;&#31614;&#30340;&#20219;&#24847;&#24615;&#21644;&#32570;&#20047;&#38480;&#21046;&#65292;&#30830;&#23450;&#20854;&#30830;&#20999;&#21547;&#20041;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20998;&#37197;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#21478;&#22806;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#20852;&#36259;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#26412;&#25991;&#36824;&#22312;&#20849;&#29616;&#26631;&#31614;&#20013;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#20197;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#21019;&#24314;&#22270;&#24418;&#27169;&#22411;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many researchers have used tag information to improve the performance of recommendation techniques in recommender systems. Examining the tags of users will help to get their interests and leads to more accuracy in the recommendations. Since user-defined tags are chosen freely and without any restrictions, problems arise in determining their exact meaning and the similarity of tags. However, using thesaurus and ontologies to find the meaning of tags is not very efficient due to their free definition by users and the use of different languages in many data sets. Therefore, this article uses mathematical and statistical methods to determine lexical similarity and co-occurrence tags solution to assign semantic similarity. On the other hand, due to the change of users' interests over time this article has considered the time of tag assignments in co-occurrence tags for determining similarity of tags. Then the graph is created based on similarity of tags. For modeling the interests of the us
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2107.11277</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24635;&#26159;&#20570;&#20986;&#39044;&#27979;&#65292;&#21363;&#20351;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#22312;&#35768;&#22810;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#65292;&#24212;&#36991;&#20813;&#36825;&#31181;&#34892;&#20026;&#65292;&#22240;&#20026;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#12290;&#23613;&#31649;&#22312;1970&#24180;&#24050;&#32463;&#30740;&#31350;&#36807;&#65292;&#20294;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#26426;&#22120;&#23398;&#20064;&#23376;&#39046;&#22495;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#36991;&#20813;&#20570;&#20986;&#39044;&#27979;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#20013;&#25298;&#32477;&#36873;&#39033;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23548;&#33268;&#20004;&#31181;&#25298;&#32477;&#24773;&#20917;&#65288;&#27169;&#31946;&#21644;&#26032;&#22855;&#25298;&#32477;&#65289;&#30340;&#26465;&#20214;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#21644;&#25298;&#32477;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#29616;&#26377;&#30340;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#25551;&#36848;&#20102;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.  This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DanceFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32423;&#32852;&#30340;&#21160;&#21147;&#23398;&#22686;&#24378;&#30340;&#21464;&#25442;&#22120;&#24341;&#23548;&#32593;&#32476;&#26469;&#29983;&#25104;&#22522;&#20110;&#38899;&#20048;&#30340;3D&#33310;&#36424;&#12290;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#20851;&#38190;&#23039;&#21183;&#65292;&#28982;&#21518;&#39044;&#27979;&#21442;&#25968;&#21270;&#36816;&#21160;&#26354;&#32447;&#65292;&#20351;&#24471;&#33310;&#36424;&#19982;&#38899;&#20048;&#33410;&#22863;&#23545;&#40784;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#38899;&#20048;&#26465;&#20214;&#19979;&#30340;3D&#33310;&#36424;&#25968;&#25454;&#38598;PhantomDance&#12290;</title><link>http://arxiv.org/abs/2103.10206</link><description>&lt;p&gt;
DanceFormer: &#22522;&#20110;&#38899;&#20048;&#30340;&#21442;&#25968;&#21270;&#36816;&#21160;&#21464;&#25442;&#22120;&#29983;&#25104;&#30340;3D&#33310;&#36424;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer. (arXiv:2103.10206v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DanceFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32423;&#32852;&#30340;&#21160;&#21147;&#23398;&#22686;&#24378;&#30340;&#21464;&#25442;&#22120;&#24341;&#23548;&#32593;&#32476;&#26469;&#29983;&#25104;&#22522;&#20110;&#38899;&#20048;&#30340;3D&#33310;&#36424;&#12290;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#20851;&#38190;&#23039;&#21183;&#65292;&#28982;&#21518;&#39044;&#27979;&#21442;&#25968;&#21270;&#36816;&#21160;&#26354;&#32447;&#65292;&#20351;&#24471;&#33310;&#36424;&#19982;&#38899;&#20048;&#33410;&#22863;&#23545;&#40784;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#38899;&#20048;&#26465;&#20214;&#19979;&#30340;3D&#33310;&#36424;&#25968;&#25454;&#38598;PhantomDance&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38899;&#20048;&#20013;&#29983;&#25104;3D&#33310;&#36424;&#26159;&#19968;&#39033;&#26032;&#20852;&#30340;&#30740;&#31350;&#20219;&#21153;&#65292;&#23545;&#35270;&#35273;&#21644;&#22270;&#24418;&#39046;&#22495;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#24207;&#21015;&#29983;&#25104;&#65292;&#28982;&#32780;&#65292;&#35201;&#28210;&#26579;&#19968;&#27573;&#19982;&#38899;&#20048;&#23545;&#40784;&#30340;&#12289;&#20855;&#26377;&#39640;&#21160;&#21147;&#23398;&#22797;&#26434;&#24615;&#21644;&#36830;&#36143;&#36816;&#21160;&#30340;&#38271;&#26399;&#24207;&#21015;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#36807;&#31243;&#36827;&#34892;&#37325;&#26500;&#65292;&#21363;&#20808;&#29983;&#25104;&#20851;&#38190;&#23039;&#21183;&#65292;&#28982;&#21518;&#39044;&#27979;&#21442;&#25968;&#21270;&#36816;&#21160;&#26354;&#32447;&#65292;&#20854;&#20013;&#20851;&#38190;&#23039;&#21183;&#26356;&#23481;&#26131;&#19982;&#38899;&#20048;&#33410;&#25293;&#21516;&#27493;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#21487;&#20197;&#26377;&#25928;&#22320;&#22238;&#24402;&#20197;&#28210;&#26579;&#24179;&#28369;&#30340;&#33410;&#22863;&#23545;&#40784;&#36816;&#21160;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;DanceFormer&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#32423;&#32852;&#30340;&#22686;&#24378;&#21160;&#21147;&#23398;&#21464;&#25442;&#22120;&#24341;&#23548;&#32593;&#32476;&#65288;&#31216;&#20026;DanTrans&#65289;&#65292;&#20998;&#21035;&#35299;&#20915;&#27599;&#20010;&#38454;&#27573;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#38899;&#20048;&#30340;3D&#33310;&#36424;&#25968;&#25454;&#38598;PhantomDance&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21160;&#30011;&#24072;&#20934;&#30830;&#26631;&#35760;&#65292;&#32780;&#19981;&#26159;&#37325;&#24314;&#25110;&#21160;&#20316;&#25429;&#25417;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, ie, a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Deep Serial Number (DSN) &#30340;&#27700;&#21360;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#12290;&#35813;&#31639;&#27861;&#22312;DNN&#20013;&#23454;&#29616;&#24207;&#21015;&#21495;&#23884;&#20837;&#65292;&#21482;&#26377;&#36755;&#20837;&#26377;&#25928;&#24207;&#21015;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;DNN&#25165;&#33021;&#27491;&#30830;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2011.08960</link><description>&lt;p&gt;
&#28145;&#24230;&#24207;&#21015;&#21495;&#65306;&#29992;&#20110; DNN &#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#35745;&#31639;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Deep Serial Number: Computational Watermarking for DNN Intellectual Property Protection. (arXiv:2011.08960v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Deep Serial Number (DSN) &#30340;&#27700;&#21360;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#12290;&#35813;&#31639;&#27861;&#22312;DNN&#20013;&#23454;&#29616;&#24207;&#21015;&#21495;&#23884;&#20837;&#65292;&#21482;&#26377;&#36755;&#20837;&#26377;&#25928;&#24207;&#21015;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;DNN&#25165;&#33021;&#27491;&#30830;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35774;&#35745;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27700;&#21360;&#31639;&#27861; Deep Serial Number&#65288;DSN&#65289;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22312;DNN&#20013;&#24341;&#20837;&#26631;&#35782;&#20449;&#21495;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;DNN&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#38459;&#27490;&#20102;&#25915;&#20987;&#32773;&#20351;&#29992;&#31363;&#21462;&#30340;&#32593;&#32476;&#12290;&#20511;&#37492;&#24207;&#21015;&#21495;&#22312;&#20445;&#25252;&#20256;&#32479;&#36719;&#20214;&#30693;&#35782;&#20135;&#26435;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;DNN&#20013;&#23454;&#29616;&#24207;&#21015;&#21495;&#23884;&#20837;&#30340;&#31532;&#19968;&#20010;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;DSN&#34987;&#38598;&#25104;&#21040;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#20013;&#65292;&#20854;&#20013;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#31169;&#26377;&#30340;&#25945;&#24072;DNN&#12290;&#38543;&#21518;&#65292;&#20854;&#30693;&#35782;&#34987;&#25552;&#28860;&#24182;&#20256;&#25480;&#32473;&#19968;&#31995;&#21015;&#23450;&#21046;&#30340;&#23398;&#29983;DNN&#12290;&#27599;&#20010;&#23458;&#25143;DNN&#20165;&#22312;&#36755;&#20837;&#26377;&#25928;&#24207;&#21015;&#21495;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#27491;&#30830;&#24037;&#20316;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DSN&#22312;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#20351;&#29992;&#30340;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#21407;&#22987;DNN&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present DSN (Deep Serial Number), a simple yet effective watermarking algorithm designed specifically for deep neural networks (DNNs). Unlike traditional methods that incorporate identification signals into DNNs, our approach explores a novel Intellectual Property (IP) protection mechanism for DNNs, effectively thwarting adversaries from using stolen networks. Inspired by the success of serial numbers in safeguarding conventional software IP, we propose the first implementation of serial number embedding within DNNs. To achieve this, DSN is integrated into a knowledge distillation framework, in which a private teacher DNN is initially trained. Subsequently, its knowledge is distilled and imparted to a series of customized student DNNs. Each customer DNN functions correctly only upon input of a valid serial number. Experimental results across various applications demonstrate DSN's efficacy in preventing unauthorized usage without compromising the original DNN performan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31867;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#23545;&#23725;&#20272;&#35745;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33021;&#22815;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#30340;&#36716;&#25442;&#21487;&#20197;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#24352;&#37327;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#65307;&#32780;&#28151;&#21512;&#25968;&#25454;&#30340;&#36716;&#25442;&#21017;&#36890;&#36807;&#36215;&#21040;&#27491;&#21017;&#21270;&#20316;&#29992;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#27169;&#22411;&#23545;&#36716;&#25442;&#21518;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25628;&#32034;&#36716;&#25442;&#31354;&#38388;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2005.00695</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#20013;&#32447;&#24615;&#36716;&#25442;&#30340;&#27867;&#21270;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31867;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#23545;&#23725;&#20272;&#35745;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33021;&#22815;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#30340;&#36716;&#25442;&#21487;&#20197;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#24352;&#37327;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#65307;&#32780;&#28151;&#21512;&#25968;&#25454;&#30340;&#36716;&#25442;&#21017;&#36890;&#36807;&#36215;&#21040;&#27491;&#21017;&#21270;&#20316;&#29992;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#27169;&#22411;&#23545;&#36716;&#25442;&#21518;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25628;&#32034;&#36716;&#25442;&#31354;&#38388;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#31561;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21508;&#31181;&#22686;&#24378;&#26041;&#27861;&#20026;&#20309;&#26377;&#25928;&#20197;&#21450;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#20005;&#26684;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#23545;&#23725;&#20272;&#35745;&#37327;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#24352;&#37327;&#26469;&#23637;&#31034;&#20102;&#33021;&#22815;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#30340;&#36716;&#25442;&#20250;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#30340;&#36716;&#25442;&#23637;&#31034;&#20102;&#23545;&#20272;&#35745;&#37327;&#36215;&#21040;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;MNIST&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#27169;&#22411;&#23545;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25628;&#32034;&#36716;&#25442;&#31354;&#38388;&#30340;&#22686;&#24378;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;Wide-ResNet&#23545;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;1.24%&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item></channel></rss>