<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#26694;&#26550;&#22312;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2311.00462</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#36827;&#34892;&#31895;&#32454;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design. (arXiv:2311.00462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#26694;&#26550;&#22312;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#35774;&#35745;&#26088;&#22312;&#21019;&#24314;&#30001;&#35768;&#22810;&#32454;&#32990;&#32452;&#25104;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#20415;&#33021;&#22815;&#39640;&#25928;&#22320;&#25511;&#21046;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29983;&#25104;&#21508;&#31181;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#30452;&#25509;&#22312;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#38590;&#20197;&#25511;&#21046;&#30340;&#22797;&#26434;&#24418;&#24577;&#30340;&#26426;&#22120;&#20154;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#12290;&#35813;&#31574;&#30053;&#39318;&#20808;&#23547;&#27714;&#26368;&#20339;&#30340;&#31895;&#31890;&#24230;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#36880;&#27493;&#23545;&#20854;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;&#31895;&#32454;&#36716;&#25442;&#36807;&#31243;&#20013;&#30830;&#23450;&#31934;&#32454;&#35843;&#25972;&#20851;&#33410;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#21452;&#26354;&#23884;&#20837; (HERD) &#26694;&#26550;&#12290;HERD &#22312;&#19968;&#20010;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#21033;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#39640;&#25928;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.00213</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23454;&#29616;&#19968;&#33268;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#39640;&#25928;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#35270;&#39057;&#27599;&#20010;&#27169;&#22411;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#24494;&#35843;&#38656;&#27714;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20026;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#21512;&#25104;&#37197;&#23545;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#21463;&#21040;Instruct Pix2Pix&#30340;&#22270;&#20687;&#36890;&#36807;&#32534;&#36753;&#25351;&#20196;&#36827;&#34892;&#36716;&#25442;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#33539;&#24335;&#24212;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#12290;&#25105;&#20204;&#23545;Prompt-to-Prompt&#36827;&#34892;&#20102;&#25299;&#23637;&#65292;&#39640;&#25928;&#29983;&#25104;&#37197;&#23545;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#36755;&#20837;&#35270;&#39057;&#21644;&#20854;&#32534;&#36753;&#21518;&#30340;&#23545;&#24212;&#35270;&#39057;&#12290;&#21516;&#26102;&#65292;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#65292;&#30830;&#20445;&#25209;&#27425;&#20043;&#38388;&#30340;&#38271;&#35270;&#39057;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;Tune-A-Video&#31561;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's image transfer via editing instruction, we adapt this paradigm to the video domain. Extending the Prompt-to-Prompt to videos, we efficiently generate paired samples, each with an input video and its edited counterpart. Alongside this, we introduce the Long Video Sampling Correction during sampling, ensuring consistent long videos across batches. Our method surpasses current methods like Tune-A-Video, heralding substantial progress in text-based video-to-video editing and suggesting exciting avenues for further exploration and deployment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;WSINDy&#36827;&#34892;&#31895;&#31890;&#21270;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;WSINDy&#22312;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#20013;&#30340;&#31895;&#31890;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#35782;&#21035;&#36817;&#20284;&#23545;&#31216;&#24615;&#21644;&#22788;&#29702;&#22806;&#37096;&#25200;&#21160;&#65292;WSINDy&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30456;&#20851;&#33258;&#30001;&#24230;&#30340;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.05879</link><description>&lt;p&gt;
&#20351;&#29992;WSINDy&#36827;&#34892;&#31895;&#31890;&#21270;&#21704;&#23494;&#39039;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Coarse-Graining Hamiltonian Systems Using WSINDy. (arXiv:2310.05879v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;WSINDy&#36827;&#34892;&#31895;&#31890;&#21270;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;WSINDy&#22312;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#20013;&#30340;&#31895;&#31890;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#35782;&#21035;&#36817;&#20284;&#23545;&#31216;&#24615;&#21644;&#22788;&#29702;&#22806;&#37096;&#25200;&#21160;&#65292;WSINDy&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30456;&#20851;&#33258;&#30001;&#24230;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#24369;&#24418;&#24577;&#31232;&#30095;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;(WSINDy)&#20855;&#26377;&#31895;&#31890;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#33021;&#21147;&#25193;&#23637;&#21040;&#20855;&#26377;&#36817;&#20284;&#23545;&#31216;&#24615;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#31895;&#31890;&#21270;&#38382;&#39064;&#19978;&#12290;&#36825;&#31181;&#36817;&#20284;&#23545;&#31216;&#24615;&#36890;&#24120;&#23548;&#33268;&#23384;&#22312;&#19968;&#20010;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#30456;&#20851;&#33258;&#30001;&#24230;&#30340;&#21160;&#21147;&#23398;&#12290;&#23548;&#20986;&#36825;&#26679;&#30340;&#38477;&#32500;&#31995;&#32479;&#65292;&#25110;&#32773;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#36817;&#20284;&#65292;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;WSINDy&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#23545;&#31216;&#19981;&#31934;&#30830;&#24615;&#21644;&#22806;&#37096;&#22122;&#22768;&#30340;&#24433;&#21709;&#19979;&#35782;&#21035;&#20986;&#36825;&#20010;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#12290;&#36825;&#22312;&#19968;&#37096;&#20998;&#26159;&#22240;&#20026;&#36825;&#26679;&#30340;&#31995;&#32479;&#22914;&#20309;&#34987;&#35299;&#26512;&#22320;&#23548;&#20986;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;WSINDy&#33258;&#28982;&#22320;&#20445;&#30041;&#20102;&#21704;&#23494;&#39039;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy) has been demonstrated to offer coarse-graining capabilities in the context of interacting particle systems ( https://doi.org/10.1016/j.physd.2022.133406 ). In this work we extend this capability to the problem of coarse-graining Hamiltonian dynamics which possess approximate symmetries. Such approximate symmetries often lead to the existence of a Hamiltonian system of reduced dimension that may be used to efficiently capture the dynamics of the relevant degrees of freedom. Deriving such reduced systems, or approximating them numerically, is an ongoing challenge. We demonstrate that WSINDy can successfully identify this reduced Hamiltonian system in the presence of large perturbations imparted from both the inexact nature of the symmetry and extrinsic noise. This is significant in part due to the nontrivial means by which such systems are derived analytically. WSINDy naturally preserves the Hamiltonian structur
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05869</link><description>&lt;p&gt;
&#36229;&#32423;&#20851;&#27880;&#21147;&#65306;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05869
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperAttention&#30340;&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#26085;&#30410;&#22797;&#26434;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26465;&#30446;&#34987;&#38480;&#21046;&#25110;&#30697;&#38453;&#20855;&#26377;&#20302;&#31283;&#23450;&#31209;&#65292;&#21542;&#21017;&#20108;&#27425;&#26102;&#38388;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21442;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#65306;&#65288;1&#65289;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#30340;&#26368;&#22823;&#21015;&#33539;&#25968;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#26816;&#27979;&#21644;&#21024;&#38500;&#22823;&#26465;&#30446;&#21518;&#65292;&#38750;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#34892;&#33539;&#25968;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#21442;&#25968;&#26469;&#25429;&#25417;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#23613;&#31649;&#20808;&#21069;&#23384;&#22312;&#19979;&#30028;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#19968;&#20010;&#32447;&#24615;&#26102;&#38388;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21363;&#20351;&#30697;&#38453;&#20855;&#26377;&#26080;&#30028;&#30340;&#26465;&#30446;&#25110;&#36739;&#22823;&#30340;&#31283;&#23450;&#31209;&#65292;&#21482;&#35201;&#19978;&#36848;&#21442;&#25968;&#36739;&#23567;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36731;&#26494;&#23481;&#32435;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;FlashAttention&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03940</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#38590;&#35270;&#22270;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#23545;&#22270;&#20687;&#36755;&#20837;&#30340;&#19981;&#21516;&#8220;&#35270;&#22270;&#8221;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#32780;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#23545;&#27492;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#22686;&#24378;&#27969;&#31243;&#20013;&#30340;&#25805;&#20316;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#65292;&#22914;&#38543;&#26426;&#35009;&#21098;&#25110;&#39068;&#33394;&#25197;&#26354;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#35270;&#22270;&#29983;&#25104;&#21450;&#20854;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22312;&#30446;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#20294;&#24378;&#22823;&#30340;&#8220;&#38590;&#35270;&#22270;&#36873;&#25321;&#8221;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#35270;&#22270;&#29983;&#25104;&#25193;&#23637;&#21040;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31574;&#30053;&#21253;&#25324;&#20197;&#19979;&#36845;&#20195;&#27493;&#39588;&#65306;1&#65289;&#38543;&#26426;&#36873;&#25321;&#22810;&#20010;&#35270;&#22270;&#24182;&#21019;&#24314;&#20004;&#20010;&#35270;&#22270;&#30340;&#37197;&#23545;&#65292;2&#65289;&#36827;&#34892;&#21521;&#21069;&#20256;&#36882;...
&lt;/p&gt;
&lt;p&gt;
Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16770</link><description>&lt;p&gt;
Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#65306;Persona&#24341;&#23548;&#30340;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#35805;AI&#12290;&#28982;&#32780;&#65292;&#35201;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#32972;&#26223;&#25110;&#20010;&#24615;&#21270;&#35843;&#25972;&#30340;&#36741;&#21161;&#20449;&#24687;&#20197;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#20173;&#28982;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#20351;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#30340;&#30740;&#31350;&#20165;&#26377;&#38480;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;AI&#25216;&#26415;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#36741;&#21161;&#25968;&#25454;&#20449;&#21495;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#20132;&#20114;&#25968;&#25454;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#31038;&#20250;&#30830;&#23450;&#22240;&#32032;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#27969;&#31243;&#32534;&#30721;&#26041;&#26696;&#20013;&#30340;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#21442;&#32771;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16713</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#36741;&#21161;&#35821;&#20041;&#36890;&#20449;&#19982;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#21033;&#29992;&#26080;&#20154;&#26426;&#36827;&#34892;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#65292;&#20197;&#25552;&#39640;&#20559;&#36828;&#22320;&#21306;&#20803;&#23431;&#23449;&#29992;&#25143;&#30340;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;&#20026;&#20102;&#22312;&#24179;&#34913;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#20943;&#23569;&#19978;&#34892;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35821;&#20041;&#27169;&#22411;&#35268;&#27169;&#12289;&#20449;&#36947;&#20998;&#37197;&#12289;&#20256;&#36755;&#21151;&#29575;&#21644;&#26080;&#20154;&#26426;&#36712;&#36857;&#19978;&#20570;&#20986;&#20915;&#31574;&#12290;&#21464;&#37327;&#34987;&#21010;&#20998;&#20026;&#31163;&#25955;&#31867;&#22411;&#21644;&#36830;&#32493;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#20248;&#21270;&#20197;&#29983;&#25104;&#32452;&#21512;&#21160;&#20316;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#26377;&#25928;&#25552;&#39640;&#19978;&#34892;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#25928;&#29575;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to explore the use of uplink semantic communications with the assistance of UAV in order to improve data collection effiicency for metaverse users in remote areas. To reduce the time for uplink data collection while balancing the trade-off between reconstruction quality and computational energy cost, we propose a hybrid action reinforcement learning (RL) framework to make decisions on semantic model scale, channel allocation, transmission power, and UAV trajectory. The variables are classified into discrete type and continuous type, which are optimized by two different RL agents to generate the combined action. Simulation results indicate that the proposed hybrid action reinforcement learning framework can effectively improve the efficiency of uplink semantic data collection under different parameter settings and outperforms the benchmark scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;</title><link>http://arxiv.org/abs/2309.14564</link><description>&lt;p&gt;
&#29983;&#25104;&#33406;&#33293;&#23572;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#12289;&#20197;&#25991;&#26412;&#20026;&#23548;&#21521;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#12289;&#21487;&#37325;&#22797;&#30340;&#20108;&#32500;&#33402;&#26415;&#20316;&#21697;&#65292;&#22914;&#22320;&#26495;&#12289;&#39532;&#36187;&#20811;&#12289;&#38518;&#29943;&#21644;&#33406;&#33293;&#23572;&#30340;&#20316;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#26080;&#32541;&#32441;&#29702;&#27010;&#24565;&#19981;&#21516;&#65292;&#21363;&#24179;&#38138;&#26080;&#32541;&#30340;&#27491;&#26041;&#24418;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#26159;&#30001;&#37325;&#22797;&#30340;&#30456;&#21516;&#23545;&#35937;&#32452;&#25104;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#20108;&#32500;&#32593;&#26684;&#30340;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#38750;&#27491;&#26041;&#24418;&#29943;&#30742;&#65292;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#32972;&#26223;&#32454;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#36129;&#29486;&#23454;&#29616;&#20102;&#38262;&#23884;&#22270;&#26696;&#30340;&#20960;&#20309;&#20248;&#21270;&#65306;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32473;&#23450;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#21487;&#38138;&#30742;&#24418;&#29366;&#31354;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20462;&#25913;&#20108;&#32500;&#32593;&#26684;&#26144;&#23556;&#25216;&#26415;Orbifold Tutte Embedding&#20013;&#20351;&#29992;&#30340;Laplacian&#31639;&#23376;&#21487;&#20197;&#23454;&#29616;&#25152;&#36873;&#24179;&#38754;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#38138;&#30742;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the standard concept of a seamless texture, i.e., square images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and color of a 2D mesh, in order to generate a non-square tile in the shape and appearance of the desired object, with close to no additional background details. We enable geometric optimization of tilings by our key technical contribution: an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. Namely, we prove that modifying the laplacian used in a 2D mesh-mapping technique Orbifold Tutte Embedding - can achieve all possible tiling configurations for a chosen planar 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#26469;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#36712;&#36857;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20132;&#36890;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35774;&#35745;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#36712;&#36857;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.12677</link><description>&lt;p&gt;
TrTr&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#27969;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#30340;&#36712;&#36857;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population. (arXiv:2309.12677v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#26469;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#36712;&#36857;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20132;&#36890;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35774;&#35745;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#36712;&#36857;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36712;&#36857;&#22810;&#26679;&#24615;&#26159;&#35299;&#20915;&#23454;&#38469;&#20132;&#36890;&#20219;&#21153;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#35268;&#27169;&#21442;&#25968;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#36712;&#36857;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#20852;&#30340;Transformer&#25216;&#26415;&#20197;&#20854;&#24182;&#34892;&#35745;&#31639;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#21487;&#20197;&#21033;&#29992;&#20855;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;Transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#65292;&#26088;&#22312;&#23398;&#20064;&#36710;&#36742;&#32676;&#20307;&#20869;&#30340;&#36712;&#36857;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;&#20854;&#36866;&#24212;&#20132;&#36890;&#20219;&#21153;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#38543;&#21518;&#35774;&#35745;&#20102;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#26102;&#31354;&#38656;&#27714;&#23545;&#24212;&#30340;&#22122;&#22768;&#65292;&#36825;&#20123;&#22122;&#22768;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#34987;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding trajectory diversity is a fundamental aspect of addressing practical traffic tasks. However, capturing the diversity of trajectories presents challenges, particularly with traditional machine learning and recurrent neural networks due to the requirement of large-scale parameters. The emerging Transformer technology, renowned for its parallel computation capabilities enabling the utilization of models with hundreds of millions of parameters, offers a promising solution. In this study, we apply the Transformer architecture to traffic tasks, aiming to learn the diversity of trajectories within vehicle populations. We analyze the Transformer's attention mechanism and its adaptability to the goals of traffic tasks, and subsequently, design specific pre-training tasks. To achieve this, we create a data structure tailored to the attention mechanism and introduce a set of noises that correspond to spatio-temporal demands, which are incorporated into the structured data during the
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15090</link><description>&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#12290;&#23427;&#38416;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#20316;&#20026;&#19968;&#20010;&#20998;&#26512;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#26059;&#36716;&#26041;&#38754;&#32479;&#19968;&#37327;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23450;&#20041;&#30340;&#26041;&#27861;&#35770;&#21453;&#26144;&#20102;&#36827;&#31243;&#32593;&#32476;&#26681;&#25454;&#32479;&#35745;&#25351;&#26631;&#21306;&#20998;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#21270;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#29702;&#35299;&#25110;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#25581;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03438</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22635;&#20889;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#23384;&#22312;&#22833;&#36133;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#32534;&#31243;&#36741;&#21161;&#21644;&#20195;&#30721;&#26234;&#33021;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#38382;&#39064;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#36825;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#21463;&#23454;&#26102;&#20195;&#30721;&#24314;&#35758;&#30340;&#29616;&#23454;&#22330;&#26223;&#21551;&#21457;&#65292;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#21487;&#33021;&#30340;&#28431;&#27934;-&#21453;&#27169;&#24335;&#65292;&#36825;&#20123;&#21453;&#27169;&#24335;&#21487;&#20197;&#25104;&#20026;&#23436;&#25104;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20174;&#35821;&#20041;&#25913;&#21464;&#25805;&#20316;&#20013;&#27966;&#29983;&#30340;&#21512;&#25104;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-HumanEval&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20174;&#29992;&#25143;&#25552;&#20132;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#27966;&#29983;&#30340;&#29616;&#23454;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-FixEval&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#24773;&#20917;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#24615;&#33021;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;CodeGen-2B-mono&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#36807;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19718</link><description>&lt;p&gt;
&#31895;&#31961;&#38598;&#19979;&#19968;&#31181;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#23558;&#20004;&#32773;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#23398;&#20064;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#20010;&#25805;&#20316;&#24456;&#26114;&#36149;&#12290;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#32467;&#21512;&#22312;&#22788;&#29702;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#21322;&#30417;&#30563;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#21644;&#29983;&#25104;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#31895;&#31961;&#38598;&#29702;&#35770;&#26159;&#35299;&#20915;&#20449;&#24687;&#31995;&#32479;&#20013;&#30693;&#35782;&#22788;&#29702;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#31961;&#38598;&#19979;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65288;RS-ABL&#65289;&#12290;&#36890;&#36807;&#23558;&#35268;&#21017;&#30340;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#36716;&#21270;&#20026;&#20449;&#24687;&#34920;&#65292;&#21033;&#29992;&#31895;&#31961;&#38598;&#29702;&#35770;&#26469;&#35299;&#20915;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#21644;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#36127;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#35268;&#21017;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#36941;&#21382;&#24615;&#65288;forecast ergodicity&#65289;&#30340;&#27010;&#24565;&#65292;&#21363;&#20174;&#36807;&#21435;&#25968;&#25454;&#20013;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#30340;&#24230;&#37327;&#12290;&#25991;&#31456;&#20351;&#29992;&#31639;&#27861;&#22797;&#26434;&#24615;&#27169;&#25311;&#36825;&#20010;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.10752</link><description>&lt;p&gt;
&#39044;&#27979;&#36941;&#21382;&#24615;&#65306;&#20351;&#29992;&#31639;&#27861;&#20449;&#24687;&#35770;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Forecast Ergodicity: Prediction Modeling Using Algorithmic Information Theory. (arXiv:2304.10752v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#36941;&#21382;&#24615;&#65288;forecast ergodicity&#65289;&#30340;&#27010;&#24565;&#65292;&#21363;&#20174;&#36807;&#21435;&#25968;&#25454;&#20013;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#30340;&#24230;&#37327;&#12290;&#25991;&#31456;&#20351;&#29992;&#31639;&#27861;&#22797;&#26434;&#24615;&#27169;&#25311;&#36825;&#20010;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26234;&#33021;&#30340;&#33021;&#21147;&#21463;&#21040;&#36807;&#21435;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#30340;&#28508;&#21147;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#29992;&#20110;&#21457;&#29616;&#21487;&#29992;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#65292;&#20197;&#20415;&#39044;&#27979;&#26410;&#26469;&#12290;&#20294;&#36825;&#20123;&#32467;&#26500;&#24517;&#39035;&#39318;&#20808;&#23384;&#22312;&#20110;&#21487;&#29992;&#25968;&#25454;&#20013;&#65292;&#24182;&#19988;&#22312;&#26410;&#26469;&#20063;&#24517;&#39035;&#36866;&#29992;&#12290;&#39044;&#27979;&#36941;&#21382;&#24615;&#26159;&#20174;&#36807;&#21435;&#25968;&#25454;&#20013;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#29992;&#25968;&#25454;&#30340;&#31639;&#27861;&#22797;&#26434;&#24615;&#26469;&#27169;&#25311;&#36825;&#20010;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of machine intelligence are bounded by the potential of data from the past to forecast the future. Deep learning tools are used to find structures in the available data to make predictions about the future. Such structures have to be present in the available data in the first place and they have to be applicable in the future. Forecast ergodicity is a measure of the ability to forecast future events from data in the past. We model this bound by the algorithmic complexity of the available data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10703</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;ReCEval
&lt;/p&gt;
&lt;p&gt;
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#26159;&#22522;&#30784;&#65292;&#20294;&#20160;&#20040;&#26500;&#25104;&#22909;&#30340;&#25512;&#29702;&#38142;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#23578;&#19981;&#28165;&#26970;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25512;&#29702;&#38142;&#26159;&#21542;&#23548;&#33268;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20294;&#36825;&#31181;&#20197;&#31572;&#26696;&#20026;&#23548;&#21521;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23558;&#22909;&#30340;&#25512;&#29702;&#36136;&#37327;&#19982;&#20854;&#20182;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#30340;&#20551;&#25463;&#24452;&#28151;&#28102;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#38142;&#35270;&#20026;&#25512;&#23548;&#26368;&#32456;&#31572;&#26696;&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#65292;&#36890;&#36807;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24615;&#8212;&#8212;&#65288;1&#65289;&#27491;&#30830;&#24615;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#22522;&#20110;&#27493;&#39588;&#65292;&#21069;&#32622;&#27493;&#39588;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20449;&#24687;&#37327;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#25552;&#20379;&#26032;&#20449;&#24687;&#26377;&#21161;&#20110;&#25512;&#23548;&#29983;&#25104;&#30340;&#31572;&#26696;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;ReCEval&#65288;&#25512;&#29702;&#38142;&#35780;&#20272;&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#21644;&#20449;&#24687;&#29702;&#35770;&#27979;&#37327;&#23454;&#29616;&#20102;ReCEval&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#25512;&#29702;&#38142;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multi
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;</title><link>http://arxiv.org/abs/2304.01811</link><description>&lt;p&gt;
HarsanyiNet: &#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#20934;&#30830;&#30340; Shapley &#20540;
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01811
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley &#20540;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#20449;&#30340;&#23646;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20154;&#20204;&#20351;&#29992; Shapley &#20540;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36755;&#20837;&#21464;&#37327;&#30340;&#23646;&#24615;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#25165;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36817;&#20284;&#35745;&#31639;&#20986;&#27604;&#36739;&#31934;&#30830;&#30340; Shapley &#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500; HarsanyiNet&#65292;&#22312;&#36755;&#20837;&#26679;&#26412;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#21069;&#21521;&#20256;&#25773;&#21363;&#21487;&#12290;HarsanyiNet &#26159;&#26500;&#24314;&#22312; Shapley &#20540;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24314;&#20026;&#32593;&#32476;&#32534;&#30721;&#30340; Harsanyi &#20132;&#20114;&#37325;&#26032;&#20998;&#37197;&#30340;&#29702;&#35770;&#22522;&#30784;&#20043;&#19978;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is widely regarded as a trustworthy attribution metric. However, when people use Shapley values to explain the attribution of input variables of a deep neural network (DNN), it usually requires a very high computational cost to approximate relatively accurate Shapley values in real-world applications. Therefore, we propose a novel network architecture, the HarsanyiNet, which makes inferences on the input sample and simultaneously computes the exact Shapley values of the input variables in a single forward propagation. The HarsanyiNet is designed on the theoretical foundation that the Shapley value can be reformulated as the redistribution of Harsanyi interactions encoded by the network.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16767</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#19987;&#21033;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65306;&#35821;&#20041;&#36317;&#31163;&#21644;&#25216;&#26415;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
A Novel Patent Similarity Measurement Methodology: Semantic Distance and Technological Distance. (arXiv:2303.16767v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#30830;&#20445;&#21019;&#26032;&#30340;&#26032;&#39062;&#24615;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#19987;&#21033;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#19987;&#23478;&#25163;&#21160;&#20998;&#31867;&#19987;&#21033;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#33258;&#21160;&#21270;&#26041;&#27861;&#21482;&#20851;&#27880;&#19987;&#21033;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19987;&#21033;&#25991;&#26412;&#20351;&#29992;BERT&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;Jaccard&#30456;&#20284;&#24615;&#35745;&#31639;&#19987;&#21033;&#30340;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#26435;&#37325;&#26469;&#23454;&#29616;&#28151;&#21512;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity between patents is an essential step to ensure novelty of innovation. However, a large number of methods of measuring the similarity between patents still rely on manual classification of patents by experts. Another body of research has proposed automated methods; nevertheless, most of it solely focuses on the semantic similarity of patents. In order to tackle these limitations, we propose a hybrid method for automatically measuring the similarity between patents, considering both semantic and technological similarities. We measure the semantic similarity based on patent texts using BERT, calculate the technological similarity with IPC codes using Jaccard similarity, and perform hybridization by assigning weights to the two similarity methods. Our evaluation result demonstrates that the proposed method outperforms the baseline that considers the semantic similarity only.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06108</link><description>&lt;p&gt;
RaLiBEV: &#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#22312;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#20809;&#38647;&#36798;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#24863;&#30693;&#20449;&#24687;&#65292;&#20294;&#22312;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38647;&#36798;&#20449;&#21495;&#30001;&#20110;&#27874;&#38271;&#30340;&#29305;&#24615;&#22312;&#36935;&#21040;&#38632;&#28404;&#25110;&#38654;&#31890;&#26102;&#20250;&#21457;&#29983;&#34893;&#23556;&#65292;&#20294;&#23427;&#21463;&#21040;&#22823;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#26368;&#36817;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#34701;&#21512;&#21487;&#20197;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#23454;&#29616;&#24378;&#20581;&#30340;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20174;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#40784;&#21644;&#27719;&#32858;&#20004;&#20010;&#20998;&#25903;&#30340;&#29305;&#24449;&#20197;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#31614;&#20998;&#37197;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#31616;&#21333;&#35774;&#35745;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#36793;&#30028;&#26694;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#26469;&#33258;&#38647;&#36798;&#30340;&#36317;&#31163;-&#26041;&#20301;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
&lt;/p&gt;</description></item></channel></rss>