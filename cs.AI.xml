<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2309.11495</link><description>&lt;p&gt;
&#38142;&#24335;&#39564;&#35777;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#29983;&#25104;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20107;&#23454;&#20449;&#24687;&#65288;&#21363;&#24187;&#35273;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#20986;&#22238;&#22797;&#26102;&#36827;&#34892;&#24605;&#32771;&#20197;&#32416;&#27491;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#65288;CoVe&#65289;&#26041;&#27861;&#65292;&#27169;&#22411;&#39318;&#20808;&#65288;i&#65289;&#36215;&#33609;&#21021;&#22987;&#22238;&#22797;&#65307;&#28982;&#21518;&#65288;ii&#65289;&#35745;&#21010;&#39564;&#35777;&#38382;&#39064;&#26469;&#20107;&#23454;&#26816;&#26597;&#33609;&#31295;&#65307;&#65288;iii&#65289;&#29420;&#31435;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#31572;&#26696;&#21463;&#20854;&#20182;&#22238;&#22797;&#30340;&#24433;&#21709;&#65307;&#26368;&#21518;&#65288;iv&#65289;&#29983;&#25104;&#26368;&#32456;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#22238;&#31572;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoVe&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;&#24187;&#35273;&#30340;&#24773;&#20917;&#65292;&#21253;&#25324;&#26469;&#33258;&#32500;&#22522;&#25968;&#25454;&#30340;&#21015;&#34920;&#38382;&#39064;&#12289;&#23553;&#38381;&#20070;&#31821;MultiSpanQA&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#25925;&#20107;&#21644;Large Language Models (LLMs)&#65292;&#23558;&#34394;&#26500;&#35282;&#33394;&#36716;&#21270;&#20026;"&#27963;&#20307;"&#31038;&#20132;&#23454;&#20307;&#65292;&#24320;&#21457;&#20102;&#24341;&#20154;&#20837;&#32988;&#19988;&#21487;&#20449;&#30340;&#31038;&#21306;&#25925;&#20107;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#22312;&#31038;&#20132;&#20114;&#21160;&#20013;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#22238;&#24518;&#25361;&#25112;&#24182;&#23547;&#27714;&#24314;&#35758;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2309.11478</link><description>&lt;p&gt;
&#34394;&#26500;&#19990;&#30028;&#65292;&#29616;&#23454;&#36830;&#25509;&#65306;&#36890;&#36807;LLMs&#24320;&#21457;&#31038;&#21306;&#25925;&#20107;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs. (arXiv:2309.11478v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#25925;&#20107;&#21644;Large Language Models (LLMs)&#65292;&#23558;&#34394;&#26500;&#35282;&#33394;&#36716;&#21270;&#20026;"&#27963;&#20307;"&#31038;&#20132;&#23454;&#20307;&#65292;&#24320;&#21457;&#20102;&#24341;&#20154;&#20837;&#32988;&#19988;&#21487;&#20449;&#30340;&#31038;&#21306;&#25925;&#20107;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#22312;&#31038;&#20132;&#20114;&#21160;&#20013;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#22238;&#24518;&#25361;&#25112;&#24182;&#23547;&#27714;&#24314;&#35758;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25925;&#20107;&#21644;Large Language Models (LLMs)&#30340;&#25972;&#21512;&#65292;&#20197;&#22312;&#31038;&#21306;&#29615;&#22659;&#20013;&#24320;&#21457;&#24341;&#20154;&#20837;&#32988;&#19988;&#21487;&#20449;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;SCs&#65289;&#12290;&#21463;&#21040;&#34394;&#26500;&#35282;&#33394;&#22686;&#24378;&#31038;&#20132;&#20114;&#21160;&#28508;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25925;&#20107;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;SSCs&#65289;&#21644;&#25925;&#20107;&#24037;&#31243;&#30340;&#27010;&#24565;&#65292;&#23558;&#34394;&#26500;&#28216;&#25103;&#35282;&#33394;&#36716;&#21270;&#20026;&#29609;&#23478;&#31038;&#21306;&#20013;&#30340;&#8220;&#27963;&#20307;&#8221;&#31038;&#20132;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#25925;&#20107;&#24037;&#31243;&#36807;&#31243;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#35282;&#33394;&#21644;&#25925;&#20107;&#21019;&#36896;&#65292;&#23450;&#20041;SC&#30340;&#20010;&#24615;&#21644;&#19990;&#30028;&#35266;&#65292;&#65288;2&#65289;&#21521;&#31038;&#21306;&#23637;&#31034;&#27963;&#20307;&#25925;&#20107;&#65292;&#35753;SC&#25551;&#36848;&#25361;&#25112;&#24182;&#23547;&#27714;&#24314;&#35758;&#65292;&#65288;3&#65289;&#19982;&#31038;&#21306;&#25104;&#21592;&#36827;&#34892;&#20132;&#27969;&#65292;&#23454;&#29616;SC&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;LLM GPT-3&#39537;&#21160;&#25105;&#20204;&#30340;SSC&#21407;&#22411;&#8220;David&#8221;&#21644;&#8220;Catherine&#8221;&#65292;&#24182;&#22312;Discord&#30340;&#22312;&#32447;&#28216;&#25103;&#31038;&#21306;&#8220;DE&#65288;Alias&#65289;&#8221;&#19978;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#20110;&#38382;&#21367;&#65288;N=15&#65289;&#21644;&#35775;&#35848;&#65288;N=8&#65289;&#30340;&#28151;&#21512;&#26041;&#27861;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into "live" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, "David" and "Catherine," and evaluated their performance in an online gaming community, "DE (Alias)," on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;ML-TSK FS&#65292;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#24314;&#27169;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32467;&#21512;&#27169;&#31946;&#25512;&#29702;&#21644;&#22810;&#26631;&#31614;&#22238;&#24402;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ML-TSK FS&#22312;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#29305;&#24449;-&#26631;&#31614;&#20851;&#31995;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11469</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;Takagi-Sugeno-Kang&#27169;&#31946;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Takagi-Sugeno-Kang Fuzzy System. (arXiv:2309.11469v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;ML-TSK FS&#65292;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#24314;&#27169;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32467;&#21512;&#27169;&#31946;&#25512;&#29702;&#21644;&#22810;&#26631;&#31614;&#22238;&#24402;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ML-TSK FS&#22312;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#29305;&#24449;-&#26631;&#31614;&#20851;&#31995;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#20998;&#31867;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#38598;&#20013;&#35782;&#21035;&#23454;&#20363;&#30340;&#30456;&#20851;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#23545;&#20998;&#31867;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#26631;&#31614;Takagi-Sugeno-Kang&#27169;&#31946;&#31995;&#32479;&#65288;ML-TSK FS&#65289;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;ML-TSK FS&#30340;&#32467;&#26500;&#20351;&#29992;&#27169;&#31946;&#35268;&#21017;&#26469;&#24314;&#27169;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27169;&#31946;&#31995;&#32479;&#36890;&#36807;&#23558;&#22522;&#20110;&#27169;&#31946;&#25512;&#29702;&#30340;&#22810;&#26631;&#31614;&#30456;&#20851;&#23398;&#20064;&#19982;&#22810;&#26631;&#31614;&#22238;&#24402;&#25439;&#22833;&#30456;&#32467;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;12&#20010;&#22522;&#20934;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;&#25552;&#20986;&#30340;ML-TSK FS&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#65292;ML-TSK FS&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#20351;&#29992;&#27169;&#31946;&#25512;&#29702;&#35268;&#21017;&#26377;&#25928;&#22320;&#24314;&#27169;&#29305;&#24449;-&#26631;&#31614;&#20851;&#31995;&#24182;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label classification can effectively identify the relevant labels of an instance from a given set of labels. However,the modeling of the relationship between the features and the labels is critical to the classification performance. To this end, we propose a new multi-label classification method, called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve the classification performance. The structure of ML-TSK FS is designed using fuzzy rules to model the relationship between features and labels. The fuzzy system is trained by integrating fuzzy inference based multi-label correlation learning with multi-label regression loss. The proposed ML-TSK FS is evaluated experimentally on 12 benchmark multi-label datasets. 1 The results show that the performance of ML-TSK FS is competitive with existing methods in terms of various evaluation metrics, indicating that it is able to model the feature-label relationship effectively using fuzzy inference rules and enhances the cl
&lt;/p&gt;</description></item><item><title>AudioFool&#26159;&#19968;&#31181;&#39640;&#36895;&#12289;&#36890;&#29992;&#21644;&#26080;&#38656;&#21516;&#27493;&#30340;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#25915;&#20987;&#65292;&#33021;&#22815;&#36890;&#36807;&#21453;&#21521;&#20613;&#37324;&#21494;&#21464;&#25442;&#26500;&#36896;&#23545;&#21516;&#27493;&#30340;&#19981;&#21464;&#24615;&#21644;&#23545;&#28388;&#27874;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#23545;ASR&#31995;&#32479;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.11462</link><description>&lt;p&gt;
AudioFool: &#39640;&#36895;&#12289;&#36890;&#29992;&#21644;&#26080;&#38656;&#21516;&#27493;&#30340;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition. (arXiv:2309.11462v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11462
&lt;/p&gt;
&lt;p&gt;
AudioFool&#26159;&#19968;&#31181;&#39640;&#36895;&#12289;&#36890;&#29992;&#21644;&#26080;&#38656;&#21516;&#27493;&#30340;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#25915;&#20987;&#65292;&#33021;&#22815;&#36890;&#36807;&#21453;&#21521;&#20613;&#37324;&#21494;&#21464;&#25442;&#26500;&#36896;&#23545;&#21516;&#27493;&#30340;&#19981;&#21464;&#24615;&#21644;&#23545;&#28388;&#27874;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#23545;ASR&#31995;&#32479;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24050;&#32463;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#35774;&#22791;&#19978;&#21629;&#20196;&#30340;&#24694;&#24847;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#25506;&#35752;&#22914;&#20309;&#21019;&#24314;&#36825;&#31181;&#25915;&#20987;&#65292;&#28982;&#32780;&#19968;&#20123;&#19982;&#31354;&#20013;&#25509;&#21475; (OTA) &#25915;&#20987;&#30456;&#20851;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22949;&#21892;&#35299;&#20915;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;OTA&#27169;&#22411;&#20860;&#23481;&#30340;&#24378;&#25915;&#20987;&#25152;&#38656;&#30340;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#25152;&#38656;&#23646;&#24615;&#30340;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#23545;&#21516;&#27493;&#30340;&#19981;&#21464;&#24615;&#21644;&#23545;&#28388;&#27874;&#30340;&#40065;&#26834;&#24615;&#65306;&#36825;&#20801;&#35768;&#23545;ASR&#31995;&#32479;&#36827;&#34892;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#25913;&#36827;&#30340;&#39057;&#22495;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#23454;&#29616;&#20102;&#36825;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#20851;&#38190;&#35789;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;OTA&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#20998;&#26512;&#20102;&#36328;&#39046;&#22495;&#25915;&#20987;&#30340;&#23646;&#24615;&#65292;&#20197;&#35299;&#37322;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#31181;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#27714;&#35299;SAT&#38382;&#39064;&#30340;&#8220;&#31070;&#35861;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35775;&#38382;&#22522;&#20110;GNN&#30340;&#31070;&#35861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11452</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20855;&#26377;&#24615;&#33021;&#36793;&#30028;&#30340;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Using deep learning to construct stochastic local search SAT solvers with performance bounds. (arXiv:2309.11452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#31181;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#27714;&#35299;SAT&#38382;&#39064;&#30340;&#8220;&#31070;&#35861;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35775;&#38382;&#22522;&#20110;GNN&#30340;&#31070;&#35861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#26159;&#26368;&#20856;&#22411;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#20855;&#26377;&#26497;&#22823;&#30340;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#27714;&#35299;&#22120;&#31867;&#21035;&#26159;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#65288;SLS&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21644;&#38543;&#26426;&#26356;&#26032;&#20505;&#36873;&#35299;&#26469;&#27714;&#35299;&#12290;&#26368;&#36817;&#65292;&#22312;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24314;&#31435;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;SLS&#27714;&#35299;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#27714;&#35299;SAT&#23454;&#20363;&#65292;&#21482;&#35201;&#23427;&#20204;&#21487;&#20197;&#35775;&#38382;&#21512;&#36866;&#30340;&#8220;&#31070;&#35861;&#8221;&#65292;&#20174;&#23454;&#20363;&#29305;&#23450;&#30340;&#20998;&#24067;&#20013;&#25552;&#20379;&#26679;&#26412;&#65292;&#21033;&#29992;&#23454;&#20363;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#21463;&#36825;&#20123;&#32467;&#26524;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#32467;&#26500;&#30340;&#33391;&#22909;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#31070;&#35861;&#65292;&#24182;&#22312;&#20004;&#20010;SLS&#27714;&#35299;&#22120;&#19978;&#23545;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#30340;&#38543;&#26426;SAT&#23454;&#20363;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35775;&#38382;&#22522;&#20110;GNN&#30340;&#31070;&#35861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20004;&#20010;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#20351;&#23427;&#20204;&#24179;&#22343;&#33021;&#22815;&#35299;&#20915;17&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete problem and of great practical relevance. One important class of solvers for this problem are stochastic local search (SLS) algorithms that iteratively and randomly update a candidate assignment. Recent breakthrough results in theoretical computer science have established sufficient conditions under which SLS solvers are guaranteed to efficiently solve a SAT instance, provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. Motivated by these results and the well established ability of neural networks to learn common structure in large datasets, in this work, we train oracles using Graph Neural Networks and evaluate them on two SLS solvers on random SAT instances of varying difficulty. We find that access to GNN-based oracles significantly boosts the performance of both solvers, allowing them, on average, to solve 17
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21457;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#24182;&#25552;&#21319;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.11433</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Few-Shot Learning in Medical Imaging. (arXiv:2309.11433v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21457;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#24182;&#25552;&#21319;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#26631;&#27880;&#30340;&#21307;&#23398;&#24433;&#20687;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;&#26412;&#31995;&#32479;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25628;&#32034;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20174;2018&#24180;&#21040;2023&#24180;&#36873;&#25321;&#20102;80&#31687;&#30456;&#20851;&#25991;&#31456;&#12290;&#25105;&#20204;&#22522;&#20110;&#21307;&#23398;&#32467;&#26524;&#65288;&#22914;&#32959;&#30244;&#20998;&#21106;&#12289;&#30142;&#30149;&#20998;&#31867;&#21644;&#22270;&#20687;&#37197;&#20934;&#65289;&#12289;&#30740;&#31350;&#30340;&#35299;&#21078;&#32467;&#26500;&#65288;&#21363;&#24515;&#33039;&#12289;&#32954;&#31561;&#65289;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23545;&#36825;&#20123;&#25991;&#31456;&#36827;&#34892;&#20102;&#32858;&#31867;&#12290;&#23545;&#20110;&#27599;&#20010;&#32858;&#31867;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35770;&#25991;&#30340;&#20998;&#24067;&#20197;&#21450;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#20379;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#25152;&#26377;&#30740;&#31350;&#20013;&#30340;&#20849;&#20139;&#36890;&#29992;&#27969;&#31243;&#12290;&#32508;&#36848;&#34920;&#26126;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#32467;&#26524;&#20013;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#19988;&#20803;&#23398;&#20064;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11427</link><description>&lt;p&gt;
&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRACE-GPT&#65288;Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers&#65289;&#65292;&#23427;&#26159;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#26816;&#27979;&#25925;&#38556;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#26469;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
&lt;/p&gt;</description></item><item><title>EDMP&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36816;&#21160;&#35268;&#21010;&#20248;&#21183;&#30340;&#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#23398;&#26377;&#25928;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.11414</link><description>&lt;p&gt;
EDMP: &#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning. (arXiv:2309.11414v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11414
&lt;/p&gt;
&lt;p&gt;
EDMP&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36816;&#21160;&#35268;&#21010;&#20248;&#21183;&#30340;&#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#23398;&#26377;&#25928;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#36816;&#21160;&#35268;&#21010;&#21253;&#25324;&#19968;&#32452;&#36890;&#29992;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#25191;&#34892;&#32473;&#23450;&#35745;&#21010;&#30340;&#29305;&#23450;&#20110;&#22330;&#26223;&#30340;&#25104;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#20110;&#20219;&#20309;&#26032;&#22330;&#26223;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23545;&#22810;&#26679;&#26377;&#25928;&#36712;&#36857;&#26377;&#20808;&#39564;&#20102;&#35299;&#65292;&#24182;&#19988;&#27809;&#26377;&#38024;&#23545;&#32473;&#23450;&#22330;&#26223;&#35774;&#35745;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#24448;&#24448;&#36739;&#20302;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#20294;&#27809;&#26377;&#19987;&#38376;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#24456;&#38590;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EDMP&#65292;&#19968;&#31181;&#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;&#38598;&#25104;&#65292;&#26088;&#22312;&#32467;&#21512;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#25193;&#25955;&#32593;&#32476;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#23398;&#26377;&#25928;&#36712;&#36857;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;&#20256;&#32479;&#35268;&#21010;&#19968;&#26679;&#65292;&#22312;&#25512;&#26029;&#26102;&#23545;&#20110;&#20219;&#20309;&#26032;&#22330;&#26223;&#65292;&#25105;&#20204;&#35745;&#31639;&#29305;&#23450;&#20110;&#22330;&#26223;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#23545;&#40784;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#32763;&#35793;&#19982;&#20998;&#21106;&#25918;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65292;&#35813;&#26041;&#27861;&#26159;&#39318;&#27425;&#23454;&#29616;&#20102;&#36825;&#31181;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.11384</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#23545;&#40784;&#20998;&#21106;&#23454;&#29616;&#38271;&#25991;&#26412;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Long-Form End-to-End Speech Translation via Latent Alignment Segmentation. (arXiv:2309.11384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#23545;&#40784;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#32763;&#35793;&#19982;&#20998;&#21106;&#25918;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65292;&#35813;&#26041;&#27861;&#26159;&#39318;&#27425;&#23454;&#29616;&#20102;&#36825;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;&#20960;&#31186;&#38047;&#30340;&#38899;&#39057;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#26681;&#25454;&#20154;&#24037;&#26631;&#27880;&#30340;&#36716;&#24405;&#21644;&#32763;&#35793;&#25552;&#20379;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#23384;&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#21106;&#12290;&#30446;&#21069;&#30340;&#35821;&#38899;&#20998;&#21106;&#26041;&#27861;&#35201;&#20040;&#25552;&#20379;&#36739;&#24046;&#30340;&#20998;&#21106;&#36136;&#37327;&#65292;&#35201;&#20040;&#38656;&#35201;&#20197;&#24310;&#36831;&#25442;&#21462;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#38899;&#32763;&#35793;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#19982; ST CTC&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#25110;&#39069;&#22806;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20998;&#21106;&#20219;&#21153;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23454;&#38469;&#30340;&#31471;&#21040;&#31471;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#21516;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#29992;&#20110;&#32763;&#35793;&#21644;&#20998;&#21106;&#12290;&#22312;&#21508;&#31181;&#35821;&#35328;&#23545;&#21644;&#22495;&#20869;&#22806;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current simultaneous speech translation models can process audio only up to a few seconds long. Contemporary datasets provide an oracle segmentation into sentences based on human-annotated transcripts and translations. However, the segmentation into sentences is not available in the real world. Current speech segmentation approaches either offer poor segmentation quality or have to trade latency for quality. In this paper, we propose a novel segmentation approach for a low-latency end-to-end speech translation. We leverage the existing speech translation encoder-decoder architecture with ST CTC and show that it can perform the segmentation task without supervision or additional parameters. To the best of our knowledge, our method is the first that allows an actual end-to-end simultaneous speech translation, as the same model is used for translation and segmentation at the same time. On a diverse set of language pairs and in- and out-of-domain data, we show that the proposed approach ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#23556;&#20987;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#35752;&#35770;&#26469;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#21319;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11382</link><description>&lt;p&gt;
&#35758;&#35770;&#20877;&#34892;&#21160;&#65306;&#22810;&#19987;&#23478;&#35752;&#35770;&#19979;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions. (arXiv:2309.11382v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#23556;&#20987;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#35752;&#35770;&#26469;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#21319;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#26159;&#19968;&#39033;&#38656;&#35201;&#21253;&#25324;&#29702;&#35299;&#12289;&#24863;&#30693;&#21644;&#35268;&#21010;&#22312;&#20869;&#30340;&#24191;&#27867;&#25216;&#33021;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#26679;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;VLN&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#19968;&#20010;&#27169;&#22411;&#33258;&#24049;&#30340;&#24605;&#32771;&#22312;&#19968;&#20010;&#22238;&#21512;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#26159;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT4&#65292;&#20173;&#28982;&#38590;&#20197;&#36890;&#36807;&#21333;&#22238;&#21512;&#33258;&#25105;&#24605;&#32771;&#26469;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19987;&#23478;&#21672;&#35810;&#20250;&#35758;&#20013;&#24471;&#21040;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#23556;&#20987;VLN&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#30340;&#22823;&#22411;&#27169;&#22411;&#34987;&#20316;&#20026;&#39046;&#22495;&#19987;&#23478;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23548;&#33322;&#20195;&#29702;DiscussNav&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20043;&#21069;&#19982;&#36825;&#20123;&#19987;&#23478;&#31215;&#26497;&#35752;&#35770;&#65292;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#20877;&#34892;&#21160;&#12290;&#36825;&#20123;&#35752;&#35770;&#28085;&#30422;&#20102;&#20851;&#38190;&#30340;&#23548;&#33322;&#23376;&#20219;&#21153;&#65292;&#22914;&#25351;&#20196;&#29702;&#35299;&#12289;&#29615;&#22659;&#24863;&#30693;&#21644;&#23436;&#25104;&#20272;&#35745;&#12290;&#36890;&#36807;&#32508;&#21512;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#35752;&#35770;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VLN&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#22810;&#26041;&#38754;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#36136;&#37327;&#19982;&#24310;&#36831;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#19968;&#33268;&#24615;&#25110;&#20445;&#25345;-n&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#19981;&#25913;&#21464;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.6-3.6 BLEU&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#25110;&#32773;&#22312;&#19981;&#25913;&#21464;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.8-1.4&#31186;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2309.11379</link><description>&lt;p&gt;
&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#29992;&#20110;&#20855;&#26377;&#21487;&#25511;&#30340;&#36136;&#37327;-&#24310;&#36831;&#26435;&#34913;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff. (arXiv:2309.11379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#36136;&#37327;&#19982;&#24310;&#36831;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#19968;&#33268;&#24615;&#25110;&#20445;&#25345;-n&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#19981;&#25913;&#21464;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.6-3.6 BLEU&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#25110;&#32773;&#22312;&#19981;&#25913;&#21464;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.8-1.4&#31186;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20998;&#22359;&#33258;&#27880;&#24847;&#32534;&#30721;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#23853;&#38706;&#22836;&#35282;&#12290;&#36825;&#20123;&#27169;&#22411;&#37319;&#29992;&#20998;&#22359;&#26463;&#25628;&#32034;&#21644;&#20551;&#35774;&#21487;&#38752;&#24615;&#35780;&#20998;&#26469;&#30830;&#23450;&#20309;&#26102;&#31561;&#24453;&#26356;&#22810;&#36755;&#20837;&#35821;&#38899;&#20197;&#20415;&#36827;&#19968;&#27493;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30452;&#21040;&#25972;&#20010;&#35821;&#38899;&#36755;&#20837;&#34987;&#28040;&#32791;&#25481;&#25165;&#33021;&#23637;&#31034;&#20986;&#21333;&#20010;&#30340;&#22686;&#37327;&#32763;&#35793;&#65292;&#26080;&#27861;&#30452;&#25509;&#21521;&#29992;&#25143;&#23637;&#31034;&#20986;&#21333;&#20010;&#22686;&#37327;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#25511;&#21046;&#36136;&#37327;&#19982;&#24310;&#36831;&#26435;&#34913;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#19968;&#33268;&#24615;&#25110;&#20445;&#25345;-n&#31574;&#30053;&#29992;&#20110;&#36136;&#37327;-&#24310;&#36831;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22312;&#32447;&#25110;&#31163;&#32447;&#32763;&#35793;&#27169;&#22411;&#20013;&#65292;&#24182;&#35777;&#26126;&#36825;&#20004;&#31181;&#31867;&#22411;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#22312;&#32447;&#27169;&#24335;&#20013;&#20351;&#29992;&#12290;MuST-C&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;0.6-3.6 BLEU&#30340;&#25552;&#21319;&#32780;&#19981;&#25913;&#21464;&#24310;&#36831;&#65292;&#25110;0.8-1.4&#31186;&#30340;&#24310;&#36831;&#25552;&#21319;&#32780;&#19981;&#25913;&#21464;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#26032;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65306;PreFed&#21644;PreFedOp&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11378</link><description>&lt;p&gt;
&#39044;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Preconditioned Federated Learning. (arXiv:2309.11378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#26032;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65306;PreFed&#21644;PreFedOp&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;FL&#20013;&#30340;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#26159;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#65292;&#23427;&#22312;&#36890;&#20449;&#36718;&#20043;&#38388;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;SGD&#27493;&#39588;&#12290;&#19982;&#29616;&#20195;&#30340;&#19968;&#38454;&#33258;&#36866;&#24212;&#20248;&#21270;&#30456;&#27604;&#65292;FedAvg&#34987;&#35748;&#20026;&#32570;&#20047;&#31639;&#27861;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#33258;&#36866;&#24212;&#26694;&#26550;&#65288;&#26412;&#22320;&#36866;&#24212;&#24615;&#21644;&#26381;&#21153;&#22120;&#31471;&#36866;&#24212;&#24615;&#65289;&#30340;&#26032;&#30340;&#39640;&#25928;&#36890;&#20449;FL&#31639;&#27861;&#65306;PreFed&#21644;PreFedOp&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#30340;&#21160;&#24577;&#25163;&#21183;&#29305;&#24449;&#30340;&#20154;&#20307;&#36816;&#21160;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#22810;&#31181;&#27169;&#22359;&#26469;&#23454;&#29616;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#20256;&#36882;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#39069;&#22806;&#22521;&#35757;&#20197;&#20351;&#29992;&#22797;&#26434;&#30340;&#20154;&#26426;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2309.11368</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#30340;&#21160;&#24577;&#25163;&#21183;&#29305;&#24449;&#30340;&#20154;&#20307;&#36816;&#21160;&#36866;&#24212;&#22312;&#24037;&#20855;&#20256;&#36882;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition. (arXiv:2309.11368v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#30340;&#21160;&#24577;&#25163;&#21183;&#29305;&#24449;&#30340;&#20154;&#20307;&#36816;&#21160;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#22810;&#31181;&#27169;&#22359;&#26469;&#23454;&#29616;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#20256;&#36882;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#39069;&#22806;&#22521;&#35757;&#20197;&#20351;&#29992;&#22797;&#26434;&#30340;&#20154;&#26426;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#26377;&#21161;&#20110;&#29992;&#25143;&#22312;&#20132;&#20114;&#20219;&#21153;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21327;&#20316;&#26041;&#26696;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#20154;&#26426;&#25509;&#21475;&#65292;&#19982;&#33258;&#28982;&#32930;&#20307;&#25511;&#21046;&#30456;&#27604;&#21487;&#33021;&#32570;&#20047;&#24517;&#35201;&#30340;&#30452;&#35266;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#20302;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#26469;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#25163;&#21183;&#21644;&#21160;&#24577;&#36816;&#21160;&#35782;&#21035;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#21487;&#20999;&#25442;&#30340;&#25511;&#21046;&#36866;&#24212;&#31574;&#30053;&#12290;&#36825;&#20123;&#27169;&#22359;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#35201;&#20256;&#36882;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#24403;&#29992;&#25143;&#21452;&#25163;&#21516;&#26102;&#24037;&#20316;&#26102;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#19987;&#27880;&#20110;&#20219;&#21153;&#25191;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#22521;&#35757;&#20154;&#26426;&#30028;&#38754;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#26426;&#22120;&#20154;&#21487;&#20197;&#35299;&#35835;&#20182;&#20204;&#30340;&#30452;&#35266;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-robot collaboration has benefited users with higher efficiency towards interactive tasks. Nevertheless, most collaborative schemes rely on complicated human-machine interfaces, which might lack the requisite intuitiveness compared with natural limb control. We also expect to understand human intent with low training data requirements. In response to these challenges, this paper introduces an innovative human-robot collaborative framework that seamlessly integrates hand gesture and dynamic movement recognition, voice recognition, and a switchable control adaptation strategy. These modules provide a user-friendly approach that enables the robot to deliver the tools as per user need, especially when the user is working with both hands. Therefore, users can focus on their task execution without additional training in the use of human-machine interfaces, while the robot interprets their intuitive gestures. The proposed multimodal interaction framework is executed in the UR5e robot pla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;KGQA4MAT&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#12290;&#36890;&#36807;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#21644;&#21033;&#29992;ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#30340;KG&#26597;&#35810;&#65292;&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;ChatGPT&#22312;&#35299;&#20915;KGQA&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11361</link><description>&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;KGQA4MAT&#65289;&#65306;&#20026;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#30693;&#35782;&#22270;&#65288;MOF-KG&#65289;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG). (arXiv:2309.11361v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;KGQA4MAT&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#12290;&#36890;&#36807;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#21644;&#21033;&#29992;ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#30340;KG&#26597;&#35810;&#65292;&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;ChatGPT&#22312;&#35299;&#20915;KGQA&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30693;&#35782;&#22270;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;KGQA4MAT&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21644;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#30693;&#35782;&#22270;&#65288;MOF-KG&#65289;&#12290;&#20026;&#20102;&#22686;&#24378;&#39046;&#22495;&#19987;&#23478;&#35775;&#38382;MOF-KG&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26469;&#26597;&#35810;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;161&#20010;&#28041;&#21450;&#27604;&#36739;&#12289;&#32858;&#21512;&#21644;&#22797;&#26434;&#22270;&#32467;&#26500;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#26377;&#19977;&#31181;&#19981;&#21516;&#30340;&#25913;&#20889;&#24418;&#24335;&#65292;&#20849;&#35745;644&#20010;&#38382;&#39064;&#21644;161&#20010;KG&#26597;&#35810;&#12290;&#20026;&#20102;&#35780;&#20272;&#22522;&#20934;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#30340;KG&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#33879;&#21517;&#30340;QALD-9&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#35299;&#20915;&#19981;&#21516;&#24179;&#21488;&#21644;&#38382;&#39064;&#19979;KGQA&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and q
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20174;&#25972;&#24418;&#25163;&#26415;&#21040;&#23089;&#20048;&#34892;&#19994;&#65292;&#20294;&#22312;&#27861;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#19968;&#20123;&#38480;&#21046;&#21644;&#38556;&#30861;&#12290;&#23545;&#20110;&#23558;3D&#20154;&#33080;&#37325;&#24314;&#20316;&#20026;&#27861;&#21307;&#23398;&#35777;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#25105;&#20204;&#20173;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.11357</link><description>&lt;p&gt;
3D&#20154;&#33080;&#37325;&#24314;&#65306;&#36890;&#24448;&#27861;&#21307;&#23398;&#30340;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
3D Face Reconstruction: the Road to Forensics. (arXiv:2309.11357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20174;&#25972;&#24418;&#25163;&#26415;&#21040;&#23089;&#20048;&#34892;&#19994;&#65292;&#20294;&#22312;&#27861;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#19968;&#20123;&#38480;&#21046;&#21644;&#38556;&#30861;&#12290;&#23545;&#20110;&#23558;3D&#20154;&#33080;&#37325;&#24314;&#20316;&#20026;&#27861;&#21307;&#23398;&#35777;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#25105;&#20204;&#20173;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#36890;&#36807;&#22270;&#20687;&#21644;&#35270;&#39057;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#65292;&#20174;&#25972;&#24418;&#25163;&#26415;&#21040;&#23089;&#20048;&#34892;&#19994;&#65292;&#24471;&#30410;&#20110;&#20854;&#20248;&#21183;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#27861;&#21307;&#24212;&#29992;&#26102;&#65292;3D&#20154;&#33080;&#37325;&#24314;&#24517;&#39035;&#36981;&#23432;&#20005;&#26684;&#30340;&#35201;&#27714;&#65292;&#36825;&#20173;&#28982;&#20351;&#20854;&#22312;&#25552;&#20379;&#35785;&#35772;&#35777;&#25454;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#19981;&#26126;&#30830;&#12290;&#23545;&#20854;&#22312;&#27861;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#32422;&#26463;&#12289;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#32570;&#22833;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#38416;&#26126;&#27861;&#21307;&#24212;&#29992;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#33080;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#23427;&#23545;&#26469;&#33258;&#30417;&#25511;&#35270;&#39057;&#21644;&#23244;&#30097;&#29359;&#29031;&#29255;&#30340;3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#30340;&#25104;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#30446;&#21069;&#38459;&#30861;3D&#20154;&#33080;&#37325;&#24314;&#22312;&#27861;&#21307;&#24212;&#29992;&#20013;&#21457;&#25381;&#31215;&#26497;&#20316;&#29992;&#30340;&#38556;&#30861;&#12290;&#26368;&#21518;&#65292;&#23427;&#36824;&#23545;&#24213;&#23618;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#21253;&#25324;&#20854;&#20248;&#28857;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#30740;&#20102;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#39046;&#22495;&#30340;&#24403;&#21069;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#32597;&#35265;&#20107;&#20214;&#25968;&#25454;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#31639;&#27861;&#26041;&#27861;&#21644;&#35780;&#20272;&#26041;&#27861;&#22235;&#20010;&#32500;&#24230;&#65292;&#24635;&#32467;&#20986;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#35299;&#20915;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11356</link><description>&lt;p&gt;
&#38024;&#23545;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Rare Event Prediction. (arXiv:2309.11356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#30740;&#20102;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#39046;&#22495;&#30340;&#24403;&#21069;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#32597;&#35265;&#20107;&#20214;&#25968;&#25454;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#31639;&#27861;&#26041;&#27861;&#21644;&#35780;&#20272;&#26041;&#27861;&#22235;&#20010;&#32500;&#24230;&#65292;&#24635;&#32467;&#20986;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#35299;&#20915;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#21644;&#39044;&#27979;&#20302;&#27010;&#29575;&#20107;&#20214;&#12290;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#26222;&#36890;&#20107;&#20214;&#30340;&#39057;&#29575;&#36828;&#36828;&#36229;&#36807;&#32597;&#35265;&#20107;&#20214;&#30340;&#39057;&#29575;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#20351;&#29992;&#19987;&#38376;&#30340;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#31639;&#27861;&#21040;&#35780;&#20272;&#21327;&#35758;&#12290;&#39044;&#27979;&#32597;&#35265;&#20107;&#20214;&#30340;&#21457;&#29983;&#23545;&#20110;&#24037;&#19994;4.0&#31561;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20063;&#26159;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#30340;&#24403;&#21069;&#26041;&#27861;&#65292;&#20174;&#32597;&#35265;&#20107;&#20214;&#25968;&#25454;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#31639;&#27861;&#26041;&#27861;&#21644;&#35780;&#20272;&#26041;&#27861;&#22235;&#20010;&#32500;&#24230;&#32771;&#34385;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;73&#20010;&#25968;&#25454;&#38598;&#65288;&#25968;&#20540;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#22235;&#20010;&#20027;&#35201;&#30340;&#25968;&#25454;&#22788;&#29702;&#20998;&#31867;&#65292;&#20116;&#20010;&#20027;&#35201;&#30340;&#31639;&#27861;&#20998;&#31867;&#21644;&#20004;&#20010;&#26356;&#24191;&#27867;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This pape
&lt;/p&gt;</description></item><item><title>C&#8901;ASE&#26159;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24322;&#26500;&#30340;&#25216;&#33021;&#21160;&#20316;&#21010;&#20998;&#20026;&#19981;&#21516;&#23376;&#38598;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#28966;&#28857;&#25216;&#33021;&#37319;&#26679;&#12289;&#39592;&#39612;&#27531;&#20313;&#21147;&#21644;&#36880;&#20803;&#32032;&#29305;&#24449;&#23631;&#34109;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#25216;&#33021;&#65292;&#24182;&#25429;&#25417;&#26356;&#19968;&#33324;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.11351</link><description>&lt;p&gt;
C&#8901;ASE&#65306;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters. (arXiv:2309.11351v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11351
&lt;/p&gt;
&lt;p&gt;
C&#8901;ASE&#26159;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24322;&#26500;&#30340;&#25216;&#33021;&#21160;&#20316;&#21010;&#20998;&#20026;&#19981;&#21516;&#23376;&#38598;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#28966;&#28857;&#25216;&#33021;&#37319;&#26679;&#12289;&#39592;&#39612;&#27531;&#20313;&#21147;&#21644;&#36880;&#20803;&#32032;&#29305;&#24449;&#23631;&#34109;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#25216;&#33021;&#65292;&#24182;&#25429;&#25417;&#26356;&#19968;&#33324;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;C&#8901;ASE&#65292;&#19968;&#20010;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#29289;&#29702;&#27169;&#25311;&#35282;&#33394;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#25216;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20197;&#30452;&#25509;&#25805;&#32437;&#25191;&#34892;&#30340;&#25216;&#33021;&#30340;&#21487;&#25511;&#24615;&#12290;C&#8901;ASE&#23558;&#24322;&#26500;&#30340;&#25216;&#33021;&#21160;&#20316;&#21010;&#20998;&#20026;&#21253;&#21547;&#21516;&#36136;&#26679;&#26412;&#30340;&#19981;&#21516;&#23376;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#20302;&#32423;&#26465;&#20214;&#27169;&#22411;&#26469;&#23398;&#20064;&#26465;&#20214;&#34892;&#20026;&#20998;&#24067;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#28966;&#28857;&#25216;&#33021;&#37319;&#26679;&#12289;&#39592;&#39612;&#27531;&#20313;&#21147;&#21644;&#36880;&#20803;&#32032;&#29305;&#24449;&#23631;&#34109;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21508;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#21160;&#21147;&#23398;&#19981;&#21305;&#37197;&#20197;&#25484;&#25569;&#25935;&#25463;&#21160;&#20316;&#65292;&#24182;&#25429;&#25417;&#26356;&#22810;&#30340;&#19968;&#33324;&#34892;&#20026;&#29305;&#24449;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#26465;&#20214;&#27169;&#22411;&#23601;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, o
&lt;/p&gt;</description></item><item><title>TRAVID&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#32763;&#35793;&#21475;&#35821;&#65292;&#36824;&#21487;&#20197;&#23558;&#32763;&#35793;&#30340;&#35821;&#38899;&#19982;&#35828;&#35805;&#32773;&#30340;&#22068;&#21767;&#21160;&#20316;&#21516;&#27493;&#65292;&#20026;&#23398;&#29983;&#21644;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#35270;&#39057;&#32763;&#35793;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.11338</link><description>&lt;p&gt;
TRAVID&#65306;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TRAVID: An End-to-End Video Translation Framework. (arXiv:2309.11338v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11338
&lt;/p&gt;
&lt;p&gt;
TRAVID&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#32763;&#35793;&#21475;&#35821;&#65292;&#36824;&#21487;&#20197;&#23558;&#32763;&#35793;&#30340;&#35821;&#38899;&#19982;&#35828;&#35805;&#32773;&#30340;&#22068;&#21767;&#21160;&#20316;&#21516;&#27493;&#65292;&#20026;&#23398;&#29983;&#21644;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#35270;&#39057;&#32763;&#35793;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#19982;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#20154;&#26377;&#25928;&#27807;&#36890;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#27861;&#65292;&#22914;&#25991;&#23383;&#25110;&#20165;&#22768;&#38899;&#30340;&#32763;&#35793;&#65292;&#21487;&#20197;&#23436;&#25104;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#36890;&#36807;&#38754;&#37096;&#34920;&#24773;&#21644;&#22068;&#21767;&#21160;&#20316;&#20256;&#36798;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#21644;&#24494;&#22937;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#32763;&#35793;&#21475;&#35821;&#65292;&#36824;&#21487;&#20197;&#23558;&#32763;&#35793;&#30340;&#35821;&#38899;&#19982;&#35828;&#35805;&#32773;&#30340;&#22068;&#21767;&#21160;&#20316;&#21516;&#27493;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19987;&#27880;&#20110;&#32763;&#35793;&#21508;&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#25945;&#32946;&#35762;&#24231;&#65292;&#24182;&#19988;&#23427;&#34987;&#35774;&#35745;&#25104;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#31995;&#32479;&#35774;&#32622;&#20013;&#20063;&#33021;&#26377;&#25928;&#12290;&#36890;&#36807;&#23558;&#19982;&#30446;&#26631;&#35821;&#35328;&#19968;&#33268;&#30340;&#22068;&#21767;&#21160;&#20316;&#19982;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#36827;&#34892;&#21305;&#37197;&#65292;&#20351;&#29992;&#35821;&#38899;&#20811;&#38534;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#24212;&#29992;&#20026;&#23398;&#29983;&#21644;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20307;&#39564;&#12290;&#36825;&#20010;&#38468;&#21152;&#21151;&#33021;&#21019;&#36896;&#20102;&#19968;&#20010;&#26356;&#23436;&#25972;&#30340;&#35270;&#39057;&#32763;&#35793;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's globalized world, effective communication with people from diverse linguistic backgrounds has become increasingly crucial. While traditional methods of language translation, such as written text or voice-only translations, can accomplish the task, they often fail to capture the complete context and nuanced information conveyed through nonverbal cues like facial expressions and lip movements. In this paper, we present an end-to-end video translation system that not only translates spoken language but also synchronizes the translated speech with the lip movements of the speaker. Our system focuses on translating educational lectures in various Indian languages, and it is designed to be effective even in low-resource system settings. By incorporating lip movements that align with the target language and matching them with the speaker's voice using voice cloning techniques, our application offers an enhanced experience for students and users. This additional feature creates a mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.11331</link><description>&lt;p&gt;
Gold-YOLO: &#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;YOLO&#31995;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#26550;&#26500;&#12289;&#22686;&#21152;&#25968;&#25454;&#21644;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#22522;&#32447;&#25552;&#21319;&#21040;&#20102;&#26356;&#39640;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#34429;&#28982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#36335;&#24452;&#32858;&#21512;&#32593;&#32476;&#65288;PANet&#65289;&#24050;&#32463;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23454;&#29616;&#12290;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#27169;&#22411;&#21517;&#20026;Gold-YOLO&#65292;&#25552;&#21319;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#33021;&#21147;&#65292;&#24182;&#22312;&#25152;&#26377;&#27169;&#22411;&#23610;&#24230;&#19978;&#23454;&#29616;&#20102;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#24819;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;YOLO&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;YOLO&#31995;&#21015;&#27169;&#22411;&#21487;&#20197;&#20174;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#21463;&#30410;&#12290;Gold-YOLO-N&#22312;COCO val2017&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;39.9%&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21338;&#24328;&#35770;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#30340;&#27169;&#22411;&#65292;&#22312;&#20113;&#24066;&#22330;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23450;&#20215;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#24066;&#22330;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11316</link><description>&lt;p&gt;
&#20351;&#29992;&#21338;&#24328;&#35770;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#22312;&#20113;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory. (arXiv:2309.11316v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21338;&#24328;&#35770;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#30340;&#27169;&#22411;&#65292;&#22312;&#20113;&#24066;&#22330;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23450;&#20215;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#24066;&#22330;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#24066;&#22330;&#30340;&#31454;&#20105;&#24615;&#20351;&#24471;&#23450;&#20215;&#31574;&#30053;&#25104;&#20026;&#20225;&#19994;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#24456;&#22810;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26222;&#36890;&#24418;&#24335;&#30340;&#21338;&#24328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#20379;&#32773;&#36890;&#36807;&#19968;&#20010;&#22996;&#21592;&#20250;&#27880;&#20876;&#20197;&#25913;&#21892;&#20182;&#20204;&#30340;&#31454;&#20105;&#23450;&#20215;&#31574;&#30053;&#12290;&#21338;&#24328;&#35770;&#30340;&#21151;&#33021;&#34987;&#24212;&#29992;&#20110;&#35774;&#35745;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#12290;&#22996;&#21592;&#20250;&#30340;&#20351;&#29992;&#20351;&#24471;&#21338;&#24328;&#25104;&#20026;&#19968;&#20010;&#23436;&#20840;&#20449;&#24687;&#30340;&#21338;&#24328;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#30693;&#36947;&#20854;&#20182;&#20154;&#30340;&#25910;&#30410;&#20989;&#25968;&#12290;&#21442;&#19982;&#32773;&#25913;&#36827;&#20182;&#20204;&#30340;&#23450;&#20215;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#20197;&#21338;&#24328;&#30340;&#24418;&#24335;&#37327;&#21270;&#20102;&#20113;&#24066;&#22330;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65307;&#36890;&#36807;&#35777;&#26126;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The competitive nature of Cloud marketplaces as new concerns in delivery of services makes the pricing policies a crucial task for firms. so that, pricing strategies has recently attracted many researchers. Since game theory can handle such competing well this concern is addressed by designing a normal form game between providers in current research. A committee is considered in which providers register for improving their competition based pricing policies. The functionality of game theory is applied to design dynamic pricing policies. The usage of the committee makes the game a complete information one, in which each player is aware of every others payoff functions. The players enhance their pricing policies to maximize their profits. The contribution of this paper is the quantitative modeling of Cloud marketplaces in form of a game to provide novel dynamic pricing strategies; the model is validated by proving the existence and the uniqueness of Nash equilibrium of the game.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#20113;&#24066;&#22330;&#23450;&#20215;&#31574;&#30053;&#65292;&#36890;&#36807;&#27169;&#25311;&#20379;&#24212;&#21830;&#20043;&#38388;&#30340;&#31454;&#20105;&#29615;&#22659;&#24182;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#20379;&#24212;&#21830;&#21033;&#28070;&#30340;&#26174;&#33879;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2309.11312</link><description>&lt;p&gt;
&#20113;&#24066;&#22330;&#20013;&#22522;&#20110;&#31454;&#20105;&#30340;&#23450;&#20215;&#31574;&#30053;&#65306;&#21033;&#29992;&#36951;&#25022;&#26368;&#23567;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques. (arXiv:2309.11312v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#20113;&#24066;&#22330;&#23450;&#20215;&#31574;&#30053;&#65292;&#36890;&#36807;&#27169;&#25311;&#20379;&#24212;&#21830;&#20043;&#38388;&#30340;&#31454;&#20105;&#29615;&#22659;&#24182;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#20379;&#24212;&#21830;&#21033;&#28070;&#30340;&#26174;&#33879;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#30456;&#23545;&#26032;&#30340;&#21830;&#19994;&#27169;&#24335;&#65292;&#24050;&#32463;&#21463;&#21040;&#19981;&#21516;&#30740;&#31350;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;&#23450;&#20215;&#26159;&#20113;&#35745;&#31639;&#24066;&#22330;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#20379;&#24212;&#21830;&#38656;&#35201;&#22312;&#19981;&#20102;&#35299;&#23545;&#25163;&#23450;&#20215;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#21560;&#24341;&#26356;&#22810;&#30340;&#23458;&#25143;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#23450;&#20215;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32771;&#34385;&#21040;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#21338;&#24328;&#27169;&#22411;&#20013;&#12290;&#22522;&#20110;&#20113;&#35745;&#31639;&#24066;&#22330;&#30340;&#31454;&#20105;&#29615;&#22659;&#65292;&#20379;&#24212;&#21830;&#21033;&#29992;&#32463;&#39564;&#36951;&#25022;&#26469;&#26356;&#26032;&#20854;&#31574;&#30053;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#31639;&#27861;&#26356;&#26032;&#31574;&#30053;&#27010;&#29575;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#20943;&#23567;&#36951;&#25022;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20854;&#20182;&#23450;&#20215;&#31574;&#30053;&#30456;&#27604;&#65292;&#20379;&#24212;&#21830;&#30340;&#21033;&#28070;&#22823;&#24133;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#36951;&#25022;&#26368;&#23567;&#21270;&#25216;&#26415;&#20063;&#24471;&#21040;&#20102;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud computing as a fairly new commercial paradigm, widely investigated by different researchers, already has a great range of challenges. Pricing is a major problem in Cloud computing marketplace; as providers are competing to attract more customers without knowing the pricing policies of each other. To overcome this lack of knowledge, we model their competition by an incomplete-information game. Considering the issue, this work proposes a pricing policy related to the regret minimization algorithm and applies it to the considered incomplete-information game. Based on the competition based marketplace of the Cloud, providers update the distribution of their strategies using the experienced regret. The idea of iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster. The experimental results show much more increase in profits of the providers in comparison with other pricing policies. Besides, the efficiency of a variety of reg
&lt;/p&gt;</description></item><item><title>FaceDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#25216;&#26415;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#21512;&#25104;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38750;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20102;&#22522;&#20110;3D&#39030;&#28857;&#21644;&#28151;&#21512;&#24418;&#29366;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.11306</link><description>&lt;p&gt;
FaceDiffuser&#65306;&#20351;&#29992;&#25193;&#25955;&#25216;&#26415;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion. (arXiv:2309.11306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11306
&lt;/p&gt;
&lt;p&gt;
FaceDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#25216;&#26415;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#21512;&#25104;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38750;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20102;&#22522;&#20110;3D&#39030;&#28857;&#21644;&#28151;&#21512;&#24418;&#29366;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#21512;&#25104;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26080;&#35770;&#22312;&#24037;&#19994;&#30028;&#36824;&#26159;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#36825;&#24847;&#21619;&#30528;&#32473;&#23450;&#19968;&#20010;&#35821;&#38899;&#36755;&#20837;&#65292;&#36755;&#20986;&#24635;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#33080;&#37096;&#20013;&#23384;&#22312;&#30340;&#38750;&#35821;&#35328;&#38754;&#37096;&#32447;&#32034;&#26159;&#38750;&#30830;&#23450;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22522;&#20110;3D&#39030;&#28857;&#30340;&#25968;&#25454;&#38598;&#21644;&#19982;&#29616;&#26377;&#38754;&#37096;&#21160;&#30011;&#27969;&#31243;&#20860;&#23481;&#30340;&#26041;&#27861;&#19978;&#26159;&#31232;&#32570;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FaceDiffuser&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#38750;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29983;&#25104;&#35821;&#38899;&#39537;&#21160;&#30340;&#38754;&#37096;&#21160;&#30011;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;3D&#39030;&#28857;&#21644;&#28151;&#21512;&#24418;&#29366;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#25216;&#26415;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;HuBERT&#26469;&#32534;&#30721;&#38899;&#39057;&#36755;&#20837;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#23558;&#25193;&#25955;&#26041;&#27861;&#29992;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#30340;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#26426;&#21046;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#21160;&#26426;&#23454;&#29616;&#23545;&#35831;&#27714;&#24212;&#29992;&#31243;&#24207;&#30340;&#39640;&#25928;&#36164;&#28304;&#37197;&#36865;&#65292;&#20026;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#26381;&#21153;&#37197;&#32622;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11299</link><description>&lt;p&gt;
&#20113;&#35745;&#31639;&#20013;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#30340;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing. (arXiv:2309.11299v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#30340;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#26426;&#21046;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#21160;&#26426;&#23454;&#29616;&#23545;&#35831;&#27714;&#24212;&#29992;&#31243;&#24207;&#30340;&#39640;&#25928;&#36164;&#28304;&#37197;&#36865;&#65292;&#20026;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#26381;&#21153;&#37197;&#32622;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20113;&#35745;&#31639;&#20013;&#35745;&#31639;&#36164;&#28304;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#36164;&#28304;&#37197;&#32622;&#25361;&#25112;&#12290;&#36164;&#28304;&#37197;&#32622;&#25216;&#26415;&#24517;&#39035;&#22312;&#28385;&#36275;&#35831;&#27714;&#35201;&#27714;&#30340;&#21516;&#26102;&#23558;&#24635;&#25104;&#26412;&#38477;&#21040;&#26368;&#20302;&#12290;&#37492;&#20110;&#20113;&#26381;&#21153;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24320;&#21457;&#25104;&#26412;&#26377;&#25928;&#30340;&#26381;&#21153;&#37197;&#32622;&#26041;&#26696;&#20284;&#20046;&#26356;&#20855;&#25361;&#25112;&#24615;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#36164;&#28304;&#37197;&#32622;&#26041;&#27861;&#65292;&#23427;&#23454;&#29616;&#20102;&#23545;&#38656;&#27714;&#30340;&#38477;&#20302;&#25104;&#26412;&#20445;&#35777;&#12290;&#25105;&#20204;&#20248;&#21270;&#30340;&#36164;&#28304;&#37197;&#32622; (ORP) &#26041;&#27861;&#30340;&#36129;&#29486;&#22914;&#19979;&#12290;&#39318;&#20808;&#65292;&#23427;&#34987;&#35774;&#35745;&#20026;&#25552;&#20379;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#22788;&#29702;&#35831;&#27714;&#30340;&#24212;&#29992;&#31243;&#24207;&#65307;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#21482;&#20801;&#35768;&#19968;&#33324;&#30340;&#27969;&#31243;&#65292;&#20851;&#24515;&#20219;&#21153;&#30340;&#20381;&#36182;&#24615;&#65292;ORP&#21017;&#22522;&#20110;&#24212;&#29992;&#31243;&#24207;&#25152;&#32452;&#25104;&#30340;&#26381;&#21153;&#36827;&#34892;&#24615;&#33021;&#20248;&#21270;&#20840;&#38754;&#20851;&#27880;&#20182;&#20204;&#30340;&#39640;&#25928;&#25552;&#20379;&#12290;&#20854;&#27425;&#65292;&#23427;&#26159;&#22522;&#20110;&#23398;&#20064;&#33258;&#21160;&#26426;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Due to the recent wide use of computational resources in cloud computing, new resource provisioning challenges have been emerged. Resource provisioning techniques must keep total costs to a minimum while meeting the requirements of the requests. According to widely usage of cloud services, it seems more challenging to develop effective schemes for provisioning services cost-effectively; we have proposed a novel learning based resource provisioning approach that achieves cost-reduction guarantees of demands. The contributions of our optimized resource provisioning (ORP) approach are as follows. Firstly, it is designed to provide a cost-effective method to efficiently handle the provisioning of requested applications; while most of the existing models allow only workflows in general which cares about the dependencies of the tasks, ORP performs based on services of which applications comprised and cares about their efficient provisioning totally. Secondly, it is a learning automata-based 
&lt;/p&gt;</description></item><item><title>CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11295</link><description>&lt;p&gt;
CPLLM: &#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11295
&lt;/p&gt;
&lt;p&gt;
CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;LLM&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#24739;&#32773;&#22312;&#19979;&#19968;&#27425;&#23601;&#35786;&#25110;&#38543;&#21518;&#30340;&#35786;&#26029;&#20013;&#26159;&#21542;&#20250;&#34987;&#35786;&#26029;&#20026;&#30446;&#26631;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;RETAIN&#21644;Med-BERT&#65292;&#21518;&#32773;&#26159;&#20351;&#29992;&#32467;&#26500;&#21270;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CPLLM&#22312;PR-AUC&#21644;ROC-AUC&#25351;&#26631;&#19978;&#22343;&#36229;&#36807;&#20102;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
&lt;/p&gt;</description></item><item><title>AuTexTification&#26159;IberLEF 2023&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#24402;&#23646;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;160,000&#22810;&#26465;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20197;&#21450;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#20116;&#20010;&#39046;&#22495;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11285</link><description>&lt;p&gt;
&#12298;IberLEF 2023&#30340;AuTexTification&#27010;&#36848;&#65306;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#21644;&#24402;&#23646;&#12299;
&lt;/p&gt;
&lt;p&gt;
Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11285
&lt;/p&gt;
&lt;p&gt;
AuTexTification&#26159;IberLEF 2023&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#24402;&#23646;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;160,000&#22810;&#26465;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20197;&#21450;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#20116;&#20010;&#39046;&#22495;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20316;&#20026;IberLEF 2023&#30740;&#35752;&#20250;&#19968;&#37096;&#20998;&#30340;AuTexTification&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#35813;&#30740;&#35752;&#20250;&#26159;&#22312;SEPLN 2023&#20250;&#35758;&#26694;&#26550;&#20869;&#30340;&#20234;&#27604;&#21033;&#20122;&#35821;&#35328;&#35780;&#20272;&#35770;&#22363;&#20013;&#36827;&#34892;&#30340;&#12290;AuTexTification&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#22312;&#23376;&#20219;&#21153;1&#20013;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#20154;&#24037;&#25776;&#20889;&#36824;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#22312;&#23376;&#20219;&#21153;2&#20013;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24402;&#23646;&#20110;&#20845;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;AuTexTification 2023&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20004;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#20116;&#20010;&#39046;&#22495;&#65288;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#65289;&#65292;&#20849;&#21253;&#21547;&#36229;&#36807;160,000&#26465;&#25991;&#26412;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#25253;&#21517;&#21442;&#19982;&#65292;&#20854;&#20013;36&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#65292;&#20854;&#20013;20&#20010;&#22242;&#38431;&#36824;&#25552;&#20132;&#20102;&#24037;&#20316;&#31508;&#35760;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AuTexTification&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#20197;&#21450;&#21442;&#19982;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20256;&#24863;&#22120;&#30340;&#20381;&#36182;&#24314;&#27169;&#38382;&#39064;&#65292;&#20174;&#21306;&#22495;&#21644;&#20840;&#23616;&#20004;&#20010;&#23618;&#27425;&#20986;&#21457;&#65292;&#36890;&#36807;&#21512;&#24182;&#21306;&#22495;&#33410;&#28857;&#21644;&#29983;&#25104;&#20840;&#23616;&#33410;&#28857;&#26469;&#21453;&#26144;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#24212;&#29992;&#20803;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#25552;&#39640;&#33410;&#28857;&#34920;&#31034;&#30340;&#26222;&#36941;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11284</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20256;&#24863;&#22120;&#24314;&#27169;: &#20998;&#23618;&#20449;&#24687;&#22686;&#24378;&#30340;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting. (arXiv:2309.11284v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20256;&#24863;&#22120;&#30340;&#20381;&#36182;&#24314;&#27169;&#38382;&#39064;&#65292;&#20174;&#21306;&#22495;&#21644;&#20840;&#23616;&#20004;&#20010;&#23618;&#27425;&#20986;&#21457;&#65292;&#36890;&#36807;&#21512;&#24182;&#21306;&#22495;&#33410;&#28857;&#21644;&#29983;&#25104;&#20840;&#23616;&#33410;&#28857;&#26469;&#21453;&#26144;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#24212;&#29992;&#20803;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#25552;&#39640;&#33410;&#28857;&#34920;&#31034;&#30340;&#26222;&#36941;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#21270;&#30340;&#21152;&#36895;&#65292;&#20132;&#36890;&#39044;&#27979;&#22312;&#26234;&#24935;&#22478;&#24066;&#24314;&#35774;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26102;&#31354;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#24314;&#27169;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#22522;&#26412;&#19978;&#21482;&#32771;&#34385;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#24494;&#35266;&#20851;&#31995;&#65292;&#23558;&#20256;&#24863;&#22120;&#31561;&#21516;&#23545;&#24453;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#23439;&#35266;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#20174;&#21306;&#22495;&#21644;&#20840;&#23616;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20256;&#24863;&#22120;&#30340;&#20381;&#36182;&#24314;&#27169;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#39640;&#20869;&#37096;&#21306;&#22495;&#30456;&#20851;&#24615;&#30340;&#21407;&#22987;&#20256;&#24863;&#22120;&#21512;&#24182;&#20026;&#21306;&#22495;&#33410;&#28857;&#65292;&#20197;&#20445;&#30041;&#21306;&#22495;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#29983;&#25104;&#20195;&#34920;&#24615;&#21644;&#24120;&#35265;&#30340;&#26102;&#31354;&#27169;&#24335;&#20316;&#20026;&#20840;&#23616;&#33410;&#28857;&#65292;&#20197;&#21453;&#26144;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20026;&#26102;&#31354;&#20381;&#36182;&#23398;&#20064;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#20026;&#20102;&#36861;&#27714;&#33410;&#28857;&#34920;&#31034;&#30340;&#26222;&#36941;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;Meta GCN&#65289;&#26469;&#26657;&#20934;&#21306;&#22495;&#21644;&#20840;&#23616;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the acceleration of urbanization, traffic forecasting has become an essential role in smart city construction. In the context of spatio-temporal prediction, the key lies in how to model the dependencies of sensors. However, existing works basically only consider the micro relationships between sensors, where the sensors are treated equally, and their macroscopic dependencies are neglected. In this paper, we argue to rethink the sensor's dependency modeling from two hierarchies: regional and global perspectives. Particularly, we merge original sensors with high intra-region correlation as a region node to preserve the inter-region dependency. Then, we generate representative and common spatio-temporal patterns as global nodes to reflect a global dependency between sensors and provide auxiliary information for spatio-temporal dependency learning. In pursuit of the generality and reality of node representations, we incorporate a Meta GCN to calibrate the regional and global nodes in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22266;&#23450;&#24418;&#24577;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#65292;&#22312;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#22330;&#26223;&#20013;&#35825;&#21457;&#20102;&#24320;&#25918;&#24335;&#36827;&#21270;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#31995;&#32479;&#65292;&#22686;&#21152;&#20102;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#22797;&#26434;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36866;&#24212;&#24615;&#31574;&#30053;&#30340;&#20986;&#29616;&#12290;&#36825;&#19968;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#36827;&#34892;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#21160;&#24577;&#26102;&#35825;&#23548;&#24320;&#25918;&#24335;&#36827;&#21270;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11275</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#36890;&#36807;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#22330;&#26223;&#35825;&#23548;&#24320;&#25918;&#24335;&#36827;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Open-endedness induced through a predator-prey scenario using modular robots. (arXiv:2309.11275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22266;&#23450;&#24418;&#24577;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#65292;&#22312;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#22330;&#26223;&#20013;&#35825;&#21457;&#20102;&#24320;&#25918;&#24335;&#36827;&#21270;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#31995;&#32479;&#65292;&#22686;&#21152;&#20102;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#22797;&#26434;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36866;&#24212;&#24615;&#31574;&#30053;&#30340;&#20986;&#29616;&#12290;&#36825;&#19968;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#36827;&#34892;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#21160;&#24577;&#26102;&#35825;&#23548;&#24320;&#25918;&#24335;&#36827;&#21270;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#22330;&#26223;&#35825;&#21457;&#24320;&#25918;&#24335;&#36827;&#21270;&#65288;OEE&#65289;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#22266;&#23450;&#24418;&#24577;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#22120;&#36827;&#34892;&#36827;&#21270;&#12290;&#22312;&#20004;&#20010;&#29289;&#31181;&#20013;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21457;&#36865;&#21644;&#25509;&#25910;&#20449;&#21495;&#65292;&#24182;&#24863;&#30693;&#29615;&#22659;&#20013;&#20854;&#20182;&#26426;&#22120;&#20154;&#30340;&#30456;&#23545;&#20301;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#26631;&#35760;&#31995;&#32479;&#30340;&#29305;&#24449;&#65306;&#23427;&#20462;&#25913;&#20102;&#20010;&#20307;&#20043;&#38388;&#30340;&#24863;&#30693;&#26041;&#24335;&#65292;&#39044;&#35745;&#21487;&#20197;&#22686;&#21152;&#34892;&#20026;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#36866;&#24212;&#24615;&#31574;&#30053;&#30340;&#20986;&#29616;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#25429;&#39135;-&#34987;&#25429;&#39135;&#32773;&#21160;&#24577;&#20351;&#29992;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#35825;&#23548;&#24320;&#25918;&#24335;&#36827;&#21270;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20986;&#29616;&#20284;&#20046;&#20381;&#36182;&#20110;&#23558;&#32321;&#27542;&#26465;&#20214;&#21270;&#20026;&#26126;&#30830;&#30340;&#34892;&#20026;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates how a predator-prey scenario can induce the emergence of Open-Ended Evolution (OEE). We utilize modular robots of fixed morphologies whose controllers are subject to evolution. In both species, robots can send and receive signals and perceive the relative positions of other robots in the environment. Specifically, we introduce a feature we call a tagging system: it modifies how individuals can perceive each other and is expected to increase behavioral complexity. Our results show the emergence of adaptive strategies, demonstrating the viability of inducing OEE through predator-prey dynamics using modular robots. Such emergence, nevertheless, seemed to depend on conditioning reproduction to an explicit behavioral criterion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25925;&#38556;&#27880;&#20837;&#30340;&#36755;&#20837;&#25968;&#25454;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#22810;&#20010;&#25925;&#24847;&#24341;&#21457;&#30340;&#25968;&#25454;&#25925;&#38556;&#30340;&#24377;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25968;&#25454;&#31361;&#21464;&#22120;&#25506;&#32034;ML&#31995;&#32479;&#23545;&#19981;&#21516;&#25925;&#38556;&#27880;&#20837;&#25928;&#26524;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#22312;&#36873;&#23450;&#30340;ML&#27169;&#22411;&#20043;&#21069;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11274</link><description>&lt;p&gt;
&#20351;&#29992;&#25925;&#38556;&#27880;&#20837;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework. (arXiv:2309.11274v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25925;&#38556;&#27880;&#20837;&#30340;&#36755;&#20837;&#25968;&#25454;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#22810;&#20010;&#25925;&#24847;&#24341;&#21457;&#30340;&#25968;&#25454;&#25925;&#38556;&#30340;&#24377;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25968;&#25454;&#31361;&#21464;&#22120;&#25506;&#32034;ML&#31995;&#32479;&#23545;&#19981;&#21516;&#25925;&#38556;&#27880;&#20837;&#25928;&#26524;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#22312;&#36873;&#23450;&#30340;ML&#27169;&#22411;&#20043;&#21069;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#24377;&#24615;&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#24050;&#25104;&#20026;&#30830;&#20445;&#20855;&#26377;&#29992;&#25143;&#20449;&#24515;&#30340;&#29983;&#20135;&#23601;&#32490;ML&#31995;&#32479;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#36755;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#36136;&#37327;&#23545;&#25968;&#25454;&#25935;&#24863;&#31995;&#32479;&#20013;&#25104;&#21151;&#30340;&#31471;&#21040;&#31471;&#27979;&#35797;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#27169;&#22411;&#27979;&#35797;&#26469;&#35828;&#65292;&#36755;&#20837;&#25968;&#25454;&#30340;&#27979;&#35797;&#26041;&#27861;&#24182;&#19981;&#20687;&#27169;&#22411;&#27979;&#35797;&#37027;&#26679;&#31995;&#32479;&#21270;&#65292;&#19988;&#25968;&#37327;&#36739;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25925;&#38556;&#27880;&#20837;&#19981;&#28385;&#24847;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;(FIUL-Data)&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;ML&#27169;&#22411;&#23545;&#22810;&#20010;&#26377;&#24847;&#35302;&#21457;&#30340;&#25968;&#25454;&#25925;&#38556;&#30340;&#24377;&#24615;&#12290;&#25968;&#25454;&#31361;&#21464;&#22120;&#21487;&#20197;&#25506;&#32034;ML&#31995;&#32479;&#23545;&#19981;&#21516;&#25925;&#38556;&#27880;&#20837;&#25928;&#26524;&#30340;&#33030;&#24369;&#24615;&#12290;&#35813;&#26694;&#26550;&#26681;&#25454;&#19977;&#20010;&#20027;&#35201;&#24605;&#24819;&#36827;&#34892;&#35774;&#35745;&#65306;&#31361;&#21464;&#22120;&#19981;&#26159;&#38543;&#26426;&#30340;&#65307;&#19968;&#20010;&#25968;&#25454;&#31361;&#21464;&#22120;&#22312;&#19968;&#20010;&#26102;&#38388;&#23454;&#20363;&#24212;&#29992;&#65307;&#22312;&#36827;&#34892;&#36873;&#25321;&#30340;ML&#27169;&#22411;&#20043;&#21069;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#26412;&#25991;&#20351;&#29992;&#20998;&#26512;&#21270;&#23398;&#30340;&#25968;&#25454;&#23545;FIUL-Data&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating resilient machine learning (ML) systems has become necessary to ensure production-ready ML systems that acquire user confidence seamlessly. The quality of the input data and the model highly influence the successful end-to-end testing in data-sensitive systems. However, the testing approaches of input data are not as systematic and are few compared to model testing. To address this gap, this paper presents the Fault Injection for Undesirable Learning in input Data (FIUL-Data) testing framework that tests the resilience of ML models to multiple intentionally-triggered data faults. Data mutators explore vulnerabilities of ML systems against the effects of different fault injections. The proposed framework is designed based on three main ideas: The mutators are not random; one data mutator is applied at an instance of time, and the selected ML models are optimized beforehand. This paper evaluates the FIUL-Data framework using data from analytical chemistry, comprising retention t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#25351;&#20196;&#36716;&#21270;&#20026;&#20250;&#35805;&#24335;&#25351;&#20196;&#65292;&#38024;&#23545;&#20250;&#35805;&#21161;&#25163;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#20998;&#21106;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#26631;&#35760;&#30340;Transformer&#27169;&#22411;&#22312;&#35745;&#31639;&#20250;&#35805;&#27493;&#39588;&#29305;&#24449;&#26041;&#38754;&#25928;&#26524;&#26368;&#22909;&#65292;&#24182;&#19988;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#21407;&#22987;&#25351;&#20196;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11271</link><description>&lt;p&gt;
&#20026;&#20250;&#35805;&#21161;&#25163;&#30340;&#22797;&#26434;&#20219;&#21153;&#20998;&#21106;&#24314;&#31435;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Grounded Complex Task Segmentation for Conversational Assistants. (arXiv:2309.11271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#25351;&#20196;&#36716;&#21270;&#20026;&#20250;&#35805;&#24335;&#25351;&#20196;&#65292;&#38024;&#23545;&#20250;&#35805;&#21161;&#25163;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#20998;&#21106;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#26631;&#35760;&#30340;Transformer&#27169;&#22411;&#22312;&#35745;&#31639;&#20250;&#35805;&#27493;&#39588;&#29305;&#24449;&#26041;&#38754;&#25928;&#26524;&#26368;&#22909;&#65292;&#24182;&#19988;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#21407;&#22987;&#25351;&#20196;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#38405;&#35835;&#30456;&#21516;&#30340;&#25351;&#20196;&#30456;&#27604;&#65292;&#20250;&#35805;&#21161;&#25163;&#22312;&#25191;&#34892;&#22797;&#26434;&#25351;&#20196;&#26102;&#24448;&#24448;&#20250;&#38754;&#20020;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#21147;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24403;&#20250;&#35805;&#21161;&#25163;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#30340;&#27493;&#39588;&#26102;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#32467;&#26500;&#21270;&#20026;&#21512;&#36866;&#38271;&#24230;&#21644;&#22797;&#26434;&#24230;&#30340;&#21487;&#31649;&#29702;&#20449;&#24687;&#12290;&#26412;&#25991;&#38024;&#23545;&#39135;&#35889;&#39046;&#22495;&#65292;&#23558;&#32467;&#26500;&#21270;&#25351;&#20196;&#36716;&#21270;&#20026;&#20250;&#35805;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#26681;&#25454;&#20250;&#35805;&#22330;&#26223;&#27880;&#37322;&#20102;&#25351;&#20196;&#30340;&#32467;&#26500;&#65292;&#20197;&#20102;&#35299;&#35813;&#22330;&#26223;&#20013;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;&#20026;&#20102;&#35745;&#31639;&#19978;&#36848;&#20250;&#35805;&#27493;&#39588;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;&#26631;&#35760;&#30340;&#26041;&#27861;&#25928;&#26524;&#26368;&#22909;&#12290;&#36827;&#19968;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#20542;&#21521;&#20110;&#25509;&#21463;&#38271;&#24230;&#21644;&#22797;&#26434;&#24230;&#36866;&#20013;&#30340;&#27493;&#39588;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#21407;&#22987;&#22522;&#20110;&#32593;&#39029;&#30340;&#25351;&#20196;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step's characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11259</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#20026;&#35768;&#22810;&#38750;&#33521;&#35821;&#35821;&#35328;&#29256;&#26412;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21253;&#25324;BERT&#12289;RoBERTa&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20248;&#21183;&#65292;&#20294;&#22312;&#28041;&#21450;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23454;&#26045;&#21644;&#35780;&#20272;&#33879;&#21517;&#30340;&#20165;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BART&#12289;T5&#21644;BERT2BERT&#39118;&#26684;&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#35821;&#29256;&#26412;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#37325;&#36848;&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;BART&#21644;T5&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#31354;&#20013;&#25112;&#26007;&#26426;&#21160;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35757;&#32451;&#20302;&#23618;&#31574;&#30053;&#23454;&#29616;&#20934;&#30830;&#30340;&#21333;&#20301;&#25112;&#26007;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11247</link><description>&lt;p&gt;
&#38024;&#23545;&#31354;&#20013;&#25112;&#26007;&#26426;&#21160;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering. (arXiv:2309.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#31354;&#20013;&#25112;&#26007;&#26426;&#21160;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35757;&#32451;&#20302;&#23618;&#31574;&#30053;&#23454;&#29616;&#20934;&#30830;&#30340;&#21333;&#20301;&#25112;&#26007;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#27169;&#25311;&#31354;&#20013;&#23545;&#31354;&#26684;&#26007;&#22330;&#26223;&#27491;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#39640;&#32500;&#29366;&#24577;&#21644;&#34892;&#21160;&#31354;&#38388;&#12289;&#24773;&#20917;&#20449;&#24687;&#30340;&#39640;&#22797;&#26434;&#24615;&#65288;&#22914;&#19981;&#23436;&#32654;&#21644;&#31579;&#36873;&#20449;&#24687;&#12289;&#38543;&#26426;&#24615;&#12289;&#20851;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#65289;&#20197;&#21450;&#38750;&#32447;&#24615;&#39134;&#34892;&#21160;&#21147;&#23398;&#32473;&#20934;&#30830;&#30340;&#31354;&#25112;&#20915;&#31574;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#28041;&#21450;&#22810;&#20010;&#24322;&#26500;&#26234;&#33021;&#20307;&#26102;&#65292;&#36825;&#20123;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#20010;&#24322;&#26500;&#26234;&#33021;&#20307;&#30340;&#31354;&#23545;&#31354;&#26684;&#26007;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#30340;&#25277;&#35937;&#65292;&#24322;&#26500;&#30340;&#20302;&#23618;&#31574;&#30053;&#25511;&#21046;&#21333;&#20010;&#21333;&#20301;&#30340;&#34892;&#21160;&#65292;&#24182;&#19988;&#22312;&#25972;&#20307;&#20219;&#21153;&#30446;&#26631;&#19979;&#65292;&#39640;&#23618;&#25351;&#25381;&#31574;&#30053;&#21457;&#20986;&#23439;&#35266;&#21629;&#20196;&#12290;&#23545;&#20110;&#20934;&#30830;&#30340;&#21333;&#20301;&#25112;&#26007;&#25511;&#21046;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20302;&#23618;&#31574;&#30053;&#12290;&#20182;&#20204;&#30340;&#35757;&#32451;&#26159;&#20197;&#19968;&#31181;&#23398;&#20064;&#26041;&#24335;&#32452;&#32455;&#36215;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learnin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#65292;&#21033;&#29992;&#36923;&#36753;&#21464;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#19982;&#29305;&#23450;&#25512;&#29702;&#31639;&#27861;&#26080;&#20851;&#30340;&#25260;&#21319;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27010;&#29575;&#25512;&#29702;&#26597;&#35810;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.11236</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#33394;&#24425;&#20256;&#36882;&#65306;&#24102;&#26377;&#20132;&#25442;&#22240;&#23376;&#30340;&#25260;&#21319;&#27169;&#22411;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Colour Passing Revisited: Lifted Model Construction with Commutative Factors. (arXiv:2309.11236v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#65292;&#21033;&#29992;&#36923;&#36753;&#21464;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#19982;&#29305;&#23450;&#25512;&#29702;&#31639;&#27861;&#26080;&#20851;&#30340;&#25260;&#21319;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27010;&#29575;&#25512;&#29702;&#26597;&#35810;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#20132;&#25442;&#22240;&#23376;&#30340;&#25260;&#21319;&#27169;&#22411;&#26500;&#24314;&#21487;&#20197;&#21033;&#29992;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#23545;&#39046;&#22495;&#22823;&#23567;&#30340;&#21487;&#34892;&#30340;&#27010;&#29575;&#25512;&#29702;&#12290;&#20026;&#20102;&#24212;&#29992;&#25260;&#21319;&#25512;&#29702;&#65292;&#24517;&#39035;&#33719;&#24471;&#19968;&#20010;&#25260;&#21319;&#34920;&#31034;&#65292;&#24182;&#19988;&#20026;&#20102;&#36825;&#26679;&#20570;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25152;&#35859;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#12290;&#19981;&#36807;&#65292;&#25105;&#20204;&#21457;&#29616;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#22312;&#26500;&#24314;&#25260;&#21319;&#34920;&#31034;&#26102;&#24573;&#30053;&#20102;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#65292;&#21033;&#29992;&#36923;&#36753;&#21464;&#37327;&#26500;&#24314;&#19968;&#20010;&#19982;&#29305;&#23450;&#25512;&#29702;&#31639;&#27861;&#26080;&#20851;&#30340;&#25260;&#21319;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;&#31163;&#32447;&#38454;&#27573;&#21033;&#29992;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#26816;&#27979;&#20986;&#26356;&#22810;&#30340;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#21387;&#32553;&#29575;&#65292;&#20351;&#24471;&#24212;&#29992;&#35813;&#27169;&#22411;&#26102;&#30340;&#27010;&#29575;&#25512;&#29702;&#26597;&#35810;&#36895;&#24230;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes. To apply lifted inference, a lifted representation has to be obtained, and to do so, the so-called colour passing algorithm is the state of the art. The colour passing algorithm, however, is bound to a specific inference algorithm and we found that it ignores commutativity of factors while constructing a lifted representation. We contribute a modified version of the colour passing algorithm that uses logical variables to construct a lifted representation independent of a specific inference algorithm while at the same time exploiting commutativity of factors during an offline-step. Our proposed algorithm efficiently detects more symmetries than the state of the art and thereby drastically increases compression, yielding significantly faster online query times for probabilistic inference when the resulting model is applied.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#20114;&#21160;&#20013;&#22914;&#20309;&#21033;&#29992;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22768;&#26126;&#24615;&#35268;&#33539;&#26469;&#36830;&#25509;&#20154;&#20204;&#20197;&#24110;&#21161;&#20182;&#20204;&#35299;&#20915;&#26085;&#24120;&#38382;&#39064;&#12290;&#35797;&#28857;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#36873;&#25321;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#22810;&#26679;&#24615;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#24182;&#24471;&#21040;&#20102;&#29992;&#25143;&#30340;&#39640;&#24230;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2309.11224</link><description>&lt;p&gt;
&#22312;&#32447;&#20114;&#21160;&#20013;&#21033;&#29992;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diversity in Online Interactions. (arXiv:2309.11224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#20114;&#21160;&#20013;&#22914;&#20309;&#21033;&#29992;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22768;&#26126;&#24615;&#35268;&#33539;&#26469;&#36830;&#25509;&#20154;&#20204;&#20197;&#24110;&#21161;&#20182;&#20204;&#35299;&#20915;&#26085;&#24120;&#38382;&#39064;&#12290;&#35797;&#28857;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#36873;&#25321;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#22810;&#26679;&#24615;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#24182;&#24471;&#21040;&#20102;&#29992;&#25143;&#30340;&#39640;&#24230;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#36830;&#25509;&#20154;&#20204;&#26469;&#24110;&#21161;&#20182;&#20204;&#35299;&#20915;&#26085;&#24120;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22768;&#26126;&#24615;&#35268;&#33539;&#26469;&#35843;&#33410;&#22312;&#32447;&#20114;&#21160;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#22312;&#36830;&#25509;&#20154;&#20204;&#26102;&#21033;&#29992;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#22823;&#23398;&#32593;&#31449;&#19978;&#36827;&#34892;&#20102;&#35797;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#36873;&#25321;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#22810;&#26679;&#24615;&#30456;&#23545;&#25104;&#21151;&#65292;&#24182;&#24471;&#21040;&#20102;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the issue of connecting people online to help them find support with their day-to-day problems. We make use of declarative norms for mediating online interactions, and we specifically focus on the issue of leveraging diversity when connecting people. We run pilots at different university sites, and the results show relative success in the diversity of the selected profiles, backed by high user satisfaction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11206</link><description>&lt;p&gt;
&#25552;&#21462;-&#25913;&#20889;-&#22238;&#31572;&#65306;&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#35760;&#24518;&#25152;&#26377;&#19990;&#30028;&#30693;&#35782;&#65292;&#23588;&#20854;&#26159;&#38271;&#23614;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20197;&#22686;&#24378;LLMs&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;KGQA&#20013;LLMs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#32570;&#20047;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#29702;&#34920;&#36848;KG&#30693;&#35782;&#65292;&#21363;&#24573;&#30053;&#20102;KG&#34920;&#31034;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;KG&#30693;&#35782;&#36716;&#21270;&#20026;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25991;&#26412;&#21270;&#38472;&#36848;&#65292;&#29992;&#20110;KGQA&#12290;&#22522;&#20110;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;KGQA&#20219;&#21153;&#30340;&#22686;&#24378;&#22411;KG-to-Text LLMS&#26694;&#26550;&#12290;&#22312;&#20960;&#20010;KGQA&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;KG-to-Text&#22686;&#24378;LLMs&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#31867;&#32534;&#32455;&#22270;&#26696;&#12290;&#27169;&#22411;&#37319;&#29992;Inception ResNet-V2&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#12289;&#23545;&#25968;&#25439;&#22833;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11202</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#32534;&#32455;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
Using Artificial Intelligence for the Automation of Knitting Patterns. (arXiv:2309.11202v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#31867;&#32534;&#32455;&#22270;&#26696;&#12290;&#27169;&#22411;&#37319;&#29992;Inception ResNet-V2&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#12289;&#23545;&#25968;&#25439;&#22833;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#32455;&#22270;&#26696;&#26159;&#21019;&#24314;&#21644;&#35774;&#35745;&#38024;&#32455;&#26448;&#26009;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#22270;&#26696;&#26159;&#36890;&#36807;&#38750;&#27491;&#24335;&#36884;&#24452;&#20256;&#25480;&#30340;&#65292;&#20294;&#30001;&#20110;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20219;&#20309;&#23545;&#32534;&#32455;&#24863;&#20852;&#36259;&#30340;&#20154;&#37117;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#22270;&#26696;&#20316;&#20026;&#25351;&#23548;&#24320;&#22987;&#32534;&#32455;&#12290;&#23613;&#31649;&#32534;&#32455;&#22823;&#22810;&#26159;&#20010;&#29233;&#22909;&#65292;&#38500;&#20102;&#21033;&#29992;&#19987;&#38376;&#30340;&#32534;&#32455;&#26426;&#36827;&#34892;&#24037;&#19994;&#21046;&#36896;&#20043;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#32534;&#32455;&#30340;&#24212;&#29992;&#36824;&#19981;&#22914;&#20854;&#20182;&#39046;&#22495;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#20351;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#36827;&#34892;&#32534;&#32455;&#22270;&#26696;&#20998;&#31867;&#26159;&#21542;&#21487;&#34892;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35782;&#21035;&#21644;&#20998;&#31867;&#32534;&#32455;&#22270;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;Inception ResNet-V2&#26159;&#35813;&#27169;&#22411;&#20013;&#20027;&#35201;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#12290;&#20351;&#29992;&#20934;&#30830;&#29575;&#12289;&#23545;&#25968;&#25439;&#22833;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;&#27169;&#22411;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Knitting patterns are a crucial component in the creation and design of knitted materials. Traditionally, these patterns were taught informally, but thanks to advancements in technology, anyone interested in knitting can use the patterns as a guide to start knitting. Perhaps because knitting is mostly a hobby, with the exception of industrial manufacturing utilising specialised knitting machines, the use of Al in knitting is less widespread than its application in other fields. However, it is important to determine whether knitted pattern classification using an automated system is viable. In order to recognise and classify knitting patterns. Using data augmentation and a transfer learning technique, this study proposes a deep learning model. The Inception ResNet-V2 is the main feature extraction and classification algorithm used in the model. Metrics like accuracy, logarithmic loss, F1-score, precision, and recall score were used to evaluate the model. The model evaluation's findings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#35748;&#35777;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32508;&#36848;&#20102;&#24050;&#32463;&#24320;&#21457;&#30340;&#29992;&#20110;&#30830;&#20445;AI&#20915;&#31574;&#23433;&#20840;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.11196</link><description>&lt;p&gt;
&#20309;&#26102;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#32593;&#32476;&#35748;&#35777;&#30340;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When to Trust AI: Advances and Challenges for Certification of Neural Networks. (arXiv:2309.11196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#35748;&#35777;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32508;&#36848;&#20102;&#24050;&#32463;&#24320;&#21457;&#30340;&#29992;&#20110;&#30830;&#20445;AI&#20915;&#31574;&#23433;&#20840;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#27491;&#20934;&#22791;&#22312;&#33258;&#20027;&#31995;&#32479;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36827;&#34892;&#37096;&#32626;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26469;&#35828;&#65292;&#26089;&#26399;&#37319;&#29992;AI&#25216;&#26415;&#24182;&#19981;&#26159;&#27809;&#26377;&#38382;&#39064;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#20854;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#20174;&#38271;&#36828;&#26469;&#30475;&#65292;&#38656;&#35201;&#24320;&#21457;&#36866;&#24403;&#30340;&#23433;&#20840;&#20445;&#35777;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#21487;&#36991;&#20813;&#30340;&#31995;&#32479;&#25925;&#38556;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#12290;&#26412;&#25991;&#20197;&#35748;&#35777;&#21644;&#21487;&#35299;&#37322;&#24615;&#20026;&#37325;&#28857;&#65292;&#32508;&#36848;&#20102;&#24050;&#32463;&#24320;&#21457;&#30340;&#29992;&#20110;&#30830;&#20445;AI&#20915;&#31574;&#23433;&#20840;&#30340;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LAGCL&#65289;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38271;&#23614;&#22686;&#24378;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#26469;&#35299;&#20915;&#22836;&#23614;&#33410;&#28857;&#20043;&#38388;&#30340;&#26174;&#33879;&#24230;&#24046;&#24322;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#24230;&#22343;&#34913;&#26426;&#21046;&#26469;&#24179;&#34913;&#25968;&#25454;&#22686;&#24378;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.11177</link><description>&lt;p&gt;
&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Long-tail Augmented Graph Contrastive Learning for Recommendation. (arXiv:2309.11177v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LAGCL&#65289;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38271;&#23614;&#22686;&#24378;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#26469;&#35299;&#20915;&#22836;&#23614;&#33410;&#28857;&#20043;&#38388;&#30340;&#26174;&#33879;&#24230;&#24046;&#24322;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#24230;&#22343;&#34913;&#26426;&#21046;&#26469;&#24179;&#34913;&#25968;&#25454;&#22686;&#24378;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#24050;&#32463;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#39640;&#38454;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#36890;&#24120;&#36935;&#21040;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;GCN&#30340;&#25512;&#33616;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24341;&#20837;&#33258;&#30417;&#30563;&#30340;&#20449;&#21495;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;&#22836;&#21644;&#23614;&#33410;&#28857;&#20043;&#38388;&#26174;&#33879;&#24230;&#24046;&#24322;&#30340;&#32771;&#34385;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#38750;&#22343;&#21248;&#30340;&#34920;&#31034;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25512;&#33616;&#30340;&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LAGCL&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#38271;&#23614;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#34917;&#20805;&#39044;&#27979;&#30340;&#37051;&#23621;&#20449;&#24687;&#26469;&#22686;&#24378;&#23614;&#33410;&#28857;&#65292;&#24182;&#22522;&#20110;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#12290;&#20026;&#20102;&#20351;&#25968;&#25454;&#22686;&#24378;&#30340;&#31243;&#24230;&#24179;&#34913;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#22343;&#34913;&#26426;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LAGCL&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. However, these methods usually encounter data sparsity issue in real-world scenarios. To address this issue, GCN-based recommendation methods employ contrastive learning to introduce self-supervised signals. Despite their effectiveness, these methods lack consideration of the significant degree disparity between head and tail nodes. This can lead to non-uniform representation distribution, which is a crucial factor for the performance of contrastive learning methods. To tackle the above issue, we propose a novel Long-tail Augmented Graph Contrastive Learning (LAGCL) method for recommendation. Specifically, we introduce a learnable long-tail augmentation approach to enhance tail nodes by supplementing predicted neighbor information, and generate contrastive views based on the resulting augmented graph. To make the data augmentation sch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.11166</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#35789;&#32423;&#25200;&#21160;&#30495;&#30340;&#20855;&#26377;&#40065;&#26834;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#27169;&#21644;&#33021;&#21147;&#19978;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#38500;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36991;&#20813;&#23545;&#29305;&#23450;&#25552;&#31034;&#30340;&#28608;&#28872;&#21453;&#39304;&#22806;&#65292;&#30830;&#20445;LLM&#30340;&#36131;&#20219;&#24615;&#36824;&#38656;&#35201;&#20851;&#27880;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#20855;&#26377;&#39044;&#23450;&#20041;&#30417;&#30563;&#26631;&#31614;&#30340;&#20256;&#32479;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36825;&#19982;&#24403;&#20195;LLMs&#30340;&#20986;&#33394;&#29983;&#25104;&#33021;&#21147;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#29702;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#26469;&#35780;&#20272;LLMs&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21512;&#29702;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;TREvaL&#65289;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;TREval&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24335;&#38382;&#39064;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#21644;&#20248;&#21270;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;ProtoExplorer&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11155</link><description>&lt;p&gt;
ProtoExplorer&#65306;&#36890;&#36807;&#21407;&#22411;&#25506;&#32034;&#21644;&#20248;&#21270;&#65292;&#23545;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21462;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement. (arXiv:2309.11155v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#21644;&#20248;&#21270;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;ProtoExplorer&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#20026;&#20154;&#31867;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22797;&#26434;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#21644;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#30340;&#20986;&#29616;&#65292;&#36825;&#19968;&#28857;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20351;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22312;&#21462;&#35777;&#29615;&#22659;&#20013;&#23545;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#30340;&#20998;&#26512;&#12290;&#23613;&#31649;&#21407;&#22411;&#22522;&#26041;&#27861;&#24050;&#32463;&#34987;&#24341;&#20837;&#29992;&#20110;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#20294;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#20351;&#29992;&#20173;&#28982;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#21407;&#22411;&#24448;&#24448;&#36807;&#20110;&#30456;&#20284;&#65292;&#21487;&#35299;&#37322;&#24615;&#22312;&#19981;&#21516;&#21407;&#22411;&#20043;&#38388;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21407;&#22411;&#23398;&#20064;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#27969;&#31243;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#27492;&#27169;&#22411;&#25552;&#20986;&#20102;ProtoExplorer&#65292;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#21644;&#20248;&#21270;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#12290;ProtoExplorer&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#21644;&#26102;&#38388;&#36807;&#28388;&#21407;&#22411;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-stakes settings, Machine Learning models that can provide predictions that are interpretable for humans are crucial. This is even more true with the advent of complex deep learning based models with a huge number of tunable parameters. Recently, prototype-based methods have emerged as a promising approach to make deep learning interpretable. We particularly focus on the analysis of deepfake videos in a forensics context. Although prototype-based methods have been introduced for the detection of deepfake videos, their use in real-world scenarios still presents major challenges, in that prototypes tend to be overly similar and interpretability varies between prototypes. This paper proposes a Visual Analytics process model for prototype learning, and, based on this, presents ProtoExplorer, a Visual Analytics system for the exploration and refinement of prototype-based deepfake detection models. ProtoExplorer offers tools for visualizing and temporally filtering prototype-based pre
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21040;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#20013;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#35821;&#20041;&#28304;&#32534;&#30721;&#12289;&#35821;&#20041;&#20449;&#36947;&#32534;&#30721;&#21644;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#31561;&#21019;&#26032;&#31639;&#27861;&#26469;&#25552;&#39640;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11127</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#36890;&#20449;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation. (arXiv:2309.11127v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21040;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#20013;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#35821;&#20041;&#28304;&#32534;&#30721;&#12289;&#35821;&#20041;&#20449;&#36947;&#32534;&#30721;&#21644;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#31561;&#21019;&#26032;&#31639;&#27861;&#26469;&#25552;&#39640;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21040;&#26032;&#20852;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#33539;&#24335;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;LSC&#65289;&#26694;&#26550;&#12290;&#22312;LSC&#20013;&#65292;&#26426;&#22120;&#20351;&#29992;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#35299;&#37322;&#21644;&#25805;&#20316;&#30340;&#20154;&#31867;&#35821;&#35328;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#65292;&#20197;&#25552;&#39640;SC&#25928;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;LSC&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#21019;&#26032;&#31639;&#27861;&#65306;1&#65289;&#35821;&#20041;&#28304;&#32534;&#30721;&#65288;SSC&#65289;&#65292;&#23558;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;&#25104;&#25429;&#25417;&#25552;&#31034;&#30340;&#21477;&#27861;&#26412;&#36136;&#30340;&#20851;&#38190;&#22836;&#35789;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#20986;&#29616;&#39034;&#24207;&#20197;&#20445;&#25345;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#65307;2&#65289;&#35821;&#20041;&#20449;&#36947;&#32534;&#30721;&#65288;SCC&#65289;&#65292;&#36890;&#36807;&#29992;&#26356;&#38271;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#22836;&#35789;&#26469;&#25552;&#39640;&#23545;&#38169;&#35823;&#30340;&#23481;&#38169;&#24615;&#65307;3&#65289;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;SKD&#65289;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#21548;&#20247;&#30340;&#35821;&#35328;&#39118;&#26684;&#26469;&#29983;&#25104;&#23450;&#21046;&#30340;&#25552;&#31034;&#12290;&#22312;&#36880;&#27493;&#36827;&#34892;&#30340;&#36890;&#20449;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;LSC&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive te
&lt;/p&gt;</description></item><item><title>AttentionMix&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#12290;&#22312;&#19977;&#20010;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AttentionMix&#22312;NLP&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#20013;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;Mixup&#26426;&#21046;&#30340;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#21644;&#26222;&#36890;BERT&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11104</link><description>&lt;p&gt;
AttentionMix: &#19968;&#31181;&#22522;&#20110;BERT&#27880;&#24847;&#26426;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11104
&lt;/p&gt;
&lt;p&gt;
AttentionMix&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#12290;&#22312;&#19977;&#20010;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AttentionMix&#22312;NLP&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#20013;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;Mixup&#26426;&#21046;&#30340;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#21644;&#26222;&#36890;BERT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#20197;&#24341;&#23548;&#26041;&#24335;&#25191;&#34892;&#22270;&#20687;&#28151;&#21512;&#30340;&#21518;&#32487;&#26041;&#27861;&#12290;&#23558;Mixup&#24605;&#24819;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#23558;Mixup&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AttentionMix&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#34429;&#28982;&#26412;&#25991;&#20851;&#27880;BERT&#27880;&#24847;&#26426;&#21046;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;AttentionMix&#22312;3&#20010;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#25152;&#26377;&#19977;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#21033;&#29992;Mixup&#26426;&#21046;&#21644;&#26222;&#36890;BERT&#26041;&#27861;&#30340;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#12290;&#32467;&#26524;&#35777;&#23454;&#65292;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;NLP&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;TT-rules&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#25903;&#25345;&#20108;&#20998;&#31867;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#19988;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.11101</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#21017;&#27169;&#22411;&#29992;&#20110;&#21307;&#30103;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making. (arXiv:2309.11101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;TT-rules&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#25903;&#25345;&#20108;&#20998;&#31867;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#19988;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#29702;&#35299;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#30495;&#20540;&#34920;&#35268;&#21017;&#8221;&#65288;TT-rules&#65289;&#65292;&#23427;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#30340;&#20840;&#23616;&#21644;&#31934;&#30830;&#21487;&#35299;&#37322;&#24615;&#24615;&#36136;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#30456;&#32467;&#21512;&#12290;TT-rules&#22522;&#20110;&#8220;&#30495;&#20540;&#34920;&#32593;&#32476;&#8221;&#65288;TTnet&#65289;&#26500;&#24314;&#65292;&#36825;&#26159;&#19968;&#26063;&#26368;&#21021;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20174;&#35757;&#32451;&#22909;&#30340;TTnet&#27169;&#22411;&#20013;&#25552;&#21462;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#35268;&#21017;$\mathcal{R}$&#26469;&#20135;&#29983;&#19982;TTnet&#30456;&#21516;&#36755;&#20986;&#30340;&#35268;&#21017;&#65288;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#65289;&#65292;TT-rules&#26377;&#25928;&#22320;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#25903;&#25345;&#23567;&#21040;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#20108;&#20998;&#31867;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#27010;&#36848;&#20102;&#26694;&#26550;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;TT-rules&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#35268;&#21017;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;dropout&#19981;&#30830;&#23450;&#24615;&#21644;&#36712;&#36857;&#37319;&#26679;&#65292;&#31283;&#23450;&#22320;&#39044;&#27979;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#65292;&#32416;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#25311;&#21512;&#35823;&#24046;&#65292;&#36807;&#28388;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#21644;&#23454;&#38469;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11089</link><description>&lt;p&gt;
&#32467;&#21512;Dropout&#19981;&#30830;&#23450;&#24615;&#19982;&#36712;&#36857;&#37319;&#26679;&#30340;&#23454;&#29992;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling. (arXiv:2309.11089v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;dropout&#19981;&#30830;&#23450;&#24615;&#21644;&#36712;&#36857;&#37319;&#26679;&#65292;&#31283;&#23450;&#22320;&#39044;&#27979;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#65292;&#32416;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#25311;&#21512;&#35823;&#24046;&#65292;&#36807;&#28388;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#21644;&#23454;&#38469;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#24314;&#31435;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25511;&#21046;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;dropout&#30340;&#27010;&#29575;&#38598;&#25104;&#21644;&#36712;&#36857;&#37319;&#26679;&#65288;DPETS&#65289;&#65292;&#22312;&#19968;&#20010;&#26694;&#26550;&#20013;&#36890;&#36807;Monte-Carlo dropout&#21644;&#36712;&#36857;&#37319;&#26679;&#26469;&#31283;&#23450;&#22320;&#39044;&#27979;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#29992;&#20110;&#32416;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#25311;&#21512;&#35823;&#24046;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#27010;&#29575;&#27169;&#22411;&#12290;&#20854;&#31574;&#30053;&#20013;&#30340;&#29366;&#24577;&#20256;&#25773;&#34987;&#25193;&#23637;&#29992;&#20110;&#28388;&#38500;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;Mujoco&#22522;&#20934;&#25511;&#21046;&#20219;&#21153;&#21644;&#19968;&#20010;&#23454;&#38469;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#19979;&#30340;&#35780;&#20272;&#65292;DPETS&#22312;&#24179;&#22343;&#22238;&#25253;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#20248;&#20110;&#30456;&#20851;MBRL&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#32988;&#36807;&#30693;&#21517;&#30340;&#26080;&#27169;&#22411;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The ope
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#22686;&#24378;&#25216;&#26415;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38169;&#35823;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#38598;&#25193;&#22823;&#21644;&#33258;&#20027;/&#20132;&#20114;&#24369;&#30417;&#30563;&#65292;&#23637;&#31034;&#20102;&#22312;&#24191;&#38420;&#30340;&#28216;&#25103;&#19990;&#30028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11077</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#22312;&#26631;&#27880;&#39640;&#25928;&#21487;&#35270;&#21270;&#38169;&#35823;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weak Supervision for Label Efficient Visual Bug Detection. (arXiv:2309.11077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11077
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#22686;&#24378;&#25216;&#26415;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38169;&#35823;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#38598;&#25193;&#22823;&#21644;&#33258;&#20027;/&#20132;&#20114;&#24369;&#30417;&#30563;&#65292;&#23637;&#31034;&#20102;&#22312;&#24191;&#38420;&#30340;&#28216;&#25103;&#19990;&#30028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#28216;&#25103;&#36827;&#21270;&#20026;&#24191;&#38420;&#12289;&#32454;&#33268;&#30340;&#19990;&#30028;&#65292;&#35270;&#35273;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#26085;&#30410;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#27979;&#35797;&#26041;&#27861;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#38590;&#20197;&#24212;&#23545;&#22823;&#37327;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#65292;&#23545;&#22823;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20005;&#37325;&#20381;&#36182;&#20173;&#28982;&#26159;&#19968;&#20010;&#21046;&#32422;&#22240;&#32032;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#28216;&#25103;&#29609;&#27861;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#22686;&#24378;&#25216;&#26415;&#29983;&#25104;&#29992;&#20110;&#39044;&#35757;&#32451;&#25110;&#22810;&#20219;&#21153;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#29992;&#20110;&#21518;&#32493;&#30340;&#35270;&#35273;&#38169;&#35823;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#25193;&#22823;&#37327;&#34920;&#21644;&#20419;&#36827;&#33258;&#20027;&#21644;&#20132;&#20114;&#24335;&#24369;&#30417;&#30563;&#65292;&#21253;&#25324;&#22522;&#20110;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;/&#25110;&#22522;&#20110;&#25991;&#26412;&#21644;&#20960;&#20309;&#25552;&#31034;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#24191;&#38420;&#30340;Giantmap&#28216;&#25103;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#23545;&#19968;&#20154;&#31216;&#29609;&#23478;&#21098;&#36753;/&#30896;&#25758;&#38169;&#35823;&#65288;FPPC&#65289;&#30340;&#26816;&#27979;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets &amp; self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effect
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24179;&#38138;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#33258;&#36866;&#24212;&#12289;&#21487;&#25193;&#23637;&#19988;&#25512;&#29702;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#37325;&#21472;&#29575;&#21644;&#24179;&#38138;&#32553;&#23567;&#22120;&#65292;&#35299;&#20915;&#20102;&#30862;&#29255;&#21270;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.11069</link><description>&lt;p&gt;
&#21160;&#24577;&#24179;&#38138;: &#39640;&#25928;&#20934;&#30830;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#30340;&#27169;&#22411;&#26080;&#20851;&#12289;&#33258;&#36866;&#24212;&#12289;&#21487;&#25193;&#23637;&#21644;&#25512;&#29702;&#25968;&#25454;&#20013;&#24515;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection. (arXiv:2309.11069v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11069
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24179;&#38138;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#33258;&#36866;&#24212;&#12289;&#21487;&#25193;&#23637;&#19988;&#25512;&#29702;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#37325;&#21472;&#29575;&#21644;&#24179;&#38138;&#32553;&#23567;&#22120;&#65292;&#35299;&#20915;&#20102;&#30862;&#29255;&#21270;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#24179;&#38138;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#33258;&#36866;&#24212;&#21644;&#21487;&#25193;&#23637;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25105;&#20204;&#30340;&#25512;&#29702;&#25968;&#25454;&#20013;&#24515;&#21270;&#21746;&#23398;&#20026;&#22522;&#30784;&#12290;&#21160;&#24577;&#24179;&#38138;&#20174;&#38750;&#37325;&#21472;&#30340;&#24179;&#38138;&#24320;&#22987;&#36827;&#34892;&#21021;&#22987;&#26816;&#27979;&#65292;&#24182;&#21033;&#29992;&#21160;&#24577;&#37325;&#21472;&#29575;&#21644;&#24179;&#38138;&#32553;&#23567;&#22120;&#12290;&#36825;&#31181;&#21452;&#37325;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30862;&#29255;&#21270;&#30340;&#29289;&#20307;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#23545;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#27491;&#21521;&#20256;&#36882;&#27425;&#25968;&#26469;&#20943;&#23567;&#35745;&#31639;&#24320;&#38144;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#25805;&#20316;&#29615;&#22659;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#37325;&#26032;&#26657;&#20934;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22823;&#23567;&#36807;&#28388;&#26426;&#21046;&#25552;&#39640;&#20102;&#19981;&#21516;&#22823;&#23567;&#29289;&#20307;&#30340;&#26816;&#27979;&#36136;&#37327;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#21160;&#24577;&#24179;&#38138;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#26080;&#20851;&#22343;&#21248;&#35009;&#21098;&#26041;&#27861;&#65292;&#20026;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#35774;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Dynamic Tiling, a model-agnostic, adaptive, and scalable approach for small object detection, anchored in our inference-data-centric philosophy. Dynamic Tiling starts with non-overlapping tiles for initial detections and utilizes dynamic overlapping rates along with a tile minimizer. This dual approach effectively resolves fragmented objects, improves detection accuracy, and minimizes computational overhead by reducing the number of forward passes through the object detection model. Adaptable to a variety of operational environments, our method negates the need for laborious recalibration. Additionally, our large-small filtering mechanism boosts the detection quality across a range of object sizes. Overall, Dynamic Tiling outperforms existing model-agnostic uniform cropping methods, setting new benchmarks for efficiency and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21021;&#27493;&#25506;&#32034;&#24615;&#35843;&#26597;&#30740;&#31350;&#20102;LLM&#30340;&#24187;&#35273;&#29616;&#35937;&#19982;&#25552;&#31034;&#35821;&#35328;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26356;&#24418;&#24335;&#21270;&#21644;&#20855;&#20307;&#30340;&#25552;&#31034;&#26377;&#21161;&#20110;&#20943;&#23569;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#20294;&#21487;&#35835;&#24615;&#30456;&#20851;&#30340;&#32467;&#26524;&#21017;&#19981;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.11064</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#24187;&#35273;&#19982;&#25552;&#31034;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#21487;&#35835;&#24615;&#12289;&#24418;&#24335;&#21270;&#21644;&#20855;&#20307;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness. (arXiv:2309.11064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21021;&#27493;&#25506;&#32034;&#24615;&#35843;&#26597;&#30740;&#31350;&#20102;LLM&#30340;&#24187;&#35273;&#29616;&#35937;&#19982;&#25552;&#31034;&#35821;&#35328;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26356;&#24418;&#24335;&#21270;&#21644;&#20855;&#20307;&#30340;&#25552;&#31034;&#26377;&#21161;&#20110;&#20943;&#23569;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#20294;&#21487;&#35835;&#24615;&#30456;&#20851;&#30340;&#32467;&#26524;&#21017;&#19981;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#24341;&#21457;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#31361;&#20986;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;LLM&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21508;&#31181;&#32531;&#35299;&#25216;&#26415;&#26469;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#28145;&#20837;&#30740;&#31350;&#20854;&#26681;&#26412;&#21407;&#22240;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20010;&#21021;&#27493;&#30340;&#25506;&#32034;&#24615;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#35821;&#35328;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#21487;&#35835;&#24615;&#12289;&#24418;&#24335;&#21270;&#21644;&#20855;&#20307;&#24615;&#65292;&#23545;&#24187;&#35273;&#21457;&#29983;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#24418;&#24335;&#21270;&#21644;&#20855;&#20307;&#24615;&#30340;&#25552;&#31034;&#20542;&#21521;&#20110;&#20943;&#23569;&#24187;&#35273;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#19982;&#21487;&#35835;&#24615;&#30456;&#20851;&#30340;&#32467;&#26524;&#26377;&#20123;&#19981;&#30830;&#23450;&#65292;&#26174;&#31034;&#20986;&#20102;&#28151;&#21512;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination. While various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. Consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. Our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. However, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.11054</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#24605;&#36335;&#38142;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24605;&#36335;&#38142;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#21508;&#31181;&#31243;&#24207;&#24605;&#36335;&#38142;&#65292;&#21253;&#25324;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#12289;&#27880;&#37322;&#25551;&#36848;&#31243;&#24207;&#21644;&#38750;&#25551;&#36848;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32534;&#31243;&#35821;&#35328;&#23545;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;Python&#21644;Wolfram&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;GSM8K&#12289;MATHQA&#21644;SVAMP&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20855;&#26377;30B&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#26126;&#26174;&#36229;&#36807;&#20102;GPT-3.5-turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#26356;&#22909;&#36873;&#25321;&#27604;Wolfram&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#32771;&#34385;&#22240;&#32032;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.11044</link><description>&lt;p&gt;
Clustered FedStack&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#22240;&#20854;&#21327;&#20316;&#23398;&#20064;&#21644;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#38750;&#29420;&#31435;&#21644;&#38750;&#29420;&#31435;&#20998;&#24067;&#65288;&#38750;IID&#65289;&#20197;&#21450;&#26412;&#22320;&#23458;&#25143;&#20043;&#38388;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#22242;&#38431;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#12289;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24050;&#21457;&#34920;&#30340;Stacked Federated Learning&#65288;FedStack&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;Clustered FedStack&#26694;&#26550;&#12290;&#26412;&#22320;&#23458;&#25143;&#31471;&#23558;&#20854;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#20010;&#20840;&#23616;&#27169;&#22411;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#22522;&#20110;&#20854;&#36755;&#20986;&#23618;&#26435;&#37325;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#32858;&#31867;&#26426;&#21046;&#65292;&#20998;&#21035;&#26159;K-Means&#12289;Agglomerative&#12289;DBSCAN&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26500;&#24314;&#28151;&#21512;&#20219;&#21153;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;NLP&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#20248;&#21270;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11042</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26500;&#24314;&#28151;&#21512;&#20219;&#21153;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;NLP&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#20248;&#21270;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#23610;&#23544;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#21644;&#22312;&#32447;&#37096;&#32626;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ALTER&#65292;&#19968;&#20010;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21442;&#25968;&lt;1B&#65289;&#19978;&#26377;&#25928;&#26500;&#24314;&#28151;&#21512;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#20197;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;NLP&#20219;&#21153;&#65292;&#24182;&#25429;&#25417;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#65292;&#20197;&#25903;&#25345;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;ALTER&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#28151;&#21512;&#65288;MTA&#65289;&#27169;&#22359;&#65292;&#20316;&#20026;&#24213;&#23618;&#27169;&#22411;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#25429;&#25417;&#20219;&#21153;&#20869;&#37096;&#30340;&#30693;&#35782;&#21644;&#20219;&#21153;&#38388;&#30340;&#30693;&#35782;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22312;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#20248;&#21270;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#22312;&#28151;&#21512;NLP&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with &lt;1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks 
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21644;&#26131;&#25193;&#23637;&#24615;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35299;&#20915;&#21160;&#24577;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.11039</link><description>&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#26368;&#36817;&#30340;&#24212;&#29992;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems. (arXiv:2309.11039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11039
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21644;&#26131;&#25193;&#23637;&#24615;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35299;&#20915;&#21160;&#24577;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#21463;&#21040;&#36890;&#20449;&#25216;&#26415;&#12289;&#20256;&#24863;&#22120;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36710;&#36742;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#21450;&#26102;&#20934;&#30830;&#22320;&#20915;&#31574;&#36710;&#36742;&#34892;&#20026;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#31227;&#21160;&#26080;&#32447;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36710;&#36742;&#20449;&#24687;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38754;&#20020;&#30528;&#25345;&#32493;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#24613;&#38656;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#24212;&#23545;&#21160;&#24577;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#21644;&#26131;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;ITS&#20013;FL&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;ITS&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#20174;&#21508;&#20010;&#35282;&#24230;&#38416;&#26126;&#24212;&#29992;FL&#30340;&#21160;&#26426;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;FL&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing depl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ModelGiF&#12290;&#36890;&#36807;&#22312;&#24322;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21516;&#36136;&#30340;&#34920;&#31034;&#65292;ModelGiF&#21487;&#36890;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#24615;&#20272;&#35745;&#12289;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#27169;&#22411;&#36951;&#24536;&#39564;&#35777;&#31561;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11013</link><description>&lt;p&gt;
ModelGiF: &#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#30340;&#26799;&#24230;&#22330;
&lt;/p&gt;
&lt;p&gt;
ModelGiF: Gradient Fields for Model Functional Distance. (arXiv:2309.11013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ModelGiF&#12290;&#36890;&#36807;&#22312;&#24322;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21516;&#36136;&#30340;&#34920;&#31034;&#65292;ModelGiF&#21487;&#36890;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#24615;&#20272;&#35745;&#12289;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#27169;&#22411;&#36951;&#24536;&#39564;&#35777;&#31561;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#35265;&#35777;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#21644;&#20844;&#24320;&#21457;&#24067;&#30340;&#35757;&#32451;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;&#21508;&#31181;&#30446;&#30340;&#30340;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#36827;&#34892;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20869;&#37096;&#24037;&#20316;&#30340;&#19981;&#36879;&#26126;&#24615;&#21644;&#20307;&#31995;&#32467;&#26500;&#25110;&#20219;&#21153;&#30340;&#24322;&#36136;&#24615;&#65292;&#37327;&#21270;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#22987;&#32456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#23398;&#20013;&#8220;&#22330;&#8221;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#27169;&#22411;&#26799;&#24230;&#22330;&#65288;&#31616;&#31216;ModelGiF&#65289;&#65292;&#20174;&#24322;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21516;&#36136;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#27599;&#20010;&#39044;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#19978;&#21807;&#19968;&#30830;&#23450;&#19968;&#20010;ModelGiF&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;ModelGiF&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;ModelGiF&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#30456;&#20851;&#24615;&#20272;&#35745;&#12289;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#27169;&#22411;&#36951;&#24536;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10987</link><description>&lt;p&gt;
Spiking NeRF&#65306;&#20351;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#31359;&#36879;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21033;&#29992;&#20854;&#20855;&#26377;&#28508;&#22312;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#33021;&#37327;&#25928;&#29575;&#21644;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#20197;&#22823;&#37327;&#33021;&#37327;&#28040;&#32791;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#28145;&#20837;&#25506;&#32034;&#20197;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#36827;&#34892;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33033;&#20914;NeRF&#65288;SpikingNeRF&#65289;&#65292;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;SNN&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;SNN&#23545;&#36752;&#23556;&#22330;&#30340;&#37325;&#24314;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#20197;&#22522;&#20110;&#33033;&#20914;&#12289;&#26080;&#20056;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;SpikingNeRF&#20013;&#65292;&#20809;&#32447;&#19978;&#30340;&#27599;&#20010;&#37319;&#26679;&#28857;&#21305;&#37197;&#21040;&#29305;&#23450;&#30340;&#26102;&#38388;&#27493;&#65292;&#24182;&#20197;&#28151;&#21512;&#26041;&#24335;&#34920;&#31034;&#65292;&#20854;&#20013;&#20307;&#32032;&#32593;&#26684;&#20063;&#24471;&#21040;&#32500;&#25252;&#12290;&#22522;&#20110;&#20307;&#32032;&#32593;&#26684;&#65292;&#30830;&#23450;&#37319;&#26679;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#34987;&#23631;&#34109;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25805;&#20316;&#20063;&#20250;&#20135;&#29983;&#19981;&#21487;&#36870;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32771;&#23519;GPT-4&#22312;&#32463;&#20856;&#20132;&#26131;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#20132;&#26131;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#33021;&#21147;&#65292;&#20197;&#39564;&#35777;&#20854;&#20316;&#20026;&#20132;&#26131;&#21592;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10982</link><description>&lt;p&gt;
GPT4&#26159;&#19968;&#20010;&#20248;&#31168;&#30340;&#20132;&#26131;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is GPT4 a Good Trader?. (arXiv:2309.10982v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32771;&#23519;GPT-4&#22312;&#32463;&#20856;&#20132;&#26131;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#20132;&#26131;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#33021;&#21147;&#65292;&#20197;&#39564;&#35777;&#20854;&#20316;&#20026;&#20132;&#26131;&#21592;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#21508;&#31181;&#35745;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#21033;&#29992;GPT-4&#30340;&#33021;&#21147;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#19982;&#29616;&#26377;&#22240;&#23376;&#24211;&#19981;&#37325;&#21472;&#30340;&#23450;&#37327;&#22240;&#23376;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#28212;&#26395;&#23454;&#29616;Alpha&#22238;&#25253;&#12290;&#19982;&#36825;&#20123;&#24037;&#20316;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#32771;&#23519;GPT-4&#23545;&#32463;&#20856;&#20132;&#26131;&#29702;&#35770;&#30340;&#29702;&#35299;&#20934;&#30830;&#24230;&#20197;&#21450;&#20854;&#22312;&#30495;&#23454;&#20132;&#26131;&#25968;&#25454;&#20998;&#26512;&#20013;&#24212;&#29992;&#20195;&#30721;&#35299;&#37322;&#33021;&#21147;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26679;&#30340;&#25506;&#32034;&#23545;&#20110;&#21028;&#26029;GPT-4&#29992;&#20110;&#20132;&#26131;&#30340;&#22522;&#26412;&#36923;&#36753;&#26159;&#21542;&#21487;&#38752;&#38750;&#24120;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#22823;&#22810;&#25968;&#20132;&#26131;&#29702;&#35770;&#20013;&#23384;&#22312;&#30340;&#35299;&#37322;&#33258;&#30001;&#24230;&#65292;&#25105;&#20204;&#24076;&#26395;&#20174;GPT&#20013;&#25552;&#28860;&#20986;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), particularly GPT-4, have demonstrated significant capabilities in various planning and reasoning tasks \cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there has been a surge of interest among researchers to harness the capabilities of GPT-4 for the automated design of quantitative factors that do not overlap with existing factor libraries, with an aspiration to achieve alpha returns \cite{webpagequant}. In contrast to these work, this study aims to examine the fidelity of GPT-4's comprehension of classic trading theories and its proficiency in applying its code interpreter abilities to real-world trading data analysis. Such an exploration is instrumental in discerning whether the underlying logic GPT-4 employs for trading is intrinsically reliable. Furthermore, given the acknowledged interpretative latitude inherent in most trading theories, we seek to distill more precise methodologies of deploying these theories from GPT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.10952</link><description>&lt;p&gt;
LMDX&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10952
&lt;/p&gt;
&lt;p&gt;
LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;&#35768;&#22810;&#29616;&#26377;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20852;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#20110;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#35768;&#22810;&#25991;&#26723;&#22788;&#29702;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#65292;&#32473;&#23450;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#27169;&#24335;&#12290;LLM&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;LLM&#20013;&#32570;&#20047;&#24067;&#23616;&#32534;&#30721;&#65292;&#36825;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25552;&#21462;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#32570;&#20047;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#26426;&#21046;&#65292;&#30830;&#20445;&#31572;&#26696;&#19981;&#26159;&#34394;&#26500;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#65288;LMDX&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;LLM&#36866;&#24212;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#12290;LMDX&#21487;&#20197;&#25552;&#21462;&#21333;&#19968;&#12289;&#37325;&#22797;&#21644;&#23618;&#27425;&#32467;&#26500;&#23454;&#20307;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#29702;&#35770;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20026;Pir\'a 2.0&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10945</link><description>&lt;p&gt;
Pir\'a 2.0&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#20010;&#20851;&#20110;&#28023;&#27915;&#12289;&#24052;&#35199;&#28023;&#23736;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10945
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20026;Pir\'a 2.0&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pir\'a&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#28023;&#27915;&#12289;&#24052;&#35199;&#28023;&#23736;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#26377;&#20851;&#36825;&#20123;&#20027;&#39064;&#30340;&#31185;&#23398;&#25688;&#35201;&#21644;&#25253;&#21578;&#30340;&#25910;&#34255;&#20013;&#26500;&#24314;&#32780;&#25104;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#35328;&#36164;&#28304;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27979;&#35797;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#21462;&#19987;&#23478;&#31185;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;Pir\'a&#23578;&#26410;&#24320;&#21457;&#20986;&#35814;&#32454;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#21019;&#24314;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#21033;&#29992;Pir\'a&#20316;&#20026;&#27979;&#35797;&#21508;&#31181;&#38382;&#31572;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;Pir\'a&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#23553;&#38381;&#29983;&#25104;&#38382;&#31572;&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#24320;&#25918;&#24335;&#38382;&#31572;&#12289;&#31572;&#26696;&#35302;&#21457;&#21644;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#12290;&#20316;&#20026;&#36825;&#19968;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#21046;&#20316;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#31934;&#36873;&#29256;&#26412;&#65292;&#20854;&#20013;&#20462;&#27491;&#20102;&#19968;&#20123;&#35821;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pir\'a. By creating these baselines, researchers can more easily utilize Pir\'a as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pir\'a dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#20351;&#31995;&#32479;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.10917</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#19978;&#19979;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#20351;&#31995;&#32479;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#34701;&#20837;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#35782;&#21035;&#35270;&#20026;&#22522;&#20110;&#39044;&#35757;&#32451;LLM&#30340;&#28151;&#21512;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#20197;&#21450;&#21487;&#36873;&#30340;&#25991;&#26412;&#26631;&#35760;&#26469;&#35757;&#32451;&#31995;&#32479;&#20197;&#35299;&#30721;&#26041;&#24335;&#23436;&#25104;&#36716;&#24405;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#20250;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#24403;&#25552;&#20379;&#39069;&#22806;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#26102;&#65292;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;6%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31454;&#20105;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#25972;&#20307;&#19978;&#23558;WER&#25552;&#39640;&#20102;7.5%&#65292;&#23545;&#20110;&#32597;&#35265;&#35789;&#35821;&#30340;WER&#25552;&#39640;&#20102;17%&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#19978;&#19979;&#25991;&#21270;RNN-T&#31995;&#32479;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#36801;&#31227;&#23398;&#20064;&#26469;&#25918;&#22823;&#33041;&#30005;&#20449;&#21495;&#36890;&#36335;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#30149;&#29702;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#35299;&#20915;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#20302;&#25104;&#26412;&#25307;&#21215;&#30495;&#23454;&#24739;&#32773;&#38431;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10910</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#36801;&#31227;&#23398;&#20064;&#26469;&#25918;&#22823;&#33041;&#30005;&#20449;&#21495;&#36890;&#36335;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning. (arXiv:2309.10910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#36801;&#31227;&#23398;&#20064;&#26469;&#25918;&#22823;&#33041;&#30005;&#20449;&#21495;&#36890;&#36335;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#30149;&#29702;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#35299;&#20915;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#20302;&#25104;&#26412;&#25307;&#21215;&#30495;&#23454;&#24739;&#32773;&#38431;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#21644;&#33041;&#27963;&#21160;&#35299;&#30721;&#30340;&#30149;&#29702;&#35786;&#26029;&#22312;&#29702;&#35299;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#35786;&#26029;&#21644;&#26377;&#25928;&#30340;&#27835;&#30103;&#30340;&#28508;&#21147;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#22312;&#22810;&#20010;&#23618;&#38754;&#19978;&#37117;&#23384;&#22312;&#21508;&#31181;&#25361;&#25112;&#12290;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#33539;&#22260;&#24773;&#26223;&#19979;&#65292;&#30001;&#20110;&#39640;&#26114;&#30340;&#25307;&#21215;&#25104;&#26412;&#23548;&#33268;&#30495;&#23454;&#24739;&#32773;&#38431;&#21015;&#26377;&#38480;&#65292;&#20984;&#26174;&#20102;&#25193;&#23637;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#20984;&#26174;&#25968;&#25454;&#21644;&#27169;&#22411;&#25193;&#23637;&#20197;&#21450;&#36328;&#25968;&#25454;&#38598;&#30693;&#35782;&#36801;&#31227;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36890;&#36807;&#25968;&#25454;&#25193;&#23637;&#21487;&#20197;&#33719;&#24471;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#34920;&#26126;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#21644;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology diagnosis based on EEG signals and decoding brain activity holds immense importance in understanding neurological disorders. With the advancement of artificial intelligence methods and machine learning techniques, the potential for accurate data-driven diagnoses and effective treatments has grown significantly. However, applying machine learning algorithms to real-world datasets presents diverse challenges at multiple levels. The scarcity of labelled data, especially in low regime scenarios with limited availability of real patient cohorts due to high costs of recruitment, underscores the vital deployment of scaling and transfer learning techniques. In this study, we explore a real-world pathology classification task to highlight the effectiveness of data and model scaling and cross-dataset knowledge transfer. As such, we observe varying performance improvements through data scaling, indicating the need for careful evaluation and labelling. Additionally, we identify the chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#26469;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10908</link><description>&lt;p&gt;
&#22810;&#21103;&#26412;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Multicopy Reinforcement Learning Agents. (arXiv:2309.10908v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#26469;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#25110;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#21333;&#20010;&#26234;&#33021;&#20307;&#20219;&#21153;&#12290;&#22914;&#26524;&#29615;&#22659;&#22024;&#26434;&#65292;&#24182;&#19988;&#21333;&#20010;&#26234;&#33021;&#20307;&#21103;&#26412;&#26377;&#26102;&#26080;&#27861;&#23436;&#25104;&#20219;&#21153;&#65292;&#21017;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#21103;&#26412;&#38382;&#39064;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#22914;&#20309;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#30340;&#26234;&#33021;&#21161;&#25163;&#65288;AIIA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20808;&#36827;&#30340;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25552;&#20379;&#20132;&#20114;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#31616;&#21270;&#20449;&#24687;&#33719;&#21462;&#12289;&#20419;&#36827;&#30693;&#35782;&#35780;&#20272;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#26469;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#20854;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#22238;&#31572;&#23398;&#29983;&#38382;&#39064;&#12289;&#29983;&#25104;&#27979;&#39564;&#21644;&#38378;&#21345;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#21487;&#23545;&#39640;&#31561;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#34394;&#25311;&#21161;&#25945;&#65288;VTA&#65289;&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#26524;&#12289;&#21442;&#19982;&#24230;&#21644;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.10892</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#26234;&#33021;&#21161;&#25163;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education. (arXiv:2309.10892v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#30340;&#26234;&#33021;&#21161;&#25163;&#65288;AIIA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20808;&#36827;&#30340;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25552;&#20379;&#20132;&#20114;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#31616;&#21270;&#20449;&#24687;&#33719;&#21462;&#12289;&#20419;&#36827;&#30693;&#35782;&#35780;&#20272;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#26469;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#20854;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#22238;&#31572;&#23398;&#29983;&#38382;&#39064;&#12289;&#29983;&#25104;&#27979;&#39564;&#21644;&#38378;&#21345;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#21487;&#23545;&#39640;&#31561;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#34394;&#25311;&#21161;&#25945;&#65288;VTA&#65289;&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#26524;&#12289;&#21442;&#19982;&#24230;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#30340;&#26234;&#33021;&#21161;&#25163;(AIIA)&#65292;&#29992;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290; AIIA&#31995;&#32479;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23398;&#20064;&#24179;&#21488;&#12290;&#35813;&#24179;&#21488;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#30340;&#31616;&#21333;&#33719;&#21462;&#12289;&#20419;&#36827;&#30693;&#35782;&#35780;&#20272;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#26469;&#20943;&#23569;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20197;&#36866;&#24212;&#20010;&#20307;&#38656;&#27714;&#21644;&#23398;&#20064;&#39118;&#26684;&#12290; AIIA&#30340;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#22238;&#31572;&#23398;&#29983;&#30340;&#38382;&#39064;&#12289;&#29983;&#25104;&#27979;&#39564;&#21644;&#38378;&#21345;&#65292;&#20197;&#21450;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#26377;&#21487;&#33021;&#23545;&#39640;&#31561;&#25945;&#32946;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#25945;&#23398;&#21161;&#25163;(VTA)&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#20026;&#24320;&#21457;&#21019;&#26032;&#30340;&#25945;&#32946;&#24037;&#20855;&#25552;&#20379;&#25351;&#23548;&#65292;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#26524;&#12289;&#21442;&#19982;&#24230;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework, Artificial Intelligence-Enabled Intelligent Assistant (AIIA), for personalized and adaptive learning in higher education. The AIIA system leverages advanced AI and Natural Language Processing (NLP) techniques to create an interactive and engaging learning platform. This platform is engineered to reduce cognitive load on learners by providing easy access to information, facilitating knowledge assessment, and delivering personalized learning support tailored to individual needs and learning styles. The AIIA's capabilities include understanding and responding to student inquiries, generating quizzes and flashcards, and offering personalized learning pathways. The research findings have the potential to significantly impact the design, implementation, and evaluation of AI-enabled Virtual Teaching Assistants (VTAs) in higher education, informing the development of innovative educational tools that can enhance student learning outcomes, engagement, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SALT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10891</link><description>&lt;p&gt;
&#33258;&#25105;&#22686;&#24378;&#25913;&#36827;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SALT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#20801;&#35768;&#22312;&#20855;&#26377;&#26356;&#20805;&#36275;&#35757;&#32451;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#20808;&#21069;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#21162;&#21147;&#20351;&#29992;&#24179;&#34892;&#35821;&#26009;&#24211;&#12289;&#21452;&#35821;&#35789;&#20856;&#25110;&#20854;&#20182;&#26631;&#27880;&#23545;&#40784;&#25968;&#25454;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#65292;&#36825;&#20123;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#33719;&#21462;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;SALT&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#65292;&#32780;&#19981;&#38656;&#35201;&#36825;&#20123;&#22806;&#37096;&#25968;&#25454;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;SALT&#26377;&#25928;&#22320;&#33976;&#39311;&#20102;&#22810;&#35821;&#35328;PLM&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;&#22312;XNLI&#21644;PAWS-X&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/luka-group/SALT&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#33258;&#21160;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#20013;&#30340;&#32452;&#32455;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10880</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#36827;&#34892;&#32452;&#32455;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Organizations for Food System Ontologies using Natural Language Processing. (arXiv:2309.10880v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#33258;&#21160;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#20013;&#30340;&#32452;&#32455;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#33258;&#21160;&#23545;&#23454;&#20307;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#36798;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#19982;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#30340;&#38598;&#25104;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#33021;&#22815;&#33258;&#21160;&#23558;&#32452;&#32455;&#26681;&#25454;&#19982;&#29615;&#22659;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#21035;&#20197;&#21450;&#32654;&#22269;&#25919;&#24220;&#29992;&#20110;&#25551;&#36848;&#21830;&#19994;&#27963;&#21160;&#30340;&#26631;&#20934;&#20135;&#19994;&#20998;&#31867;&#65288;SIC&#65289;&#20195;&#30721;&#36827;&#34892;&#20998;&#31867;&#30340;NLP&#27169;&#22411;&#12290;NLP&#27169;&#22411;&#30340;&#36755;&#20837;&#20026;&#27599;&#20010;&#32452;&#32455;&#36890;&#36807;Google&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#21040;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#35813;&#25991;&#26412;&#29255;&#27573;&#29992;&#20316;&#29992;&#20110;&#23398;&#20064;&#30340;&#32452;&#32455;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#30456;&#24403;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20854;&#20182;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NLP&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research explores the use of natural language processing (NLP) methods to automatically classify entities for the purpose of knowledge graph population and integration with food system ontologies. We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification (SIC) codes, which are used by the U.S. government to characterize business activities. As input, the NLP models are provided with text snippets retrieved by the Google search engine for each organization, which serves as a textual description of the organization that is used for learning. Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks, and they rely on a general framework that could be applied to many other classification problems as well. We believe that NLP models represent a promising approach for automatically harvesting informatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#25955;&#30340;&#12289;&#36845;&#20195;&#30340;&#35268;&#21010;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Minecraft&#23450;&#23621;&#28857;&#29983;&#25104;&#31454;&#36187;&#20013;&#33719;&#32988;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20135;&#29983;&#36924;&#30495;&#19988;&#21487;&#20449;&#36182;&#30340;&#31243;&#24207;&#21270;&#22478;&#24066;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.10871</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#36845;&#20195;&#35268;&#21010;&#30340;&#21487;&#20449;&#36182;Minecraft&#23450;&#23621;&#28857;
&lt;/p&gt;
&lt;p&gt;
Believable Minecraft Settlements by Means of Decentralised Iterative Planning. (arXiv:2309.10871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#25955;&#30340;&#12289;&#36845;&#20195;&#30340;&#35268;&#21010;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Minecraft&#23450;&#23621;&#28857;&#29983;&#25104;&#31454;&#36187;&#20013;&#33719;&#32988;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20135;&#29983;&#36924;&#30495;&#19988;&#21487;&#20449;&#36182;&#30340;&#31243;&#24207;&#21270;&#22478;&#24066;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#31243;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#39046;&#22495;&#20013;&#65292;&#20197;&#21487;&#20449;&#24230;&#21644;&#36866;&#24212;&#24615;&#20026;&#37325;&#28857;&#30340;&#31243;&#24207;&#21270;&#22478;&#24066;&#29983;&#25104;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#20687;Minecraft&#20013;&#30340;&#29983;&#25104;&#23450;&#23621;&#35774;&#35745;&#65288;GDMC&#65289;&#36825;&#26679;&#30340;&#25361;&#25112;&#20013;&#65292;&#25968;&#21313;&#20301;&#30740;&#31350;&#20154;&#21592;&#31454;&#20105;&#25552;&#20986;&#29616;&#23454;&#30340;&#26041;&#27861;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;2022&#24180;&#30340;&#27604;&#36187;&#20013;&#33719;&#32988;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#20998;&#25955;&#30340;&#12289;&#36845;&#20195;&#30340;&#35268;&#21010;&#36807;&#31243;&#23454;&#29616;&#65292;&#35813;&#36807;&#31243;&#21487;&#22312;&#31867;&#20284;&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#36716;&#31227;&#65292;&#20197;&#20135;&#29983;&#31243;&#24207;&#21270;&#30340;&#8220;&#26377;&#26426;&#8221;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural city generation that focuses on believability and adaptability to random terrain is a difficult challenge in the field of Procedural Content Generation (PCG). Dozens of researchers compete for a realistic approach in challenges such as the Generative Settlement Design in Minecraft (GDMC), in which our method has won the 2022 competition. This was achieved through a decentralised, iterative planning process that is transferable to similar generation processes that aims to produce "organic" content procedurally.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#22522;&#30784;&#27169;&#22411;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#20351;&#29992;transformers&#12289;&#29289;&#29702;&#30693;&#35782;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20840;&#29699;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;AI&#27169;&#22411;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2309.10808</link><description>&lt;p&gt;
&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#65306;&#24212;&#29992;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;
&lt;/p&gt;
&lt;p&gt;
AI Foundation Models for Weather and Climate: Applications, Design, and Implementation. (arXiv:2309.10808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#22522;&#30784;&#27169;&#22411;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#20351;&#29992;transformers&#12289;&#29289;&#29702;&#30693;&#35782;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20840;&#29699;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;AI&#27169;&#22411;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35299;&#22823;&#27668;&#30340;&#28151;&#27788;&#34892;&#20026;&#21644;&#25512;&#36827;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#31185;&#25216;&#20844;&#21496;&#12289;&#25919;&#24220;&#26426;&#26500;&#21644;&#27668;&#35937;&#26426;&#26500;&#23545;&#24314;&#31435;&#22320;&#29699;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#20351;&#29992;transformers&#12289;&#21463;&#29289;&#29702;&#30693;&#35782;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#22312;&#30456;&#23545;&#29421;&#31364;&#30340;&#26102;&#31354;&#33539;&#22260;&#21644;&#29305;&#23450;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;transformers&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#21644;&#35270;&#35273;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#25105;&#20204;&#27491;&#22312;&#26397;&#30528;&#21487;&#25512;&#24191;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36808;&#36827;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#26377;&#31454;&#20105;&#21147;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20840;&#29699;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;AI&#27169;&#22411;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MelodyGLM&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#30340;&#26059;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#26469;&#25429;&#25417;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10738</link><description>&lt;p&gt;
MelodyGLM: &#38899;&#20048;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation. (arXiv:2309.10738v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MelodyGLM&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#30340;&#26059;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#26469;&#25429;&#25417;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#38899;&#20048;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#25429;&#25417;&#38899;&#31526;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#32500;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#25991;&#26412;&#21644;&#38899;&#20048;&#20043;&#38388;&#39046;&#22495;&#30693;&#35782;&#24046;&#24322;&#30340;&#32536;&#25925;&#12290;&#27492;&#22806;&#65292;&#21487;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MelodyGLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#26059;&#24459;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#65292;&#20026;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#24314;&#31435;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#31354;&#30333;&#22635;&#20805;&#20219;&#21153;&#65292;&#20197;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38899;&#39640;n-gram&#12289;&#33410;&#22863;n-gram&#21450;&#20854;&#32452;&#21512;&#30340;n-gram&#32435;&#20837;&#38899;&#20048;n-gram&#31354;&#30333;&#22635;&#20805;&#20219;&#21153;&#20013;&#65292;&#20197;&#24314;&#27169;&#26059;&#24459;&#30340;&#22810;&#32500;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have achieved impressive results in various music understanding and generation tasks. However, existing pre-training methods for symbolic melody generation struggle to capture multi-scale, multi-dimensional structural information in note sequences, due to the domain knowledge discrepancy between text and music. Moreover, the lack of available large-scale symbolic melody datasets limits the pre-training improvement. In this paper, we propose MelodyGLM, a multi-task pre-training framework for generating melodies with long-term structure. We design the melodic n-gram and long span sampling strategies to create local and global blank infilling tasks for modeling the local and global structures in melodies. Specifically, we incorporate pitch n-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram blank infilling tasks for modeling the multi-dimensional structures in melodies. To this end, we have constructed a large-scale symbolic melody datas
&lt;/p&gt;</description></item><item><title>NusaWrites&#39033;&#30446;&#36890;&#36807;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#26500;&#24314;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#65292;&#24357;&#34917;&#20102;&#22312;&#32447;&#29228;&#21462;&#21644;&#32763;&#35793;&#25991;&#26723;&#25152;&#24102;&#26469;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#26041;&#35821;&#35328;&#19978;&#36827;&#34892;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;datasetname&#8221;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;12&#31181;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2309.10661</link><description>&lt;p&gt;
NusaWrites: &#20026;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#26500;&#24314;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages. (arXiv:2309.10661v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10661
&lt;/p&gt;
&lt;p&gt;
NusaWrites&#39033;&#30446;&#36890;&#36807;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#26500;&#24314;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#65292;&#24357;&#34917;&#20102;&#22312;&#32447;&#29228;&#21462;&#21644;&#32763;&#35793;&#25991;&#26723;&#25152;&#24102;&#26469;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#26041;&#35821;&#35328;&#19978;&#36827;&#34892;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;datasetname&#8221;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;12&#31181;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27665;&#20027;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#36890;&#36807;&#22312;&#32447;&#29228;&#21462;&#21644;&#25991;&#20214;&#32763;&#35793;&#20026;&#36825;&#20123;&#35821;&#35328;&#24320;&#21457;&#24102;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#30340;&#35821;&#26009;&#24211;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#21644;&#36153;&#29992;&#25928;&#30410;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#24471;&#21040;&#30340;&#35821;&#26009;&#24211;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19982;&#24403;&#22320;&#31038;&#21306;&#30340;&#25991;&#21270;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#22320;&#26041;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#32447;&#29228;&#21462;&#12289;&#20154;&#24037;&#32763;&#35793;&#21644;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#22312;&#26500;&#24314;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#25991;&#21270;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#8220;datasetname &#8221;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;12&#20010;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Democratizing access to natural language processing (NLP) technology is crucial, especially for underrepresented and extremely low-resource languages. Previous research has focused on developing labeled and unlabeled corpora for these languages through online scraping and document translation. While these methods have proven effective and cost-efficient, we have identified limitations in the resulting corpora, including a lack of lexical diversity and cultural relevance to local communities. To address this gap, we conduct a case study on Indonesian local languages. We compare the effectiveness of online scraping, human translation, and paragraph writing by native speakers in constructing datasets. Our findings demonstrate that datasets generated through paragraph writing by native speakers exhibit superior quality in terms of lexical diversity and cultural content. In addition, we present the \datasetname{} benchmark, encompassing 12 underrepresented and extremely low-resource languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2309.10576</link><description>&lt;p&gt;
PDRL&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#20197;&#24448;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#22312;&#30417;&#25511;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#30417;&#25511;&#24212;&#29992;&#22823;&#22810;&#26159;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#35757;&#32451;&#26631;&#31614;&#25968;&#25454;&#65292;&#26080;&#27861;&#22312;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21363;&#20855;&#26377;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#39044;&#27979;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;PDRL&#65289;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29615;&#22659;&#12290;&#35813;&#25552;&#20986;&#30340;&#36890;&#29992;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#34394;&#25311;&#28145;&#24230; Q &#32593;&#32476;&#65288;DQN&#65289;&#26234;&#33021;&#20307;&#65292;&#20197;&#30417;&#27979;&#22797;&#26434;&#29615;&#22659;&#30340;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#26681;&#25454;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#31574;&#30053;&#20351;&#26234;&#33021;&#20307;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#21516;&#26102;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#12290;&#22312;&#35780;&#20272;&#35813;&#26694;&#26550;&#30340;&#36807;&#31243;&#20013;&#65292;&#37096;&#32626;&#20102;&#19977;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20197;&#30417;&#27979;&#36890;&#36807; BiLSTM &#27169;&#22411;&#39044;&#27979;&#30340;&#21463;&#35797;&#32773;&#26410;&#26469;&#30340;&#24515;&#29575;&#12289;&#21628;&#21560;&#29575;&#21644;&#20307;&#28201;&#12290;&#38543;&#30528;&#27599;&#27425;&#36845;&#20195;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#23454;&#38469;&#21453;&#39304;&#35843;&#25972;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10447</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#23454;&#29616;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22522;&#26412;&#26041;&#38754;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#19981;&#21516;&#32422;&#26463;&#31867;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#37325;&#22823;&#30340;&#26550;&#26500;&#25110;&#35299;&#30721;&#20462;&#25913;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#38468;&#21152;&#32422;&#26463;&#25110;&#35299;&#20915;&#19981;&#21516;&#32422;&#26463;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#65292;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#26426;&#21046;&#20805;&#20998;&#21033;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#20248;&#21183;&#65292;&#32479;&#19968;&#24314;&#27169;&#21508;&#31181;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;REI&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#39118;&#26684;&#30340;&#25351;&#20196;&#25903;&#25345;&#25152;&#26377;&#27969;&#34892;&#30340;&#32454;&#31890;&#24230;&#21487;&#25511;&#29983;&#25104;&#32422;&#26463;&#65292;&#21363;&#35789;&#27719;&#12289;&#20301;&#32622;&#21644;&#38271;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#20013;&#31561;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#25110;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#26102;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demon
&lt;/p&gt;</description></item><item><title>QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.10326</link><description>&lt;p&gt;
QASnowball: &#19968;&#20010;&#29992;&#20110;&#39640;&#36136;&#37327;&#38382;&#31572;&#25968;&#25454;&#29983;&#25104;&#30340;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10326
&lt;/p&gt;
&lt;p&gt;
QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#20854;&#22312;&#24212;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#31283;&#23450;&#30340;QA&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;QASnowball&#65292;&#29992;&#20110;QA&#25968;&#25454;&#22686;&#24378;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#36845;&#20195;&#22320;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QASnowball&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22238;&#31572;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#20174;&#26080;&#26631;&#31614;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20505;&#36873;&#31572;&#26696;&#30340;&#26680;&#24515;&#30701;&#35821;&#65307;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#26681;&#25454;&#25991;&#26723;&#21644;&#20505;&#36873;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#65307;QA&#25968;&#25454;&#36807;&#28388;&#22120;&#65292;&#29992;&#20110;&#36807;&#28388;&#20986;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;QASnowball&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#31181;&#23376;&#38598;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#65292;&#20174;&#32780;&#19981;&#26029;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10293</link><description>&lt;p&gt;
QXAI&#65306;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems. (arXiv:2309.10293v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21487;&#29992;&#20110;&#23545;&#24739;&#32773;&#30340;&#36523;&#20307;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#30340;&#29983;&#21629;&#20307;&#24449;&#12290;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65289;&#30340;&#22238;&#24402;&#20998;&#26512;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#32780;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#21487;&#33021;&#38656;&#35201;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#30450;&#30446;&#20915;&#31574;&#12290;&#22312;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#20013;&#65292;&#36319;&#36394;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#24739;&#32773;&#25968;&#25454;&#21644;&#20854;&#30456;&#20851;&#20020;&#24202;&#29305;&#24449;&#20316;&#20026;&#39044;&#27979;&#26410;&#26469;&#29983;&#21629;&#20307;&#24449;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#35299;&#37322;&#21508;&#31181;&#29305;&#24449;&#23545;&#30417;&#27979;&#24212;&#29992;&#25972;&#20307;&#36755;&#20986;&#30340;&#36129;&#29486;&#23545;&#20110;&#20020;&#24202;&#21307;&#24072;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20107;&#21518;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;Shapley&#20540;&#36827;&#34892;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values c
&lt;/p&gt;</description></item><item><title>Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2309.10150</link><description>&lt;p&gt;
Q-Transformer&#65306;&#36890;&#36807;&#33258;&#22238;&#24402;Q-&#20989;&#25968;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10150
&lt;/p&gt;
&lt;p&gt;
Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#21644;&#33258;&#20027;&#37319;&#38598;&#25968;&#25454;&#30340;&#22823;&#22411;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Transformer&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;Q&#20989;&#25968;&#34920;&#31034;&#65292;&#36890;&#36807;&#31163;&#32447;&#26102;&#24046;&#22791;&#20221;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;Q-Transformer&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#30340;Q&#20540;&#34920;&#31034;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#24212;&#29992;&#39640;&#23481;&#37327;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;Q&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35774;&#35745;&#20915;&#31574;&#65292;&#20351;&#20854;&#22312;&#31163;&#32447;RL&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;Q-Transformer&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#39033;&#30446;&#30340;&#32593;&#31449;&#21644;&#35270;&#39057;&#21487;&#20197;&#22312;https://q-transformer.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#29702;&#35770;&#19982;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#27880;&#24847;&#21147;&#25918;&#22312;&#19978;&#19979;&#25991;-&#26410;&#26631;&#35760;&#26679;&#26412;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.09888</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#8776;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context $\approx$ Environment. (arXiv:2309.09888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09888
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#29702;&#35770;&#19982;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#27880;&#24847;&#21147;&#25918;&#22312;&#19978;&#19979;&#25991;-&#26410;&#26631;&#35760;&#26679;&#26412;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30740;&#31350;&#30340;&#20013;&#24515;&#22312;&#20110;&#20004;&#20010;&#26041;&#38754;&#12290;&#19968;&#26041;&#38754;&#65292;&#31038;&#21306;&#27491;&#22312;&#21162;&#21147;&#26500;&#24314;&#33021;&#22815;&#20002;&#24323;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#22312;&#26032;&#39062;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#20219;&#20309;&#25552;&#26696;&#33021;&#22815;&#20196;&#20154;&#20449;&#26381;&#22320;&#36229;&#36234;&#31616;&#21333;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22522;&#32447;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#12289;&#26681;&#25454;&#29992;&#25143;&#36890;&#36807;&#25552;&#31034;&#26045;&#21152;&#30340;&#22810;&#31181;&#19978;&#19979;&#25991;&#32972;&#26223;&#28789;&#27963;&#27867;&#21270;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#19978;&#19979;&#25991;&#8776;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#38544;&#34255;&#30528;&#26356;&#22909;&#30340;&#39046;&#22495;&#27867;&#21270;&#20043;&#38053;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#19982;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27880;&#24847;&#19978;&#19979;&#25991;-&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#31181;&#27880;&#24847;&#21487;&#20197;&#20351;&#25105;&#20204;&#25552;&#20986;&#30340;In-Context Risk Minimization (ICRM)&#31639;&#27861;&#32858;&#28966;&#20110;&#27979;&#35797;&#29615;&#22659;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to the eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context $\approx$ environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#25351;&#23548;&#65292;&#21457;&#29616;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.09736</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#37327;&#23376;&#20248;&#21270;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Quantum Optimization Case Study for a Transport Robot Scheduling Problem. (arXiv:2309.09736v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#25351;&#23548;&#65292;&#21457;&#29616;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#26368;&#20808;&#36827;&#32463;&#20856;&#27714;&#35299;&#22120;&#22312;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#19968;&#20010;&#20855;&#26377;&#24037;&#19994;&#30456;&#20851;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#35774;&#35745;&#29702;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19981;&#21516;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#32452;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;Gurobi&#30452;&#25509;&#27604;&#36739;&#65292;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;&#19981;&#21516;&#31574;&#30053;&#35299;&#20915;&#24212;&#29992;&#23548;&#21521;&#20248;&#21270;&#38382;&#39064;&#30340;&#24037;&#20316;&#27969;&#31243;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive case study comparing the performance of D-Waves' quantum-classical hybrid framework, Fujitsu's quantum-inspired digital annealer, and Gurobi's state-of-the-art classical solver in solving a transport robot scheduling problem. This problem originates from an industrially relevant real-world scenario. We provide three different models for our problem following different design philosophies. In our benchmark, we focus on the solution quality and end-to-end runtime of different model and solver combinations. We find promising results for the digital annealer and some opportunities for the hybrid quantum annealer in direct comparison with Gurobi. Our study provides insights into the workflow for solving an application-oriented optimization problem with different strategies, and can be useful for evaluating the strengths and weaknesses of different approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#25925;&#20107;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25913;&#36827;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21382;&#21490;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09553</link><description>&lt;p&gt;
&#22240;&#26524;&#25925;&#20107;&#65306;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#23454;&#29616;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis. (arXiv:2309.09553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#25925;&#20107;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25913;&#36827;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21382;&#21490;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21270;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#36830;&#36143;&#35270;&#35273;&#25925;&#20107;&#30340;&#21512;&#25104;&#36827;&#23637;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#26631;&#39064;&#12289;&#21382;&#21490;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#30340;&#29305;&#24449;&#20316;&#20026;&#29983;&#25104;&#24403;&#21069;&#24103;&#30340;&#26465;&#20214;&#36827;&#34892;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21382;&#21490;&#24103;&#21644;&#26631;&#39064;&#37117;&#35270;&#20026;&#21516;&#26679;&#30340;&#36129;&#29486;&#65292;&#24182;&#20197;&#30456;&#31561;&#30340;&#26435;&#37325;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#65292;&#24573;&#35270;&#20102;&#24182;&#38750;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#37117;&#19982;&#29983;&#25104;&#24403;&#21069;&#24103;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#26681;&#25454;&#36825;&#31181;&#20851;&#31995;&#20998;&#37197;&#26435;&#37325;&#65292;&#22240;&#26524;&#25925;&#20107;&#29983;&#25104;&#24403;&#21069;&#24103;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25925;&#20107;&#29983;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;PororoSV&#21644;FlintstonesSV&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID score
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08182</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#32431;&#25968;&#23398;&#39064;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#29289;&#29702;&#35789;&#38382;&#39064;-&#21363;&#22522;&#20110;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#31532;&#19968;&#20010;&#29289;&#29702;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;-PhysQA&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;1000&#20010;&#21021;&#20013;&#29289;&#29702;&#35789;&#38382;&#39064;&#65288;&#21253;&#25324;&#36816;&#21160;&#23398;&#12289;&#36136;&#37327;&#21644;&#23494;&#24230;&#12289;&#21147;&#23398;&#12289;&#28909;&#23398;&#21644;&#30005;&#23398;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;GPT3.5&#26469;&#29983;&#25104;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#21457;&#29616;GPT3.5&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#19978;&#33258;&#21160;&#35299;&#20915;49.3%&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#21017;&#20026;73.2%&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#20316;&#20026;&#25552;&#31034;&#65292;LLM&#21487;&#20197;&#35299;&#20915;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#22522;&#30784;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;&#38500;&#20102;&#33258;&#21160;&#35299;&#20915;&#38382;&#39064;&#65292;GPT3.5&#36824;&#21487;&#20197;&#24635;&#32467;&#38382;&#39064;&#28041;&#21450;&#30340;&#30693;&#35782;&#25110;&#20027;&#39064;&#65292;&#29983;&#25104;&#30456;&#20851;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#32508;&#21512;&#20986;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems-problems to be solved by calculation and inference based on some prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (on Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning and 73.2% on few-shot learning. This result show that by using similar problem and its answer as prompt, LLM could solve elementary physics word problems approaching human level. Besides automatically solving problems, GPT3.5 could also summarize the knowledge or topic examined by the problem, generate the relevant explanation, and synthesis new physics word problems according tothe input problem
&lt;/p&gt;</description></item><item><title>VAPOR&#26159;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23460;&#22806;&#22797;&#26434;&#26893;&#34987;&#29615;&#22659;&#20013;&#36827;&#34892;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#30495;&#23454;&#26893;&#34987;&#25968;&#25454;&#20013;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#26893;&#34987;&#30340;&#29289;&#29702;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#33258;&#36866;&#24212;&#35268;&#21010;&#22120;&#29983;&#25104;&#21160;&#24577;&#21487;&#34892;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#22312;&#29421;&#31364;&#36890;&#36947;&#20013;&#23548;&#33322;&#65292;&#24182;&#36991;&#20813;&#34987;&#39640;&#33609;&#21644;&#28748;&#26408;&#19995;&#25152;&#22256;&#20303;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.07832</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#22312;&#23460;&#22806;&#26893;&#34987;&#20013;&#36827;&#34892;&#23548;&#33322;&#30340;VAPOR&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning. (arXiv:2309.07832v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07832
&lt;/p&gt;
&lt;p&gt;
VAPOR&#26159;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23460;&#22806;&#22797;&#26434;&#26893;&#34987;&#29615;&#22659;&#20013;&#36827;&#34892;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#30495;&#23454;&#26893;&#34987;&#25968;&#25454;&#20013;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#26893;&#34987;&#30340;&#29289;&#29702;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#33258;&#36866;&#24212;&#35268;&#21010;&#22120;&#29983;&#25104;&#21160;&#24577;&#21487;&#34892;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#22312;&#29421;&#31364;&#36890;&#36947;&#20013;&#23548;&#33322;&#65292;&#24182;&#36991;&#20813;&#34987;&#39640;&#33609;&#21644;&#28748;&#26408;&#19995;&#25152;&#22256;&#20303;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VAPOR&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#32467;&#26500;&#28151;&#20081;&#12289;&#23494;&#38598;&#26893;&#34987;&#30340;&#23460;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#20840;&#21521;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;&#19977;&#32500;LiDAR&#28857;&#20113;&#23548;&#20986;&#30340;&#39640;&#24230;&#21644;&#24378;&#24230;&#22522;&#20110;&#25104;&#26412;&#22320;&#22270;&#12289;&#30446;&#26631;&#25104;&#26412;&#22320;&#22270;&#20197;&#21450;&#32463;&#36807;&#22788;&#29702;&#30340;&#22266;&#26377;&#24863;&#30693;&#25968;&#25454;&#20316;&#20026;&#29366;&#24577;&#36755;&#20837;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#23454;&#38469;&#23460;&#22806;&#26893;&#34987;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#19968;&#31181;&#26032;&#30340;RL&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#23398;&#20064;&#20102;&#21608;&#22260;&#26893;&#34987;&#30340;&#29289;&#29702;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#22914;&#39640;&#24230;&#12289;&#23494;&#24230;&#21644;&#22362;&#23454;&#24230;/&#21018;&#24615;&#65292;&#20197;&#36827;&#34892;&#23548;&#33322;&#12290;&#19982;&#20351;&#29992;&#31471;&#21040;&#31471;&#31574;&#30053;&#21160;&#20316;&#19981;&#21516;&#65292;&#23436;&#20840;&#35757;&#32451;&#22909;&#30340;RL&#31574;&#30053;&#30340;Q&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#30001;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#35268;&#21010;&#22120;&#29983;&#25104;&#30340;&#21160;&#24577;&#21487;&#34892;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#35813;&#35268;&#21010;&#22120;&#33021;&#22815;&#31359;&#36807;&#29421;&#31364;&#30340;&#36890;&#36947;&#24182;&#38450;&#27490;&#34987;&#26893;&#34987;&#65292;&#20363;&#22914;&#39640;&#33609;&#21644;&#28748;&#26408;&#19995;&#25152;&#22256;&#20303;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#23460;&#22806;&#26893;&#34987;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using Offline Reinforcement Learning (RL). Our method trains a novel RL policy from unlabeled data collected in real outdoor vegetation. This policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding vegetation such as height, density, and solidity/stiffness for navigation. Instead of using end-to-end policy actions, the fully-trained RL policy's Q network is used to evaluate dynamically feasible robot actions generated from a novel adaptive planner capable of navigating through dense narrow passages and preventing entrapment in vegetation such as tall grass and bushes. We demonstrate our method's capabilities on a legged robot in complex outdoor vegetation. We observe an improvement in success rates
&lt;/p&gt;</description></item><item><title>REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.07832</link><description>&lt;p&gt;
REFORMS: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07832
&lt;/p&gt;
&lt;p&gt;
REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#37319;&#29992;&#20063;&#20276;&#38543;&#30528;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#30340;&#22833;&#36133;&#12290;&#36825;&#20123;&#22833;&#36133;&#21487;&#33021;&#20250;&#38459;&#30861;&#31185;&#23398;&#36827;&#23637;&#65292;&#23548;&#33268;&#23545;&#26080;&#25928;&#32467;&#35770;&#30340;&#38169;&#35823;&#20849;&#35782;&#65292;&#24182;&#21066;&#24369;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#30340;&#21487;&#20449;&#24230;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#31185;&#20013;&#24120;&#24120;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#24212;&#29992;&#19988;&#22833;&#36133;&#12290;&#20986;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25552;&#20379;&#28165;&#26224;&#30340;&#25253;&#21578;&#26631;&#20934;&#12290;&#22522;&#20110;&#23545;&#36807;&#21435;&#25991;&#29486;&#30340;&#24191;&#27867;&#35780;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REFORMS&#26816;&#26597;&#34920;&#65288;$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience&#65289;&#12290;&#23427;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#37197;&#22871;&#30340;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#12290;REFORMS&#26159;&#22522;&#20110;19&#20301;&#30740;&#31350;&#20154;&#21592;&#30340;&#20849;&#35782;&#24320;&#21457;&#30340;&#65292;&#36825;&#20123;&#20154;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#12290;REFORMS&#21487;&#20197;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05809</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#20013;&#37319;&#29992;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#37319;&#29992;&#19968;&#31181;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#65292;&#21521;&#27599;&#20010;&#29992;&#25143;&#25552;&#20379;&#30456;&#21516;&#31243;&#24230;&#30340;&#35299;&#37322;&#65292;&#32780;&#19981;&#32771;&#34385;&#20182;&#20204;&#30340;&#20010;&#20307;&#38656;&#27714;&#21644;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#22823;&#22810;&#20197;&#38745;&#24577;&#21644;&#38750;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#22522;&#20110;&#20854;&#38656;&#27714;&#21644;&#20559;&#22909;&#36827;&#34892;&#20132;&#20114;&#12289;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65288;&#22522;&#26412;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#65289;&#65292;&#24182;&#22312;&#36879;&#26126;&#30340;&#25512;&#33616;&#21644;&#20852;&#36259;&#24314;&#27169;&#24212;&#29992;&#65288;RIMA&#65289;&#20013;&#23454;&#29616;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#65292;&#20197;&#35843;&#26597;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#23545;&#29992;&#25143;&#23545;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#23616;&#37096;-&#36828;&#31243;&#36965;&#25805;&#20316;&#20013;&#30340;&#26102;&#24310;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#25913;&#21892;&#21516;&#27493;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#38543;&#26426;&#24310;&#36831;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16979</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#22312;&#20855;&#26377;&#38543;&#26426;&#26102;&#24310;&#30340;&#23616;&#37096;-&#36828;&#31243;&#36965;&#25805;&#20316;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays. (arXiv:2305.16979v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#23616;&#37096;-&#36828;&#31243;&#36965;&#25805;&#20316;&#20013;&#30340;&#26102;&#24310;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#25913;&#21892;&#21516;&#27493;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#38543;&#26426;&#24310;&#36831;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;-&#36828;&#31243;&#31995;&#32479;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#21361;&#38505;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#22826;&#31354;&#21644;&#26680;&#30005;&#31449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#23041;&#32961;&#21040;&#31995;&#32479;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#26102;&#24310;&#65292;&#30830;&#31435;&#23616;&#37096;&#21644;&#36828;&#31243;&#35774;&#22791;&#20043;&#38388;&#30340;&#20934;&#30830;&#20301;&#32622;&#26144;&#23556;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#22686;&#24378;&#23616;&#37096;-&#36828;&#31243;&#31995;&#32479;&#30340;&#21516;&#27493;&#24615;&#21644;&#31283;&#23450;&#24615;&#23545;&#20110;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26356;&#36828;&#36317;&#31163;&#21644;&#39640;&#24230;&#25361;&#25112;&#30340;&#32593;&#32476;&#29615;&#22659;&#20013;&#19982;&#29615;&#22659;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26102;&#24310;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#26102;&#24310;&#25511;&#21046;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#36825;&#31181;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#21487;&#20197;&#34917;&#20607;&#38543;&#26426;&#24310;&#36831;&#24182;&#25913;&#21892;&#23616;&#37096;&#21644;&#36828;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#20043;&#38388;&#30340;&#21516;&#27493;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22810;&#27493;&#24310;&#36831;&#32435;&#20837;&#21040;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of local-remote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23545;GPT-3.5&#12289;GPT-4&#21644;BARD&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#20013;&#65292;ChatGPT-4&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;GPT-4&#30456;&#23545;&#20110;GPT-3.5&#30340;&#20248;&#21183;&#21487;&#33021;&#26159;&#30001;&#20854;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;NLP&#25928;&#29575;&#25152;&#24341;&#36215;&#30340;&#65292;&#20294;&#23545;&#20110;BARD&#26469;&#35828;&#24182;&#19981;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;&#24402;&#32435;&#12289;&#25968;&#23398;&#21644;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.12477</link><description>&lt;p&gt;
GPT-3.5&#12289;GPT-4&#36824;&#26159;BARD&#65311;&#23545;LLM&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#36890;&#36807;&#25552;&#31034;&#25552;&#21319;&#24615;&#33021;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts. (arXiv:2305.12477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23545;GPT-3.5&#12289;GPT-4&#21644;BARD&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#20013;&#65292;ChatGPT-4&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;GPT-4&#30456;&#23545;&#20110;GPT-3.5&#30340;&#20248;&#21183;&#21487;&#33021;&#26159;&#30001;&#20854;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;NLP&#25928;&#29575;&#25152;&#24341;&#36215;&#30340;&#65292;&#20294;&#23545;&#20110;BARD&#26469;&#35828;&#24182;&#19981;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;&#24402;&#32435;&#12289;&#25968;&#23398;&#21644;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#23384;&#22312;&#30528;&#24403;&#21069;&#30340;&#28909;&#35758;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#21313;&#19968;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#19981;&#21516;&#30340;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#24443;&#24213;&#25216;&#26415;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;GPT-3.5&#12289;GPT-4&#21644;BARD&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#19979;&#65292;ChatGPT-4&#30456;&#23545;&#20110;ChatGPT-3.5&#21644;BARD&#22312;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;GPT-4&#30456;&#23545;&#20110;GPT-3.5&#30340;&#20248;&#21183;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#20854;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;NLP&#25928;&#29575;&#26469;&#35299;&#37322;&#65292;&#20294;&#23545;&#20110;BARD&#26469;&#35828;&#24182;&#19981;&#26126;&#26174;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;&#24402;&#32435;&#12289;&#25968;&#23398;&#21644;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24378;&#21270;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23545;&#36825;&#19977;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#24037;&#31243;&#21270;&#30340;&#25552;&#31034;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable performance on various Natural Language Processing (NLP) tasks. However, there is a current hot debate regarding their reasoning capacity. In this paper, we examine the performance of GPT-3.5, GPT-4, and BARD models, by performing a thorough technical evaluation on different reasoning tasks across eleven distinct datasets. Our paper provides empirical evidence showcasing the superior performance of ChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting throughout almost all evaluated tasks. While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD. We also demonstrate that the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To bolster our findings, we present a detailed and comprehensive analysis of the results from these three models. Furthermore, we propose a set of engineered prompt
&lt;/p&gt;</description></item><item><title>Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.08415</link><description>&lt;p&gt;
Marsellus: &#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC
&lt;/p&gt;
&lt;p&gt;
Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08415
&lt;/p&gt;
&lt;p&gt;
Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#20114;&#32852;&#29289;&#32852;&#32593;&#65288;AI-IoT&#65289;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#38656;&#35201;&#22312;&#33539;&#22260;&#24191;&#27867;&#30340;&#24037;&#20316;&#26465;&#20214;&#19979;&#65292;&#22312;&#20960;&#21313;&#27627;&#29926;&#30340;&#21151;&#32791;&#38480;&#21046;&#19979;&#36816;&#34892;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#23494;&#38598;&#22411;&#20294;&#24378;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#20197;&#21450;&#38656;&#35201;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Marsellus&#65292;&#19968;&#20010;&#22312;GlobalFoundries 22nm FDX&#19978;&#21046;&#36896;&#30340;&#20840;&#25968;&#23383;&#24322;&#26500;SoC&#65292;&#29992;&#20110;AI-IoT&#26411;&#31471;&#33410;&#28857;&#65292;&#23427;&#32467;&#21512;&#20102;&#65306;1&#65289;&#19968;&#20010;16&#20010;RISC-V&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#26680;&#24515;&#30340;&#36890;&#29992;&#38598;&#32676;&#65292;&#29992;&#20110;&#25191;&#34892;&#21508;&#31181;&#25903;&#25345;4&#20301;&#21644;2&#20301;&#31639;&#26415;&#25193;&#23637;&#65288;XpulpNN&#65289;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#34701;&#21512;&#30340;MAC&#21644;LOAD&#25805;&#20316;&#21644;&#28014;&#28857;&#25903;&#25345;&#65307;2&#65289;&#19968;&#20010;2-8&#20301;&#21487;&#37325;&#26500;&#20108;&#36827;&#21046;&#24341;&#25806;&#65288;RBE&#65289;&#65292;&#29992;&#20110;&#21152;&#36895;DNN&#20013;&#30340;3x3&#21644;1x1&#65288;&#36880;&#28857;&#65289;&#21367;&#31215;&#65307;3&#65289;&#19968;&#32452;&#36830;&#25509;&#21040;&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#29255;&#19978;&#30417;&#35270;&#65288;OCM&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT) System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and nano-robotics need to run many diverse tasks within a power envelope of a few tens of mW over a wide range of operating conditions: compute-intensive but strongly quantized Deep Neural Network (DNN) inference, as well as signal processing and control requiring high-precision floating-point. We present Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16 RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions (XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support; 2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1 (pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks connected to an Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06395</link><description>&lt;p&gt;
ACTC: &#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;(KGC)&#20381;&#36182;&#20110;&#20272;&#35745;&#24471;&#20998;&#27169;&#22411;(&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;)-&#20803;&#32452;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#23884;&#20837;&#21021;&#22987;&#30693;&#35782;&#22270;&#12290;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#38408;&#20540;(&#20351;&#29992;&#25163;&#21160;&#27880;&#37322;&#30340;&#31034;&#20363;)&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#36136;&#37327;&#12290;&#26412;&#25991;&#23581;&#35797;&#39318;&#27425;&#38024;&#23545;KGC&#36827;&#34892;&#20919;&#21551;&#21160;&#26657;&#20934;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#21021;&#22987;&#27809;&#26377;&#27880;&#37322;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;ACTC&#22522;&#20110;&#26377;&#38480;&#30340;&#27880;&#37322;&#20803;&#32452;&#26377;&#25928;&#22320;&#25214;&#21040;&#22909;&#30340;&#27599;&#20010;&#20851;&#31995;&#30340;&#38408;&#20540;&#12290;&#38500;&#20102;&#19968;&#20123;&#27880;&#37322;&#30340;&#20803;&#32452;&#22806;&#65292;ACTC&#36824;&#21033;&#29992;Logistic&#22238;&#24402;&#25110;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#22120;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20803;&#32452;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23494;&#24230;&#21644;&#38543;&#26426;&#36873;&#25321;&#31561;&#19981;&#21516;&#26041;&#27861;&#36873;&#25321;&#20505;&#36873;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#35780;&#20998;&#27169;&#22411;&#21644;&#19968;&#20010;oracle&#27880;&#37322;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25945;&#31243;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24110;&#21161;&#25552;&#21319;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36827;&#32780;&#20419;&#36827;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04933</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#24037;&#31243;&#35774;&#35745;&#19982;&#20581;&#24247;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial. (arXiv:2305.04933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25945;&#31243;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24110;&#21161;&#25552;&#21319;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36827;&#32780;&#20419;&#36827;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#20316;&#20026;&#23433;&#20840;&#20445;&#38556;&#30340;&#22522;&#26412;&#23618;&#65292;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#21512;&#29702;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#26469;&#20419;&#36827;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;UQ&#20351;&#24471;ML&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#24471;&#21040;&#25913;&#21892;&#65292;&#26377;&#28508;&#21147;&#26174;&#33879;&#20419;&#36827;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#21046;&#36896;&#19994;&#21644;&#33322;&#31354;&#31561;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#26041;&#20301;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26032;&#20852;UQ&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#36825;&#20123;UQ&#26041;&#27861;&#22312;&#35299;&#20915;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#28041;&#21450;ML&#27169;&#22411;UQ&#30340;&#19981;&#30830;&#23450;&#24615;&#31867;&#22411;&#65292;&#26469;&#28304;&#21644;&#21407;&#22240;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#30340;&#25945;&#31243;&#24335;&#25551;&#36848;&#65306;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
On top of machine learning models, uncertainty quantification (UQ) functions as an essential layer of safety assurance that could lead to more principled decision making by enabling sound risk assessment and management. The safety and reliability improvement of ML models empowered by UQ has the potential to significantly facilitate the broad adoption of ML solutions in high-stakes decision settings, such as healthcare, manufacturing, and aviation, to name a few. In this tutorial, we aim to provide a holistic lens on emerging UQ methods for ML models with a particular focus on neural networks and the applications of these UQ methods in tackling engineering design as well as prognostics and health management problems. Toward this goal, we start with a comprehensive classification of uncertainty types, sources, and causes pertaining to UQ of ML models. Next, we provide a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22330;&#26223;&#20851;&#38190;&#28857;&#36319;&#36394;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#23545;&#31216;&#29289;&#20307;&#21644;&#36974;&#25377;&#21644;&#35270;&#37326;&#20043;&#22806;&#29289;&#20307;&#30340;&#20851;&#38190;&#28857;&#36319;&#36394;&#65292;&#25552;&#39640;&#20102;&#30456;&#26426;&#35266;&#23519;&#30340;&#25928;&#29992;&#65292;&#23545;&#20110;&#31574;&#30053;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.04718</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#32972;&#21467;&#65306;&#36125;&#21494;&#26031;&#22330;&#26223;&#20851;&#38190;&#28857;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation. (arXiv:2305.04718v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22330;&#26223;&#20851;&#38190;&#28857;&#36319;&#36394;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#23545;&#31216;&#29289;&#20307;&#21644;&#36974;&#25377;&#21644;&#35270;&#37326;&#20043;&#22806;&#29289;&#20307;&#30340;&#20851;&#38190;&#28857;&#36319;&#36394;&#65292;&#25552;&#39640;&#20102;&#30456;&#26426;&#35266;&#23519;&#30340;&#25928;&#29992;&#65292;&#23545;&#20110;&#31574;&#30053;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#20174;&#30456;&#26426;&#35266;&#23519;&#20013;&#23398;&#20064;&#21644;&#25552;&#21462;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#22330;&#26223;&#30340;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#22788;&#29702;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#24773;&#26223;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#32463;&#24120;&#34987;&#36974;&#25377;&#25110;&#32773;&#20301;&#20110;&#30456;&#26426;&#30340;&#35270;&#37326;&#20043;&#22806;&#65292;&#23548;&#33268;&#30456;&#26426;&#35266;&#23519;&#22312;&#29289;&#20307;&#20301;&#32622;&#26041;&#38754;&#20135;&#29983;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BASK&#65292;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#36319;&#36394;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#23610;&#24230;&#19981;&#21464;&#20851;&#38190;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22270;&#20687;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#31216;&#29289;&#20307;&#21644;&#36974;&#25377;&#21644;&#35270;&#37326;&#20043;&#22806;&#30340;&#29289;&#20307;&#30340;&#20851;&#38190;&#28857;&#36319;&#36394;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25163;&#33109;&#30456;&#26426;&#35266;&#23519;&#20013;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#29289;&#20307;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20102;&#26356;&#20986;&#33394;&#30340;&#31574;&#30053;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach successfully resolves inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ our method to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.13032</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27880;&#37322;&#22270;&#24418;&#25968;&#25454;&#24182;&#24212;&#29992;&#20110;&#36719;&#20214;&#20195;&#30721;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#24037;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#21644;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#21487;&#33021;&#20107;&#20808;&#24182;&#19981;&#21487;&#29992;&#12290;&#33719;&#21462;&#27880;&#37322;&#36890;&#24120;&#38656;&#35201;&#26174;&#30528;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#24320;&#22987;&#65292;&#28982;&#21518;&#29992;&#25968;&#25454;&#21644;&#25511;&#21046;&#27969;&#36793;&#26469;&#22686;&#24378;&#23427;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#28304;&#20195;&#30721;&#30340;&#26641;&#24418;&#34920;&#31034;&#36716;&#25442;&#20026;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#65288;FA-AST&#65289;&#34920;&#31034;&#27861;&#12290;&#22522;&#20110;&#22270;&#24418;&#34920;&#31034;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#22270;&#23884;&#20837;&#65288;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#65289;&#26500;&#36896;&#25104;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#12290;&#37492;&#20110;&#36825;&#26679;&#30340;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#21464;&#24471;&#20219;&#21153;&#19981;&#21487;&#30693;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#22238;&#24402;&#26041;&#27861;&#21644;&#36866;&#29992;&#20110;&#22238;&#24402;&#30340;&#26597;&#35810;&#31574;&#30053;&#26469;&#25191;&#34892;&#20027;&#21160;&#23398;&#20064;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#29992;&#20110;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#31574;&#30053;&#21644;&#22238;&#24402;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#20114;&#27169;&#22359;&#36827;&#34892;&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04688</link><description>&lt;p&gt;
&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection. (arXiv:2304.04688v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04688
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#20114;&#27169;&#22359;&#36827;&#34892;&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#27599;&#20010;&#20154;&#22312;&#35270;&#39057;&#20013;&#21160;&#20316;&#21457;&#29983;&#30340;&#26102;&#38388;&#21644;&#20301;&#32622;&#65292;&#24182;&#23545;&#30456;&#24212;&#30340;&#21160;&#20316;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20840;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#20132;&#20114;&#27169;&#22359;&#24314;&#27169;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#24471;&#21040;&#20132;&#20114;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#36825;&#20010;&#29305;&#24449;&#25552;&#31034;&#27599;&#20010;&#26631;&#31614;&#20197;&#33719;&#21462;&#26356;&#21512;&#36866;&#30340;&#25991;&#26412;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#26631;&#31614;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#30830;&#23450;&#21160;&#20316;&#31867;&#21035;&#12290;&#22312;J-HMDB&#21644;UCF101-24&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20132;&#20114;&#27169;&#22359;&#21644;&#25552;&#31034;&#20351;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text features. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achievi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23567;&#40736;&#39045;&#39592;&#19981;&#22343;&#21248;&#38382;&#39064;&#30340;&#33258;&#20027;&#24335;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.12265</link><description>&lt;p&gt;
&#33258;&#20027;&#24335;&#23567;&#40736;&#39045;&#31383;&#39592;&#25277;&#26679;&#26426;&#22120;&#20154;&#38075;&#23380;&#31995;&#32479;&#65306;&#19968;&#20010;&#22522;&#20110;&#34507;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Robotic Drilling System for Mice Cranial Window Creation: An Evaluation with an Egg Model. (arXiv:2303.12265v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23567;&#40736;&#39045;&#39592;&#19981;&#22343;&#21248;&#38382;&#39064;&#30340;&#33258;&#20027;&#24335;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#31185;&#23398;&#23454;&#39564;&#20013;&#65292;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#24212;&#29992;&#21487;&#20197;&#20351;&#24471;&#23545;&#23454;&#39564;&#26679;&#26412;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#65292;&#19981;&#21463;&#31185;&#23398;&#23478;&#25216;&#33021;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20197;&#23567;&#40736;&#39045;&#31383;&#30340;&#39592;&#25277;&#26679;&#25805;&#20316;&#20026;&#20363;&#65292;&#32771;&#34385;&#23567;&#40736;&#39045;&#39592;&#30340;&#19981;&#22343;&#21248;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#24335;&#25805;&#20316;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic assistance for experimental manipulation in the life sciences is expected to enable precise manipulation of valuable samples, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and deformation, and therefore require autonomous robotic control. As an example, we are studying the installation of a cranial window in a mouse. This operation requires the removal of the skull, which is approximately 300 um thick, to cut it into a circular shape 8 mm in diameter, but the shape of the mouse skull varies depending on the strain of mouse, sex and week of age. The thickness of the skull is not uniform, with some areas being thin and others thicker. It is also difficult to ensure that the skulls of the mice are kept in the same position for each operation. It is not realistically possible to measure all these features and pre-program a robotic trajectory for individual mice. The paper therefore proposes an autonomous 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#22810;&#35270;&#35282;&#26041;&#27861;&#35782;&#21035;&#21270;&#30707;&#22270;&#20687;&#12290;&#36890;&#36807;&#25910;&#38598;&#21270;&#30707;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#36719;&#25237;&#31080;&#36827;&#34892;&#26368;&#32456;&#20915;&#31574;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#19977;&#20010;&#21407;&#22987;&#35270;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2302.08062</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#22810;&#35270;&#35282;&#26041;&#27861;&#35782;&#21035;&#21270;&#30707;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews. (arXiv:2302.08062v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#22810;&#35270;&#35282;&#26041;&#27861;&#35782;&#21035;&#21270;&#30707;&#22270;&#20687;&#12290;&#36890;&#36807;&#25910;&#38598;&#21270;&#30707;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#36719;&#25237;&#31080;&#36827;&#34892;&#26368;&#32456;&#20915;&#31574;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#19977;&#20010;&#21407;&#22987;&#35270;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#30707;&#29289;&#31181;&#30340;&#35782;&#21035;&#23545;&#20110;&#36827;&#21270;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#21270;&#30707;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#26377;&#21069;&#26223;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21270;&#30707;&#20445;&#23384;&#12289;&#26377;&#26465;&#20214;&#37319;&#26679;&#20197;&#21450;&#39046;&#22495;&#19987;&#23478;&#26114;&#36149;&#19988;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#27880;&#37322;&#30340;&#38480;&#21046;&#65292;&#26631;&#35760;&#21270;&#30707;&#22270;&#20687;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#36890;&#24120;&#37117;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#32473;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36981;&#24490;&#8220;&#32676;&#20247;&#30340;&#26234;&#24935;&#8221;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25910;&#38598;&#27599;&#20010;&#21270;&#30707;&#22270;&#20687;&#30340;&#21407;&#22987;&#35270;&#22270;&#12289;&#28784;&#24230;&#35270;&#22270;&#21644;&#39592;&#26550;&#35270;&#22270;&#26469;&#35757;&#32451;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36719;&#25237;&#31080;&#36827;&#34892;&#26368;&#32456;&#20915;&#31574;&#12290;&#22312;&#21253;&#21547;2400&#24352;&#22270;&#20687;&#30340;&#26368;&#22823;&#36896;&#38024;&#34746;&#21270;&#30707;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;OGS&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65288;&#38024;&#23545;&#27599;&#20010;&#35270;&#22270;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#65289;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#21487;&#19982;OOO&#26041;&#27861;&#30456;&#23218;&#32654;&#65288;&#20351;&#29992;&#19977;&#20010;&#21407;&#22987;&#35270;&#22270;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification of fossil species is crucial to evolutionary studies. Recent advances from deep learning have shown promising prospects in fossil image identification. However, the quantity and quality of labeled fossil images are often limited due to fossil preservation, conditioned sampling, and expensive and inconsistent label annotation by domain experts, which pose great challenges to training deep learning based image classification models. To address these challenges, we follow the idea of the wisdom of crowds and propose a multiview ensemble framework, which collects Original (O), Gray (G), and Skeleton (S) views of each fossil image reflecting its different characteristics to train multiple base models, and then makes the final decision via soft voting. Experiments on the largest fusulinid dataset with 2400 images show that the proposed OGS consistently outperforms baselines (using a single model for each view), and obtains superior or comparable performance compared to OOO (us
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36879;&#26126;&#27169;&#22411;&#12289;&#36719;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#22122;&#22768;&#25239;&#24615;&#30340;&#40065;&#26834;&#22810;&#26631;&#35760;&#26041;&#27861;&#65288;R-MLTSK-FS&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#26631;&#35760;&#23398;&#20064;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.03283</link><description>&lt;p&gt;
&#19968;&#20010;&#34701;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36879;&#26126;&#27169;&#22411;&#12289;&#36719;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#22122;&#22768;&#25239;&#24615;&#30340;&#40065;&#26834;&#22810;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance. (arXiv:2301.03283v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36879;&#26126;&#27169;&#22411;&#12289;&#36719;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#22122;&#22768;&#25239;&#24615;&#30340;&#40065;&#26834;&#22810;&#26631;&#35760;&#26041;&#27861;&#65288;R-MLTSK-FS&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#26631;&#35760;&#23398;&#20064;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#35760;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#36879;&#26126;&#24615;&#12289;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#26041;&#27861;&#21516;&#26102;&#30740;&#31350;&#36825;&#19977;&#20010;&#29305;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22810;&#26631;&#35760;Takagi-Sugeno-Kang&#27169;&#31946;&#31995;&#32479;&#65288;R-MLTSK-FS&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#26426;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36719;&#26631;&#31614;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#26126;&#30830;&#27979;&#37327;&#26631;&#31614;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#26469;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#20063;&#26159;&#20854;&#20182;&#20004;&#31181;&#26426;&#21046;&#30340;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;TSK FS&#20316;&#20026;&#22522;&#27169;&#22411;&#65292;&#20197;&#27604;&#35768;&#22810;&#29616;&#26377;&#30340;&#22810;&#26631;&#35760;&#27169;&#22411;&#26356;&#36879;&#26126;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24314;&#27169;&#29305;&#24449;&#21644;&#36719;&#26631;&#31614;&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#26631;&#35760;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22522;&#20110;&#36719;&#26631;&#31614;&#31354;&#38388;&#21644;&#27169;&#31946;&#29305;&#24449;&#31354;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#22686;&#24378;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model transparency, label correlation learning and the robust-ness to label noise are crucial for multilabel learning. However, few existing methods study these three characteristics simultaneously. To address this challenge, we propose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) with three mechanisms. First, we design a soft label learning mechanism to reduce the effect of label noise by explicitly measuring the interactions between labels, which is also the basis of the other two mechanisms. Second, the rule-based TSK FS is used as the base model to efficiently model the inference relationship be-tween features and soft labels in a more transparent way than many existing multilabel models. Third, to further improve the performance of multilabel learning, we build a correlation enhancement learning mechanism based on the soft label space and the fuzzy feature space. Extensive experiments are conducted to demonstrate the superiority of the proposed method.
&lt;/p&gt;</description></item><item><title>RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.12070</link><description>&lt;p&gt;
RouteNet-Fermi: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32593;&#32476;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12070
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27169;&#22411;&#26159;&#29616;&#20195;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24191;&#27867;&#29992;&#20110;&#32593;&#32476;&#35268;&#21010;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#19968;&#20123;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#25490;&#38431;&#29702;&#35770;&#27169;&#22411;&#20013;&#23545;&#39532;&#23572;&#21487;&#22827;&#27969;&#37327;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#32593;&#32476;&#27169;&#25311;&#22120;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#27491;&#22312;&#25512;&#21160;&#19968;&#20195;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RouteNet-Fermi&#30340;&#33258;&#23450;&#20041;GNN&#27169;&#22411;&#65292;&#23427;&#19982;&#25490;&#38431;&#29702;&#35770;&#20855;&#26377;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#35268;&#27169;&#65288;&#26368;&#22823;&#36798;&#21040;300&#20010;&#33410;&#28857;&#65289;&#21644;&#21253;&#25324;&#20855;&#26377;&#28151;&#21512;&#27969;&#37327;&#29305;&#24615;&#30340;&#26679;&#26412;&#65288;&#22914;&#22797;&#26434;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#20013;&#27979;&#35797;&#20102;RouteNet-Fermi&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
&lt;/p&gt;</description></item><item><title>DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.16906</link><description>&lt;p&gt;
DyG2Vec: &#24102;&#26377;&#33258;&#30417;&#30563;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16906
&lt;/p&gt;
&lt;p&gt;
DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#26102;&#38388;&#27169;&#24335;&#26469;&#23398;&#20064;&#24402;&#32435;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#24120;&#24120;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#35760;&#24518;&#27169;&#22359;&#25110;&#20302;&#25928;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#26469;&#26500;&#24314;&#26102;&#38388;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#32534;&#30721;&#22120;&#19981;&#23481;&#26131;&#36866;&#24212;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#26102;&#38388;&#36793;&#32534;&#30721;&#21644;&#22522;&#20110;&#31383;&#21475;&#30340;&#23376;&#22270;&#37319;&#26679;&#26469;&#29983;&#25104;&#20219;&#21153;&#26080;&#20851;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#27604;SSL&#30340;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#20197;&#23398;&#20064;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#12290;&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20256;&#23548;&#35774;&#32622;&#21644;&#24402;&#32435;&#35774;&#32622;&#30340;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#24179;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;SoTA&#22522;&#32447;4.23&#65285;&#21644;3.30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#22411;&#27169;&#31946;&#31995;&#32479;&#65292;&#21517;&#20026;&#22270;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#65292;&#36890;&#36807;&#23450;&#20041;&#30456;&#20851;&#27010;&#24565;&#12289;&#26500;&#24314;&#27169;&#22411;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#26102;&#20445;&#30041;&#27169;&#31946;&#31995;&#32479;&#20248;&#21183;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.16730</link><description>&lt;p&gt;
&#22270;&#27169;&#31946;&#31995;&#32479;: &#27010;&#24565;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Fuzzy System: Concepts, Models and Algorithms. (arXiv:2210.16730v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#22411;&#27169;&#31946;&#31995;&#32479;&#65292;&#21517;&#20026;&#22270;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#65292;&#36890;&#36807;&#23450;&#20041;&#30456;&#20851;&#27010;&#24565;&#12289;&#26500;&#24314;&#27169;&#22411;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#26102;&#20445;&#30041;&#27169;&#31946;&#31995;&#32479;&#20248;&#21183;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#31995;&#32479;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27169;&#24335;&#35782;&#21035;&#12289;&#26234;&#33021;&#25511;&#21046;&#12289;&#25968;&#25454;&#25366;&#25496;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#24378;&#22823;&#30340;&#35299;&#37322;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#20256;&#32479;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#27169;&#31946;&#31995;&#32479;&#20027;&#35201;&#29992;&#20110;&#24314;&#27169;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25968;&#25454;&#65292;&#26080;&#27861;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#20132;&#36890;&#36335;&#32447;&#22270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24182;&#33021;&#20445;&#30041;&#20256;&#32479;&#27169;&#31946;&#31995;&#32479;&#20248;&#21183;&#30340;&#24314;&#27169;&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#22411;&#27169;&#31946;&#31995;&#32479;&#65292;&#31216;&#20026;&#22270;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#65292;&#24182;&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#30456;&#20851;&#27010;&#24565;&#12289;&#24314;&#27169;&#26694;&#26550;&#21644;&#26500;&#24314;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#23450;&#20041;&#20102;GFS&#30456;&#20851;&#27010;&#24565;&#65292;&#21253;&#25324;&#22270;&#27169;&#31946;&#35268;&#21017;&#24211;&#12289;&#22270;&#27169;&#31946;&#38598;&#21644;&#22270;&#21518;&#20214;&#22788;&#29702;&#21333;&#20803;&#65288;GCPU&#65289;&#12290;&#28982;&#21518;&#26500;&#24314;&#20102;&#19968;&#20010;GFS&#24314;&#27169;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy systems (FSs) have enjoyed wide applications in various fields, including pattern recognition, intelligent control, data mining and bioinformatics, which is attributed to the strong interpretation and learning ability. In traditional application scenarios, FSs are mainly applied to model Euclidean space data and cannot be used to handle graph data of non-Euclidean structure in nature, such as social networks and traffic route maps. Therefore, development of FS modeling method that is suitable for graph data and can retain the advantages of traditional FSs is an important research. To meet this challenge, a new type of FS for graph data modeling called Graph Fuzzy System (GFS) is proposed in this paper, where the concepts, modeling framework and construction algorithms are systematically developed. First, GFS related concepts, including graph fuzzy rule base, graph fuzzy sets and graph consequent processing unit (GCPU), are defined. A GFS modeling framework is then constructed and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2209.14295</link><description>&lt;p&gt;
Conformal Prediction&#23545;&#20998;&#25955;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction is Robust to Dispersive Label Noise. (arXiv:2209.14295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#27491;&#30830;&#35206;&#30422;&#26410;&#35266;&#23519;&#21040;&#30340;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#36827;&#34892;&#20102;&#30028;&#23450;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#27491;&#30830;&#25511;&#21046;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#20551;&#38452;&#24615;&#27604;&#20363;&#65289;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#65292;&#38500;&#20102;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#36890;&#36807;&#23545;Conformal Prediction&#31639;&#27861;&#36827;&#34892;&#26377;&#30028;&#23610;&#23544;&#30340;&#22122;&#22768;&#20462;&#27491;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#20998;&#25968;&#25110;&#25968;&#25454;&#30340;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels except in adversarial cases. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03680</link><description>&lt;p&gt;
&#36890;&#36807;NeurVec&#21152;&#36895;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#22312;&#20272;&#35745;&#31215;&#20998;&#26102;&#30001;&#20110;&#27493;&#38271;&#36873;&#25321;&#30340;&#38480;&#21046;&#65292;&#23384;&#22312;&#30528;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#31216;&#20026;NeurVec&#65292;&#23427;&#21487;&#20197;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26377;&#38480;&#21644;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;NeurVec&#22312;&#36830;&#32493;&#30456;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;NeurVec&#26174;&#33879;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#23454;&#29616;&#20102;&#20960;&#21313;&#21040;&#20960;&#30334;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;NeurVec&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#35745;&#32467;&#21512;&#20102;&#26131;&#20110;&#23454;&#29616;&#30340;&#29305;&#28857;&#65292;&#26377;&#28508;&#21147;&#24314;&#31435;&#36215;&#19968;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#26512;&#30005;&#32593;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;GNN&#22312;&#20165;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#38750;&#32447;&#24615;&#30446;&#26631;&#65292;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#30005;&#32593;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2206.06369</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#30005;&#32593;&#25299;&#25169;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks. (arXiv:2206.06369v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#26512;&#30005;&#32593;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;GNN&#22312;&#20165;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#38750;&#32447;&#24615;&#30446;&#26631;&#65292;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#30005;&#32593;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#21147;&#29983;&#20135;&#20013;&#30340;&#20221;&#39069;&#38656;&#35201;&#22686;&#21152;&#12290;&#21487;&#20877;&#29983;&#33021;&#28304;&#24341;&#20837;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#30005;&#32593;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#20998;&#25955;&#21270;&#12289;&#38477;&#20302;&#30340;&#24815;&#24615;&#21644;&#29983;&#20135;&#30340;&#27874;&#21160;&#24615;&#12290;&#30001;&#20110;&#23545;&#20110;&#22823;&#22411;&#30005;&#32593;&#32780;&#35328;&#65292;&#21160;&#24577;&#31283;&#23450;&#24615;&#27169;&#25311;&#26159;&#26840;&#25163;&#19988;&#26497;&#20854;&#26114;&#36149;&#30340;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#26512;&#30005;&#32593;&#21160;&#24577;&#31283;&#23450;&#24615;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;&#20316;&#20026;GNN&#27169;&#22411;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26032;&#30340;&#12289;&#22823;&#22411;&#30340;&#21512;&#25104;&#30005;&#32593;&#21160;&#24577;&#31283;&#23450;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24320;&#28304;&#36164;&#28304;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20973;&#25299;&#25169;&#20449;&#24687;&#65292;GNN&#33021;&#22815;&#20986;&#22855;&#22320;&#26377;&#25928;&#22320;&#39044;&#27979;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#30446;&#26631;&#12290;&#39318;&#27425;&#23454;&#29616;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20934;&#30830;&#35782;&#21035;&#30005;&#32593;&#20013;&#29305;&#23450;&#33030;&#24369;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate climate change, the share of renewable energies in power production needs to be increased. Renewables introduce new challenges to power grids regarding the dynamic stability due to decentralization, reduced inertia, and volatility in production. Since dynamic stability simulations are intractable and exceedingly expensive for large grids, graph neural networks (GNNs) are a promising method to reduce the computational effort of analyzing the dynamic stability of power grids. As a testbed for GNN models, we generate new, large datasets of dynamic stability of synthetic power grids, and provide them as an open-source resource to the research community. We find that GNNs are surprisingly effective at predicting the highly non-linear targets from topological information only. For the first time, performance that is suitable for practical use cases is achieved. Furthermore, we demonstrate the ability of these models to accurately identify particular vulnerable nodes in power grid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#22120;&#20154;&#35821;&#38899;&#39118;&#26684;&#30340;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#20197;&#36798;&#21040;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#39564;&#35777;&#35821;&#38899;&#25968;&#25454;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#25237;&#24433;&#12289;&#28783;&#20809;&#21644;&#22768;&#38899;&#22312;&#37325;&#26032;&#21019;&#36896;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#32858;&#31867;&#20154;&#31867;&#30340;&#35821;&#38899;&#35805;&#35821;&#20197;&#35782;&#21035;&#20027;&#35201;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#24182;&#20197;&#39184;&#39278;&#26381;&#21153;&#22330;&#26223;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#29615;&#22659;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#30740;&#31350;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#20154;&#22312;&#29305;&#23450;&#35821;&#22659;&#19979;&#30340;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#26234;&#33021;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2205.04952</link><description>&lt;p&gt;
&#20998;&#26512;&#29615;&#22659;&#21644;&#31038;&#20132;&#35821;&#22659;&#65292;&#20351;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#36866;&#24212;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts. (arXiv:2205.04952v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#22120;&#20154;&#35821;&#38899;&#39118;&#26684;&#30340;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#20197;&#36798;&#21040;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#39564;&#35777;&#35821;&#38899;&#25968;&#25454;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#25237;&#24433;&#12289;&#28783;&#20809;&#21644;&#22768;&#38899;&#22312;&#37325;&#26032;&#21019;&#36896;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#32858;&#31867;&#20154;&#31867;&#30340;&#35821;&#38899;&#35805;&#35821;&#20197;&#35782;&#21035;&#20027;&#35201;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#24182;&#20197;&#39184;&#39278;&#26381;&#21153;&#22330;&#26223;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#29615;&#22659;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#30740;&#31350;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#20154;&#22312;&#29305;&#23450;&#35821;&#22659;&#19979;&#30340;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#26234;&#33021;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#27491;&#24335;&#12289;&#23433;&#38745;&#12289;&#40657;&#26263;&#30340;&#29615;&#22659;&#25110;&#26126;&#20142;&#12289;&#28909;&#38393;&#12289;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#24212;&#35813;&#22914;&#20309;&#35828;&#35805;&#65311;&#36890;&#36807;&#35774;&#35745;&#26426;&#22120;&#20154;&#20197;&#26356;&#31038;&#20132;&#21644;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#24335;&#35828;&#35805;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#20154;&#20204;&#23545;&#36825;&#20123;&#20195;&#29702;&#20154;&#30340;&#24863;&#30693;&#24847;&#35782;&#21644;&#26234;&#33021;&#31243;&#24230;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#22120;&#20154;&#35821;&#38899;&#39118;&#26684;&#20197;&#36798;&#21040;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#30340;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#30001;&#20110;&#37326;&#22806;&#35821;&#38899;&#33719;&#21462;&#30340;&#22256;&#38590;&#65292;&#29702;&#35299;&#20154;&#31867;&#22312;&#19981;&#21516;&#22768;&#23398;&#29615;&#22659;&#20013;&#22914;&#20309;&#35843;&#25972;&#33258;&#24049;&#30340;&#22768;&#38899;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;(a) &#22312;&#34394;&#25311;&#30340;Zoom&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#39564;&#35777;&#35821;&#38899;&#25968;&#25454;&#20132;&#20114;&#65292;(b) &#25506;&#32034;&#21644;&#32858;&#31867;&#20154;&#31867;&#30340;&#35821;&#38899;&#35805;&#35821;&#20197;&#35782;&#21035;&#20027;&#35201;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#20197;&#21450;(c) &#20351;&#29992;&#25237;&#24433;&#12289;&#28783;&#20809;&#21644;&#22768;&#38899;&#22312;&#20877;&#29616;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#39118;&#26684;&#12290;&#25105;&#20204;&#20197;&#39184;&#39278;&#26381;&#21153;&#22330;&#26223;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;Pepper&#26426;&#22120;&#20154;&#30340;&#19981;&#21516;&#39118;&#26684;&#30340;&#35821;&#38899;&#30340;&#32467;&#26524;&#65292;&#26397;&#30528;&#22312;&#29305;&#23450;&#35821;&#22659;&#19979;&#35828;&#35805;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
How should a robot speak in a formal, quiet and dark, or a bright, lively and noisy environment? By designing robots to speak in a more social and ambient-appropriate manner we can improve perceived awareness and intelligence for these agents. We describe a process and results toward selecting robot voice styles for perceived social appropriateness and ambiance awareness. Understanding how humans adapt their voices in different acoustic settings can be challenging due to difficulties in voice capture in the wild. Our approach includes 3 steps: (a) Collecting and validating voice data interactions in virtual Zoom ambiances, (b) Exploration and clustering human vocal utterances to identify primary voice styles, and (c) Testing robot voice styles in recreated ambiances using projections, lighting and sound. We focus on food service scenarios as a proof-of-concept setting. We provide results using the Pepper robot's voice with different styles, towards robots that speak in a contextually a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23558;&#38169;&#35823;&#20449;&#24687;&#27880;&#20837;&#38382;&#31572;&#31995;&#32479;&#30340;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#38382;&#31572;&#27169;&#22411;&#23545;&#20110;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#21363;&#20351;&#23569;&#37327;&#38169;&#35823;&#20449;&#24687;&#30340;&#27745;&#26579;&#20063;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2110.07803</link><description>&lt;p&gt;
&#23558;&#38169;&#35823;&#20449;&#24687;&#27880;&#20837;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacking Open-domain Question Answering by Injecting Misinformation. (arXiv:2110.07803v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23558;&#38169;&#35823;&#20449;&#24687;&#27880;&#20837;&#38382;&#31572;&#31995;&#32479;&#30340;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#38382;&#31572;&#27169;&#22411;&#23545;&#20110;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#21363;&#20351;&#23569;&#37327;&#38169;&#35823;&#20449;&#24687;&#30340;&#27745;&#26579;&#20063;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23459;&#20256;&#12289;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#34394;&#20551;&#12289;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#20449;&#24687;&#30340;&#22686;&#21152;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#31572;&#31995;&#32479;&#38754;&#20020;&#30528;&#22312;&#34987;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#30340;&#35821;&#22659;&#20013;&#32508;&#21512;&#21644;&#25512;&#29702;&#20197;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#32039;&#36843;&#24615;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#20351;&#38382;&#31572;&#31995;&#32479;&#23545;&#38169;&#35823;&#20449;&#24687;&#20855;&#22791;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#26159;&#20043;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#19968;&#20010;&#20027;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#23545;&#24102;&#26377;&#38169;&#35823;&#20449;&#24687;&#25991;&#26723;&#30340;&#35821;&#26009;&#24211;&#27745;&#26579;&#30340;&#25935;&#24863;&#24615;&#65292;&#26469;&#30740;&#31350;&#38169;&#35823;&#20449;&#24687;&#23545;&#38382;&#31572;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#20154;&#24037;&#25776;&#20889;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25991;&#26723;&#65292;&#23558;&#20854;&#27880;&#20837;&#21040;&#38382;&#31572;&#27169;&#22411;&#30340;&#35777;&#25454;&#35821;&#26009;&#24211;&#20013;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#23569;&#37327;&#30340;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#20063;&#20250;&#20351;&#38382;&#31572;&#27169;&#22411;&#33030;&#24369;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#20250;&#22823;&#24133;&#24230;&#19979;&#38477;&#12290;&#24403;&#31070;&#32463;&#27169;&#22411;&#25209;&#37327;&#29983;&#25104;&#34394;&#20551;&#25991;&#26723;&#26102;&#25110;&#25915;&#20987;&#32773;&#38024;&#23545;&#38382;&#31572;&#27169;&#22411;&#26102;&#65292;&#38169;&#35823;&#20449;&#24687;&#25915;&#20987;&#30340;&#23041;&#32961;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over misinformation-polluted contexts to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the sensitivity of open-domain QA models to corpus pollution with misinformation documents. We curate both human-written and model-generated false documents that we inject into the evidence corpus of QA models and assess the impact on the performance of these systems. Experiments show that QA models are vulnerable to even small amounts of evidence contamination brought by misinformation, with large absolute performance drops on all models. Misinformation attack brings more threat when fake documents are produced at scale by neural models or the attacker targ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#31243;&#24207;&#20026;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#25351;&#20196;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#23618;&#32423;&#27169;&#22359;&#21270;&#32593;&#32476;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.08214</link><description>&lt;p&gt;
&#20197;&#31243;&#24207;&#20026;&#36807;&#31243;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#22788;&#22659;&#20195;&#29702;&#36827;&#34892;&#23618;&#32423;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#31243;&#24207;&#20026;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#25351;&#20196;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#23618;&#32423;&#27169;&#22359;&#21270;&#32593;&#32476;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#26500;&#24605;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#26102;&#65292;&#20182;&#20204;&#20250;&#20197;&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#24335;&#36827;&#34892;&#65306;&#23558;&#39640;&#32423;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#23545;&#22788;&#22659;&#20195;&#29702;&#30340;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#23558;&#35201;&#25191;&#34892;&#30340;&#36807;&#31243;&#35270;&#20026;&#31616;&#21333;&#21160;&#20316;&#30340;&#24179;&#22374;&#24207;&#21015;&#65292;&#25110;&#32773;&#26368;&#22810;&#21482;&#26377;&#27973;&#23618;&#30340;&#31243;&#24207;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31243;&#24207;&#20316;&#20026;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#30452;&#35266;&#30340;&#34920;&#31034;&#23618;&#32423;&#36807;&#31243;&#30693;&#35782;&#20197;&#36827;&#34892;&#20195;&#29702;&#25351;&#20196;&#21644;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#27169;&#22359;&#21270;&#32593;&#32476;&#30340;&#24314;&#27169;&#33539;&#24335;&#65292;&#35813;&#32593;&#32476;&#30001;&#35268;&#21010;&#32773;&#21644;&#21453;&#24212;&#22120;&#32452;&#25104;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#39044;&#27979;&#65292;&#24182;&#25506;&#27979;&#29615;&#22659;&#20197;&#33719;&#21462;&#23436;&#25104;&#31243;&#24207;&#25191;&#34892;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;IQA&#21644;ALFRED&#25968;&#25454;&#38598;&#19978;&#23454;&#20363;&#21270;&#36825;&#20010;&#26694;&#26550;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36319;&#38543;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#21453;&#24212;&#24335;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;o...
&lt;/p&gt;
&lt;p&gt;
When humans conceive how to perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, in the literature on natural language (NL) command of situated agents, most works have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a powerful yet intuitive method of representing hierarchical procedural knowledge for agent command and control. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that o
&lt;/p&gt;</description></item><item><title>Co-GAIL&#26159;&#19968;&#31181;&#23398;&#20064;&#20154;&#26426;&#22810;&#26679;&#21270;&#21327;&#20316;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#20154;&#31867;&#31574;&#30053;&#21644;&#26426;&#22120;&#20154;&#31574;&#30053;&#65292;&#23454;&#29616;&#20174;&#20154;-&#20154;&#21327;&#20316;&#31034;&#33539;&#20013;&#23398;&#20064;&#65292;&#24182;&#33021;&#22312;&#22312;&#32447;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#24212;&#23545;&#20154;&#31867;&#31574;&#30053;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.06038</link><description>&lt;p&gt;
Co-GAIL: &#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20154;&#26426;&#21327;&#20316;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration. (arXiv:2108.06038v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06038
&lt;/p&gt;
&lt;p&gt;
Co-GAIL&#26159;&#19968;&#31181;&#23398;&#20064;&#20154;&#26426;&#22810;&#26679;&#21270;&#21327;&#20316;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#20154;&#31867;&#31574;&#30053;&#21644;&#26426;&#22120;&#20154;&#31574;&#30053;&#65292;&#23454;&#29616;&#20174;&#20154;-&#20154;&#21327;&#20316;&#31034;&#33539;&#20013;&#23398;&#20064;&#65292;&#24182;&#33021;&#22312;&#22312;&#32447;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#24212;&#23545;&#20154;&#31867;&#31574;&#30053;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;-&#20154;&#21327;&#20316;&#31034;&#33539;&#20013;&#23398;&#20064;&#20154;&#26426;&#21327;&#20316;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#21161;&#25163;&#24517;&#39035;&#23398;&#20250;&#22788;&#29702;&#31034;&#33539;&#20013;&#23637;&#31034;&#30340;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#24182;&#22312;&#22312;&#32447;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#24403;&#20154;&#31867;&#35843;&#25972;&#31574;&#30053;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#23398;&#20064;&#36807;&#31243;&#20013;&#20849;&#21516;&#20248;&#21270;&#20154;&#31867;&#31574;&#30053;&#21644;&#26426;&#22120;&#20154;&#31574;&#30053;&#65306;&#20154;&#31867;&#31574;&#30053;&#36890;&#36807;&#31034;&#33539;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#21487;&#20449;&#30340;&#21327;&#20316;&#34892;&#20026;&#65292;&#32780;&#26426;&#22120;&#20154;&#31574;&#30053;&#21017;&#36890;&#36807;&#20272;&#35745;&#20854;&#20154;&#31867;&#21512;&#20316;&#32773;&#26410;&#35266;&#27979;&#21040;&#30340;&#28508;&#22312;&#31574;&#30053;&#26469;&#36827;&#34892;&#36741;&#21161;&#12290;&#22312;2D&#31574;&#30053;&#28216;&#25103;&#12289;&#20154;&#26426;&#20132;&#25509;&#20219;&#21153;&#21644;&#22810;&#27493;&#21327;&#20316;&#25805;&#32437;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#35780;&#20272;&#21644;&#19982;&#30495;&#23454;&#20154;&#31867;&#25805;&#20316;&#21592;&#19968;&#36215;&#25191;&#34892;&#20219;&#21153;&#26102;&#22343;&#20248;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;&#38468;&#21152;&#26448;&#26009;&#21644;&#35270;&#39057;&#35831;&#21442;&#35265; https://sites.google.com/view/co-gail-web/home
&lt;/p&gt;
&lt;p&gt;
We present a method for learning a human-robot collaboration policy from human-human collaboration demonstrations. An effective robot assistant must learn to handle diverse human behaviors shown in the demonstrations and be robust when the humans adjust their strategies during online task execution. Our method co-optimizes a human policy and a robot policy in an interactive learning process: the human policy learns to generate diverse and plausible collaborative behaviors from demonstrations while the robot policy learns to assist by estimating the unobserved latent strategy of its human collaborator. Across a 2D strategy game, a human-robot handover task, and a multi-step collaborative manipulation task, our method outperforms the alternatives in both simulated evaluations and when executing the tasks with a real human operator in-the-loop. Supplementary materials and videos at https://sites.google.com/view/co-gail-web/home
&lt;/p&gt;</description></item></channel></rss>