<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#26356;&#20855;&#20307;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#21487;&#20197;&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;&#27169;&#22411;&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#24182;&#19988;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02412</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#30340;LLMs&#65306;&#36890;&#36807;&#32452;&#21512;&#25193;&#23637;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#26356;&#20855;&#20307;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#21487;&#20197;&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;&#27169;&#22411;&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#24182;&#19988;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#38750;&#24179;&#20961;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#22686;&#24378;&#25110;&#36171;&#20104;&#26032;&#30340;&#25216;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#36866;&#24212;&#33021;&#21147;&#65292;&#27491;&#22312;&#35757;&#32451;&#22810;&#20010;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#27169;&#22411;&#23454;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#19982;&#26356;&#20855;&#20307;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#23454;&#29992;&#30340;&#32452;&#21512;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;&#21151;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CALM -&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#27169;&#22411;-&#65292;&#23427;&#24341;&#20837;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#20197;&#32452;&#21512;&#23427;&#20204;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#30340;&#26174;&#33879;&#29305;&#28857;&#21253;&#25324;&#65306;(i)&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;LLMs&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;LLMs&#30340;&#35268;&#27169;&#65292;(ii)&#20445;&#25345;&#29616;&#26377;&#27169;&#22411;&#26435;&#37325;&#19981;&#21464;&#65292;&#20174;&#32780;&#20445;&#30041;&#29616;&#26377;&#21151;&#33021;&#65292;(iii)&#24212;&#29992;&#26032;&#21151;&#33021;&#21482;&#38656;&#35201;&#23545;&#22686;&#21152;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Appli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#25193;&#23637;&#21040;&#26412;&#22320;2D&#22270;&#20687;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;3D GANs&#26080;&#27861;&#35299;&#20915;2D&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02411</link><description>&lt;p&gt;
&#20320;&#25152;&#35265;&#21363;&#25152;GAN: &#22312;3D GAN&#20013;&#20026;&#39640;&#20445;&#30495;&#24230;&#20960;&#20309;&#22270;&#24418;&#28210;&#26579;&#27599;&#20010;&#20687;&#32032;
&lt;/p&gt;
&lt;p&gt;
What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#25193;&#23637;&#21040;&#26412;&#22320;2D&#22270;&#20687;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;3D GANs&#26080;&#27861;&#35299;&#20915;2D&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#22312;&#36890;&#36807;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#38598;&#21512;&#20013;&#23398;&#20064;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#22270;&#20687;&#21644;&#22330;&#26223;3D&#20960;&#20309;&#22270;&#24418;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20307;&#31215;&#28210;&#26579;&#20013;&#23494;&#38598;&#37319;&#26679;&#23548;&#33268;&#30340;&#26174;&#33879;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#36843;&#20351;3D GANs&#37319;&#29992;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#25110;&#37319;&#29992;&#20302;&#20998;&#36776;&#29575;&#28210;&#26579;&#19982;&#21518;&#22788;&#29702;&#30340;2D&#36229;&#20998;&#36776;&#29575;&#65292;&#36825;&#25439;&#23475;&#20102;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#21644;&#35299;&#20915;&#20960;&#20309;&#22270;&#24418;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;3D GANs&#23578;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;2D&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#25193;&#23637;&#21040;&#26412;&#22320;2D&#22270;&#20687;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;&#65292;&#22240;&#27492;&#33021;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#33410;&#35299;&#20915;&#32454;&#31890;&#24230;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#37319;&#26679;&#22120;&#26469;&#21152;&#36895;3D GAN&#35757;&#32451;&#20013;&#30340;&#31070;&#32463;&#28210;&#26579;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#28145;&#24230;&#37319;&#26679;&#27425;&#25968;&#39640;&#36798;5&#20493;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26126;&#30830;&#22320;&#8220;&#28210;&#26579;&#27599;&#20010;&#20687;&#32032;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly "render every pixel" o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#20108;&#32500;&#28201;&#24230;&#22330;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36755;&#20837;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#26102;&#28201;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#12289;&#27785;&#31215;&#27169;&#24335;&#21644;&#24037;&#33402;&#30340;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2401.02403</link><description>&lt;p&gt;
&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#23454;&#26102;&#20108;&#32500;&#28201;&#24230;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks. (arXiv:2401.02403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#20108;&#32500;&#28201;&#24230;&#22330;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36755;&#20837;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#26102;&#28201;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#12289;&#27785;&#31215;&#27169;&#24335;&#21644;&#24037;&#33402;&#30340;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#28201;&#24230;&#22330;&#23545;&#20110;&#38450;&#27490;&#36807;&#28909;&#12289;&#35843;&#25972;&#24037;&#33402;&#21442;&#25968;&#21644;&#30830;&#20445;&#24037;&#33402;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#65292;&#20294;&#24448;&#24448;&#32791;&#26102;&#19988;&#19981;&#36866;&#29992;&#20110;&#36845;&#20195;&#35774;&#35745;&#22330;&#26223;&#20013;&#30340;&#23454;&#26102;&#39044;&#27979;&#21644;&#22312;&#32447;&#25511;&#21046;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#20869;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#28201;&#24230;&#22330;&#39044;&#27979;&#30340;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36755;&#20837;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;(ConvLSTM)&#26550;&#26500;&#12290;&#21033;&#29992;&#24037;&#33402;&#36807;&#31243;&#20013;&#30340;&#23454;&#26102;&#28201;&#24230;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19979;&#21508;&#31181;&#20960;&#20309;&#24418;&#29366;&#12289;&#27785;&#31215;&#27169;&#24335;&#21644;&#24037;&#33402;&#30340;&#20108;&#32500;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the temperature field in metal additive manufacturing (AM) processes is critical to preventing overheating, adjusting process parameters, and ensuring process stability. While physics-based computational models offer precision, they are often time-consuming and unsuitable for real-time predictions and online control in iterative design scenarios. Conversely, machine learning models rely heavily on high-quality datasets, which can be costly and challenging to obtain within the metal AM domain. Our work addresses this by introducing a physics-informed neural network framework specifically designed for temperature field prediction in metal AM. This framework incorporates a physics-informed input, physics-informed loss function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture. Utilizing real-time temperature data from the process, our model predicts 2D temperature fields for future timestamps across diverse geometries, deposition patterns, and proce
&lt;/p&gt;</description></item><item><title>TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.02385</link><description>&lt;p&gt;
TinyLlama&#65306;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLlama: An Open-Source Small Language Model. (arXiv:2401.02385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02385
&lt;/p&gt;
&lt;p&gt;
TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TinyLlama&#65292;&#19968;&#20010;&#26377;&#38480;&#30340;1.1B&#35821;&#35328;&#27169;&#22411;&#65292;&#22823;&#32422;&#39044;&#35757;&#32451;&#20102;1&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#35757;&#32451;&#36718;&#25968;&#32422;&#20026;3&#36718;&#12290;TinyLlama&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#22312;&#24320;&#28304;&#31038;&#21306;&#30340;&#36129;&#29486;&#22522;&#30784;&#19978;&#65288;&#20363;&#22914;FlashAttention&#65289;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;TinyLlama&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;&#29616;&#26377;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;https://github.com/jzhang38/TinyLlama&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#29616;&#20195;&#33310;&#21644;&#34920;&#28436;&#33402;&#26415;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#24103;&#26041;&#27861;&#22312;&#29616;&#20195;&#33310;&#36424;&#34920;&#28436;&#20013;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#27604;&#21333;&#24103;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.02383</link><description>&lt;p&gt;
&#29616;&#20195;&#33310;&#24212;&#29992;&#20013;&#19977;&#32500;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#26041;&#27861;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications. (arXiv:2401.02383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#29616;&#20195;&#33310;&#21644;&#34920;&#28436;&#33402;&#26415;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#24103;&#26041;&#27861;&#22312;&#29616;&#20195;&#33310;&#36424;&#34920;&#28436;&#20013;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#27604;&#21333;&#24103;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;RGB&#22270;&#20687;&#20013;&#20272;&#35745;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#22686;&#24378;/&#34394;&#25311;&#29616;&#23454;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#36523;&#25216;&#26415;&#20197;&#21450;&#34394;&#25311;&#38646;&#21806;&#31561;&#28508;&#22312;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65306;i&#65289;&#21333;&#20010;&#22270;&#20687;&#65292;ii&#65289;&#22810;&#35270;&#22270;&#22270;&#20687;&#21644;iii&#65289;&#35270;&#39057;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#24182;&#27604;&#36739;&#20102;&#29616;&#20195;&#33310;&#21644;&#34920;&#28436;&#33402;&#26415;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#20307;&#23039;&#21183;&#21644;&#31359;&#30528;&#12289;&#25668;&#20687;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#21644;&#32972;&#26223;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#29616;&#20195;&#33310;&#36424;&#34920;&#28436;&#20013;&#30340;&#23039;&#21183;&#20272;&#35745;&#65292;&#22914;PHALP&#36825;&#26679;&#30340;&#22810;&#24103;&#26041;&#27861;&#27604;&#21333;&#24103;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human body shape and pose estimation from RGB images is a challenging problem with potential applications in augmented/virtual reality, healthcare and fitness technology and virtual retail. Recent solutions have focused on three types of inputs: i) single images, ii) multi-view images and iii) videos. In this study, we surveyed and compared 3D body shape and pose estimation methods for contemporary dance and performing arts, with a special focus on human body pose and dressing, camera viewpoint, illumination conditions and background conditions. We demonstrated that multi-frame methods, such as PHALP, provide better results than single-frame method for pose estimation when dancers are performing contemporary dances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#20102;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.02349</link><description>&lt;p&gt;
&#20998;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#20102;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#25110;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25104;&#21151;&#21644;&#20851;&#27880;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30446;&#21069;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#27491;&#22312;&#34987;&#24212;&#29992;&#65292;&#20174;&#21307;&#30103;&#24212;&#29992;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#20294;&#20851;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26377;&#35768;&#22810;&#24453;&#35299;&#31572;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36935;&#21040;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20013;&#30340;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#20026;&#24403;&#21069;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#19968;&#20010;&#31616;&#27905;&#31995;&#32479;&#30340;&#32479;&#19968;&#20998;&#26512;&#65292;&#24182;&#26377;&#21161;&#20110;&#26500;&#24314;&#20581;&#22766;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;CLIP&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;CLIP&#30340;&#35270;&#35273;&#29305;&#24449;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#37197;&#23545;&#30340;&#23383;&#24149;&#65292;&#32780;&#22270;&#20687;-&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#24314;&#27169;&#20026;&#19968;&#20010;&#38646;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2401.02347</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#25991;&#26412;&#35757;&#32451;&#25366;&#25496;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#36827;&#34892;&#38646;&#26679;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training. (arXiv:2401.02347v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02347
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;CLIP&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;CLIP&#30340;&#35270;&#35273;&#29305;&#24449;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#37197;&#23545;&#30340;&#23383;&#24149;&#65292;&#32780;&#22270;&#20687;-&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#24314;&#27169;&#20026;&#19968;&#20010;&#38646;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#23545;&#22270;&#20687;&#36827;&#34892;&#25551;&#36848;&#30340;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#27867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#24212;&#29992;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#23545;&#27604;&#22270;&#20687;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#30340;&#21147;&#37327;&#21487;&#20197;&#26377;&#26395;&#23454;&#29616;&#38646;&#26679;&#26412;&#23383;&#24149;&#29983;&#25104;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#23383;&#24149;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;CLIP&#28508;&#22312;&#31354;&#38388;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27169;&#24577;&#24046;&#36317;&#30772;&#22351;&#20102;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#38646;&#26679;&#26412;&#23383;&#24149;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;CLIP&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20004;&#20010;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#20110;&#25991;&#26412;&#25551;&#36848;&#20013;&#22266;&#26377;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;CLIP&#22270;&#20687;&#23376;&#21306;&#22495;&#30340;&#35270;&#35273;&#29305;&#24449;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#37197;&#23545;&#30340;&#23383;&#24149;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37197;&#23545;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#24314;&#27169;&#20026;&#19968;&#20010;&#38646;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions. In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.02290</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02290
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20132;&#20114;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#21364;&#27809;&#26377;&#24471;&#21040;&#24517;&#35201;&#30340;&#20851;&#27880;&#12290;&#23545;&#22522;&#20110;GNN&#30340;KGC&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#36866;&#24403;&#35299;&#37322;&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;KGC&#35299;&#37322;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23454;&#20363;/&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#36335;&#24452;&#21487;&#20197;&#25552;&#20379;&#26356;&#21451;&#22909;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#23545;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Power-Link&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#20351;&#24471;&#21487;&#20197;&#20197;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37492;&#20110;&#22797;&#26434;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24322;&#36136;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#20540;&#21450;&#20854;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#20851;&#27880;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25554;&#34917;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02258</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#28145;&#24230;&#20851;&#27880;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#36136;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02258
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37492;&#20110;&#22797;&#26434;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24322;&#36136;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#20540;&#21450;&#20854;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#20851;&#27880;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25554;&#34917;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26222;&#36941;&#23384;&#22312;&#32570;&#22833;&#65292;&#32473;&#21487;&#38752;&#30340;&#19979;&#28216;&#20998;&#26512;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#23613;&#31649;&#36882;&#24402;&#32593;&#32476;&#25554;&#34917;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#19981;&#33021;&#25193;&#23637;&#21040;&#21487;&#20197;&#32531;&#35299;&#22797;&#26434;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#30340;&#28145;&#24230;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25554;&#34917;&#36824;&#23384;&#22312;&#20272;&#35745;&#22320;&#38754;&#30495;&#20540;&#20559;&#24046;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#23545;&#25554;&#34917;&#20540;&#30340;&#32622;&#20449;&#24230;&#22987;&#32456;&#26159;&#26410;&#34987;&#27979;&#37327;&#30340;&#25110;&#20174;&#27169;&#22411;&#36755;&#20986;&#21518;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DEep Attention Recurrent Imputation (DEARI)&#65292;&#23427;&#22312;&#24322;&#36136;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#21516;&#26102;&#20272;&#35745;&#32570;&#22833;&#20540;&#21450;&#20854;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#32852;&#21512;&#34920;&#31034;&#29305;&#24449;&#30456;&#20851;&#24615;&#21644;&#26102;&#24207;&#21160;&#24577;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#26377;&#25928;&#30340;&#27531;&#24046;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#25554;&#34917;&#24615;&#33021;&#21644;&#31283;&#23450;&#25910;&#25947;&#24615;&#30340;&#28145;&#24230;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#33258;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#26469;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis. Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data. Moreover, imputation carries the risk of biased estimations of the ground truth. Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output. We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series. By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence. We also leverage self-supervised metric learning to boost performance by optimizing sample similarity. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31163;&#32447;&#35268;&#33539;&#21270;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20197;&#21033;&#29992;&#31163;&#32447;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#22810;&#30446;&#26631;&#25919;&#31574;&#12290;&#22312;&#38754;&#23545;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#38382;&#39064;&#26102;&#65292;&#25552;&#20986;&#20102;&#36807;&#28388;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#20559;&#22909;&#26465;&#20214;&#21270;&#26631;&#37327;&#21270;&#26356;&#26032;&#19982;&#25919;&#31574;&#35268;&#33539;&#21270;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.02244</link><description>&lt;p&gt;
&#25919;&#31574;&#35268;&#33539;&#21270;&#30340;&#31163;&#32447;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31163;&#32447;&#35268;&#33539;&#21270;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20197;&#21033;&#29992;&#31163;&#32447;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#22810;&#30446;&#26631;&#25919;&#31574;&#12290;&#22312;&#38754;&#23545;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#38382;&#39064;&#26102;&#65292;&#25552;&#20986;&#20102;&#36807;&#28388;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#20559;&#22909;&#26465;&#20214;&#21270;&#26631;&#37327;&#21270;&#26356;&#26032;&#19982;&#25919;&#31574;&#35268;&#33539;&#21270;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#20165;&#20351;&#29992;&#31163;&#32447;&#36712;&#36857;&#25968;&#25454;&#26469;&#35757;&#32451;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#23558;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#21333;&#30446;&#26631;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31163;&#32447;&#35268;&#33539;&#21270;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#35774;&#32622;&#65292;&#20197;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#32447;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#36817;&#20284;&#34892;&#20026;&#20559;&#22909;&#26469;&#36807;&#28388;&#20986;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#65292;&#21644;2&#65289;&#37319;&#29992;&#20855;&#26377;&#39640;&#31574;&#30053;&#34920;&#36798;&#33021;&#21147;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20559;&#22909;&#26465;&#20214;&#21270;&#26631;&#37327;&#21270;&#26356;&#26032;&#26041;&#27861;&#34701;&#20837;&#21040;&#25919;&#31574;&#35268;&#33539;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20197;&#20351;&#29992;&#21333;&#20010;&#31574;&#30053;&#32593;&#32476;&#21516;&#26102;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20174;&#32780;&#20943;&#23569;&#20026;&#21508;&#31181;&#20559;&#22909;&#35757;&#32451;&#22823;&#37327;&#20010;&#20307;&#31574;&#30053;&#25152;&#20135;&#29983;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#26435;&#37325;&#35843;&#25972;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adapta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#65288;JMFRN&#65289;&#29992;&#20110;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#22312;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#65292;&#36890;&#36807;&#32858;&#21512;&#23454;&#20307;&#21644;&#26102;&#38388;&#25139;&#20449;&#24687;&#26469;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02212</link><description>&lt;p&gt;
&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#29992;&#20110;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#22312;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph. (arXiv:2401.02212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#65288;JMFRN&#65289;&#29992;&#20110;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#22312;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#65292;&#36890;&#36807;&#32858;&#21512;&#23454;&#20307;&#21644;&#26102;&#38388;&#25139;&#20449;&#24687;&#26469;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#22312;&#24120;&#35268;&#30693;&#35782;&#22270;&#30340;&#22522;&#30784;&#19978;&#21152;&#20837;&#26102;&#38388;&#33539;&#22260;&#30340;&#25193;&#23637;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;TKGQA&#65289;&#27169;&#22411;&#20165;&#22788;&#29702;&#31616;&#21333;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20808;&#21069;&#20551;&#35774;&#27599;&#20010;&#38382;&#39064;&#21482;&#21253;&#21547;&#19968;&#20010;&#20855;&#26377;&#26174;&#24335;/&#38544;&#24335;&#26102;&#38388;&#32422;&#26463;&#30340;&#26102;&#38388;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#26102;&#38388;&#20107;&#23454;&#30340;&#38382;&#39064;&#34920;&#29616;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#65288;JMFRN&#65289;&#65292;&#29992;&#20110;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;JMFRN&#39318;&#20808;&#20174;TKG&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#22797;&#26434;&#38382;&#39064;&#30340;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#26102;&#38388;&#20107;&#23454;&#12290;&#20026;&#20102;&#36827;&#34892;&#32852;&#21512;&#25512;&#29702;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;&#21363;&#23454;&#20307;&#24863;&#30693;&#21644;&#26102;&#38388;&#24863;&#30693;&#65289;&#65292;&#36866;&#29992;&#20110;&#36890;&#29992;&#35774;&#32622;&#65292;&#20197;&#32858;&#21512;&#23454;&#20307;&#21644;&#26102;&#38388;&#25139;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by attaching the time scope. Existing temporal knowledge graph question answering (TKGQA) models solely approach simple questions, owing to the prior assumption that each question only contains a single temporal fact with explicit/implicit temporal constraints. Hence, they perform poorly on questions which own multiple temporal facts. In this paper, we propose \textbf{\underline{J}}oint \textbf{\underline{M}}ulti \textbf{\underline{F}}acts \textbf{\underline{R}}easoning \textbf{\underline{N}}etwork (JMFRN), to jointly reasoning multiple temporal facts for accurately answering \emph{complex} temporal questions. Specifically, JMFRN first retrieves question-related temporal facts from TKG for each entity of the given complex question. For joint reasoning, we design two different attention (\ie entity-aware and time-aware) modules, which are suitable for universal settings, to aggregate entities and timestamps inform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#20998;&#31867;&#23454;&#26102;&#30340;&#36710;&#36733;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20197;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#27700;&#24179;&#21644;&#24773;&#22659;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.02199</link><description>&lt;p&gt;
LADRI: &#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#39118;&#38505;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System. (arXiv:2401.02199v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#20998;&#31867;&#23454;&#26102;&#30340;&#36710;&#36733;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20197;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#27700;&#24179;&#21644;&#24773;&#22659;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#30340;&#28436;&#36827;&#65292;&#26234;&#33021;&#20132;&#36890;&#30340;&#35270;&#37326;&#19981;&#26029;&#25193;&#22823;&#65292;&#30830;&#20445;&#26497;&#20854;&#23433;&#20840;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#20026;&#36843;&#20999;&#12290;&#20256;&#32479;&#30340;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#20154;&#24037;&#39550;&#39542;&#30340;&#36710;&#36742;&#65292;&#26080;&#27861;&#20805;&#20998;&#36866;&#24212;ADS&#22810;&#26041;&#38754;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#23454;&#26102;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#65288;DRA&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#31361;&#30772;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26680;&#24515;&#37096;&#20998;&#8212;&#8212;ANN&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#20998;&#31867;&#23454;&#26102;&#30340;&#36710;&#36733;&#20256;&#24863;&#22120;&#65288;OBS&#65289;&#25968;&#25454;&#26469;&#32454;&#33268;&#22320;&#20998;&#26512;&#21644;&#20998;&#31867;&#39118;&#38505;&#32500;&#24230;&#12290;&#36825;&#31181;&#20197;&#23398;&#20064;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#19981;&#20165;&#25552;&#21319;&#20102;ADS&#30340;&#24773;&#22659;&#24847;&#35782;&#65292;&#36824;&#20016;&#23500;&#20102;&#20854;&#23545;&#21363;&#26102;&#36816;&#34892;&#29615;&#22659;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;OBS&#25968;&#25454;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20854;&#24403;&#21069;&#30340;&#39118;&#38505;&#37197;&#32622;&#25991;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20056;&#23458;&#21644;&#26356;&#24191;&#27867;&#26053;&#36884;&#20013;&#30340;&#23433;&#20840;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the horizon of intelligent transportation expands with the evolution of Automated Driving Systems (ADS), ensuring paramount safety becomes more imperative than ever. Traditional risk assessment methodologies, primarily crafted for human-driven vehicles, grapple to adequately adapt to the multifaceted, evolving environments of ADS. This paper introduces a framework for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of Artificial Neural Networks (ANNs).  Our proposed solution transcends these limitations, drawing upon ANNs, a cornerstone of deep learning, to meticulously analyze and categorize risk dimensions using real-time On-board Sensor (OBS) data. This learning-centric approach not only elevates the ADS's situational awareness but also enriches its understanding of immediate operational contexts. By dissecting OBS data, the system is empowered to pinpoint its current risk profile, thereby enhancing safety prospects for onboard passengers and the broader tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;FairGridSearch&#65292;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#32452;&#21512;&#24182;&#25512;&#33616;&#26368;&#20339;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#20934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#24230;&#37327;&#23545;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.02183</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#27604;&#36739;&#30340;&#26694;&#26550;: FairGridSearch
&lt;/p&gt;
&lt;p&gt;
FairGridSearch: A Framework to Compare Fairness-Enhancing Models. (arXiv:2401.02183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;FairGridSearch&#65292;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#32452;&#21512;&#24182;&#25512;&#33616;&#26368;&#20339;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#20934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#24230;&#37327;&#23545;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#20915;&#31574;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#22797;&#21046;&#25110;&#29978;&#33267;&#25918;&#22823;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#23384;&#22312;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#21644;&#22522;&#26412;&#20272;&#35745;&#22120;&#65292;&#20294;&#36873;&#25321;&#29305;&#23450;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#20108;&#20803;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27604;&#36739;&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;FairGridSearch&#12290;FairGridSearch&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#32452;&#21512;&#65292;&#24182;&#25512;&#33616;&#26368;&#20339;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#23558;FairGridSearch&#24212;&#29992;&#20110;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;(&#25104;&#24180;&#20154;&#12289;COMPAS&#21644;&#24503;&#22269;&#20449;&#29992;)&#65292;&#24182;&#20998;&#26512;&#24230;&#37327;&#36873;&#25321;&#12289;&#22522;&#26412;&#20272;&#35745;&#22120;&#36873;&#25321;&#21644;&#20998;&#31867;&#38408;&#20540;&#23545;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#36873;&#25321;&#21512;&#36866;&#30340;&#20934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#24230;&#37327;&#23545;&#27169;&#22411;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#22522;&#26412;&#20272;&#35745;&#22120;&#21644;&#20998;&#31867;&#38408;&#20540;&#20063;&#20250;&#23545;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are increasingly used in critical decision-making applications. However, these models are susceptible to replicating or even amplifying bias present in real-world data. While there are various bias mitigation methods and base estimators in the literature, selecting the optimal model for a specific application remains challenging.  This paper focuses on binary classification and proposes FairGridSearch, a novel framework for comparing fairness-enhancing models. FairGridSearch enables experimentation with different model parameter combinations and recommends the best one. The study applies FairGridSearch to three popular datasets (Adult, COMPAS, and German Credit) and analyzes the impacts of metric selection, base estimator choice, and classification threshold on model fairness.  The results highlight the significance of selecting appropriate accuracy and fairness metrics for model evaluation. Additionally, different base estimators and classification threshold va
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#21592;&#20877;&#35782;&#21035;&#30340;&#25552;&#31034;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#31163;&#39046;&#22495;&#36866;&#24212;&#21644;&#20219;&#21153;&#36866;&#24212;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#31454;&#20105;&#24615;&#24615;&#33021;&#30340;&#21516;&#26102;&#36991;&#20813;&#20102;&#21516;&#26102;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#21644;&#20219;&#21153;&#36866;&#24212;&#30340;&#23376;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02173</link><description>&lt;p&gt;
&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#21592;&#20877;&#35782;&#21035;&#30340;&#25552;&#31034;&#35299;&#32806;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt Decoupling for Text-to-Image Person Re-identification. (arXiv:2401.02173v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#21592;&#20877;&#35782;&#21035;&#30340;&#25552;&#31034;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#31163;&#39046;&#22495;&#36866;&#24212;&#21644;&#20219;&#21153;&#36866;&#24212;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#31454;&#20105;&#24615;&#24615;&#33021;&#30340;&#21516;&#26102;&#36991;&#20813;&#20102;&#21516;&#26102;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#21644;&#20219;&#21153;&#36866;&#24212;&#30340;&#23376;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;TIReID&#65289;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26597;&#35810;&#20174;&#22270;&#20687;&#30011;&#24266;&#20013;&#26816;&#32034;&#30446;&#26631;&#20154;&#21592;&#12290;&#26368;&#36817;&#65292;&#20687;CLIP&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#65292;&#24182;&#24191;&#27867;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#20041;&#27010;&#24565;&#23398;&#20064;&#21644;&#20016;&#23500;&#30340;&#22810;&#27169;&#24335;&#30693;&#35782;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22522;&#20110;CLIP&#30340;TIReID&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#30452;&#25509;&#24494;&#35843;&#25972;&#20010;&#32593;&#32476;&#20197;&#36866;&#24212;TIReID&#20219;&#21153;&#30340;CLIP&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#21644;&#20219;&#21153;&#36866;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23581;&#35797;&#22312;&#35757;&#32451;&#38454;&#27573;&#23545;&#36825;&#20004;&#20010;&#36807;&#31243;&#36827;&#34892;&#35299;&#32806;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#26469;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#36866;&#24212;&#19982;&#20219;&#21153;&#36866;&#24212;&#20998;&#31163;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20923;&#32467;CLIP&#20013;&#30340;&#20004;&#20010;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image person re-identification (TIReID) aims to retrieve the target person from an image gallery via a textual description query. Recently, pre-trained vision-language models like CLIP have attracted significant attention and have been widely utilized for this task due to their robust capacity for semantic concept learning and rich multi-modal knowledge. However, recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the entire network to adapt the CLIP model for the TIReID task. Although these methods show competitive performance on this topic, they are suboptimal as they necessitate simultaneous domain adaptation and task adaptation. To address this issue, we attempt to decouple these two processes during the training stage. Specifically, we introduce the prompt tuning strategy to enable domain adaptation and propose a two-stage training approach to disentangle domain adaptation from task adaptation. In the first stage, we freeze the two encoders from CLIP an
&lt;/p&gt;</description></item><item><title>Shayona&#22242;&#38431;&#22312;SMMH4-23&#20013;&#20351;&#29992;&#20102;BERT&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;COVID-19&#33258;&#25105;&#35786;&#26029;&#20998;&#31867;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;0.94&#12290;</title><link>http://arxiv.org/abs/2401.02158</link><description>&lt;p&gt;
Shayona@SMM4H23&#65306;&#20351;&#29992;BERT&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;COVID-19&#33258;&#25105;&#35786;&#26029;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models. (arXiv:2401.02158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02158
&lt;/p&gt;
&lt;p&gt;
Shayona&#22242;&#38431;&#22312;SMMH4-23&#20013;&#20351;&#29992;&#20102;BERT&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;COVID-19&#33258;&#25105;&#35786;&#26029;&#20998;&#31867;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;0.94&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Shayona&#22242;&#38431;&#22312;SMMH4-23&#30340;&#20849;&#20139;&#20219;&#21153;1&#21644;4&#20013;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#20849;&#20139;&#20219;&#21153;1&#26159;&#23545;&#33258;&#25253;COVID-19&#35786;&#26029;&#30340;&#33521;&#25991;&#25512;&#25991;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#20849;&#20139;&#20219;&#21153;4&#26159;&#23545;&#33258;&#25253;&#31038;&#20132;&#28966;&#34385;&#38556;&#30861;&#35786;&#26029;&#30340;&#33521;&#25991;Reddit&#24086;&#23376;&#36827;&#34892;&#20108;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#26368;&#39640;&#30340;F1&#20998;&#25968;0.94&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#37117;&#20351;&#29992;&#20102;Transformer&#27169;&#22411;&#65288;BERT&#65289;&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes approaches and results for shared Task 1 and 4 of SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary classification of English Reddit posts self-reporting a social anxiety disorder diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all participants. We have leveraged the Transformer model (BERT) in combination with the LightGBM model for both tasks.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26550;&#26500;&#26469;&#35299;&#20915;&#36328;&#24179;&#21488;&#25968;&#25454;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#32422;&#26463;&#26465;&#20214;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02154</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Disentangle Estimation of Causal Effects from Cross-Silo Data. (arXiv:2401.02154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02154
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26550;&#26500;&#26469;&#35299;&#20915;&#36328;&#24179;&#21488;&#25968;&#25454;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#32422;&#26463;&#26465;&#20214;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#33647;&#29289;&#30740;&#21457;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#20272;&#35745;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#29305;&#24449;&#21487;&#33021;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#24179;&#21488;&#19978;&#65292;&#24182;&#19988;&#22312;&#21508;&#26041;&#20043;&#38388;&#20445;&#25345;&#31169;&#23494;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30452;&#25509;&#20449;&#24687;&#20132;&#27969;&#12290;&#36825;&#21453;&#36807;&#26469;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#22240;&#26524;&#25928;&#24212;&#30340;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#65292;&#20381;&#36182;&#20110;&#20165;&#23376;&#38598;&#21327;&#21464;&#37327;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#32806;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#20849;&#20139;&#21644;&#31169;&#26377;&#20998;&#25903;&#30340;&#32452;&#21512;&#20419;&#36827;&#27169;&#22411;&#21442;&#25968;&#30340;&#26080;&#32541;&#36328;&#24179;&#21488;&#20256;&#36882;&#65292;&#20016;&#23500;&#22240;&#26524;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#23616;&#32422;&#26463;&#26465;&#20214;&#26469;&#26377;&#25928;&#20943;&#36731;&#21508;&#20010;&#32570;&#22833;&#22495;&#20869;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25105;&#20204;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26032;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects among different events is of great importance to critical fields such as drug development. Nevertheless, the data features associated with events may be distributed across various silos and remain private within respective parties, impeding direct information exchange between them. This, in turn, can result in biased estimations of local causal effects, which rely on the characteristics of only a subset of the covariates. To tackle this challenge, we introduce an innovative disentangle architecture designed to facilitate the seamless cross-silo transmission of model parameters, enriched with causal mechanisms, through a combination of shared and private branches. Besides, we introduce global constraints into the equation to effectively mitigate bias within the various missing domains, thereby elevating the accuracy of our causal effect estimation. Extensive experiments conducted on new semi-synthetic datasets show that our method outperforms state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;ASP&#20013;&#30340;&#21333;&#20803;&#27979;&#35797;&#35821;&#35328;&#21644;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#32852;&#27979;&#35797;&#35268;&#33539;&#35821;&#35328;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#24212;&#20219;&#21153;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.02153</link><description>&lt;p&gt;
ASP&#20013;&#30340;&#21333;&#20803;&#27979;&#35797;&#37325;&#26032;&#23457;&#35270;&#65306;&#35821;&#35328;&#21644;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Unit Testing in ASP Revisited: Language and Test-Driven Development Environment. (arXiv:2401.02153v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;ASP&#20013;&#30340;&#21333;&#20803;&#27979;&#35797;&#35821;&#35328;&#21644;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#32852;&#27979;&#35797;&#35268;&#33539;&#35821;&#35328;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#24212;&#20219;&#21153;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20803;&#27979;&#35797;&#26694;&#26550;&#29616;&#22914;&#20170;&#34987;&#35270;&#20026;&#19968;&#31181;&#26368;&#20339;&#23454;&#36341;&#65292;&#20960;&#20046;&#21253;&#21547;&#22312;&#25152;&#26377;&#29616;&#20195;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#24320;&#21457;&#27491;&#30830;&#30340;&#35268;&#33539;&#12290;Answer Set Programming&#65288;ASP&#65289;&#31561;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#33539;&#20363;&#22312;&#24037;&#19994;&#32423;&#24212;&#29992;&#20013;&#20063;&#19981;&#20363;&#22806;&#12290;&#20107;&#23454;&#19978;&#65292;ASPIDE&#24320;&#21457;&#29615;&#22659;&#30340;&#39318;&#20010;ASP&#21333;&#20803;&#27979;&#35797;&#35268;&#33539;&#35821;&#35328;&#26159;&#22312;2011&#24180;&#25552;&#20986;&#30340;&#12290;&#38543;&#21518;&#65292;&#22312;LANA&#27880;&#37322;&#35821;&#35328;&#20013;&#21253;&#21547;&#20102;&#26356;&#21152;&#21487;&#31227;&#26893;&#30340;&#21333;&#20803;&#27979;&#35797;&#35821;&#35328;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;ASP&#20013;&#30340;&#20004;&#31181;&#35821;&#35328;&#21644;&#24037;&#20855;&#20197;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#20803;&#27979;&#35797;&#35268;&#33539;&#35821;&#35328;&#65292;&#21487;&#20197;&#22312;ASP&#31243;&#24207;&#20869;&#32852;&#27979;&#35797;&#65292;&#24182;&#30830;&#23450;&#19982;&#26816;&#26597;&#21508;&#31181;&#31243;&#24207;&#27491;&#30830;&#24615;&#26029;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#29992;&#20363;&#35268;&#33539;&#23545;&#20256;&#32479;&#35780;&#20272;&#26469;&#35828;&#26159;&#36879;&#26126;&#30340;&#65292;&#20294;&#21487;&#20197;&#30001;&#29305;&#23450;&#30340;&#27979;&#35797;&#24037;&#20855;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unit testing frameworks are nowadays considered a best practice, included in almost all modern software development processes, to achieve rapid development of correct specifications. Knowledge representation and reasoning paradigms such as Answer Set Programming (ASP), that have been used in industry-level applications, are not an exception. Indeed, the first unit testing specification language for ASP was proposed in 2011 as a feature of the ASPIDE development environment. Later, a more portable unit testing language was included in the LANA annotation language. In this paper we revisit both languages and tools for unit testing in ASP. We propose a new unit test specification language that allows one to inline tests within ASP programs, and we identify the computational complexity of the tasks associated with checking the various program-correctness assertions. Test-case specifications are transparent to the traditional evaluation, but can be interpreted by a specific testing tool. Th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36798;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#33021;&#21147;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;TDL&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;GNN4TDL&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#28436;&#21270;&#39046;&#22495;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.02143</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#24102;&#26377;&#20998;&#31867;&#21644;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions. (arXiv:2401.02143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36798;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#33021;&#21147;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;TDL&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;GNN4TDL&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#28436;&#21270;&#39046;&#22495;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#35813;&#32508;&#36848;&#31361;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;TDL&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65306;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36848;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#22825;&#28982;&#33021;&#21147;&#26469;&#27169;&#25311;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;TDL&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20852;&#36259;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#35774;&#35745;&#21644;&#23454;&#29616;GNN&#29992;&#20110;TDL&#65288;GNN4TDL&#65289;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#23427;&#21253;&#25324;&#23545;&#22522;&#30784;&#38382;&#39064;&#30340;&#35814;&#32454;&#30740;&#31350;&#21644;&#22522;&#20110;GNN&#30340;TDL&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#20026;&#20854;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#27880;&#26500;&#24314;&#22270;&#32467;&#26500;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#20840;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural Networks (GNNs), a domain where deep learning-based approaches have increasingly shown superior performance in both classification and regression tasks compared to traditional methods. The survey highlights a critical gap in deep neural TDL methods: the underrepresentation of latent correlations among data instances and feature values. GNNs, with their innate capability to model intricate relationships and interactions between diverse elements of tabular data, have garnered significant interest and application across various TDL domains. Our survey provides a systematic review of the methods involved in designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed investigation into the foundational aspects and an overview of GNN-based TDL methods, offering insights into their evolving landscape. We present a comprehensive taxonomy focused on constructing graph structures and representation learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SyCoCa&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#21644;&#26412;&#22320;&#34920;&#31034;&#23618;&#38754;&#19978;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#21521;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2401.02137</link><description>&lt;p&gt;
SyCoCa: &#29992;&#20851;&#27880;&#25513;&#30721;&#23545;&#22810;&#27169;&#24577;&#23545;&#40784;&#36827;&#34892;&#23545;&#31216;&#21270;&#30340;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment. (arXiv:2401.02137v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SyCoCa&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#21644;&#26412;&#22320;&#34920;&#31034;&#23618;&#38754;&#19978;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#21521;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26159;&#24403;&#21069;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20013;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#22120;&#65288;CoCa&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#23558;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#65288;IC&#65289;&#25972;&#21512;&#21040;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;IC&#22312;&#26412;&#22320;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#21333;&#21521;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#23427;&#32570;&#20047;&#23545;&#26412;&#22320;&#25991;&#26412;&#21040;&#22270;&#20687;&#37325;&#26500;&#30340;&#20219;&#20309;&#32422;&#26463;&#65292;&#22312;&#19982;&#25991;&#26412;&#23545;&#40784;&#26102;&#38480;&#21046;&#20102;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#20174;&#20840;&#23616;&#21644;&#26412;&#22320;&#20004;&#20010;&#35282;&#24230;&#23454;&#29616;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#31216;&#21270;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#22120;&#65288;SyCoCa&#65289;&#65292;&#23427;&#22312;&#20840;&#23616;&#21644;&#26412;&#22320;&#34920;&#31034;&#23618;&#38754;&#19978;&#24341;&#20837;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#21521;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;ITC&#30340;&#25991;&#26412;&#24341;&#23548;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;TG-MIM&#65289;&#22836;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal alignment between language and vision is the fundamental topic in current vision-language model research. Contrastive Captioners (CoCa), as a representative method, integrates Contrastive Language-Image Pretraining (CLIP) and Image Caption (IC) into a unified framework, resulting in impressive results. CLIP imposes a bidirectional constraints on global representation of entire images and sentences. Although IC conducts an unidirectional image-to-text generation on local representation, it lacks any constraint on local text-to-image reconstruction, which limits the ability to understand images at a fine-grained level when aligned with texts. To achieve multimodal alignment from both global and local perspectives, this paper proposes Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional interactions on images and texts across the global and local representation levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM) head based on ITC
&lt;/p&gt;</description></item><item><title>DCR-Consistency&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27573;&#33853;&#23545;&#27573;&#33853;&#27604;&#36739;&#21010;&#20998;&#20026;&#21477;&#23376;&#23545;&#27573;&#33853;&#30340;&#27604;&#36739;&#65292;&#24182;&#26681;&#25454;&#39044;&#23450;&#20041;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02132</link><description>&lt;p&gt;
DCR-Consistency: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#21644;&#25913;&#36827;&#30340;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models. (arXiv:2401.02132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02132
&lt;/p&gt;
&lt;p&gt;
DCR-Consistency&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27573;&#33853;&#23545;&#27573;&#33853;&#27604;&#36739;&#21010;&#20998;&#20026;&#21477;&#23376;&#23545;&#27573;&#33853;&#30340;&#27604;&#36739;&#65292;&#24182;&#26681;&#25454;&#39044;&#23450;&#20041;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#21464;&#24322;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#23578;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22914;ROUGE&#21644;BERTScore&#65292;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#25972;&#20307;&#35821;&#20041;&#30340;&#31561;&#20215;&#24615;&#12290;&#36825;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#21644;&#30452;&#35273;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#24378;&#22823;&#30340;&#20915;&#31574;&#33021;&#21147;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DCR&#26694;&#26550;&#65292;&#19968;&#31181;&#20351;&#29992;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#29983;&#25104;&#25991;&#26412;&#19968;&#33268;&#24615;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#19981;&#21516;&#65292;&#26412;&#26041;&#27861;&#37319;&#29992;&#20102;&#21010;&#20998;&#21644;&#24449;&#26381;&#35780;&#20272;&#22120;&#65288;DCE&#65289;&#65292;&#23558;&#20004;&#20010;&#29983;&#25104;&#30340;&#22238;&#31572;&#20043;&#38388;&#30340;&#27573;&#33853;&#23545;&#27573;&#33853;&#27604;&#36739;&#20998;&#35299;&#20026;&#26681;&#25454;&#39044;&#23450;&#20041;&#26631;&#20934;&#35780;&#20272;&#30340;&#27599;&#20010;&#21477;&#23376;&#23545;&#27573;&#33853;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge. Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical. This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach. Unlike existing LLM-based evaluators that operate at the paragraph level, our method employs a divide-and-conquer evaluator (DCE) that breaks down the paragraph-to-paragraph comparison between two generated responses into individual sentence-to-paragraph comparisons, each evaluated based on predefined criteria. T
&lt;/p&gt;</description></item><item><title>ACP-ESM&#26159;&#19968;&#20010;&#20351;&#29992;&#20197;&#34507;&#30333;&#36136;&#20026;&#23548;&#21521;&#30340;Transformer&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#25239;&#30284;&#32957;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25239;&#30284;&#32957;&#30340;&#31283;&#23450;&#24615;&#65292;&#25913;&#21892;&#36873;&#25321;&#24615;&#65292;&#24182;&#25552;&#39640;&#23545;&#30284;&#32454;&#32990;&#30340;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02124</link><description>&lt;p&gt;
ACP-ESM:&#19968;&#31181;&#20351;&#29992;&#20197;&#34507;&#30333;&#36136;&#20026;&#23548;&#21521;&#30340;Transformer&#26041;&#27861;&#36827;&#34892;&#25239;&#30284;&#32957;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach. (arXiv:2401.02124v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02124
&lt;/p&gt;
&lt;p&gt;
ACP-ESM&#26159;&#19968;&#20010;&#20351;&#29992;&#20197;&#34507;&#30333;&#36136;&#20026;&#23548;&#21521;&#30340;Transformer&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#25239;&#30284;&#32957;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25239;&#30284;&#32957;&#30340;&#31283;&#23450;&#24615;&#65292;&#25913;&#21892;&#36873;&#25321;&#24615;&#65292;&#24182;&#25552;&#39640;&#23545;&#30284;&#32454;&#32990;&#30340;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#30284;&#32957;(ACPs)&#26159;&#19968;&#31867;&#22312;&#30284;&#30151;&#30740;&#31350;&#21644;&#27835;&#30103;&#39046;&#22495;&#24341;&#36215;&#37325;&#35270;&#30340;&#20998;&#23376;&#12290; ACPs&#26159;&#30001;&#27688;&#22522;&#37240;&#26500;&#25104;&#30340;&#30701;&#38142;&#65292;&#26159;&#34507;&#30333;&#36136;&#30340;&#26500;&#24314;&#22522;&#22359;&#65292;&#24182;&#20855;&#26377;&#36873;&#25321;&#24615;&#38774;&#21521;&#21644;&#26432;&#27515;&#30284;&#32454;&#32990;&#30340;&#33021;&#21147;&#12290; ACPs&#20043;&#25152;&#20197;&#20855;&#26377;&#38024;&#23545;&#30284;&#32454;&#32990;&#30340;&#36873;&#25321;&#24615;&#65292;&#24448;&#24448;&#24402;&#22240;&#20110;&#30284;&#32454;&#32990;&#34920;&#38754;&#29305;&#24615;&#19982;&#27491;&#24120;&#32454;&#32990;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;ACP&#34987;&#35748;&#20026;&#26159;&#30284;&#30151;&#27835;&#30103;&#30340;&#28508;&#22312;&#20505;&#36873;&#33647;&#29289;&#12290;ACPs&#21487;&#20197;&#21333;&#29420;&#20351;&#29992;&#25110;&#19982;&#21270;&#30103;&#21644;&#25918;&#30103;&#31561;&#27835;&#30103;&#26041;&#24335;&#32852;&#21512;&#20351;&#29992;&#12290;&#34429;&#28982;ACP&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30284;&#30151;&#27835;&#30103;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20248;&#21270;&#20854;&#31283;&#23450;&#24615;&#65292;&#25913;&#21892;&#36873;&#25321;&#24615;&#65292;&#25552;&#39640;&#23545;&#30284;&#32454;&#32990;&#30340;&#20256;&#36882;&#24615;&#65292;&#24182;&#19988;&#24212;&#23545;&#32957;&#24207;&#21015;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, develo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31227;&#21160;ALOHA&#31995;&#32479;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#25163;&#31227;&#21160;&#25805;&#20316;&#12290;&#36890;&#36807;&#20302;&#25104;&#26412;&#30340;&#36828;&#31243;&#25805;&#20316;&#21644;&#25972;&#20307;&#36523;&#20307;&#25511;&#21046;&#65292;&#31995;&#32479;&#33021;&#22815;&#23436;&#25104;&#22797;&#26434;&#30340;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25552;&#39640;&#25104;&#21151;&#29575;&#36798;&#21040;90%&#12290;</title><link>http://arxiv.org/abs/2401.02117</link><description>&lt;p&gt;
&#31227;&#21160;ALOHA&#65306;&#20302;&#25104;&#26412;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#23398;&#20064;&#21452;&#25163;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation. (arXiv:2401.02117v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31227;&#21160;ALOHA&#31995;&#32479;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#25163;&#31227;&#21160;&#25805;&#20316;&#12290;&#36890;&#36807;&#20302;&#25104;&#26412;&#30340;&#36828;&#31243;&#25805;&#20316;&#21644;&#25972;&#20307;&#36523;&#20307;&#25511;&#21046;&#65292;&#31995;&#32479;&#33021;&#22815;&#23436;&#25104;&#22797;&#26434;&#30340;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25552;&#39640;&#25104;&#21151;&#29575;&#36798;&#21040;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#28436;&#31034;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30340;&#32467;&#26524;&#38598;&#20013;&#22312;&#26700;&#38754;&#25805;&#20316;&#19978;&#65292;&#32570;&#20047;&#23545;&#20110;&#36890;&#24120;&#26377;&#29992;&#20219;&#21153;&#25152;&#38656;&#35201;&#30340;&#31227;&#21160;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#20223;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#21452;&#25163;&#25805;&#20316;&#19988;&#38656;&#35201;&#20840;&#36523;&#25511;&#21046;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#31227;&#21160;ALOHA&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#21644;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;&#12290;&#23427;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#31227;&#21160;&#24213;&#30424;&#21644;&#19968;&#20010;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#25509;&#21475;&#26469;&#22686;&#24378;ALOHA&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36890;&#36807;&#31227;&#21160;ALOHA&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#30417;&#30563;&#24335;&#34892;&#20026;&#20811;&#38534;&#65292;&#24182;&#21457;&#29616;&#19982;&#29616;&#26377;&#30340;&#38745;&#24577;ALOHA&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;50&#27425;&#28436;&#31034;&#65292;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#23558;&#25104;&#21151;&#29575;&#25552;&#39640;90%&#65292;&#20351;&#24471;&#31227;&#21160;ALOHA&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#22797;&#26434;&#30340;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#28818;&#34430;&#21644;&#19978;&#33756;&#65292;&#24182;&#25171;&#24320;&#19968;&#20010;&#21452;&#38376;&#22721;&#26588;&#23384;&#25918;&#37325;&#22411;&#28921;&#39274;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;k-Winners-Take-All&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;kWTA-ENN&#65289;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#23376;&#32593;&#32476;&#26469;&#25913;&#36827;&#20197;&#24448;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;kWTA&#28608;&#27963;&#20989;&#25968;&#20316;&#20026;&#36755;&#20986;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.02092</link><description>&lt;p&gt;
k-Winners-Take-All&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
k-Winners-Take-All Ensemble Neural Network. (arXiv:2401.02092v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;k-Winners-Take-All&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;kWTA-ENN&#65289;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#23376;&#32593;&#32476;&#26469;&#25913;&#36827;&#20197;&#24448;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;kWTA&#28608;&#27963;&#20989;&#25968;&#20316;&#20026;&#36755;&#20986;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#26159;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#24120;&#36890;&#36807;&#23545;&#23427;&#20204;&#30340;&#21333;&#29420;&#36755;&#20986;&#36827;&#34892;&#24179;&#22343;&#25110;&#27714;&#21644;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#31181;&#38598;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#23376;&#32593;&#32476;&#32780;&#19981;&#26159;&#29420;&#31435;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#31181;&#24182;&#21457;&#35757;&#32451;&#20351;&#23376;&#32593;&#32476;&#30456;&#20114;&#21512;&#20316;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#21512;&#20316;&#38598;&#21512;&#8221;&#12290;&#21516;&#26102;&#65292;&#19987;&#23478;&#32452;&#21512;&#26041;&#27861;&#36890;&#36807;&#23558;&#32473;&#23450;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#32593;&#32476;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#38376;&#25511;&#32593;&#32476;&#65292;&#32473;&#23427;&#30340;&#23376;&#32593;&#32476;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#19987;&#19994;&#21270;&#35282;&#33394;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#19978;&#36848;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;k-Winners-Take-All&#65288;kWTA&#65289;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#20316;&#20026;&#38598;&#21512;&#20013;&#27599;&#20010;&#23376;&#32593;&#32476;&#36755;&#20986;&#30340;&#32452;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;&#8220;kWTA&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#8221;&#65288;kWTA-ENN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling is one approach that improves the performance of a neural network by combining a number of independent neural networks, usually by either averaging or summing up their individual outputs. We modify this ensembling approach by training the sub-networks concurrently instead of independently. This concurrent training of sub-networks leads them to cooperate with each other, and we refer to them as "cooperative ensemble". Meanwhile, the mixture-of-experts approach improves a neural network performance by dividing up a given dataset to its sub-networks. It then uses a gating network that assigns a specialization to each of its sub-networks called "experts". We improve on these aforementioned ways for combining a group of neural networks by using a k-Winners-Take-All (kWTA) activation function, that acts as the combination method for the outputs of each sub-network in the ensemble. We refer to this proposed model as "kWTA ensemble neural networks" (kWTA-ENN). With the kWTA activati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#36827;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02051</link><description>&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20987;&#36133;&#20154;&#31867;&#65306;&#39640;&#25928;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#35774;&#35745;&#30340;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search. (arXiv:2401.02051v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02051
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#36827;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#65292;&#35774;&#35745;&#39640;&#25928;&#31639;&#27861;&#36890;&#24120;&#38750;&#24120;&#32321;&#29712;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36827;&#21270;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;AEL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#12290;AEL&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#36827;&#21270;&#35745;&#31639;&#30340;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#33258;&#21160;&#35774;&#35745;&#12289;&#32452;&#21512;&#21644;&#20462;&#25913;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;AEL&#26469;&#35774;&#35745;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#65288;GLS&#65289;&#30340;&#24341;&#23548;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#33879;&#21517;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;AEL&#33258;&#21160;&#28436;&#21270;&#20986;&#20248;&#31168;&#30340;GLS&#31639;&#27861;&#65292;&#22312;&#20004;&#22825;&#20869;&#23454;&#29616;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#21644;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;1,000&#20010;TSP20-TSP100&#23454;&#20363;&#21644;TSPLib&#23454;&#20363;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AEL&#35774;&#35745;&#30340;GLS&#31639;&#27861;&#22312;&#30456;&#21516;&#30340;&#36845;&#20195;&#39044;&#31639;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#35774;&#35745;&#30340;GLS&#31639;&#27861;&#12290;&#22312;1,000&#27425;&#36845;&#20195;&#20013;&#65292;&#23427;&#22312;TSP20&#21644;TSP50&#19978;&#36798;&#21040;0%&#38388;&#38553;&#65292;&#22312;TSP100&#19978;&#36798;&#21040;0.032%&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often very tedious for human experts to design efficient algorithms. Recently, we have proposed a novel Algorithm Evolution using Large Language Model (AEL) framework for automatic algorithm design. AEL combines the power of a large language model and the paradigm of evolutionary computation to design, combine, and modify algorithms automatically. In this paper, we use AEL to design the guide algorithm for guided local search (GLS) to solve the well-known traveling salesman problem (TSP). AEL automatically evolves elite GLS algorithms in two days, with minimal human effort and no model training. Experimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show that AEL-designed GLS outperforms state-of-the-art human-designed GLS with the same iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap on TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in automatic algorithm design.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02009</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#65306;&#36890;&#36807;&#19981;&#19968;&#33268;&#30340;&#27714;&#35299;&#35270;&#35282;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24605;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02009
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#20107;&#21518;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;&#21453;&#24605;&#21644;&#33258;&#25105;&#25913;&#36827;&#65292;&#26681;&#25454;&#33258;&#25105;&#35780;&#20272;&#25110;&#22806;&#37096;&#21453;&#39304;&#26469;&#25913;&#21892;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#20869;&#22312;&#21453;&#24605;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#33258;&#25105;&#35780;&#20272;&#21453;&#39304;&#36136;&#37327;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#33258;&#25105;&#35780;&#20272;&#26102;&#24120;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#22266;&#25191;&#25110;&#19981;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#23548;&#33268;&#21453;&#24605;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#23545;&#27604;&#30340;&#26041;&#27861;&#65306;&#23427;&#26681;&#25454;&#35831;&#27714;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#22810;&#26679;&#30340;&#27714;&#35299;&#35270;&#35282;&#65292;&#23545;&#27604;&#24046;&#24322;&#65292;&#24182;&#23558;&#36825;&#20123;&#24046;&#24322;&#24635;&#32467;&#20026;&#19968;&#20010;&#26816;&#26597;&#34920;&#65292;&#29992;&#20110;&#37325;&#26032;&#23457;&#35270;&#21644;&#28040;&#38500;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;LLM&#22810;&#26679;&#30340;&#35270;&#35282;&#20197;&#20943;&#36731;&#22266;&#25191;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24046;&#24322;&#25351;&#31034;&#20102;&#28508;&#22312;&#30340;&#38169;&#35823;&#25110;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#36890;&#36807;&#25353;&#22266;&#23450;&#25345;&#32493;&#26102;&#38388;&#20381;&#27425;&#25191;&#34892;&#19981;&#21516;&#30340;&#34892;&#21160;&#22836;&#65292;&#20351;&#24471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#39034;&#24207;&#25805;&#25511;&#20219;&#21153;&#26102;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32467;&#26500;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01993</link><description>&lt;p&gt;
&#20851;&#20110;&#29992;&#26102;&#38388;&#32034;&#24341;&#20316;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#22909;&#22788;&#29702;&#39034;&#24207;&#25805;&#25511;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
On Time-Indexing as Inductive Bias in Deep RL for Sequential Manipulation Tasks. (arXiv:2401.01993v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#36890;&#36807;&#25353;&#22266;&#23450;&#25345;&#32493;&#26102;&#38388;&#20381;&#27425;&#25191;&#34892;&#19981;&#21516;&#30340;&#34892;&#21160;&#22836;&#65292;&#20351;&#24471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#39034;&#24207;&#25805;&#25511;&#20219;&#21153;&#26102;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32467;&#26500;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#25805;&#25511;&#20219;&#21153;&#26102;&#65292;&#25805;&#25511;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#26469;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#36825;&#32452;&#25216;&#33021;&#36890;&#24120;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#27599;&#20010;&#25216;&#33021;&#21487;&#33021;&#20855;&#26377;&#30456;&#24403;&#19981;&#21516;&#30340;&#34892;&#21160;&#21644;&#29366;&#24577;&#20998;&#24067;&#12290;&#26631;&#20934;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#23558;&#31574;&#30053;&#24314;&#27169;&#20026;&#20855;&#26377;&#21333;&#20010;&#36755;&#20986;&#22836;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#65289;&#12290;&#36825;&#31181;&#32467;&#26500;&#35201;&#27714;&#32593;&#32476;&#22312;&#20869;&#37096;&#23398;&#20064;&#22312;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#20999;&#25442;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#20302;&#21644;&#24615;&#33021;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#23398;&#20064;&#35768;&#22810;&#25805;&#25511;&#20219;&#21153;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26550;&#26500;&#65292;&#20854;&#25353;&#22266;&#23450;&#25345;&#32493;&#26102;&#38388;&#20381;&#27425;&#25191;&#34892;&#19981;&#21516;&#30340;&#34892;&#21160;&#22836;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35832;&#22914;&#21040;&#36798;&#21644;&#25235;&#21462;&#31561;&#22522;&#26412;&#25216;&#33021;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23545;Metaworld&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#31616;&#21333;&#30340;&#32467;&#26500;&#20248;&#20110;&#26631;&#20934;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#20984;&#26174;&#20986;&#20854;&#21019;&#26032;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
While solving complex manipulation tasks, manipulation policies often need to learn a set of diverse skills to accomplish these tasks. The set of skills is often quite multimodal - each one may have a quite distinct distribution of actions and states. Standard deep policy-learning algorithms often model policies as deep neural networks with a single output head (deterministic or stochastic). This structure requires the network to learn to switch between modes internally, which can lead to lower sample efficiency and poor performance. In this paper we explore a simple structure which is conducive to skill learning required for so many of the manipulation tasks. Specifically, we propose a policy architecture that sequentially executes different action heads for fixed durations, enabling the learning of primitive skills such as reaching and grasping. Our empirical evaluation on the Metaworld tasks reveals that this simple structure outperforms standard policy learning methods, highlightin
&lt;/p&gt;</description></item><item><title>GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01990</link><description>&lt;p&gt;
GPS-SSL: &#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01990
&lt;/p&gt;
&lt;p&gt;
GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GPS-SSL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27491;&#26679;&#26412;&#36873;&#25321;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;SSL&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#29983;&#25104;&#27491;&#26679;&#26412;&#65292;&#24182;&#23558;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#36827;&#21435;&#65292;&#20294;&#26159;&#38169;&#35823;&#25110;&#32773;&#36807;&#24369;&#30340;DA&#20250;&#20005;&#37325;&#38477;&#20302;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;GPS-SSL&#21017;&#25552;&#20986;&#35774;&#35745;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20351;&#24471;&#27431;&#27663;&#36317;&#31163;&#25104;&#20026;&#35821;&#20041;&#20851;&#31995;&#30340;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#12290;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#37117;&#21487;&#20197;&#29420;&#31435;&#22320;&#23884;&#20837;&#21040;&#36825;&#20010;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#32780;&#19981;&#21463;&#25152;&#20351;&#29992;&#30340;DA&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;GPS-SSL&#36866;&#29992;&#20110;&#20219;&#20309;SSL&#26041;&#27861;&#65292;&#22914;SimCLR&#25110;BYOL&#12290;GPS-SSL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#20943;&#23569;&#20102;&#23450;&#21046;&#24378;DA&#30340;&#21387;&#21147;&#12290;&#20363;&#22914;&#65292;GPS-SSL&#22312;Cifar10&#19978;&#20351;&#29992;&#24369;DA&#36798;&#21040;&#20102;85.58&#65285;&#65292;&#32780;&#22522;&#20934;&#20540;&#21482;&#36798;&#21040;&#20102;37.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a general method to inject a priori knowledge into Self-Supervised Learning (SSL) positive samples selection. Current SSL methods leverage Data-Augmentations (DA) for generating positive samples and incorporate prior knowledge - an incorrect, or too weak DA will drastically reduce the quality of the learned representation. GPS-SSL proposes instead to design a metric space where Euclidean distances become a meaningful proxy for semantic relationship. In that space, it is now possible to generate positive samples from nearest neighbor sampling. Any prior knowledge can now be embedded into that metric space independently from the employed DA. From its simplicity, GPS-SSL is applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches 85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We therefore move a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01989</link><description>&lt;p&gt;
&#37325;&#35775;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#20174;&#20301;&#32622;&#20559;&#35265;&#30340;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#26469;&#34920;&#24449;&#21644;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20808;&#21069;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26356;&#20026;&#38480;&#21046;&#24615;&#30340;&#24341;&#23548;&#20559;&#35265;&#29616;&#35937;&#30340;&#19968;&#33324;&#34920;&#36848;&#12290;&#20301;&#32622;&#20559;&#35265;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#26576;&#20123;&#37096;&#20998;&#19978;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#65292;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLM&#27169;&#22411;&#22914;GPT 3.5-Turbo&#65292;Llama-2&#21644;Dolly-v2&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#65292;&#20197;&#21450;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22914;Pegasus&#21644;BART&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20363;&#31243;&#20197;&#21450;&#21033;&#29992;&#23569;&#37327;&#30340;nun</title><link>http://arxiv.org/abs/2401.01974</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#27491;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65306;&#20197;LLMs&#20026;&#31243;&#24207;&#21592;
&lt;/p&gt;
&lt;p&gt;
Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20363;&#31243;&#20197;&#21450;&#21033;&#29992;&#23569;&#37327;&#30340;nun
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25512;&#29702;&#20027;&#35201;&#37319;&#29992;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#25512;&#29702;&#12289;&#27867;&#21270;&#12289;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#20197;&#21450;&#35745;&#25968;&#26041;&#38754;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#25511;&#21046;&#22120;&#36827;&#34892;&#35270;&#35273;&#25512;&#29702;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35843;&#24230;&#19968;&#32452;(&#35270;&#35273;)&#24037;&#20855;&#26469;&#35299;&#20915;&#23376;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#12289;&#35270;&#35273; grounding &#21644;&#35270;&#39057;&#30340;&#26102;&#38388;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#24403;&#21069;&#24418;&#24335;&#19979;&#20005;&#37325;&#20381;&#36182;&#20110;&#22312;&#25552;&#31034;&#20013;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#35774;&#35745;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#36825;&#38656;&#35201;&#39640;&#25216;&#33021;&#31243;&#24207;&#21592;&#25237;&#20837;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#30340;&#20363;&#31243;&#65292;&#24182;&#21033;&#29992;&#23569;&#37327;&#30340;nu
&lt;/p&gt;
&lt;p&gt;
Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small nu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#22522;&#30784;&#27169;&#22411;&#21040;3D&#39640;&#26031;&#20998;&#21106;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.01970</link><description>&lt;p&gt;
FMGS&#65306;&#22522;&#20110;&#23884;&#20837;&#24335;3D&#39640;&#26031;&#20998;&#21106;&#30340;&#20840;&#38754;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01970
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#22522;&#30784;&#27169;&#22411;&#21040;3D&#39640;&#26031;&#20998;&#21106;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#24863;&#30693;&#29616;&#23454;&#19990;&#30028;3D&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#29305;&#24615;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;&#21644;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25345;&#32493;&#36827;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FMGS&#65288;Foundation Model Embedded 3D Gaussian Splatting&#65289;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#22522;&#30784;&#27169;&#22411;&#21040;3D&#39640;&#26031;&#20998;&#21106;&#20013;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21644;&#34920;&#31034;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#29305;&#24449;&#22270;&#34701;&#21512;&#21040;&#25105;&#20204;&#30340;3D&#27169;&#22411;&#20013;&#28210;&#26579;&#23454;&#29616;&#30340;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#21644;&#24555;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;GS&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#65288;MHE&#65289;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26377;&#25928;&#35757;&#32451;&#36807;&#31243;&#36824;&#24341;&#20837;&#20102;&#20687;&#32032;&#23545;&#40784;&#25439;&#22833;&#65292;&#20351;&#30456;&#21516;&#35821;&#20041;&#23454;&#20307;&#30340;&#28210;&#26579;&#29305;&#24449;&#36317;&#31163;&#25509;&#36817;&#65292;&#36981;&#24490;&#20687;&#32032;&#32423;&#35821;&#20041;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#22810;&#35270;&#22270;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \algfull{} (\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, faci
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#23545;&#40784;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23545;&#40784;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21462;&#28040;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#20351;&#20854;&#24674;&#22797;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.01967</link><description>&lt;p&gt;
&#23545;&#40784;&#31639;&#27861;&#30340;&#26426;&#21046;&#29702;&#35299;&#65306;&#22522;&#20110;DPO&#21644;&#27602;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. (arXiv:2401.01967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#23545;&#40784;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23545;&#40784;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21462;&#28040;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#20351;&#20854;&#24674;&#22797;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#40784;&#31639;&#27861;&#29616;&#22312;&#24120;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#21916;&#22909;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35299;&#37322;&#27169;&#22411;&#22914;&#20309;&#8220;&#23545;&#40784;&#8221;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#22240;&#27492;&#38590;&#20197;&#35299;&#37322;&#35832;&#22914;&#36234;&#29425;&#31561;&#29616;&#35937;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;&#31639;&#27861;&#8212;&#8212;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#38477;&#20302;&#27602;&#24615;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#27602;&#24615;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;GPT2-medium&#20013;&#30340;&#34920;&#31034;&#21644;&#21796;&#36215;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25104;&#23545;&#25968;&#25454;&#38598;&#24212;&#29992;DPO&#26469;&#38477;&#20302;&#27602;&#24615;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#27169;&#22411;&#26159;&#22914;&#20309;&#36991;&#20813;&#36755;&#20986;&#26377;&#23475;&#32467;&#26524;&#30340;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#23398;&#21040;&#30340;&#33021;&#21147;&#24182;&#27809;&#26377;&#34987;&#31227;&#38500;&#65292;&#32780;&#26159;&#34987;&#32469;&#36807;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21462;&#28040;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23558;&#20854;&#24674;&#22797;&#20026;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.
&lt;/p&gt;</description></item><item><title>Instruct-Imagen&#26159;&#19968;&#31181;&#22788;&#29702;&#24322;&#26500;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#24182;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24335;&#25351;&#20196;&#20197;&#23454;&#29616;&#21508;&#31181;&#29983;&#25104;&#24847;&#22270;&#30340;&#32479;&#19968;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35757;&#32451;&#25552;&#21319;&#27169;&#22411;&#22312;&#22806;&#37096;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23545;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01952</link><description>&lt;p&gt;
Instruct-Imagen: &#24102;&#26377;&#22810;&#27169;&#24335;&#25351;&#20196;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruct-Imagen: Image Generation with Multi-modal Instruction. (arXiv:2401.01952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01952
&lt;/p&gt;
&lt;p&gt;
Instruct-Imagen&#26159;&#19968;&#31181;&#22788;&#29702;&#24322;&#26500;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#24182;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24335;&#25351;&#20196;&#20197;&#23454;&#29616;&#21508;&#31181;&#29983;&#25104;&#24847;&#22270;&#30340;&#32479;&#19968;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35757;&#32451;&#25552;&#21319;&#27169;&#22411;&#22312;&#22806;&#37096;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23545;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Instruct-Imagen&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#24322;&#26500;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#24182;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24335;&#25351;&#20196;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#65292;&#36825;&#26159;&#19968;&#31181;&#20219;&#21153;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#34920;&#36798;&#21508;&#31181;&#29983;&#25104;&#24847;&#22270;&#12290;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#25972;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#12289;&#36793;&#32536;&#12289;&#39118;&#26684;&#12289;&#20027;&#39064;&#31561;&#65289;&#65292;&#20351;&#24471;&#20016;&#23500;&#30340;&#29983;&#25104;&#24847;&#22270;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26684;&#24335;&#26631;&#20934;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;Instruct-Imagen&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35757;&#32451;&#26469;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22806;&#37096;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#22522;&#20110;&#20854;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#38656;&#35201;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#30340;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23545;&#36825;&#20010;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#37197;&#23545;&#19968;&#20010;&#21253;&#21547;&#20219;&#21153;&#26412;&#36136;&#30340;&#22810;&#27169;&#24335;&#25351;&#20196;&#12290;&#23545;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper presents instruct-imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce *multi-modal instruction* for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format.  We then build instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01951</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#33021;&#21542;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#25163;&#37096;&#22270;&#20687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36798;&#21313;&#24180;&#20043;&#20037;&#65292;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19968;&#30452;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#20363;&#22914;&#20154;&#25163;&#21644;&#25163;&#25351;&#20013;&#25152;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#12290;&#34429;&#28982;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#25351;&#21521;&#20102;&#24213;&#23618;&#32467;&#26500;&#30340;&#26681;&#26412;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#20854;&#20013;&#21253;&#21547;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#65292;&#26469;&#23637;&#31034;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;GAN&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative $n$-dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.01943</link><description>&lt;p&gt;
&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#22312;&#30701;&#35821;&#22659;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#34920;&#29616;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#21644;&#35299;&#20915;&#26041;&#26696;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39640;&#24230;&#20851;&#38190;&#21644;&#25935;&#24863;&#30340;&#39046;&#22495;&#20013;&#20351;&#29992;&#23427;&#20204;&#23545;&#20854;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#36755;&#20837;&#21464;&#21270;&#21644;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;ICD-10-CM&#20195;&#30721;&#25551;&#36848;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#25968;&#25454;&#38598;&#24191;&#27867;&#24212;&#29992;&#20110;&#32654;&#22269;&#21307;&#38498;&#65292;&#21253;&#21547;&#35768;&#22810;&#20020;&#24202;&#26415;&#35821;&#21450;&#20854;&#26131;&#20110;&#22797;&#21046;&#30340;&#25913;&#20889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#25628;&#32034;&#20219;&#21153;&#20013;&#23545;&#29616;&#26377;&#30340;&#36890;&#29992;&#25110;&#20020;&#24202;&#19987;&#19994;&#21270;&#30340;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#30446;&#26631;&#26159;&#27491;&#30830;&#21305;&#37197;&#25913;&#20889;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#27604;&#20020;&#24202;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#65292;&#20174;&#32780;&#20351;&#20854;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of tools and solutions based on Large Language Models (LLMs) for various tasks in the medical domain has become a prominent trend. Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs. This study addresses these questions by constructing a textual dataset based on the ICD-10-CM code descriptions, widely used in US hospitals and containing many clinical terms, and their easily reproducible rephrasing. We then benchmarked existing embedding models, either generalist or specialized in the clinical domain, in a semantic search task where the goal was to correctly match the rephrased text to the original description. Our results showed that generalist models performed better than clinical models, suggesting that existing clinical specialized models are more sensitive to small changes in input that confuse them. The highl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.01078</link><description>&lt;p&gt;
&#36234;&#21335;&#35799;&#27468;&#29983;&#25104;&#19982;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#30340;&#36807;&#31243;&#21644;&#22686;&#24378;&#30340;&#20869;&#23481;&#25511;&#21046;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;GPT-3 Babbage&#21464;&#31181;&#65292;&#22312;&#36234;&#21335;&#35799;&#27468;&#30340;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#23454;&#29616;&#20102;0.8&#30340;&#33258;&#23450;&#20041;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#25913;&#20889;&#25104;&#27491;&#24120;&#25991;&#26412;&#25552;&#31034;&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#33719;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;0.718&#20998;&#25968;&#12290;&#36825;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#20197;&#32763;&#35793;&#21518;&#30340;&#35799;&#27468;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the "luc bat" genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00926</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#21464;&#24418;DETR&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#36741;&#21161;&#34880;&#28082;&#30142;&#30149;&#35786;&#26029;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#21307;&#38498;&#34880;&#28082;&#26816;&#27979;&#20013;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#38656;&#35201;&#21307;&#29983;&#20351;&#29992;&#26174;&#24494;&#38236;&#20174;&#24739;&#32773;&#30340;&#34880;&#28082;&#26174;&#24494;&#22270;&#20687;&#20013;&#25163;&#21160;&#20998;&#31163;&#30333;&#32454;&#32990;&#12290;&#28982;&#21518;&#36890;&#36807;&#33258;&#21160;&#30333;&#32454;&#32990;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#20998;&#31163;&#30340;&#30333;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#34880;&#26679;&#20013;&#19981;&#21516;&#31867;&#22411;&#30333;&#32454;&#32990;&#30340;&#27604;&#20363;&#21644;&#20307;&#31215;&#65292;&#20174;&#32780;&#21327;&#21161;&#30142;&#30149;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32791;&#26102;&#12289;&#32791;&#21147;&#65292;&#32780;&#19988;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#20026;&#22270;&#20687;&#36136;&#37327;&#21644;&#29615;&#22659;&#26465;&#20214;&#31561;&#22240;&#32032;&#65292;&#21487;&#33021;&#23548;&#33268;&#21518;&#32493;&#20998;&#31867;&#38169;&#35823;&#21644;&#35823;&#35786;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65306;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65288;MFDS-DETR&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#32423;&#31579;&#36873;&#29305;&#24449;&#34701;&#21512;&#37329;&#23383;&#22612;&#65288;HS-FPN&#65289;&#65292;&#23454;&#29616;&#20102;&#22810;&#32423;&#34701;&#21512;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#39640;&#32423;&#29305;&#24449;&#20316;&#20026;&#29305;&#24449;&#34701;&#21512;&#30340;&#36755;&#20837;&#65292;&#21516;&#26102;&#37319;&#29992;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#23454;&#29616;&#31934;&#30830;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard hospital blood tests, the traditional process requires doctors to manually isolate leukocytes from microscopic images of patients' blood using microscopes. These isolated leukocytes are then categorized via automatic leukocyte classifiers to determine the proportion and volume of different types of leukocytes present in the blood samples, aiding disease diagnosis. This methodology is not only time-consuming and labor-intensive, but it also has a high propensity for errors due to factors such as image quality and environmental conditions, which could potentially lead to incorrect subsequent classifications and misdiagnosis. To address these issues, this paper proposes an innovative method of leukocyte detection: the Multi-level Feature Fusion and Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte scale disparity, we designed the High-level Screening-feature Fusion Pyramid (HS-FPN), enabling multi-level fusion. This model uses high-level features as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.16713</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26465;&#20214;&#33258;&#27880;&#24847;&#21147;&#25554;&#34917;&#65288;CSAI&#65289;&#27169;&#22411;&#20197;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32454;&#33410;&#30340;&#26465;&#20214;&#38544;&#34255;&#29366;&#24577;&#21021;&#22987;&#21270;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25554;&#34917;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;CSAI&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16430</link><description>&lt;p&gt;
&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20174;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#23398;&#20064;&#65292;&#39318;&#20808;&#25311;&#21512;&#20559;&#22909;&#20998;&#25968;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;RLHF&#30340;&#22788;&#29702;&#36807;&#31243;&#22797;&#26434;&#12289;&#32791;&#26102;&#19988;&#19981;&#31283;&#23450;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#20351;&#29992;&#31163;&#31574;&#30053;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;DPO&#20351;&#29992;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#21644;&#23545;&#25968;&#25439;&#22833;&#65292;&#23548;&#33268;&#22312;&#20559;&#22909;&#25509;&#36817;&#30830;&#23450;&#24615;&#26102;&#24573;&#30053;&#20102;KL&#27491;&#21017;&#21270;&#39033;&#32780;&#36807;&#24230;&#25311;&#21512;&#20559;&#22909;&#25968;&#25454;&#12290;IPO&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26681;&#26597;&#25214;&#30340;&#25104;&#23545;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#26469;&#35299;&#20915;&#24573;&#30053;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#24182;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#26159;IPO&#30340;&#25104;&#23545;&#25439;&#22833;&#20173;&#28982;&#26080;&#27861;&#20351;KL&#27491;&#21017;&#21270;&#29983;&#25928;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20559;&#22909;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.12728</link><description>&lt;p&gt;
Lookahead:&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#26080;&#25439;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12289;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#25903;&#20184;&#23453;&#36825;&#26679;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#25552;&#20379;&#37325;&#35201;&#37329;&#34701;&#20135;&#21697;&#30340;&#38656;&#35201;&#20934;&#30830;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25903;&#20184;&#23453;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#23558;LLMs&#19982;&#26368;&#20934;&#30830;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#30495;&#23454;&#20135;&#21697;&#26469;&#35828;&#65292;LLMs&#30340;&#25512;&#29702;&#36895;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#23454;&#39564;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;RAG&#31995;&#32479;&#30340;&#36895;&#24230;&#22823;&#24133;&#25552;&#21319;&#21644;&#25104;&#26412;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#26080;&#25439;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#22312;&#20256;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#20196;&#29260;&#37117;&#30001;LLMs&#25353;&#39034;&#24207;&#29983;&#25104;&#65292;&#23548;&#33268;&#30340;&#26102;&#38388;&#28040;&#32791;&#19982;&#29983;&#25104;&#30340;&#20196;&#29260;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26377;&#19968;&#23450;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2312.11671</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26377;&#19968;&#23450;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#37326;&#22806;&#33719;&#21462;&#36164;&#28304;&#12289;&#22797;&#21046;&#33258;&#36523;&#21644;&#36866;&#24212;&#26032;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#33021;&#21147;&#20026;"&#33258;&#20027;&#22797;&#21046;&#21644;&#36866;&#24212;"&#25110;&#32773;ARA&#12290;&#25105;&#20204;&#35748;&#20026;&#20855;&#22791;ARA&#33021;&#21147;&#30340;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#24191;&#27867;&#32780;&#38590;&#20197;&#39044;&#27979;&#30340;&#21518;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#34913;&#37327;&#21644;&#39044;&#27979;ARA&#33021;&#21147;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#30456;&#20851;&#30340;&#23433;&#20840;&#12289;&#30417;&#27979;&#21644;&#23545;&#40784;&#25514;&#26045;&#12290;&#27492;&#22806;&#65292;&#19968;&#26086;&#31995;&#32479;&#20855;&#22791;ARA&#33021;&#21147;&#65292;&#23545;&#31995;&#32479;&#33021;&#21147;&#30340;&#38480;&#21046;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#20195;&#29702;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20801;&#35768;&#20854;&#22312;&#19990;&#30028;&#20013;&#37319;&#21462;&#34892;&#21160;&#30340;&#24037;&#20855;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20195;&#29702;&#22312;&#19982;ARA&#30456;&#20851;&#30340;12&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#20219;&#21153;&#21015;&#34920;&#20013;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23613;&#31649;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20063;&#26377;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#35780;&#20272;&#36824;&#27809;&#26377;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#38899;&#20048;&#26631;&#35760;&#20013;&#25506;&#32034;&#20102;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#26469;&#25552;&#21462;&#38899;&#39057;&#25991;&#20214;&#20013;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2312.11234</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#38899;&#39057;&#26631;&#35760;&#30340;&#24863;&#30693;&#38899;&#20048;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Perceptual Musical Features for Interpretable Audio Tagging. (arXiv:2312.11234v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#38899;&#20048;&#26631;&#35760;&#20013;&#25506;&#32034;&#20102;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#26469;&#25552;&#21462;&#38899;&#39057;&#25991;&#20214;&#20013;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;&#30340;&#26102;&#20195;&#65292;&#33258;&#21160;&#26631;&#35760;&#38899;&#20048;&#38899;&#39057;&#30340;&#20219;&#21153;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#26088;&#22312;&#25552;&#39640;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25351;&#26631;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23613;&#31649;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20063;&#20855;&#26377;&#19981;&#36879;&#26126;&#24615;&#65292;&#20351;&#24471;&#38590;&#20197;&#35299;&#37322;&#20854;&#23545;&#32473;&#23450;&#36755;&#20837;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#38382;&#39064;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#22791;&#21463;&#24378;&#35843;&#65292;&#20294;&#22312;&#38899;&#20048;&#30456;&#20851;&#20219;&#21153;&#20013;&#24182;&#26410;&#24471;&#21040;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#33258;&#21160;&#38899;&#20048;&#26631;&#35760;&#30340;&#32972;&#26223;&#19979;&#35299;&#37322;&#24615;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#65292;&#32467;&#21512;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#65306;a&#65289;&#21033;&#29992;&#31526;&#21495;&#30693;&#35782;&#65292;b&#65289;&#21033;&#29992;&#36741;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;c&#65289;&#21033;&#29992;&#20449;&#21495;&#22788;&#29702;&#20174;&#38899;&#39057;&#25991;&#20214;&#20013;&#25552;&#21462;&#24863;&#30693;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of music streaming platforms, the task of automatically tagging music audio has garnered significant attention, driving researchers to devise methods aimed at enhancing performance metrics on standard datasets. Most recent approaches rely on deep neural networks, which, despite their impressive performance, possess opacity, making it challenging to elucidate their output for a given input. While the issue of interpretability has been emphasized in other fields like medicine, it has not received attention in music-related tasks. In this study, we explored the relevance of interpretability in the context of automatic music tagging. We constructed a workflow that incorporates three different information extraction techniques: a) leveraging symbolic knowledge, b) utilizing auxiliary deep neural networks, and c) employing signal processing to extract perceptual features from audio files. These features were subsequently used to train an interpretable machine-learning model for ta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;</title><link>http://arxiv.org/abs/2312.10302</link><description>&lt;p&gt;
&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#25968;&#25454;&#25506;&#32034;&#32773;&#30340;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#26159;&#26377;&#25928;&#21033;&#29992;&#20854;&#39044;&#35757;&#32451;&#33021;&#21147;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25193;&#23637;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#20294;&#32570;&#20047;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#30340;&#26126;&#30830;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#20013;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;Nuggets&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#12290;Nuggets&#35780;&#20272;&#21333;&#20010;&#25351;&#23548;&#31034;&#20363;&#20316;&#20026;&#26377;&#25928;&#21333;&#27425;&#31034;&#20363;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#35782;&#21035;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#21508;&#31181;&#20219;&#21153;&#24615;&#33021;&#30340;&#31034;&#20363;&#12290;Nuggets&#21033;&#29992;&#22522;&#20110;&#20505;&#36873;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;MT-Bench&#21644;Alpaca-Ev&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;</title><link>http://arxiv.org/abs/2312.01520</link><description>&lt;p&gt;
Bayesian&#32593;&#32476;&#30340;&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#65306;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#22270;&#32467;&#26500;&#21487;&#20197;&#22788;&#29702;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#31232;&#30095;&#30340;&#19968;&#31995;&#21015;&#36739;&#23567;&#38382;&#39064;&#65292;&#36825;&#26159;Judea Pearl&#30340;&#22240;&#26524;&#24615;&#30340;&#22522;&#30784;&#65292;&#20063;&#20915;&#23450;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#22914;&#20309;&#22312;&#26368;&#24120;&#35265;&#30340;&#20998;&#24067;&#20551;&#35774;&#19979;&#35745;&#31639;BNs&#30340;Shannon&#29109;&#21644;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;BNs&#30340;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#24182;&#29992;&#19968;&#25972;&#22871;&#25968;&#20540;&#31034;&#20363;&#35828;&#26126;&#20102;&#23427;&#20204;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#23558;&#39640;&#26031;BNs&#30340;KL&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian networks (BNs) are a foundational model in machine learning and causal inference. Their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies Judea Pearl's causality, and determines their explainability and interpretability. Despite their popularity, there are almost no resources in the literature on how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for BNs under their most common distributional assumptions. In this paper, we provide computationally efficient algorithms for both by leveraging BNs' graphical structure, and we illustrate them with a complete set of numerical examples. In the process, we show it is possible to reduce the computational complexity of KL from cubic to quadratic for Gaussian BNs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;DiffAttack&#65292;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#20928;&#21270;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20559;&#24046;&#37325;&#26500;&#25439;&#22833;&#21644;&#20998;&#27573;&#24335;&#21069;&#21521;&#21518;&#21521;&#31639;&#27861;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20854;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.16124</link><description>&lt;p&gt;
DiffAttack&#65306;&#38024;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#20928;&#21270;&#30340;&#35268;&#36991;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification. (arXiv:2311.16124v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;DiffAttack&#65292;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#20928;&#21270;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20559;&#24046;&#37325;&#26500;&#25439;&#22833;&#21644;&#20998;&#27573;&#24335;&#21069;&#21521;&#21518;&#21521;&#31639;&#27861;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20854;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20928;&#21270;&#38450;&#24481;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#28040;&#38500;&#23545;&#25239;&#31034;&#20363;&#30340;&#21046;&#36896;&#25200;&#21160;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#20808;&#36827;&#30340;&#25915;&#20987;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#30772;&#22351;&#36825;&#31181;&#38450;&#24481;&#65292;&#22240;&#20026;&#20928;&#21270;&#36807;&#31243;&#24341;&#21457;&#20102;&#19968;&#20010;&#26497;&#20854;&#28145;&#23618;&#30340;&#35745;&#31639;&#22270;&#65292;&#21487;&#33021;&#23548;&#33268;&#26799;&#24230;&#27169;&#31946;&#12289;&#39640;&#20869;&#23384;&#28040;&#32791;&#21644;&#26080;&#38480;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;DiffAttack&#26469;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#20928;&#21270;&#38450;&#24481;&#25191;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25915;&#20987;&#65292;&#21253;&#25324;DDPM&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#20013;&#38388;&#30340;&#25193;&#25955;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;&#37325;&#26500;&#25439;&#22833;&#65292;&#20197;&#24341;&#36215;&#19981;&#20934;&#30830;&#30340;&#23494;&#24230;&#26799;&#24230;&#20272;&#35745;&#65292;&#20197;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#27573;&#24335;&#21069;&#21521;&#21518;&#21521;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#20854;&#20182;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#39564;&#35777;&#20102;DiffAttack&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based purification defenses leverage diffusion models to remove crafted perturbations of adversarial examples and achieve state-of-the-art robustness. Recent studies show that even advanced attacks cannot break such defenses effectively, since the purification process induces an extremely deep computational graph which poses the potential problem of gradient obfuscation, high memory cost, and unbounded randomness. In this paper, we propose a unified framework DiffAttack to perform effective and efficient attacks against diffusion-based purification defenses, including both DDPM and score-based approaches. In particular, we propose a deviated-reconstruction loss at intermediate diffusion steps to induce inaccurate density gradient estimation to tackle the problem of vanishing/exploding gradients. We also provide a segment-wise forwarding-backwarding algorithm, which leads to memory-efficient gradient backpropagation. We validate the attack effectiveness of DiffAttack compared 
&lt;/p&gt;</description></item><item><title>TEAL&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#29983;&#25104;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2311.04589</link><description>&lt;p&gt;
TEAL: &#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#23558;&#25152;&#26377;&#27169;&#24577;&#36827;&#34892;&#20998;&#35789;&#21644;&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04589
&lt;/p&gt;
&lt;p&gt;
TEAL&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#29983;&#25104;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#29983;&#25104;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEAL&#65288;Tokenize and Embed ALL&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#20309;&#27169;&#24577;&#30340;&#36755;&#20837;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#25152;&#26377;&#27169;&#24577;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;TEAL&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#20998;&#35789;&#22120;&#23558;&#20854;&#31163;&#25955;&#21270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#30697;&#38453;&#23558;&#20196;&#29260;&#24207;&#21015;&#23884;&#20837;&#21040;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;MM-LLMs&#21482;&#38656;&#35201;&#20687;&#25991;&#26412;LLMs&#37027;&#26679;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#22810;&#27169;&#24577;&#20196;&#29260;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#39044;&#27979;&#30340;&#20196;&#29260;&#24207;&#21015;&#65292;&#24212;&#29992;&#30456;&#24212;&#30340;&#21435;&#20998;&#35789;&#22120;&#29983;&#25104;&#27599;&#20010;&#27169;&#24577;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;TEAL&#20351;&#20923;&#32467;&#30340;LLMs&#33021;&#22815;&#25191;&#34892;&#28041;&#21450;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#29702;&#35299;&#21644;&#29983;&#25104;&#22270;&#20687;&#25110;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33539;&#24335;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19251</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22240;&#26524;&#21435;&#20559;&#35265;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33539;&#24335;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#24314;&#31435;&#33539;&#24335;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#25551;&#36848;&#36890;&#29992;&#20219;&#21153;&#31354;&#38388;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#25104;&#21151;&#22320;&#36866;&#24212;&#35299;&#20915;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#65288;&#22914;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#65289;&#12290;&#21463;&#21040;&#36825;&#26679;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#30740;&#31350;&#20102;&#23558;&#36825;&#31181;&#33539;&#24335;&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35282;&#19979;&#36739;&#23569;&#34987;&#35843;&#26597;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30340;&#36890;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#21040;&#36890;&#29992;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#25552;&#21319;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#65288;&#25968;&#25454;&#26377;&#38480;&#65289;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).  However, unlike vision/language data which share strong conformity in the semant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.12081</link><description>&lt;p&gt;
DHOT-GM&#65306;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#23454;&#29616;&#40065;&#26834;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#21305;&#37197;&#26159;&#26368;&#37325;&#35201;&#30340;&#22270;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#21305;&#37197;&#22270;&#26102;&#20381;&#36182;&#20110;&#37051;&#25509;&#30697;&#38453;&#25110;&#33410;&#28857;&#23884;&#20837;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22914;&#33410;&#28857;&#23646;&#24615;&#12289;&#23376;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#31216;&#20026;DHOT-GM&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22270;&#34920;&#31034;&#20026;&#19982;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23545;&#24212;&#30340;&#19968;&#32452;&#20851;&#31995;&#30697;&#38453;&#12290;&#32473;&#23450;&#20004;&#20010;&#22270;&#65292;&#25105;&#20204;&#26522;&#20030;&#25152;&#26377;&#20851;&#31995;&#30697;&#38453;&#23545;&#65292;&#24182;&#33719;&#21462;&#23427;&#20204;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20026;&#35745;&#31639;&#20004;&#20010;&#22270;&#20043;&#38388;&#30340;HOT&#36317;&#31163;&#65292;&#27599;&#20010;&#22270;&#37117;&#26159;&#30001;&#20851;&#31995;&#30697;&#38453;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Factorized Hyperspectral Transformer&#65292;&#32467;&#21512;&#20102;&#20998;&#35299;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27969;&#31243;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09431</link><description>&lt;p&gt;
FactoFormer: &#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20998;&#35299;&#39640;&#20809;&#35889;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining. (arXiv:2309.09431v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09431
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Factorized Hyperspectral Transformer&#65292;&#32467;&#21512;&#20102;&#20998;&#35299;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27969;&#31243;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#65288;HSIs&#65289;&#21253;&#21547;&#20016;&#23500;&#30340;&#20809;&#35889;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#21464;&#21387;&#22120;&#22312;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22240;&#27492;&#36817;&#26399;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23558;&#21464;&#21387;&#22120;&#29992;&#20110;HSIs&#19978;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39640;&#20809;&#35889;&#21464;&#21387;&#22120;&#21482;&#22312;&#20809;&#35889;&#32500;&#24230;&#19978;&#23545;&#36755;&#20837;&#30340;HSI&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#21464;&#21387;&#22120;&#23545;&#25968;&#25454;&#38656;&#27714;&#37327;&#22823;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#65292;&#32780;&#30001;&#20110;&#26377;&#38480;&#30340;&#26631;&#27880;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#39640;&#20809;&#35889;&#21464;&#21387;&#22120;&#30340;&#20840;&#37096;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#21457;&#25381;&#20986;&#26469;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#20809;&#35889;-&#31354;&#38388;&#21464;&#21387;&#22120;&#65292;&#23427;&#34701;&#21512;&#20102;&#20998;&#35299;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27969;&#31243;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral images (HSIs) contain rich spectral and spatial information. Motivated by the success of transformers in the field of natural language processing and computer vision where they have shown the ability to learn long range dependencies within input data, recent research has focused on using transformers for HSIs. However, current state-of-the-art hyperspectral transformers only tokenize the input HSI sample along the spectral dimension, resulting in the under-utilization of spatial information. Moreover, transformers are known to be data-hungry and their performance relies heavily on large-scale pretraining, which is challenging due to limited annotated hyperspectral data. Therefore, the full potential of HSI transformers has not been fully realized. To overcome these limitations, we propose a novel factorized spectral-spatial transformer that incorporates factorized self-supervised pretraining procedures, leading to significant improvements in performance. The factorization
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLN-net&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#20934;&#30830;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#28304;&#22270;&#20687;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#22810;&#28304;&#22270;&#20687;&#65292;&#36890;&#36807;&#22810;&#23618;&#24402;&#19968;&#21270;&#23618;&#32467;&#26500;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20998;&#21106;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02742</link><description>&lt;p&gt;
MLN-net&#65306;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#24402;&#19968;&#21270;&#36827;&#34892;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#22810;&#28304;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLN-net: A multi-source medical image segmentation method for clustered microcalcifications using multiple layer normalization. (arXiv:2309.02742v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLN-net&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#20934;&#30830;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#28304;&#22270;&#20687;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#22810;&#28304;&#22270;&#20687;&#65292;&#36890;&#36807;&#22810;&#23618;&#24402;&#19968;&#21270;&#23618;&#32467;&#26500;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20998;&#21106;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20083;&#33146;X&#32447;&#25668;&#24433;&#20013;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#20934;&#30830;&#20998;&#21106;&#23545;&#20110;&#20083;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#19987;&#23478;&#32423;&#20934;&#30830;&#24615;&#65292;&#20294;&#30001;&#20110;&#24739;&#32773;&#20307;&#20301;&#12289;&#20010;&#20307;&#33146;&#20307;&#23494;&#24230;&#21644;&#20083;&#33146;X&#32447;&#25668;&#24433;&#25104;&#20687;&#27169;&#24335;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#36896;&#25104;&#20102;&#39046;&#22495;&#36716;&#31227;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36129;&#29486;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLN-net&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20165;&#20351;&#29992;&#21333;&#19968;&#28304;&#22270;&#20687;&#21363;&#21487;&#20934;&#30830;&#22320;&#20998;&#21106;&#22810;&#28304;&#22270;&#20687;&#65292;&#29992;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#20998;&#21106;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#22495;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#26469;&#29983;&#25104;&#22810;&#28304;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#37319;&#29992;&#20102;&#22810;&#23618;&#24402;&#19968;&#21270;&#65288;LN&#65289;&#23618;&#30340;&#32467;&#26500;&#26469;&#26500;&#24314;&#20998;&#21106;&#32593;&#32476;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23545;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#20998;&#21106;&#20855;&#26377;&#33391;&#22909;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#36335;&#36873;&#25321;&#31574;&#30053;&#26469;&#20248;&#21270;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation of clustered microcalcifications in mammography is crucial for the diagnosis and treatment of breast cancer. Despite exhibiting expert-level accuracy, recent deep learning advancements in medical image segmentation provide insufficient contribution to practical applications, due to the domain shift resulting from differences in patient postures, individual gland density, and imaging modalities of mammography etc. In this paper, a novel framework named MLN-net, which can accurately segment multi-source images using only single source images, is proposed for clustered microcalcification segmentation. We first propose a source domain image augmentation method to generate multi-source images, leading to improved generalization. And a structure of multiple layer normalization (LN) layers is used to construct the segmentation network, which can be found efficient for clustered microcalcification segmentation in different domains. Additionally, a branch selection strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15256</link><description>&lt;p&gt;
&#35753;&#22768;&#38899;&#23384;&#22312;&#65306;&#20174;&#26080;&#22768;&#35270;&#39057;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20165;&#36890;&#36807;&#21767;&#36816;&#21160;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20063;&#34987;&#31216;&#20026;&#21767;&#35821;&#36716;&#35821;&#38899;&#12290;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30001;&#20110;&#21516;&#24418;&#24322;&#38899;&#21644;&#22810;&#26679;&#21270;&#35821;&#38899;&#21464;&#21270;&#32780;&#36896;&#25104;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#65292;&#23548;&#33268;&#21457;&#38899;&#38169;&#35823;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#32531;&#35299;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#65288;1&#65289;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#26469;&#28040;&#38500;&#21516;&#24418;&#24322;&#38899;&#65292;&#21644;&#65288;2&#65289;&#22768;&#23398;&#21464;&#24322;&#20449;&#24687;&#26469;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#35821;&#38899;&#39118;&#26684;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#27969;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#65292;&#25429;&#25417;&#21644;&#31934;&#28860;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#30495;&#23454;&#20154;&#31867;&#35821;&#38899;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11586</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25913;&#36827;&#26041;&#27861;&#21253;&#25324;&#22810;&#22270;&#31471;&#21475;&#32534;&#21495;&#12289;&#20010;&#20307;ID&#21644;&#21453;&#21521;&#28040;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20960;&#20046;&#21487;&#20197;&#24471;&#21040;&#23436;&#32654;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26816;&#27979;&#27927;&#38065;&#20132;&#26131;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;GNN&#30340;&#23569;&#25968;&#31867;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#39640;&#36798;30%&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#26641;&#21644;GNN&#30340;&#22522;&#20934;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#25552;&#21319;&#20102;&#19977;&#20010;&#26631;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#37319;&#29992;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26469;&#35745;&#31639;&#27169;&#22411;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#24182;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.00876</link><description>&lt;p&gt;
&#22312;&#31526;&#21512;&#24615;&#39044;&#27979;&#20013;&#37327;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#37319;&#29992;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26469;&#35745;&#31639;&#27169;&#22411;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#24182;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#20013;&#65292;&#31934;&#30830;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#21487;&#38752;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#12290;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#32622;&#20449;&#27700;&#24179;&#26469;&#34920;&#31034;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#36827;&#34892;&#21333;&#20010;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#31526;&#21512;&#24615;&#39044;&#27979;&#20013;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#36824;&#27809;&#26377;&#23436;&#20840;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#26041;&#27861;&#26469;&#37327;&#21270;&#31526;&#21512;&#24615;&#39044;&#27979;&#20013;&#20135;&#29983;&#30340;&#39044;&#27979;&#38598;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20026;&#35745;&#31639;&#24471;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#35748;&#35777;&#36793;&#30028;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36890;&#36807;CP&#27979;&#37327;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65288;&#22914;&#36125;&#21494;&#26031;&#26041;&#27861;&#12289;MC-Dropout&#21644;DeepEnsemble&#65289;&#36827;&#34892;&#27604;&#36739;.
&lt;/p&gt;
&lt;p&gt;
Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decision-making in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidentia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07520</link><description>&lt;p&gt;
STAS: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07520
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#26377;&#25928;&#30340;&#33539;&#20363;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#36171;&#20449;&#29992;&#20540;&#65292;&#21363;&#36890;&#36807;&#20195;&#29702;&#30340;&#36129;&#29486;&#26469;&#32473;&#20195;&#29702;&#36171;&#20449;&#29992;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#38544;&#24335;&#22320;&#20998;&#35299;&#32852;&#21512;&#20215;&#20540;&#20989;&#25968;&#25110;&#26174;&#24335;&#22320;&#35745;&#31639;&#25152;&#26377;&#20195;&#29702;&#30340;&#25903;&#20184;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#26377;&#22312;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20840;&#23616;&#22870;&#21169;&#21482;&#33021;&#22312;&#21608;&#26399;&#32467;&#26463;&#26102;&#26174;&#31034;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#12290;&#23427;&#20204;&#32570;&#20047;&#23545;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#22312;&#26102;&#38388;&#32500;&#24230;&#20013;&#22797;&#26434;&#20851;&#31995;&#30340;&#24314;&#27169;&#21151;&#33021;&#65292;&#24182;&#19988;&#21463;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#38388;&#26102;&#38388;&#20851;&#27880;&#19982; Shapley&#65288;STAS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22238;&#25253;&#20998;&#35299;&#65307;STAS &#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#23398;&#20064;&#20449;&#29992;&#20998;&#37197;&#12290;&#23427;&#39318;&#20808;&#23558;&#20840;&#23616;&#22238;&#25253;&#20998;&#35299;&#22238;&#21040;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#28982;&#21518;&#20351;&#29992;Shapley&#20540;&#26469;&#35780;&#20272;&#21327;&#20316;MARL&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#36129;&#29486;&#12290; STAS &#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388; - &#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#25429;&#33719;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#20013;&#65292;STAS &#33021;&#22815;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is yet credit assignment, which aims to credit agents by their contributions. Prior studies focus on either implicitly decomposing the joint value function or explicitly computing the payoff distribution of all agents. However, in episodic reinforcement learning settings where global rewards can only be revealed at the end of the episode, existing methods usually fail to work. They lack the functionality of modeling complicated relations of the delayed global reward in the temporal dimension and suffer from large variance and bias. We propose a novel method named Spatial-Temporal Attention with Shapley (STAS) for return decomposition; STAS learns credit assignment in both the temporal and the spatial dimension. It first decomposes the global return back to each time step, then utilizes Shapley Value to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;&#22120;NSGA-II&#22312;&#22810;&#27169;&#24335;&#38382;&#39064;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#24403;&#31181;&#32676;&#22823;&#23567;&#36275;&#22815;&#22823;&#26102;&#65292;NSGA-II&#33021;&#22815;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(N n^k)&#20869;&#20248;&#21270;OneJumpZeroJump&#22522;&#20934;&#38382;&#39064;&#65292;&#19988;&#20351;&#29992;&#24555;&#36895;&#31361;&#21464;&#31639;&#23376;&#21487;&#20197;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.13750</link><description>&lt;p&gt;
NSGA-II&#22312;&#22810;&#27169;&#24335;&#38382;&#39064;&#19978;&#30340;&#31532;&#19968;&#27425;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A First Runtime Analysis of the NSGA-II on a Multimodal Problem. (arXiv:2204.13750v5 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;&#22120;NSGA-II&#22312;&#22810;&#27169;&#24335;&#38382;&#39064;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#24403;&#31181;&#32676;&#22823;&#23567;&#36275;&#22815;&#22823;&#26102;&#65292;NSGA-II&#33021;&#22815;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(N n^k)&#20869;&#20248;&#21270;OneJumpZeroJump&#22522;&#20934;&#38382;&#39064;&#65292;&#19988;&#20351;&#29992;&#24555;&#36895;&#31361;&#21464;&#31639;&#23376;&#21487;&#20197;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;&#22120;NSGA-II&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#12290;&#25105;&#20204;&#32487;&#32493;&#36825;&#39033;&#30740;&#31350;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35813;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#31181;&#32676;&#22823;&#23567;N&#33267;&#23569;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#65292;&#21017;&#24102;&#26377;&#22235;&#31181;&#19981;&#21516;&#36873;&#25321;&#29238;&#20195;&#21644;&#20301;&#31361;&#21464;&#30340;NSGA-II&#33021;&#22815;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(N n^k)&#20869;&#20248;&#21270;OneJumpZeroJump&#22522;&#20934;&#38382;&#39064;&#65292;&#20854;&#20013;2&#8804;k&#8804;n/4&#12290;&#24403;&#20351;&#29992;&#24555;&#36895;&#31361;&#21464;&#26102;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#37325;&#23614;&#31361;&#21464;&#31639;&#23376;&#21487;&#20197;&#23558;&#36825;&#19968;&#20445;&#35777;&#25552;&#39640;&#33267;k^&#937;(k)&#20493;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;NSGA-II&#33267;&#23569;&#19982;&#20840;&#23616;SEMO&#31639;&#27861;&#19968;&#26679;&#33021;&#22815;&#22788;&#29702;OneJumpZeroJump&#38382;&#39064;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very recently, the first mathematical runtime analyses of the multi-objective evolutionary optimizer NSGA-II have been conducted. We continue this line of research with a first runtime analysis of this algorithm on a benchmark problem consisting of two multimodal objectives. We prove that if the population size $N$ is at least four times the size of the Pareto front, then the NSGA-II with four different ways to select parents and bit-wise mutation optimizes the OneJumpZeroJump benchmark with jump size~$2 \le k \le n/4$ in time $O(N n^k)$. When using fast mutation, a recently proposed heavy-tailed mutation operator, this guarantee improves by a factor of $k^{\Omega(k)}$. Overall, this work shows that the NSGA-II copes with the local optima of the OneJumpZeroJump problem at least as well as the global SEMO algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;FedValue&#65292;&#35813;&#26041;&#27861;&#26159;&#38544;&#31169;&#20445;&#25252;&#12289;&#38024;&#23545;&#20219;&#21153;&#32780;&#26080;&#38656;&#27169;&#22411;&#30340;&#12290;&#23427;&#21253;&#25324;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;MShapley-CMI&#21644;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#35780;&#20272;&#25968;&#25454;&#26041;&#25968;&#25454;&#20215;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.08364</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65306;&#19968;&#31181;&#26080;&#27169;&#22411;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method. (arXiv:2112.08364v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;FedValue&#65292;&#35813;&#26041;&#27861;&#26159;&#38544;&#31169;&#20445;&#25252;&#12289;&#38024;&#23545;&#20219;&#21153;&#32780;&#26080;&#38656;&#27169;&#22411;&#30340;&#12290;&#23427;&#21253;&#25324;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;MShapley-CMI&#21644;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#35780;&#20272;&#25968;&#25454;&#26041;&#25968;&#25454;&#20215;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#39044;&#27979;&#20998;&#26512;&#33539;&#24335;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#25968;&#25454;&#25552;&#20379;&#26041;&#65288;&#21363;&#25968;&#25454;&#26041;&#65289;&#30340;&#21512;&#20316;&#65292;&#22312;&#20998;&#25955;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#19979;&#65292;&#36171;&#20104;&#19968;&#20010;&#32452;&#32455;&#65288;&#21363;&#20219;&#21153;&#26041;&#65289;&#25552;&#39640;&#20854;&#39044;&#27979;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23545;VFL&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#19988;&#23433;&#20840;&#30340;&#24037;&#20855;&#26469;&#35780;&#20272;&#25968;&#25454;&#26041;&#25317;&#26377;&#30340;&#25968;&#25454;&#20215;&#20540;&#65292;&#21046;&#32422;&#20102;VFL&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedValue&#65292;&#19968;&#31181;&#38024;&#23545;VFL&#30340;&#20445;&#25252;&#38544;&#31169;&#19988;&#38024;&#23545;&#20219;&#21153;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#30001;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;&#21644;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#32452;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;&#65292;&#21363;MShapley-CMI&#12290;&#35813;&#24230;&#37327;&#35780;&#20272;&#25968;&#25454;&#26041;&#23545;&#39044;&#27979;&#20998;&#26512;&#20219;&#21153;&#30340;&#36129;&#29486;&#65292;&#32780;&#26080;&#38656;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#20110;VFL&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated learning (VFL) is a promising paradigm for predictive analytics, empowering an organization (i.e., task party) to enhance its predictive models through collaborations with multiple data suppliers (i.e., data parties) in a decentralized and privacy-preserving way. Despite the fast-growing interest in VFL, the lack of effective and secure tools for assessing the value of data owned by data parties hinders the application of VFL in business contexts. In response, we propose FedValue, a privacy-preserving, task-specific but model-free data valuation method for VFL, which consists of a data valuation metric and a federated computation method. Specifically, we first introduce a novel data valuation metric, namely MShapley-CMI. The metric evaluates a data party's contribution to a predictive analytics task without the need of executing a machine learning model, making it well-suited for real-world applications of VFL. Next, we develop an innovative federated computation met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.14956</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#21450;&#20854;&#22312;&#24189;&#38376;&#34746;&#26438;&#33740;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#32570;&#20047;&#20934;&#30830;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#22240;&#27492;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#39318;&#20808;&#23545;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#36827;&#34892;&#19968;&#20123;&#32416;&#27491;&#65292;&#28982;&#21518;&#29992;&#32416;&#27491;&#20449;&#24687;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#65288;MHWSIA&#65289;&#31561;&#29305;&#23450;&#39046;&#22495;&#20013;&#65292;&#19987;&#23478;&#24448;&#24448;&#38590;&#20197;&#25110;&#29978;&#33267;&#26080;&#27861;&#25163;&#21160;&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#23548;&#33268;&#26631;&#31614;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#12290;&#36825;&#31181;&#24773;&#20917;&#24341;&#21457;&#20102;&#20004;&#20010;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65306;1&#65289;&#30001;&#20110;&#26631;&#31614;&#20013;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#65292;&#20808;&#21069;&#26041;&#27861;&#32416;&#27491;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#30340;&#26041;&#27861;&#23398;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;2&#65289;&#30001;&#20110;&#25910;&#38598;&#26080;&#22122;&#22768;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#38750;&#24120;&#22256;&#38590;&#65292;&#39564;&#35777;/&#27979;&#35797;&#30340;&#36866;&#24403;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#32531;&#35299;&#20197;&#19978;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is an important concern because of the lack of accurate ground-truth labels in plenty of real-world scenarios. In practice, various approaches for this concern first make some corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or even impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. In this paper, we focus on allevia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#20050;&#20051;&#29699;&#20013;&#30340;&#39640;&#25928;&#26679;&#26412;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#21040;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#22312;&#23569;&#25968;&#23581;&#35797;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2011.03275</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20050;&#20051;&#29699;&#20013;&#30340;&#39640;&#25928;&#26679;&#26412;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Reinforcement Learning in Robotic Table Tennis. (arXiv:2011.03275v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.03275
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#20050;&#20051;&#29699;&#20013;&#30340;&#39640;&#25928;&#26679;&#26412;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#21040;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#22312;&#23569;&#25968;&#23581;&#35797;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#28216;&#25103;&#21644;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25104;&#21151;&#26696;&#20363;&#37117;&#26159;&#22522;&#20110;&#22823;&#37327;&#35797;&#39564;&#27425;&#25968;&#30340;&#23398;&#20064;&#12290;&#32780;&#22312;&#20856;&#22411;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#21487;&#34892;&#23581;&#35797;&#27425;&#25968;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20050;&#20051;&#29699;&#26426;&#22120;&#20154;&#30340;&#39640;&#25928;&#26679;&#26412;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#20050;&#20051;&#29699;&#20013;&#65292;&#27599;&#20010;&#20987;&#29699;&#37117;&#26159;&#19981;&#21516;&#30340;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20301;&#32622;&#12289;&#36895;&#24230;&#21644;&#26059;&#36716;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#26681;&#25454;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#25214;&#21040;&#20934;&#30830;&#30340;&#22238;&#29699;&#26041;&#24335;&#12290;&#20026;&#20102;&#22312;&#23569;&#25968;&#23581;&#35797;&#20013;&#23454;&#29616;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#34987;&#23884;&#20837;&#21040;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#19968;&#27493;&#29615;&#22659;&#12290;&#29366;&#24577;&#31354;&#38388;&#21462;&#20915;&#20110;&#20987;&#29699;&#26102;&#29699;&#30340;&#20301;&#32622;&#12289;&#36895;&#24230;&#21644;&#26059;&#36716;&#65292;&#21160;&#20316;&#26159;&#20987;&#29699;&#26102;&#29699;&#25293;&#30340;&#29366;&#24577;&#65288;&#26041;&#21521;&#12289;&#36895;&#24230;&#65289;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26469;&#21152;&#36895;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26679;&#26412;&#27425;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has achieved some impressive recent successes in various computer games and simulations. Most of these successes are based on having large numbers of episodes from which the agent can learn. In typical robotic applications, however, the number of feasible attempts is very limited. In this paper we present a sample-efficient RL algorithm applied to the example of a table tennis robot. In table tennis every stroke is different, with varying placement, speed and spin. An accurate return therefore has to be found depending on a high-dimensional continuous state space. To make learning in few trials possible the method is embedded into our robot system. In this way we can use a one-step environment. The state space depends on the ball at hitting time (position, velocity, spin) and the action is the racket state (orientation, velocity) at hitting. An actor-critic based deterministic policy gradient algorithm was developed for accelerated learning. Our approach per
&lt;/p&gt;</description></item></channel></rss>