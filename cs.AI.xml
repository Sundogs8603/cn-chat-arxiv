<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26465;&#20214;&#20271;&#21162;&#21033;&#25193;&#25955;&#27169;&#22411;&#65288;BerDiff&#65289;&#65292;&#20351;&#29992;&#20271;&#21162;&#21033;&#22122;&#22768;&#20316;&#20026;&#25193;&#25955;&#26680;&#22686;&#24378;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20108;&#36827;&#21046;&#20998;&#21106;&#20219;&#21153;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#21021;&#22122;&#22768;&#29983;&#25104;&#22810;&#26679;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04429</link><description>&lt;p&gt;
BerDiff: &#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26465;&#20214;&#20271;&#21162;&#21033;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation. (arXiv:2304.04429v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04429
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26465;&#20214;&#20271;&#21162;&#21033;&#25193;&#25955;&#27169;&#22411;&#65288;BerDiff&#65289;&#65292;&#20351;&#29992;&#20271;&#21162;&#21033;&#22122;&#22768;&#20316;&#20026;&#25193;&#25955;&#26680;&#22686;&#24378;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20108;&#36827;&#21046;&#20998;&#21106;&#20219;&#21153;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#21021;&#22122;&#22768;&#29983;&#25104;&#22810;&#26679;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#22266;&#26377;&#27169;&#31946;&#24615;&#21644;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24402;&#22240;&#20110;&#27169;&#31946;&#30340;&#32959;&#30244;&#36793;&#30028;&#21644;&#22810;&#20010;&#21487;&#33021;&#30340;&#27880;&#37322;&#31561;&#22240;&#32032;&#12290;&#20998;&#21106;&#25513;&#27169;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#25552;&#20379;&#20020;&#24202;&#23454;&#36341;&#20013;&#25918;&#23556;&#23398;&#23478;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#22343;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#29983;&#25104;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22788;&#29702;&#20998;&#21106;&#20013;&#30340;&#31163;&#25955;&#25513;&#27169;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#19988;&#22810;&#26679;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25513;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26465;&#20214;&#20271;&#21162;&#21033;&#25193;&#25955;&#27169;&#22411;&#65288;BerDiff&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#20271;&#21162;&#21033;&#22122;&#22768;&#20316;&#20026;&#25193;&#25955;&#26680;&#26469;&#22686;&#24378;&#29992;&#20110;&#20108;&#36827;&#21046;&#20998;&#21106;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;BerDiff&#22312;&#25512;&#29702;&#26399;&#38388;&#38543;&#26426;&#37319;&#26679;&#21021;&#22987;&#30340;&#20271;&#21162;&#21033;&#22122;&#22768;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#22312;BraTS&#21644;LiTS&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;BerDiff&#22312;&#20998;&#21106;&#25513;&#27169;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#32988;&#36807;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation is a challenging task with inherent ambiguity and high uncertainty, attributed to factors such as unclear tumor boundaries and multiple plausible annotations. The accuracy and diversity of segmentation masks are both crucial for providing valuable references to radiologists in clinical practice. While existing diffusion models have shown strong capacities in various visual generation tasks, it is still challenging to deal with discrete masks in segmentation. To achieve accurate and diverse medical image segmentation masks, we propose a novel conditional Bernoulli Diffusion model for medical image segmentation (BerDiff). Instead of using the Gaussian noise, we first propose to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for binary segmentation tasks, resulting in more accurate segmentation masks. Second, by leveraging the stochastic nature of the diffusion model, our BerDiff randomly samples the initial Bernou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21382;&#21490;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20004;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#65292;PCR&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04408</link><description>&lt;p&gt;
PCR: &#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#22312;&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning. (arXiv:2304.04408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21382;&#21490;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20004;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#65292;PCR&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#65292;&#20294;&#25968;&#25454;&#27969;&#20013;&#30340;&#26679;&#26412;&#20165;&#38656;&#35266;&#23519;&#19968;&#27425;&#65292;&#36825;&#23481;&#26131;&#23548;&#33268;&#21382;&#21490;&#31867;&#21035;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#20195;&#29702;&#20026;&#22522;&#30784;&#25110;&#20197;&#23545;&#27604;&#20026;&#22522;&#30784;&#30340;&#22238;&#25918;&#26041;&#24335;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#22238;&#25918;&#26041;&#24335;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#21069;&#32773;&#20250;&#22240;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#32780;&#20542;&#21521;&#20110;&#26032;&#31867;&#65292;&#21518;&#32773;&#21017;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#32780;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#25910;&#25947;&#12290;&#26412;&#25991;&#23545;&#36825;&#20004;&#31181;&#22238;&#25918;&#26041;&#24335;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#20114;&#34917;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#25918;-based&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#65288;PCR&#65289;&#12290;&#20851;&#38190;&#25805;&#20316;&#26159;&#23558;&#38170;&#23450;&#28857;&#30340;&#23545;&#27604;&#26679;&#26412;&#26367;&#25442;&#20026;&#30456;&#24212;&#20195;&#29702;&#30340;&#23545;&#27604;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#20004;&#31181;&#20195;&#29702;&#65292;&#21363;&#26087;&#20195;&#29702;&#21644;&#26032;&#20195;&#29702;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#24182;&#32531;&#35299;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PCR&#22312;&#20004;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online class-incremental continual learning is a specific task of continual learning. It aims to continuously learn new classes from data stream and the samples of data stream are seen only once, which suffers from the catastrophic forgetting issue, i.e., forgetting historical knowledge of old classes. Existing replay-based methods effectively alleviate this issue by saving and replaying part of old data in a proxy-based or contrastive-based replay manner. Although these two replay manners are effective, the former would incline to new classes due to class imbalance issues, and the latter is unstable and hard to converge because of the limited number of samples. In this paper, we conduct a comprehensive analysis of these two replay manners and find that they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR). The key operation is to replace the contrastive samples of anchors with corresponding proxies in th
&lt;/p&gt;</description></item><item><title>H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04403</link><description>&lt;p&gt;
H2RBox-v2&#65306;&#36890;&#36807;&#23545;&#31216;&#23398;&#20064;&#25552;&#39640;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning. (arXiv:2304.04403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04403
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#31561;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#38656;&#27714;&#30340;&#26085;&#30410;&#22686;&#38271;&#65292;&#26377;&#21521;&#27880;&#37322;&#21464;&#24471;&#38750;&#24120;&#36153;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30340;&#27700;&#24179;&#27880;&#37322;&#25968;&#25454;&#38598;&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#26816;&#27979;&#22120;H2RBox&#65292;&#29992;&#20110;&#20174;&#27700;&#24179;&#26694;Box&#20013;&#23398;&#20064;&#26059;&#36716;&#26694;RBox&#65292;&#24182;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;H2RBox-v2&#30340;&#26032;&#29256;&#26412;&#65292;&#20197;&#36827;&#19968;&#27493;&#24357;&#21512;HBox&#30417;&#30563;&#21644;RBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21033;&#29992;&#32763;&#36716;&#21644;&#26059;&#36716;&#19968;&#33268;&#24615;&#26469;&#24320;&#21457;&#36724;&#23545;&#31216;&#24615;&#26159;&#21487;&#34892;&#30340;&#65292;H2RBox-v2&#21017;&#37319;&#29992;&#19982;H2RBox&#31867;&#20284;&#30340;&#24369;&#30417;&#30563;&#20998;&#25903;&#65292;&#24182;&#23884;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#20998;&#25903;&#65292;&#23427;&#21487;&#20197;&#20174;&#23545;&#35937;&#22270;&#20687;&#20013;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#26041;&#21521;&#12290;&#36890;&#36807;&#22788;&#29702;&#21608;&#36793;&#38382;&#39064;&#30340;&#27169;&#22359;&#65288;&#20363;&#22914;&#35282;&#21608;&#26399;&#24615;&#65289;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#31283;&#23450;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing demand for oriented object detection e.g. in autonomous driving and remote sensing, the oriented annotation has become a labor-intensive work. To make full use of existing horizontally annotated datasets and reduce the annotation cost, a weakly-supervised detector H2RBox for learning the rotated box (RBox) from the horizontal box (HBox) has been proposed and received great attention. This paper presents a new version, H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. While exploiting axisymmetry via flipping and rotating consistencies is available through our theoretical analysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, is embedded with a novel self-supervised branch that learns orientations from the symmetry inherent in the image of objects. Complemented by modules to cope with peripheral issues, e.g. angular periodicity, a stable and effective solution is achieved. To our knowledge, H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;CAVL&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#25972;&#20010;&#21477;&#23376;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#24341;&#20837;&#36731;&#37327;&#32423;&#33258;&#36866;&#24212;&#32593;&#32476;&#65292;&#23454;&#29616;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04399</link><description>&lt;p&gt;
CAVL&#65306;&#23398;&#20064;&#23545;&#27604;&#21644;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CAVL: Learning Contrastive and Adaptive Representations of Vision and Language. (arXiv:2304.04399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;CAVL&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#25972;&#20010;&#21477;&#23376;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#24341;&#20837;&#36731;&#37327;&#32423;&#33258;&#36866;&#24212;&#32593;&#32476;&#65292;&#23454;&#29616;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#26088;&#22312;&#19968;&#36215;&#23398;&#20064;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21487;&#36716;&#31227;&#21040;&#35270;&#35273;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23545;&#27604;&#21644;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;&#65292;&#21363;CAVL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#23545;&#19968;&#23545;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#21477;&#23376;&#21644;&#21516;&#19968;&#25209;&#27425;&#20013;&#27599;&#20010;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#36731;&#37327;&#32423;&#33258;&#36866;&#24212;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#24182;&#22686;&#21152;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12289;&#35270;&#35273;&#36890;&#35782;&#25512;&#29702;&#65288;VCL&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#31561;&#20845;&#20010;&#20027;&#35201;&#19979;&#28216;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;CAVL&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAVL&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual and linguistic pre-training aims to learn vision and language representations together, which can be transferred to visual-linguistic downstream tasks. However, there exists semantic confusion between language and vision during the pre-training stage. Moreover, current pre-trained models tend to take lots of computation resources for fine-tuning when transferred to downstream tasks. In this work, we present a simple but effective approach for learning Contrastive and Adaptive representations of Vision and Language, namely CAVL. Specifically, we introduce a pair-wise contrastive loss to learn alignments between the whole sentence and each image in the same batch during the pre-training process. At the fine-tuning stage, we introduce two lightweight adaptation networks to reduce model parameters and increase training speed for saving computation resources. We evaluate our CAVL on six main downstream tasks, including Visual Question Answering (VQA), Visual Commonsense Reasoning (VC
&lt;/p&gt;</description></item><item><title>CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04391</link><description>&lt;p&gt;
CAFIN: &#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04391
&lt;/p&gt;
&lt;p&gt;
CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25152;&#23398;&#23884;&#20837;&#30340;&#32039;&#20945;&#24615;&#21644;&#20016;&#23500;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#22312;(&#22823;&#22411;)&#22270;&#19978;&#24050;&#32463;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#24403;&#36825;&#20123;&#33410;&#28857;&#34920;&#31034;&#34987;&#37096;&#32626;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#36866;&#24403;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#20197;&#20943;&#23569;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#36896;&#25104;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#35843;&#26597;&#20102;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36825;&#20123;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#27809;&#26377;&#32771;&#34385;&#36830;&#25509;&#27169;&#24335;&#22312;&#22270;&#20013;&#23548;&#33268;&#30340;&#19981;&#21516;&#33410;&#28857;&#24433;&#21709;(&#25110;&#20013;&#24515;&#24615;&#33021;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#24402;&#32435;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CAFIN&#65288;Centrality Aware Fairness inducing IN-processing&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25913;&#36827;GraphSAGE&#34920;&#31034;&#30340;&#36827;&#31243;&#25216;&#26415;&#8212;&#8212;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;KG&#23545;&#40784;&#26041;&#27861;DAAKG&#65292;&#21487;&#20197;&#32852;&#21512;&#23545;&#40784;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#65307;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04389</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#26550;&#26500;&#30340;&#28145;&#24230;&#20027;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Deep Active Alignment of Knowledge Graph Entities and Schemata. (arXiv:2304.04389v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;KG&#23545;&#40784;&#26041;&#27861;DAAKG&#65292;&#21487;&#20197;&#32852;&#21512;&#23545;&#40784;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#65307;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23384;&#20648;&#20102;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20016;&#23500;&#20107;&#23454;&#12290;&#26412;&#25991;&#30740;&#31350;KG&#23545;&#40784;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;KG&#20013;&#25214;&#21040;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#30340;&#23545;&#40784;&#12290;&#23454;&#20307;&#32423;&#21035;&#30340;&#23545;&#40784;&#21487;&#20197;&#20419;&#36827;&#26550;&#26500;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KG&#23545;&#40784;&#26041;&#27861;&#65292;&#31216;&#20026;DAAKG&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65292;&#23427;&#23398;&#20064;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#26041;&#24335;&#19979;&#32852;&#21512;&#23545;&#40784;&#23427;&#20204;&#65307;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#20272;&#35745;&#23454;&#20307;&#12289;&#20851;&#31995;&#25110;&#31867;&#21035;&#23545;&#30340;&#25512;&#26029;&#21487;&#33021;&#24615;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#36817;&#20284;&#31639;&#27861;&#20197;&#39640;&#25928;&#36873;&#25321;&#25209;&#27425;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;DAAKG&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25152;&#26377;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) store rich facts about the real world. In this paper, we study KG alignment, which aims to find alignment between not only entities but also relations and classes in different KGs. Alignment at the entity level can cross-fertilize alignment at the schema level. We propose a new KG alignment approach, called DAAKG, based on deep learning and active learning. With deep learning, it learns the embeddings of entities, relations and classes, and jointly aligns them in a semi-supervised manner. With active learning, it estimates how likely an entity, relation or class pair can be inferred, and selects the best batch for human labeling. We design two approximation algorithms for efficient solution to batch selection. Our experiments on benchmark datasets show the superior accuracy and generalization of DAAKG and validate the effectiveness of all its modules.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35762;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;WebBrain&#65292;&#23427;&#36890;&#36807;&#25366;&#25496;Web&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#65292;&#20026;&#26597;&#35810;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#31616;&#30701;&#25991;&#31456;&#12290;&#25105;&#20204;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#20102;&#25968;&#25454;&#38598;WebBrain-Raw&#65292;&#26500;&#24314;&#20102;WebBrain-R&#21644;WebBrain-G&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;ReGen&#12290;</title><link>http://arxiv.org/abs/2304.04358</link><description>&lt;p&gt;
WebBrain: &#22522;&#20110;&#22823;&#22411;Web&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25366;&#25496;&#25903;&#25345;&#35777;&#25454;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#25991;&#31456;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus. (arXiv:2304.04358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35762;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;WebBrain&#65292;&#23427;&#36890;&#36807;&#25366;&#25496;Web&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#65292;&#20026;&#26597;&#35810;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#31616;&#30701;&#25991;&#31456;&#12290;&#25105;&#20204;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#20102;&#25968;&#25454;&#38598;WebBrain-Raw&#65292;&#26500;&#24314;&#20102;WebBrain-R&#21644;WebBrain-G&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;ReGen&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#20174;Web&#25366;&#25496;&#25903;&#25345;&#35777;&#25454;&#65292;&#20026;&#26597;&#35810;&#29983;&#25104;&#24102;&#21442;&#32771;&#25991;&#29486;&#30340;&#31616;&#30701;&#20107;&#23454;&#25991;&#31456;&#12290;&#22312;&#36825;&#20010;&#21517;&#20026;WebBrain&#30340;&#20219;&#21153;&#20013;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#20026;&#32500;&#22522;&#30334;&#31185;&#20013;&#26410;&#20986;&#29616;&#30340;&#20107;&#23454;&#26597;&#35810;&#29983;&#25104;&#27969;&#30021;&#12289;&#20449;&#24687;&#20016;&#23500;&#12289;&#20107;&#23454;&#27491;&#30830;&#30340;&#31616;&#30701;&#25991;&#31456;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;WebBrain&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25353;&#29031;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#25991;&#31456;&#21644;&#21487;&#29228;&#34892;&#30340;&#32500;&#22522;&#30334;&#31185;&#21442;&#32771;&#25991;&#29486;&#25552;&#21462;&#33521;&#35821;&#25968;&#25454;&#38598;WebBrain-Raw&#12290;WebBrain-Raw&#27604;&#20197;&#21069;&#26368;&#22823;&#30340;&#21516;&#34892;&#25968;&#25454;&#38598;&#22823;&#21313;&#20493;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#24800;&#21450;&#30740;&#31350;&#31038;&#21306;&#12290;&#20174;WebBrain-Raw&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#65306;WebBrain-R&#21644;WebBrain-G&#65292;&#20998;&#21035;&#29992;&#20110;&#35757;&#32451;&#39046;&#22495;&#20869;&#30340;&#26816;&#32034;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;WebBrain&#19978;&#23454;&#35777;&#20998;&#26512;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22686;&#24378;&#35777;&#25454;&#25903;&#25345;&#30340;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#26032;&#26694;&#26550;ReGen&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new NLP task -- generating short factual articles with references for queries by mining supporting evidence from the Web. In this task, called WebBrain, the ultimate goal is to generate a fluent, informative, and factually-correct short article (e.g., a Wikipedia article) for a factual query unseen in Wikipedia. To enable experiments on WebBrain, we construct a large-scale dataset WebBrain-Raw by extracting English Wikipedia articles and their crawlable Wikipedia references. WebBrain-Raw is ten times larger than the previous biggest peer dataset, which can greatly benefit the research community. From WebBrain-Raw, we construct two task-specific datasets: WebBrain-R and WebBrain-G, which are used to train in-domain retriever and generator, respectively. Besides, we empirically analyze the performances of the current state-of-the-art NLP techniques on WebBrain and introduce a new framework ReGen, which enhances the generation factualness by improved evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20844;&#20849;&#20449;&#24687;&#26041;&#27861;&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#31639;&#27861;&#65292;&#32467;&#21512;&#28857;&#22788;&#29702;POMDP&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#34892;&#21160;&#31354;&#38388;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04346</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#31639;&#27861;&#65306;&#22522;&#20110;&#20844;&#20849;&#20449;&#24687;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Point-based Algorithm for Multi-agent Control Using the Common Information Approach. (arXiv:2304.04346v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20844;&#20849;&#20449;&#24687;&#26041;&#27861;&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#31639;&#27861;&#65292;&#32467;&#21512;&#28857;&#22788;&#29702;POMDP&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#34892;&#21160;&#31354;&#38388;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#20449;&#24687;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#19968;&#26234;&#33021;&#20307;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;POMDP&#65289;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#31216;&#20026;&#21327;&#35843;&#32773;&#30340;POMDP&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26497;&#20854;&#24222;&#22823;&#30340;&#34892;&#21160;&#31354;&#38388;&#65292;&#36825;&#26679;&#30340;POMDP&#21487;&#33021;&#24456;&#38590;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#21327;&#35843;&#32773;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#20540;&#36845;&#20195;&#65288;CHSVI&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#20844;&#20849;&#20449;&#24687;&#26041;&#27861;&#21644;&#22522;&#20110;&#28857;&#30340;POMDP&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22823;&#34892;&#21160;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#35299;&#20915;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#26469;&#28436;&#31034;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Common Information (CI) approach provides a systematic way to transform a multi-agent stochastic control problem to a single-agent partially observed Markov decision problem (POMDP) called the coordinator's POMDP. However, such a POMDP can be hard to solve due to its extraordinarily large action space. We propose a new algorithm for multi-agent stochastic control problems, called coordinator's heuristic search value iteration (CHSVI), that combines the CI approach and point-based POMDP algorithms for large action spaces. We demonstrate the algorithm through optimally solving several benchmark problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.04339</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#24773;&#24863;&#20998;&#26512;&#22120;&#21527;&#65311;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#22312;&#30740;&#31350;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#19979;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#29305;&#21035;&#24819;&#30693;&#36947;&#23427;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#22120;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#24847;&#35265;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#12290;&#20197;&#19978;&#35780;&#20272;&#28041;&#21450;18&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;5&#20010;&#20195;&#34920;&#24615;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;ChatGPT&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#30456;&#24212;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#22312;&#26411;&#31471;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#23450;&#24615;&#26696;&#20363;&#30740;&#31350;&#20197;&#28145;&#20837;&#29702;&#35299;&#20854;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
&lt;/p&gt;</description></item><item><title>ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.04321</link><description>&lt;p&gt;
ARNOLD&#65306;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#23454;&#29616;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04321
&lt;/p&gt;
&lt;p&gt;
ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#29702;&#35299;&#29289;&#20307;&#30340;&#36830;&#32493;&#29366;&#24577;&#23545;&#20110;&#20219;&#21153;&#23398;&#20064;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20551;&#23450;&#30446;&#26631;&#29366;&#24577;&#26159;&#31163;&#25955;&#30340;(&#20363;&#22914;&#20108;&#36827;&#21046;&#29366;&#24577;)&#65292;&#36825;&#32473;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#21644;&#23558;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29366;&#24577;&#31163;&#25955;&#21270;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#26681;&#25454;&#21160;&#20316;&#21644;&#29366;&#24577;&#30340;&#24341;&#23548;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARNOLD&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;ARNOLD&#30001;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#32452;&#25104;&#65292;&#28041;&#21450;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#20419;&#36827;&#35821;&#35328;&#24341;&#23548;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#26495;&#29983;&#25104;&#30340;&#35821;&#35328;&#25551;&#36848;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23398;&#20064;&#27169;&#22411;&#26469;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ARNOLD&#20026;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#30340;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;&#20174;&#27169;&#25311;&#22330;&#26223;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#24378;&#40065;&#26834;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#27604;&#26368;&#20339;&#38598;&#25104;&#25104;&#21592;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04308</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach. (arXiv:2304.04308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#24378;&#40065;&#26834;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#27604;&#26368;&#20339;&#38598;&#25104;&#25104;&#21592;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#28041;&#21450;&#26102;&#38388;&#25968;&#25454;&#30340;&#24191;&#27867;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#38598;&#25104;&#24314;&#27169;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#39044;&#27979;&#27169;&#22411;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#25104;&#29087;&#25216;&#26415;&#65292;&#22240;&#20026;&#21333;&#20010;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#22240;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#25442;&#32780;&#39640;&#24230;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#24378;&#40065;&#26834;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#65288;ARO&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#26435;&#37325;&#21487;&#20197;&#38543;&#26102;&#38388;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#23454;&#39564;&#21644;&#29616;&#23454;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#27668;&#27745;&#26579;&#31649;&#29702;&#12289;&#33021;&#28304;&#28040;&#32791;&#39044;&#27979;&#21644;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#31561;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#38598;&#25104;&#27169;&#22411;&#22312;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#22909;&#30340;&#38598;&#25104;&#25104;&#21592;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;16-26%&#21644;14-28%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate time series forecasting is critical for a wide range of problems with temporal data. Ensemble modeling is a well-established technique for leveraging multiple predictive models to increase accuracy and robustness, as the performance of a single predictor can be highly variable due to shifts in the underlying data distribution. This paper proposes a new methodology for building robust ensembles of time series forecasting models. Our approach utilizes Adaptive Robust Optimization (ARO) to construct a linear regression ensemble in which the models' weights can adapt over time. We demonstrate the effectiveness of our method through a series of synthetic experiments and real-world applications, including air pollution management, energy consumption forecasting, and tropical cyclone intensity forecasting. Our results show that our adaptive ensembles outperform the best ensemble member in hindsight by 16-26% in root mean square error and 14-28% in conditional value at risk and improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;CILG&#25216;&#26415;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;CILG&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20811;&#26381;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04300</link><description>&lt;p&gt;
&#22270;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Class-Imbalanced Learning on Graphs: A Survey. (arXiv:2304.04300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;CILG&#25216;&#26415;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;CILG&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20811;&#26381;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#30340;&#24555;&#36895;&#21457;&#23637;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#30340;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#22312;&#22270;&#19978;&#65288;CILG&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;CILG&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;CILG&#30340;&#29616;&#26377;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#35265;&#35299;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#25345;&#32493;&#26356;&#26032;&#30340;&#35770;&#25991;&#38405;&#35835;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement in data-driven research has increased the demand for effective graph data analysis. However, real-world data often exhibits class imbalance, leading to poor performance of machine learning models. To overcome this challenge, class-imbalanced learning on graphs (CILG) has emerged as a promising solution that combines the strengths of graph representation learning and class-imbalanced learning. In recent years, significant progress has been made in CILG. Anticipating that such a trend will continue, this survey aims to offer a comprehensive understanding of the current state-of-the-art in CILG and provide insights for future research directions. Concerning the former, we introduce the first taxonomy of existing work and its connection to existing imbalanced learning literature. Concerning the latter, we critically analyze recent work in CILG and discuss urgent lines of inquiry within the topic. Moreover, we provide a continuously maintained reading list of papers an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20013;&#25104;&#21151;&#27169;&#25311;&#20986;&#38750;&#39640;&#26031;&#22810;&#27169;&#24335;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#20854;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2304.04290</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#29992;&#20110;&#21512;&#25104;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distributed Conditional GAN (discGAN) For Synthetic Healthcare Data Generation. (arXiv:2304.04290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20013;&#25104;&#21151;&#27169;&#25311;&#20986;&#38750;&#39640;&#26031;&#22810;&#27169;&#24335;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#20854;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#26469;&#29983;&#25104;&#29305;&#23450;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;&#34429;&#28982;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#22270;&#20687;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#20851;&#27880;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;&#24314;&#27169;&#31163;&#25955;&#21644;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#26159;&#19968;&#39033;&#38750;&#24120;&#26377;&#29992;&#30340;&#38750;&#24179;&#20961;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;discGAN&#24212;&#29992;&#20110;&#27169;&#25311;&#38750;&#39640;&#26031;&#22810;&#27169;&#24335;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#21407;&#22987;&#30340;2,027&#20010;eICU&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20102;249,000&#20010;&#21512;&#25104;&#35760;&#24405;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21151;&#25928;&#12289;&#36830;&#32493;&#21464;&#37327;&#30340;Kolmogorov-Smirnov&#65288;KS&#65289;&#26816;&#39564;&#21644;&#31163;&#25955;&#21464;&#37327;&#30340;&#21345;&#26041;&#26816;&#39564;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;discGAN&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19982;&#30495;&#23454;&#25968;&#25454;&#31867;&#20284;&#30340;&#20998;&#24067;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a distributed Generative Adversarial Networks (discGANs) to generate synthetic tabular data specific to the healthcare domain. While using GANs to generate images has been well studied, little to no attention has been given to generation of tabular data. Modeling distributions of discrete and continuous tabular data is a non-trivial task with high utility. We applied discGAN to model non-Gaussian multi-modal healthcare data. We generated 249,000 synthetic records from original 2,027 eICU dataset. We evaluated the performance of the model using machine learning efficacy, the Kolmogorov-Smirnov (KS) test for continuous variables and chi-squared test for discrete variables. Our results show that discGAN was able to generate data with distributions similar to the real data.
&lt;/p&gt;</description></item><item><title>FrenchMedMCQA&#26159;&#27861;&#35821;&#21307;&#23398;MCQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3,105&#36947;&#30495;&#23454;&#32771;&#35797;&#39064;&#30446;&#12290;&#38656;&#35201;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#25110;MCQA&#20219;&#21153;&#19987;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04280</link><description>&lt;p&gt;
FrenchMedMCQA: &#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27861;&#35821;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain. (arXiv:2304.04280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04280
&lt;/p&gt;
&lt;p&gt;
FrenchMedMCQA&#26159;&#27861;&#35821;&#21307;&#23398;MCQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3,105&#36947;&#30495;&#23454;&#32771;&#35797;&#39064;&#30446;&#12290;&#38656;&#35201;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#25110;MCQA&#20219;&#21153;&#19987;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FrenchMedMCQA&#65292;&#36825;&#26159;&#20844;&#24320;&#21457;&#24067;&#30340;&#21307;&#23398;&#39046;&#22495;&#27861;&#35821;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;3,105&#36947;&#30495;&#23454;&#30340;&#27861;&#22269;&#33647;&#23398;&#19987;&#19994;&#25991;&#20973;&#32771;&#35797;&#39064;&#30446;&#32452;&#25104;&#65292;&#21253;&#25324;&#21333;&#39033;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#27599;&#20010;&#23454;&#20363;&#21253;&#21547;&#26631;&#35782;&#31526;&#12289;&#38382;&#39064;&#12289;&#20116;&#20010;&#21487;&#33021;&#30340;&#31572;&#26696;&#21644;&#23427;&#20204;&#30340;&#25163;&#21160;&#32416;&#27491;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#32447;&#27169;&#22411;&#26469;&#33258;&#21160;&#22788;&#29702;&#35813;MCQA&#20219;&#21153;&#65292;&#20197;&#25253;&#21578;&#24403;&#21069;&#30340;&#24615;&#33021;&#21644;&#31361;&#20986;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#32467;&#26524;&#30340;&#35814;&#32454;&#20998;&#26512;&#26174;&#31034;&#65292;&#38656;&#35201;&#26377;&#36866;&#24212;&#20110;&#21307;&#23398;&#39046;&#22495;&#25110;MCQA&#20219;&#21153;&#30340;&#34920;&#31034;&#24418;&#24335;&#65306;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;FrenchMedMCQA&#26159;&#20197;&#27861;&#35821;&#20070;&#20889;&#30340;&#65292;&#33521;&#35821;&#19987;&#38376;&#30340;&#27169;&#22411;&#20063;&#27604;&#36890;&#29992;&#30340;&#27861;&#35821;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#35821;&#26009;&#24211;&#12289;&#27169;&#22411;&#21644;&#24037;&#20855;&#37117;&#21487;&#22312;&#32447;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MixUp&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;MixUp++&#21644;LatentMixUp++&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;1&#65285;- 15&#65285;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.04271</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23604;&#23596;&#31616;&#21333;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Embarrassingly Simple MixUp for Time-series. (arXiv:2304.04271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MixUp&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;MixUp++&#21644;LatentMixUp++&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;1&#65285;- 15&#65285;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#25968;&#25454;&#30340;&#21160;&#24577;&#24615;&#65292;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35832;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#31867;&#30340;&#39046;&#22495;&#65292;&#20197;&#21033;&#29992;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#20854;&#20013;&#19968;&#31181;&#26368;&#24120;&#29992;&#30340;&#25216;&#26415;MixUp&#65292;&#24212;&#29992;&#20110;&#26102;&#24207;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;MixUp++&#21644;LatentMixUp++&#26041;&#27861;&#20998;&#21035;&#23545;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#25554;&#20540;&#30340;&#31616;&#21333;&#20462;&#25913;&#12290;&#25105;&#20204;&#20063;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#20302;&#26631;&#35760;&#25968;&#25454;&#36824;&#26159;&#39640;&#26631;&#35760;&#25968;&#25454;&#21046;&#24230;&#65292;&#36890;&#36807;LatentMixUp ++&#65292;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;1&#65285;- 15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling time series data is an expensive task because of domain expertise and dynamic nature of the data. Hence, we often have to deal with limited labeled data settings. Data augmentation techniques have been successfully deployed in domains like computer vision to exploit the use of existing labeled data. We adapt one of the most commonly used technique called MixUp, in the time series domain. Our proposed, MixUp++ and LatentMixUp++, use simple modifications to perform interpolation in raw time series and classification model's latent space, respectively. We also extend these methods with semi-supervised learning to exploit unlabeled data. We observe significant improvements of 1\% - 15\% on time series classification on two public datasets, for both low labeled data as well as high labeled data regimes, with LatentMixUp++.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#23433;&#20840;&#36335;&#30001;&#31639;&#27861;&#65288;SRABC&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#28304;&#33410;&#28857;&#21040;&#30446;&#30340;&#33410;&#28857;&#30340;&#23433;&#20840;&#12289;&#32463;&#36807;&#35748;&#35777;&#21644;&#38450;&#31713;&#25913;&#30340;&#36335;&#30001;&#26469;&#20445;&#25252;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#39044;&#38450;MANET&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#25163;&#27573;&#65292;&#24182;&#23545;&#33410;&#28857;&#36827;&#34892;&#35748;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SRABC&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#26159;&#30830;&#20445;MANET&#23433;&#20840;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2304.04254</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;MANET&#20013;&#30340;&#23433;&#20840;&#36335;&#30001;&#21327;&#35758;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Secure Routing Protocol To Mitigate Attacks By Using Blockchain Technology In Manet. (arXiv:2304.04254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04254
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#23433;&#20840;&#36335;&#30001;&#31639;&#27861;&#65288;SRABC&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#28304;&#33410;&#28857;&#21040;&#30446;&#30340;&#33410;&#28857;&#30340;&#23433;&#20840;&#12289;&#32463;&#36807;&#35748;&#35777;&#21644;&#38450;&#31713;&#25913;&#30340;&#36335;&#30001;&#26469;&#20445;&#25252;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#39044;&#38450;MANET&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#25163;&#27573;&#65292;&#24182;&#23545;&#33410;&#28857;&#36827;&#34892;&#35748;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SRABC&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#26159;&#30830;&#20445;MANET&#23433;&#20840;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MANET&#26159;&#19968;&#32452;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36890;&#20449;&#30340;&#31227;&#21160;&#33410;&#28857;&#65292;&#23427;&#20204;&#20174;&#19968;&#20010;&#28857;&#31227;&#21160;&#21040;&#21478;&#19968;&#20010;&#28857;&#12290;&#30001;&#20110;MANET&#26159;&#19968;&#20010;&#27809;&#26377;&#22522;&#30784;&#35774;&#26045;&#19988;&#25299;&#25169;&#32467;&#26500;&#21487;&#21464;&#30340;&#32593;&#32476;&#65292;&#22240;&#27492;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#24694;&#24847;&#32593;&#32476;&#33410;&#28857;&#26159;&#32593;&#32476;&#25915;&#20987;&#30340;&#28304;&#22836;&#12290;&#22312;MANET&#20013;&#65292;&#25915;&#20987;&#21487;&#20197;&#37319;&#21462;&#21508;&#31181;&#24418;&#24335;&#65292;&#24182;&#20197;&#20854;&#29420;&#29305;&#30340;&#26041;&#24335;&#25913;&#21464;&#32593;&#32476;&#30340;&#36816;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35768;&#22810;&#24418;&#24335;&#30340;&#25915;&#20987;&#12289;&#23427;&#20204;&#23545;MANET&#30340;&#24433;&#21709;&#20197;&#21450;&#30446;&#21069;&#23454;&#26045;&#30340;MANET&#38450;&#24481;&#25514;&#26045;&#12290;&#25152;&#25552;&#20986;&#30340;&#37319;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#23433;&#20840;&#36335;&#30001;&#31639;&#27861;&#65288;SRABC&#65289;&#21487;&#20445;&#25252;MANET&#20813;&#21463;&#25915;&#20987;&#24182;&#23545;&#33410;&#28857;&#36827;&#34892;&#35748;&#35777;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#28304;&#33410;&#28857;&#21040;&#30446;&#30340;&#33410;&#28857;&#30340;&#23433;&#20840;&#12289;&#32463;&#36807;&#35748;&#35777;&#21644;&#38450;&#31713;&#25913;&#30340;&#36335;&#30001;&#26469;&#20445;&#25252;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#38450;&#33539;&#23041;&#32961;&#12290;&#20351;&#29992;NS2&#27169;&#25311;&#22120;&#35780;&#20272;&#20102;SRABC&#31639;&#27861;&#30340;&#20851;&#38190;&#24615;&#33021;&#21442;&#25968;&#65292;&#22914;&#25968;&#25454;&#21253;&#20256;&#36882;&#29575;&#12289;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#21534;&#21520;&#37327;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SRABC&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#26159;&#30830;&#20445;MANET&#23433;&#20840;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
MANET is a collection of mobile nodes that communicate through wireless networks as they move from one point to another. MANET is an infrastructure-less network with a changeable topology; as a result, it is very susceptible to attacks. MANET attack prevention represents a serious difficulty. Malicious network nodes are the source of network-based attacks. In a MANET, attacks can take various forms, and each one alters the network's operation in its unique way. In general, attacks can be separated into two categories: those that target the data traffic on a network and those that target the control traffic. This article explains the many sorts of assaults, their impact on MANET, and the MANET-based defence measures that are currently in place. The suggested SRA that employs blockchain technology (SRABC) protects MANET from attacks and authenticates nodes. The secure routing algorithm (SRA) proposed by blockchain technology safeguards control and data flow against threats. This is achie
&lt;/p&gt;</description></item><item><title>Video ChatCaptioner&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#21644;&#31639;&#27861;&#29983;&#25104;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.04227</link><description>&lt;p&gt;
&#35270;&#39057;&#32842;&#22825;&#23383;&#24149;&#29983;&#25104;&#22120;&#65306; &#36808;&#21521;&#20016;&#23500;&#26102;&#31354;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions. (arXiv:2304.04227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04227
&lt;/p&gt;
&lt;p&gt;
Video ChatCaptioner&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#21644;&#31639;&#27861;&#29983;&#25104;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#35270;&#39057;&#20013;&#30340;&#21160;&#24577;&#22330;&#26223;&#65292;&#20419;&#36827;&#25105;&#20204;&#23545;&#29615;&#22659;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#29983;&#25104;&#32454;&#33268;&#21644;&#20016;&#23500;&#30340;&#35270;&#39057;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;Video ChatCaptioner&#65292;&#29992;&#20110;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992; ChatGPT &#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#26694;&#26550;&#20197;&#25552;&#20986;&#35270;&#39057;&#20869;&#23481;&#39537;&#21160;&#30340;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#31639;&#27861;&#22238;&#31572;&#36825;&#20123;&#35270;&#35273;&#26597;&#35810;&#12290;&#36825;&#31181;&#38382;&#31572;&#26694;&#26550;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#35270;&#39057;&#32454;&#33410;&#65292;&#24182;&#26174;&#31034;&#20986;&#22686;&#24378;&#35270;&#39057;&#20869;&#23481;&#30340;&#26041;&#27861;&#30340;&#21069;&#36884;&#12290;&#22312;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#20043;&#21518;&#65292;ChatGPT &#21487;&#20197;&#26681;&#25454;&#20043;&#21069;&#30340;&#23545;&#35805;&#24635;&#32467;&#20016;&#23500;&#30340;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#23450;&#24615;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340; Video ChatCaptioner &#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#26356;&#22810;&#32454;&#33410;&#30340;&#35270;&#39057;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video captioning aims to convey dynamic scenes from videos using natural language, facilitating the understanding of spatiotemporal information within our environment. Although there have been recent advances, generating detailed and enriched video descriptions continues to be a substantial challenge. In this work, we introduce Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions. Our method employs a ChatGPT model as a controller, specifically designed to select frames for posing video content-driven questions. Subsequently, a robust algorithm is utilized to answer these visual queries. This question-answer framework effectively uncovers intricate video details and shows promise as a method for enhancing video content. Following multiple conversational rounds, ChatGPT can summarize enriched video content based on previous conversations. We qualitatively demonstrate that our Video ChatCaptioner can generate captions containing mo
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#26368;&#26377;&#25928;&#30340;&#35774;&#35745;&#26159;&#20276;&#38543;&#30528;&#26174;&#24335;&#30340;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#21333;&#29420;&#20351;&#29992;&#19981;&#33021;&#38450;&#27490;&#34920;&#31034;&#30340;&#21487;&#26367;&#25442;&#24615;&#65307;&#24212;&#24910;&#29992;&#20027;&#35201;&#31354;&#38388;&#19979;&#37319;&#26679;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.04225</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#20013;&#20351;&#29992;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer Utilization in Medical Image Segmentation Networks. (arXiv:2304.04225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04225
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#26368;&#26377;&#25928;&#30340;&#35774;&#35745;&#26159;&#20276;&#38543;&#30528;&#26174;&#24335;&#30340;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#21333;&#29420;&#20351;&#29992;&#19981;&#33021;&#38450;&#27490;&#34920;&#31034;&#30340;&#21487;&#26367;&#25442;&#24615;&#65307;&#24212;&#24910;&#29992;&#20027;&#35201;&#31354;&#38388;&#19979;&#37319;&#26679;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#65292;Transformer&#26368;&#36817;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;Transformer&#19982;&#21367;&#31215;&#22359;&#30340;&#21305;&#37197;&#20197;&#21450;&#19981;&#21516;&#26550;&#26500;&#25490;&#21015;&#26041;&#24335;&#30340;&#36873;&#25321;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Transformer&#21066;&#24369;&#23454;&#39564;&#65292;&#29992;&#26222;&#36890;&#32447;&#24615;&#31639;&#23376;&#26367;&#20195;Transformer&#22359;&#65292;&#20197;&#37327;&#21270;&#35813;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;8&#20010;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#65306;1&#65289;Transformer&#23398;&#20064;&#34920;&#31034;&#30340;&#21487;&#26367;&#20195;&#24615;&#65307;2&#65289;Transformer&#30340;&#23481;&#37327;&#21333;&#29420;&#24182;&#19981;&#33021;&#38450;&#27490;&#34920;&#31034;&#30340;&#21487;&#26367;&#25442;&#24615;&#65292;&#24182;&#38656;&#35201;&#19982;&#26377;&#25928;&#30340;&#35774;&#35745;&#25645;&#37197;&#20351;&#29992;&#65307;3&#65289;Transformer&#22359;&#20013;&#26174;&#24335;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#30340;&#23384;&#22312;&#26412;&#36523;&#27604;&#20276;&#38543;&#27880;&#24847;&#21147;&#26426;&#21046;&#26356;&#26377;&#30410;&#65307;4&#65289;&#24212;&#35880;&#24910;&#20351;&#29992;Transformer&#27169;&#22359;&#20043;&#21069;&#30340;&#20027;&#35201;&#31354;&#38388;&#19979;&#37319;&#26679;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to success in the data-rich domain of natural images, Transformers have recently become popular in medical image segmentation. However, the pairing of Transformers with convolutional blocks in varying architectural permutations leaves their relative effectiveness to open interpretation. We introduce Transformer Ablations that replace the Transformer blocks with plain linear operators to quantify this effectiveness. With experiments on 8 models on 2 medical image segmentation tasks, we explore -- 1) the replaceable nature of Transformer-learnt representations, 2) Transformer capacity alone cannot prevent representational replaceability and works in tandem with effective design, 3) The mere existence of explicit feature hierarchies in transformer blocks is more beneficial than accompanying self-attention modules, 4) Major spatial downsampling before Transformer modules should be used with caution.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#36816;&#34892;&#26102;&#38388;&#25351;&#25968;&#22686;&#38271;&#21644;&#27515;&#38145;&#12289;&#37325;&#26032;&#36335;&#30001;&#31561;&#19981;&#33391;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#39640;&#36895;&#20844;&#36335;&#30340;&#27010;&#24565;&#32435;&#20837;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#23454;&#29616;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;&#24182;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.04217</link><description>&lt;p&gt;
&#38754;&#21521;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#39640;&#36895;&#20844;&#36335;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Study of Highway for Lifelong Multi-Agent Path Finding. (arXiv:2304.04217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#36816;&#34892;&#26102;&#38388;&#25351;&#25968;&#22686;&#38271;&#21644;&#27515;&#38145;&#12289;&#37325;&#26032;&#36335;&#30001;&#31561;&#19981;&#33391;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#39640;&#36895;&#20844;&#36335;&#30340;&#27010;&#24565;&#32435;&#20837;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#23454;&#29616;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;&#24182;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#30340;&#37197;&#36865;&#20179;&#24211;&#20013;&#65292;&#26234;&#33021;&#20307;&#22312;&#22320;&#22270;&#19978;&#31359;&#34892;&#23436;&#25104;&#28304;&#28304;&#19981;&#26029;&#30340;&#20219;&#21153;&#65292;&#36825;&#34987;&#23450;&#20041;&#20026;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36335;&#24452;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22320;&#22270;&#22823;&#23567;&#25110;&#26234;&#33021;&#20307;&#23494;&#24230;&#22686;&#38271;&#26102;&#20250;&#36935;&#21040;&#36816;&#34892;&#26102;&#38388;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#27515;&#38145;&#20197;&#21450;&#37325;&#26032;&#36335;&#30001;&#31561;&#19981;&#33391;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#38754;&#21521;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#36895;&#20844;&#36335;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#20027;&#35201;&#29992;&#20110;&#19968;&#27425;&#24615;&#36335;&#24452;&#35268;&#21010;&#65292;&#36890;&#36807;&#40723;&#21169;&#26234;&#33021;&#20307;&#26397;&#30528;&#21516;&#19968;&#26041;&#21521;&#31227;&#21160;&#65292;&#20943;&#23569;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#23558;&#39640;&#36895;&#20844;&#36335;&#24605;&#24819;&#32435;&#20837;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#23567;&#21270;&#27515;&#38145;&#21644;&#37325;&#26032;&#36335;&#30001;&#38382;&#39064;&#30340;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36816;&#34892;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#65292;&#21534;&#21520;&#37327;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern fulfillment warehouses, agents traverse the map to complete endless tasks that arrive on the fly, which is formulated as a lifelong Multi-Agent Path Finding (lifelong MAPF) problem. The goal of tackling this challenging problem is to find the path for each agent in a finite runtime while maximizing the throughput. However, existing methods encounter exponential growth of runtime and undesirable phenomena of deadlocks and rerouting as the map size or agent density grows. To address these challenges in lifelong MAPF, we explore the idea of highways mainly studied for one-shot MAPF (i.e., finding paths at once beforehand), which reduces the complexity of the problem by encouraging agents to move in the same direction. We utilize two methods to incorporate the highway idea into the lifelong MAPF framework and discuss the properties that minimize the existing problems of deadlocks and rerouting. The experimental results demonstrate that the runtime is considerably reduced and the 
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#26469;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#65292;&#24182;&#21457;&#29616;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#22909;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#29992;&#20110;&#24212;&#23545;&#22810;&#25968;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#22312;SemEval2023&#20219;&#21153;3&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2304.04190</link><description>&lt;p&gt;
QUST&#38431;&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#65306;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques. (arXiv:2304.04190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#26469;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#65292;&#24182;&#21457;&#29616;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#22909;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#29992;&#20110;&#24212;&#23545;&#22810;&#25968;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#22312;SemEval2023&#20219;&#21153;3&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;QUST&#22242;&#38431;&#21442;&#21152;SemEval2023&#20219;&#21153;3&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#21333;&#35821;&#27169;&#22411;&#22312;&#20219;&#21153;&#26089;&#26399;&#23545;&#22810;&#25968;&#31867;&#36827;&#34892;&#20102;&#27424;&#37319;&#26679;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20004;&#31181;&#19981;&#21516;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#20998;&#21035;&#20026;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#30456;&#20851;&#30340;&#12290;&#25152;&#26377;&#23454;&#39564;&#37117;&#22312;10&#25240;&#20132;&#21449;&#39564;&#35777;&#19979;&#36827;&#34892;&#65292;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system achieves the second best in Italian and Spanish (zero-shot) in subtask-1.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#32531;&#35299;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#19978;&#20256;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04164</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#39640;&#25928;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy. (arXiv:2304.04164v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#32531;&#35299;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#19978;&#20256;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#20351;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#19978;&#20256;&#27169;&#22411;&#32780;&#27844;&#28431;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#20256;&#36755;&#24102;&#23485;&#65292;&#35757;&#32451;&#24310;&#36831;&#22686;&#21152;&#65292;&#21516;&#26102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#25252;&#26102;&#27169;&#22411;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#19968;&#37096;&#20998;&#26799;&#24230;&#20803;&#32032;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#20102;&#26080;&#32447;&#20449;&#36947;&#19978;&#20256;&#36755;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#38750;&#20984;FL&#38382;&#39064;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25910;&#25947;&#24230;&#30028;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;Alternating Direction Method of Multipliers&#65288;ADMM&#65289;&#35299;&#20915;&#20854;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables distributed clients to collaboratively train a machine learning model without sharing raw data with each other. However, it suffers the leakage of private information from uploading models. In addition, as the model size grows, the training latency increases due to limited transmission bandwidth and the model performance degrades while using differential privacy (DP) protection. In this paper, we propose a gradient sparsification empowered FL framework over wireless channels, in order to improve training efficiency without sacrificing convergence performance. Specifically, we first design a random sparsification algorithm to retain a fraction of the gradient elements in each client's local training, thereby mitigating the performance degradation induced by DP and and reducing the number of transmission parameters over wireless channels. Then, we analyze the convergence bound of the proposed algorithm, by modeling a non-convex FL problem. Next, we formula
&lt;/p&gt;</description></item><item><title>RoboPianist&#26159;&#19968;&#20010;&#26032;&#30340;&#39640;&#32500;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#39640;&#31934;&#24230;&#12289;&#21327;&#35843;&#21644;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#25509;&#35302;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#36827;&#34892;&#38050;&#29748;&#28436;&#22863;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#24615;&#33021;&#29305;&#24449;&#30340;&#23450;&#37327;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04150</link><description>&lt;p&gt;
RoboPianist&#65306;&#29992;&#20110;&#39640;&#32500;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RoboPianist: A Benchmark for High-Dimensional Robot Control. (arXiv:2304.04150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04150
&lt;/p&gt;
&lt;p&gt;
RoboPianist&#26159;&#19968;&#20010;&#26032;&#30340;&#39640;&#32500;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#39640;&#31934;&#24230;&#12289;&#21327;&#35843;&#21644;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#25509;&#35302;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#36827;&#34892;&#38050;&#29748;&#28436;&#22863;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#24615;&#33021;&#29305;&#24449;&#30340;&#23450;&#37327;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#38024;&#23545;&#27979;&#35797;&#39640;&#31354;&#38388;&#21644;&#26102;&#38388;&#31934;&#24230;&#12289;&#21327;&#35843;&#21644;&#35268;&#21010;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26159;&#22312;&#39057;&#32321;&#36827;&#34892;&#25509;&#35302;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#20013;&#36827;&#34892;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#25361;&#25112;&#26159;&#36890;&#36807;&#21452;&#25163;&#28789;&#24039;&#65292;&#20351;&#29992;&#19968;&#23545;&#20223;&#20154;&#26426;&#22120;&#20154;&#25163;&#26469;&#25484;&#25569;&#38050;&#29748;&#28436;&#22863;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;RoboPianist&#65292;&#26368;&#21021;&#29256;&#26412;&#28085;&#30422;&#20102;150&#39318;&#38590;&#24230;&#19981;&#21516;&#30340;&#27468;&#26354;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#20934;&#27979;&#35797;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#34920;&#24449;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#29305;&#24449;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#26576;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#26576;&#20123;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26576;&#20123;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;RoboPianist&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12289;&#36890;&#36807;&#31616;&#21333;&#22686;&#21152;&#26032;&#27468;&#26354;&#26469;&#25193;&#23637;&#26354;&#30446;&#30340;&#39640;&#26131;&#29992;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new benchmarking suite for high-dimensional control, targeted at testing high spatial and temporal precision, coordination, and planning, all with an underactuated system frequently making-and-breaking contacts. The proposed challenge is mastering the piano through bi-manual dexterity, using a pair of simulated anthropomorphic robot hands. We call it RoboPianist, and the initial version covers a broad set of 150 variable-difficulty songs. We investigate both model-free and model-based methods on the benchmark, characterizing their performance envelopes. We observe that while certain existing methods, when well-tuned, can achieve impressive levels of performance in certain aspects, there is significant room for improvement. RoboPianist provides a rich quantitative benchmarking environment, with human-interpretable results, high ease of expansion by simply augmenting the repertoire with new songs, and opportunities for further research, including in multi-task learning, ze
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36827;&#21270;&#32858;&#31867;&#26041;&#27861;&#21644;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26469;&#23454;&#29616;&#19968;&#27425;&#32852;&#37030;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#35299;&#20915;&#36890;&#20449;&#24320;&#38144;&#21644;&#26377;&#38480;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04147</link><description>&lt;p&gt;
&#36827;&#21270;&#32858;&#31867;&#26041;&#27861;&#21644;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#29992;&#20110;&#19968;&#27425;&#32852;&#37030;&#23398;&#20064;&#20998;&#31867;&#65306;FedPNN
&lt;/p&gt;
&lt;p&gt;
FedPNN: One-shot Federated Classification via Evolving Clustering Method and Probabilistic Neural Network hybrid. (arXiv:2304.04147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36827;&#21270;&#32858;&#31867;&#26041;&#27861;&#21644;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26469;&#23454;&#29616;&#19968;&#27425;&#32852;&#37030;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#35299;&#20915;&#36890;&#20449;&#24320;&#38144;&#21644;&#26377;&#38480;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#12289;&#38134;&#34892;&#21644;&#21307;&#30103;&#31561;&#39046;&#22495;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#20854;&#20998;&#25955;&#12289;&#20998;&#24067;&#24335;&#30340;&#35757;&#32451;&#21644;&#21516;&#26102;&#33719;&#24471;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;FL&#38754;&#20020;&#30528;&#36890;&#20449;&#24320;&#38144;&#21644;&#26377;&#38480;&#30340;&#36164;&#28304;&#33021;&#21147;&#31561;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#30446;&#26631;&#65292;&#24182;&#36827;&#34892;&#20102;&#20197;&#19979;&#39318;&#27425;&#30740;&#31350;&#65306;&#65288;i&#65289;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#24067;&#20316;&#20026;&#22122;&#22768;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25913;&#36827;&#30340;&#26465;&#20214;&#34920;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;CTGAN&#65289;&#65292;&#65288;ii&#65289;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#24320;&#21457;&#21644;&#37319;&#29992;&#32852;&#37030;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;FedPNN&#65289;&#26469;&#26500;&#24314;&#20840;&#23616;&#20849;&#20139;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#25351;&#26631;&#26469;&#26816;&#26597;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting data privacy is paramount in the fields such as finance, banking, and healthcare. Federated Learning (FL) has attracted widespread attention due to its decentralized, distributed training and the ability to protect the privacy while obtaining a global shared model. However, FL presents challenges such as communication overhead, and limited resource capability. This motivated us to propose a two-stage federated learning approach toward the objective of privacy protection, which is a first-of-its-kind study as follows: (i) During the first stage, the synthetic dataset is generated by employing two different distributions as noise to the vanilla conditional tabular generative adversarial neural network (CTGAN) resulting in modified CTGAN, and (ii) In the second stage, the Federated Probabilistic Neural Network (FedPNN) is developed and employed for building globally shared classification model. We also employed synthetic dataset metrics to check the quality of the generated syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#30340;&#31995;&#32479;&#26435;&#37325;&#21098;&#26525;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#21152;&#24555;&#27169;&#22411;&#21098;&#26525;&#38382;&#39064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#24471;&#21040;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.04120</link><description>&lt;p&gt;
&#26367;&#20195;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#65306;&#19968;&#31181;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Surrogate Lagrangian Relaxation: A Path To Retrain-free Deep Neural Network Pruning. (arXiv:2304.04120v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#30340;&#31995;&#32479;&#26435;&#37325;&#21098;&#26525;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#21152;&#24555;&#27169;&#22411;&#21098;&#26525;&#38382;&#39064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#24471;&#21040;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#19977;&#38454;&#27573;&#31649;&#36947;&#26174;&#33879;&#22686;&#21152;&#20102;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#30340;&#31995;&#32479;&#26435;&#37325;&#21098;&#26525;&#20248;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#26435;&#37325;&#21098;&#26525;&#38382;&#39064;&#30340;&#31163;&#25955;&#24615;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#20102;&#27169;&#22411;&#21387;&#32553;&#38382;&#39064;&#30340;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#20108;&#27425;&#24809;&#32602;&#26469;&#21152;&#36895;SLR&#30340;&#25910;&#25947;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;SLR&#22312;&#35757;&#32451;&#38454;&#27573;&#24471;&#21040;&#30340;&#27169;&#22411;&#21442;&#25968;&#36317;&#31163;&#20854;&#26368;&#20248;&#20540;&#26356;&#36817;&#12290;&#25105;&#20204;&#20351;&#29992;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;MLP-Mixer&#12289;Swin Transformer&#12289;VGG-16&#12289;ResNet-18&#12289;ResNet-50&#12289;ResNet-110&#21644;MobileNetV2&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network pruning is a widely used technique to reduce computation cost and model size for deep neural networks. However, the typical three-stage pipeline significantly increases the overall training time. In this paper, we develop a systematic weight-pruning optimization approach based on Surrogate Lagrangian relaxation, which is tailored to overcome difficulties caused by the discrete nature of the weight-pruning problem. We prove that our method ensures fast convergence of the model compression problem, and the convergence of the SLR is accelerated by using quadratic penalties. Model parameters obtained by SLR during the training phase are much closer to their optimal values as compared to those obtained by other state-of-the-art methods. We evaluate our method on image classification tasks using CIFAR-10 and ImageNet with state-of-the-art MLP-Mixer, Swin Transformer, and VGG-16, ResNet-18, ResNet-50 and ResNet-110, MobileNetV2. We also evaluate object detection and segmentation tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.04103</link><description>&lt;p&gt;
TC-VAE&#65306;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
TC-VAE: Uncovering Out-of-Distribution Data Generative Factors. (arXiv:2304.04103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#26159;&#35299;&#20915;&#35299;&#32544;&#32467;&#23398;&#20064;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;-TC-VAE&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#23398;&#30340;&#28508;&#22312;&#34920;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#24635;&#30456;&#20851;&#24615;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#21457;&#29616;&#19981;&#22312;&#25968;&#25454;&#38598;&#20013;&#26174;&#24335;&#20986;&#29616;&#30340;&#21464;&#21270;&#22240;&#32032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#20855;&#26377;&#19981;&#24179;&#34913;&#30340;&#29983;&#25104;&#22240;&#32032;&#25968;&#25454;&#38598;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#20013;&#34920;&#26126;&#20102;TC-VAE&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#12290;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20934;&#30830;&#29575;&#21644;&#21152;&#26435;F1-score&#22343;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.04087</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#23391;&#21152;&#25289;&#26377;&#23475;&#35780;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning. (arXiv:2304.04087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#12290;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20934;&#30830;&#29575;&#21644;&#21152;&#26435;F1-score&#22343;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#26696;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#65292;&#39318;&#20808;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#30830;&#23450;&#35780;&#35770;&#26159;&#21542;&#26377;&#23475;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30830;&#23450;&#35813;&#35780;&#35770;&#23646;&#20110;&#21738;&#31181;&#27602;&#24615;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20934;&#22791;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;16,073&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;8,488&#20010;&#26159;&#26377;&#23475;&#30340;&#65292;&#24182;&#19988;&#20219;&#20309;&#26377;&#23475;&#30340;&#35780;&#35770;&#21487;&#33021;&#21516;&#26102;&#23646;&#20110;&#20845;&#31181;&#26377;&#23475;&#31867;&#22411;-&#20302;&#20439;&#65292;&#20167;&#24680;&#65292;&#23447;&#25945;&#65292;&#23041;&#32961;&#65292;&#24694;&#24847;&#21644;&#20398;&#36785;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;89.42&#65285;&#30340;&#20934;&#30830;&#29575;&#65307;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#26041;&#38754;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#65288;CNN-BiLSTM&#65289;&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#65292;&#33719;&#24471;&#20102;78.92&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;0.86&#30340;&#21152;&#26435;F1-score&#12290;&#20026;&#20102;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#24182;&#35299;&#37322;&#20998;&#31867;&#26399;&#38388;&#30340;&#21333;&#35789;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;LIME&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning-based pipeline for categorizing Bengali toxic comments, in which at first a binary classification model is used to determine whether a comment is toxic or not, and then a multi-label classifier is employed to determine which toxicity type the comment belongs to. For this purpose, we have prepared a manually labeled dataset consisting of 16,073 instances among which 8,488 are Toxic and any toxic comment may correspond to one or more of the six toxic categories - vulgar, hate, religious, threat, troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT Embedding achieved 89.42% accuracy for the binary classification task while as a multi-label classifier, a combination of Convolutional Neural Network and Bi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism achieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the predictions and interpret the word feature importance during classification by the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#21516;&#26102;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#21644;&#19981;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2304.04071</link><description>&lt;p&gt;
&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#25913;&#36827;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#24615;&#33021;&#19981;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Performance Insensitivity of Large-scale Multiobjective Optimization via Monte Carlo Tree Search. (arXiv:2304.04071v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#21516;&#26102;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#21644;&#19981;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;(LSMOP)&#30340;&#29305;&#28857;&#26159;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20914;&#31361;&#30446;&#26631;&#24182;&#28041;&#21450;&#25968;&#30334;&#20010;&#20915;&#31574;&#21464;&#37327;&#12290; &#35768;&#22810;&#24037;&#31243;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21487;&#20197;&#24314;&#27169;&#20026;LSMOP&#12290;&#21516;&#26102;&#65292;&#24037;&#31243;&#24212;&#29992;&#35201;&#27714;&#24615;&#33021;&#19981;&#25935;&#24863;&#12290;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#31639;&#27861;&#36816;&#34892;&#30340;&#32467;&#26524;&#19981;&#20165;&#22312;&#24615;&#33021;&#26041;&#38754;&#23545;&#27599;&#27425;&#36816;&#34892;&#37117;&#24456;&#22909;&#65292;&#32780;&#19988;&#22810;&#27425;&#36816;&#34892;&#30340;&#24615;&#33021;&#19981;&#24212;&#27874;&#21160;&#22826;&#22823;&#65292;&#21363;&#31639;&#27861;&#21576;&#29616;&#33391;&#22909;&#30340;&#19981;&#25935;&#24863;&#24615;&#12290;&#32771;&#34385;&#21040;&#27599;&#27425;&#36816;&#34892;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#25913;&#36827;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#31639;&#27861;&#30340;&#19981;&#25935;&#24863;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#21482;&#20851;&#27880;&#20110;&#25552;&#39640;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#19981;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale multiobjective optimization problem (LSMOP) is characterized by simultaneously optimizing multiple conflicting objectives and involving hundreds of decision variables. {Many real-world applications in engineering fields can be modeled as LSMOPs; simultaneously, engineering applications require insensitivity in performance.} This requirement usually means that the results from the algorithm runs should not only be good for every run in terms of performance but also that the performance of multiple runs should not fluctuate too much, i.e., the algorithm shows good insensitivity. Considering that substantial computational resources are requested for each run, it is essential to improve upon the performance of the large-scale multiobjective optimization algorithm, as well as the insensitivity of the algorithm. However, existing large-scale multiobjective optimization algorithms solely focus on improving the performance of the algorithms, leaving the insensitivity characteri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#35299;&#20915;&#26041;&#26696;&#35270;&#20026;&#29992;&#25143;&#65292;&#36890;&#36807;&#27748;&#26222;&#26862;&#25277;&#26679;&#21644;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817; Pareto-&#26368;&#20248;&#21069;&#27839;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04067</link><description>&lt;p&gt;
&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Recommender System Approach for Very Large-scale Multiobjective Optimization. (arXiv:2304.04067v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#35299;&#20915;&#26041;&#26696;&#35270;&#20026;&#29992;&#25143;&#65292;&#36890;&#36807;&#27748;&#26222;&#26862;&#25277;&#26679;&#21644;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817; Pareto-&#26368;&#20248;&#21069;&#27839;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23450;&#20041;&#20102;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#36229;&#36807;100,000&#20010;&#32428;&#24230;&#30340;&#38382;&#39064;&#20026;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#38656;&#35201;&#20248;&#21270;&#21313;&#19975;&#32423;&#21035;&#30340;&#21464;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31181;&#35268;&#27169;&#38750;&#24120;&#22823;&#30340;&#38382;&#39064;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#21463;&#21040;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#25104;&#21151;&#22788;&#29702;&#21382;&#21490;&#20132;&#20114;&#26377;&#38480;&#30340;&#22823;&#35268;&#27169;&#29289;&#21697;&#30340;&#21551;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#8221;&#65288;VMORS&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#23558;&#36825;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#35299;&#20915;&#26041;&#26696;&#34987;&#35270;&#20026;&#29992;&#25143;&#65292;&#19981;&#21516;&#30340;&#36827;&#21270;&#26041;&#21521;&#26159;&#31561;&#24453;&#25512;&#33616;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#20351;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817; Pareto-&#26368;&#20248;&#21069;&#27839;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#25512;&#33616;&#36827;&#21270;&#26041;&#21521;&#12290;&#23545;&#22522;&#20934;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define very large multi-objective optimization problems to be multiobjective optimization problems in which the number of decision variables is greater than 100,000 dimensions. This is an important class of problems as many real-world problems require optimizing hundreds of thousands of variables. Existing evolutionary optimization methods fall short of such requirements when dealing with problems at this very large scale. Inspired by the success of existing recommender systems to handle very large-scale items with limited historical interactions, in this paper we propose a method termed Very large-scale Multiobjective Optimization through Recommender Systems (VMORS). The idea of the proposed method is to transform the defined such very large-scale problems into a problem that can be tackled by a recommender system. In the framework, the solutions are regarded as users, and the different evolution directions are items waiting for the recommendation. We use Thompson sampling to recom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24739;&#32773;&#30340;&#22810;&#27169;&#24577;EHR&#25968;&#25454;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#65292;&#20197;&#20415;&#23454;&#29616;&#26089;&#26399;&#24178;&#39044;&#21644;&#27835;&#30103;&#12290;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04062</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting multiple sclerosis disease severity with multimodal deep neural networks. (arXiv:2304.04062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24739;&#32773;&#30340;&#22810;&#27169;&#24577;EHR&#25968;&#25454;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#65292;&#20197;&#20415;&#23454;&#29616;&#26089;&#26399;&#24178;&#39044;&#21644;&#27835;&#30103;&#12290;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65288;MS&#65289;&#26159;&#19968;&#31181;&#21457;&#23637;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#33034;&#39635;&#20013;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#31070;&#32463;&#27704;&#20037;&#24615;&#25439;&#20260;&#25110;&#24694;&#21270;&#12290;MS&#30149;&#24773;&#30340;&#20005;&#37325;&#31243;&#24230;&#26159;&#36890;&#36807;&#25193;&#23637;&#27531;&#30142;&#29366;&#24577;&#35780;&#20998;&#65288;EDSS&#65289;&#26469;&#30417;&#27979;&#30340;&#65292;&#35813;&#35780;&#20998;&#30001;&#20960;&#20010;&#21151;&#33021;&#23376;&#20998;&#25968;&#32452;&#25104;&#12290;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;MS&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#23545;&#20110;&#36890;&#36807;&#24212;&#29992;&#26089;&#26399;&#27835;&#30103;&#24178;&#39044;&#31574;&#30053;&#26469;&#20943;&#32531;&#25110;&#39044;&#38450;&#30142;&#30149;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290; &#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#20026;&#24212;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#39044;&#27979;&#24314;&#27169;&#24037;&#20855;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20197;&#24448;&#19987;&#27880;&#20110;&#21033;&#29992;&#21333;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#25110;&#27169;&#22411;&#31616;&#21333;&#32780;&#38480;&#21046;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#24739;&#32773;&#30340;&#22810;&#27169;&#24577;&#32437;&#21521;&#21644;&#27178;&#21521;EHR&#25968;&#25454;&#39044;&#27979;&#21307;&#38498;&#35775;&#38382;&#26102;&#30340;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Sclerosis (MS) is a chronic disease developed in human brain and spinal cord, which can cause permanent damage or deterioration of the nerves. The severity of MS disease is monitored by the Expanded Disability Status Scale (EDSS), composed of several functional sub-scores. Early and accurate classification of MS disease severity is critical for slowing down or preventing disease progression via applying early therapeutic intervention strategies. Recent advances in deep learning and the wide use of Electronic Health Records (EHR) creates opportunities to apply data-driven and predictive modeling tools for this goal. Previous studies focusing on using single-modal machine learning and deep learning algorithms were limited in terms of prediction accuracy due to the data insufficiency or model simplicity. In this paper, we proposed an idea of using patients' multimodal longitudinal and longitudinal EHR data to predict multiple sclerosis disease severity at the hospital visit. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.04054</link><description>&lt;p&gt;
tmn&#22312;SemEval-2023&#20219;&#21153;9&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;XLM-T&#12289;Google&#32763;&#35793;&#21644;&#38598;&#25104;&#23398;&#20064;&#36827;&#34892;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#38024;&#23545;SemEval-2023&#20219;&#21153;9&#65306;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#20998;&#26512;&#36827;&#34892;&#35774;&#35745;&#12290;&#20219;&#21153;&#30340;&#30446;&#30340;&#26159;&#39044;&#27979;&#19968;&#31995;&#21015;&#25512;&#29305;&#30340;&#20146;&#23494;&#24230;&#65292;&#33539;&#22260;&#20174;1&#65288;&#23436;&#20840;&#19981;&#20146;&#23494;&#65289;&#21040;5&#65288;&#38750;&#24120;&#20146;&#23494;&#65289;&#12290;&#27604;&#36187;&#30340;&#23448;&#26041;&#35757;&#32451;&#38598;&#21253;&#21547;&#20845;&#31181;&#35821;&#35328;&#30340;&#25512;&#29305;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#20013;&#25991;&#65289;&#12290;&#27979;&#35797;&#38598;&#21253;&#25324;&#20845;&#31181;&#32473;&#23450;&#30340;&#35821;&#35328;&#20197;&#21450;&#22806;&#37096;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#35757;&#32451;&#38598;&#20013;&#26410;&#20986;&#29616;&#30340;&#22235;&#31181;&#35821;&#35328;&#65288;&#21360;&#22320;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#38889;&#35821;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XLM-T&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36866;&#29992;&#20110;Twitter&#39046;&#22495;&#30340;&#22810;&#35821;&#31181;RoBERTa&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25105;&#20204;&#23545;&#27599;&#26465;&#25512;&#29305;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#32763;&#35793;&#25968;&#25454;&#24212;&#29992;&#20110;&#24494;&#35843;&#20013;&#30475;&#21040;&#30340;&#35821;&#35328;&#19982;&#26410;&#30475;&#21040;&#30340;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20272;&#35745;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;50&#20010;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;4&#65292;&#24182;&#23454;&#29616;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04052</link><description>&lt;p&gt;
&#20165;&#35299;&#30721;&#22120;&#25110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65311;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20026;&#27491;&#21017;&#21270;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. (arXiv:2304.04052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#28304;&#24207;&#21015;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#12290; &#20256;&#32479;&#19978;&#65292;&#22823;&#22810;&#25968;seq2seq&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#35299;&#20915;&#30340;&#65292;&#35813;&#26694;&#26550;&#38656;&#35201;&#32534;&#30721;&#22120;&#26469;&#32534;&#30721;&#28304;&#24207;&#21015;&#65292;&#24182;&#19988;&#38656;&#35201;&#35299;&#30721;&#22120;&#26469;&#29983;&#25104;&#30446;&#26631;&#25991;&#26412;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26032;&#26041;&#27861;&#65292;&#23558;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;seq2seq&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;seq2seq&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#26377;&#25928;&#24615;&#30340;&#24443;&#24213;&#20998;&#26512;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#12290;&#35813;&#32467;&#26500;&#26088;&#22312;&#22797;&#21046;&#32463;&#20856;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26356;&#23481;&#26131;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to b
&lt;/p&gt;</description></item><item><title>RescueSNN&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;SNN&#33455;&#29255;&#35745;&#31639;&#24341;&#25806;&#20013;&#27704;&#20037;&#25925;&#38556;&#30340;&#26041;&#27861;&#65292;&#21487;&#32500;&#25345;&#24615;&#33021;&#21644;&#36136;&#37327;&#24182;&#20943;&#23569;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.04041</link><description>&lt;p&gt;
RescueSNN: &#22312;&#27704;&#20037;&#25925;&#38556;&#19979;&#25552;&#39640;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
RescueSNN: Enabling Reliable Executions on Spiking Neural Network Accelerators under Permanent Faults. (arXiv:2304.04041v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04041
&lt;/p&gt;
&lt;p&gt;
RescueSNN&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;SNN&#33455;&#29255;&#35745;&#31639;&#24341;&#25806;&#20013;&#27704;&#20037;&#25925;&#38556;&#30340;&#26041;&#27861;&#65292;&#21487;&#32500;&#25345;&#24615;&#33021;&#21644;&#36136;&#37327;&#24182;&#20943;&#23569;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#26368;&#22823;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22788;&#29702;&#30340;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#37319;&#29992;&#20102;&#19987;&#38376;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;/&#33455;&#29255;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;SNN&#33455;&#29255;&#21487;&#33021;&#20250;&#21463;&#21040;&#27704;&#20037;&#25925;&#38556;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#37325;&#22823;&#30340;&#31934;&#24230;&#38477;&#20302;&#21644;&#31995;&#32479;&#25925;&#38556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RescueSNN&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;SNN&#33455;&#29255;&#35745;&#31639;&#24341;&#25806;&#20013;&#30340;&#27704;&#20037;&#25925;&#38556;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;&#35774;&#35745;&#26102;&#38388;&#21644;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21534;&#21520;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
To maximize the performance and energy efficiency of Spiking Neural Network (SNN) processing on resource-constrained embedded systems, specialized hardware accelerators/chips are employed. However, these SNN chips may suffer from permanent faults which can affect the functionality of weight memory and neuron behavior, thereby causing potentially significant accuracy degradation and system malfunctioning. Such permanent faults may come from manufacturing defects during the fabrication process, and/or from device/transistor damages (e.g., due to wear out) during the run-time operation. However, the impact of permanent faults in SNN chips and the respective mitigation techniques have not been thoroughly investigated yet. Toward this, we propose RescueSNN, a novel methodology to mitigate permanent faults in the compute engine of SNN chips without requiring additional retraining, thereby significantly cutting down the design time and retraining costs, while maintaining the throughput and qu
&lt;/p&gt;</description></item><item><title>EnforceSNN &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32771;&#34385;&#36817;&#20284;DRAM&#65292;&#20351;&#29992;&#37327;&#21270;&#26435;&#37325;&#38477;&#20302;DRAM&#30340;&#35775;&#38382;&#33021;&#37327;&#65292;&#23454;&#29616;&#20102;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340;SNN&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.04039</link><description>&lt;p&gt;
EnforceSNN: &#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32771;&#34385;&#36817;&#20284;DRAM&#65292;&#23454;&#29616;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnforceSNN: Enabling Resilient and Energy-Efficient Spiking Neural Network Inference considering Approximate DRAMs for Embedded Systems. (arXiv:2304.04039v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04039
&lt;/p&gt;
&lt;p&gt;
EnforceSNN &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32771;&#34385;&#36817;&#20284;DRAM&#65292;&#20351;&#29992;&#37327;&#21270;&#26435;&#37325;&#38477;&#20302;DRAM&#30340;&#35775;&#38382;&#33021;&#37327;&#65292;&#23454;&#29616;&#20102;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340;SNN&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30001;&#20110;&#20854;&#29983;&#29289;&#21487;&#34892;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20302;&#25805;&#20316;&#21151;&#29575;/&#33021;&#37327;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;DRAM&#30340; off-chip &#20869;&#23384;&#35775;&#38382;&#21344;&#25454;&#20102;SNN&#22788;&#29702;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24182;&#26410;&#20248;&#21270;DRAM&#30340;&#27599;&#27425;&#35775;&#38382;&#30340;&#33021;&#37327;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22522;&#20110;SNN&#30340;&#31995;&#32479;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#33410;&#33021;&#25928;&#30410;&#12290;&#20026;&#20102;&#22823;&#24133;&#24230;&#38477;&#20302;DRAM&#30340;&#27599;&#27425;&#35775;&#38382;&#30340;&#33021;&#37327;&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#38477;&#20302;DRAM&#20379;&#30005;&#30005;&#21387;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;DRAM&#21333;&#20803;&#30340;&#38169;&#35823;&#65288;&#21363;&#25152;&#35859;&#30340;&#36817;&#20284;DRAM&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EnforceSNN&#65292;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#29992;&#38477;&#21387;DRAM&#23454;&#29616;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340; SNN &#25512;&#29702;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204; EnforceSNN &#30340;&#20851;&#38190;&#26426;&#21046;&#26159;:(1)&#37319;&#29992;&#37327;&#21270;&#26435;&#37325;&#38477;&#20302;DRAM&#30340;&#35775;&#38382;&#33021;&#37327;&#65307;(2)&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;DRAM&#26144;&#23556;p
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have shown capabilities of achieving high accuracy under unsupervised settings and low operational power/energy due to their bio-plausible computations. Previous studies identified that DRAM-based off-chip memory accesses dominate the energy consumption of SNN processing. However, state-of-the-art works do not optimize the DRAM energy-per-access, thereby hindering the SNN-based systems from achieving further energy efficiency gains. To substantially reduce the DRAM energy-per-access, an effective solution is to decrease the DRAM supply voltage, but it may lead to errors in DRAM cells (i.e., so-called approximate DRAM). Towards this, we propose \textit{EnforceSNN}, a novel design framework that provides a solution for resilient and energy-efficient SNN inference using reduced-voltage DRAM for embedded systems. The key mechanisms of our EnforceSNN are: (1) employing quantized weights to reduce the DRAM access energy; (2) devising an efficient DRAM mapping p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35266;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#30340;&#26031;&#27931;&#20240;&#20811;NER&#25968;&#25454;&#38598;WikiGoldSK&#65292;&#36890;&#36807;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#29616;&#26377;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04026</link><description>&lt;p&gt;
WikiGoldSK:&#26031;&#27931;&#20240;&#20811;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24102;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#22522;&#20934;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition. (arXiv:2304.04026v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35266;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#30340;&#26031;&#27931;&#20240;&#20811;NER&#25968;&#25454;&#38598;WikiGoldSK&#65292;&#36890;&#36807;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#29616;&#26377;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#30340;NLP&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;NER&#26041;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#39640;&#36136;&#37327;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#35821;&#35328;&#20173;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;WikiGoldSK&#26469;&#35299;&#20915;&#26031;&#27931;&#20240;&#20811;&#35821;&#20013;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35266;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#30340;&#26031;&#27931;&#20240;&#20811;NER&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#38134;&#26631;&#20934;&#26031;&#27931;&#20240;&#20811;&#35821;NER&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#26469;&#23545;&#20854;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#22312;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25903;&#25345;&#26410;&#26469;&#22522;&#20110;&#26031;&#27931;&#20240;&#20811;NER&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312;https://github.com/NaiveNeuron/WikiGoldSK&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#65292;&#37319;&#29992;&#21487;&#20801;&#35768;&#30340;&#35768;&#21487;&#26465;&#27454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a fundamental NLP tasks with a wide range of practical applications. The performance of state-of-the-art NER methods depends on high quality manually anotated datasets which still do not exist for some languages. In this work we aim to remedy this situation in Slovak by introducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. We benchmark it by evaluating state-of-the-art multilingual Pretrained Language Models and comparing it to the existing silver-standard Slovak NER dataset. We also conduct few-shot experiments and show that training on a sliver-standard dataset yields better results. To enable future work that can be based on Slovak NER, we release the dataset, code, as well as the trained models publicly under permissible licensing terms at https://github.com/NaiveNeuron/WikiGoldSK.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#65292;&#37319;&#29992;&#38598;&#21512;&#31181;&#32676;&#31574;&#30053;&#21644;&#20195;&#29702;&#27169;&#22411;&#21152;&#24555;&#31639;&#27861;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04022</link><description>&lt;p&gt;
&#19968;&#31181;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning-assisted Genetic Programming Algorithm for Team Formation Problem Considering Person-Job Matching. (arXiv:2304.04022v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04022
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#65292;&#37319;&#29992;&#38598;&#21512;&#31181;&#32676;&#31574;&#30053;&#21644;&#20195;&#29702;&#27169;&#22411;&#21152;&#24555;&#31639;&#27861;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#22242;&#38431;&#23545;&#20110;&#20844;&#21496;&#25104;&#21151;&#23436;&#25104;&#26032;&#39033;&#30446;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#65288;TFP-PJM&#65289;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;0-1&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20154;-&#24037;&#21305;&#37197;&#21644;&#22242;&#38431;&#25104;&#21592;&#36890;&#20449;&#24847;&#24895;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#30452;&#35273;&#27169;&#31946;&#25968;&#35745;&#31639;&#20154;-&#24037;&#21305;&#37197;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;&#65288;RL-GP&#65289;&#20197;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;RL-GP&#37319;&#29992;&#38598;&#21512;&#31181;&#32676;&#31574;&#30053;&#12290;&#22312;&#27599;&#19968;&#20195;&#31181;&#32676;&#36827;&#21270;&#20043;&#21069;&#65292;&#20195;&#29702;&#26681;&#25454;&#33719;&#24471;&#30340;&#20449;&#24687;&#20174;&#22235;&#31181;&#31181;&#32676;&#25628;&#32034;&#27169;&#24335;&#20013;&#36873;&#25321;&#19968;&#31181;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#29983;&#25104;&#30340;&#32452;&#24314;&#26041;&#26696;&#65292;&#21152;&#24555;&#31639;&#27861;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23545;&#27604;&#23454;&#39564;&#20197;&#39564;&#35777;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An efficient team is essential for the company to successfully complete new projects. To solve the team formation problem considering person-job matching (TFP-PJM), a 0-1 integer programming model is constructed, which considers both person-job matching and team members' willingness to communicate on team efficiency, with the person-job matching score calculated using intuitionistic fuzzy numbers. Then, a reinforcement learning-assisted genetic programming algorithm (RL-GP) is proposed to enhance the quality of solutions. The RL-GP adopts the ensemble population strategies. Before the population evolution at each generation, the agent selects one from four population search modes according to the information obtained, thus realizing a sound balance of exploration and exploitation. In addition, surrogate models are used in the algorithm to evaluate the formation plans generated by individuals, which speeds up the algorithm learning process. Afterward, a series of comparison experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>DREAM&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#30340;&#32570;&#22833;&#20803;&#32032;&#21644;&#29702;&#35299;&#25512;&#29702;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.03984</link><description>&lt;p&gt;
DREAM: &#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DREAM: Adaptive Reinforcement Learning based on Attention Mechanism for Temporal Knowledge Graph Reasoning. (arXiv:2304.03984v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03984
&lt;/p&gt;
&lt;p&gt;
DREAM&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#30340;&#32570;&#22833;&#20803;&#32032;&#21644;&#29702;&#35299;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#27169;&#22411;&#25551;&#32472;&#20102;&#20107;&#20214;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#30001;&#20110;TKG&#22266;&#26377;&#30340;&#19981;&#23436;&#22791;&#24615;&#65292;&#38656;&#35201;&#25512;&#29702;&#20986;&#32570;&#22833;&#30340;&#20803;&#32032;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#26410;&#26469;&#20107;&#20214;&#65292;&#20294;&#26159;&#32570;&#20047;&#26174;&#24335;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30001;&#20110;&#20256;&#32479;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22810;&#36339;&#25512;&#29702;&#22312;&#26368;&#36817;&#30340;&#36827;&#23637;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;TKG&#25512;&#29702;&#19978;&#25506;&#32034;RL&#25216;&#26415;&#30340;&#26426;&#20250;&#24050;&#32463;&#24320;&#21551;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;RL&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#32570;&#20047;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#28436;&#21270;&#21644;&#35821;&#20041;&#20381;&#36182;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#36807;&#24230;&#20381;&#36182;&#25163;&#21160;&#35774;&#35745;&#30340;&#22870;&#21169;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65288;DREAM&#65289;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#32570;&#22833;&#20803;&#32032;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graphs (TKGs) model the temporal evolution of events and have recently attracted increasing attention. Since TKGs are intrinsically incomplete, it is necessary to reason out missing elements. Although existing TKG reasoning methods have the ability to predict missing future events, they fail to generate explicit reasoning paths and lack explainability. As reinforcement learning (RL) for multi-hop reasoning on traditional knowledge graphs starts showing superior explainability and performance in recent advances, it has opened up opportunities for exploring RL techniques on TKG reasoning. However, the performance of RL-based TKG reasoning methods is limited due to: (1) lack of ability to capture temporal evolution and semantic dependence jointly; (2) excessive reliance on manually designed rewards. To overcome these challenges, we propose an adaptive reinforcement learning model based on attention mechanism (DREAM) to predict missing elements in the future. Specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EMP-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;&#27599;&#20010;&#22270;&#20687;&#23454;&#20363;&#30340;&#35009;&#21098;&#25968;&#37327;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#20195;&#25968;&#37327;&#65292;&#24182;&#22312;CIFAR-10&#12289;CIFAR-100&#12289;Tiny ImageNet&#21644;ImageNet-100&#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#35757;&#32451;&#26102;&#20195;&#32780;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03977</link><description>&lt;p&gt;
EMP-SSL&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#27425;&#35757;&#32451;&#26102;&#20195;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
EMP-SSL: Towards Self-Supervised Learning in One Training Epoch. (arXiv:2304.03977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EMP-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;&#27599;&#20010;&#22270;&#20687;&#23454;&#20363;&#30340;&#35009;&#21098;&#25968;&#37327;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#20195;&#25968;&#37327;&#65292;&#24182;&#22312;CIFAR-10&#12289;CIFAR-100&#12289;Tiny ImageNet&#21644;ImageNet-100&#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#35757;&#32451;&#26102;&#20195;&#32780;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#23454;&#39564;&#35777;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#30456;&#24403;&#8220;&#20302;&#25928;&#8221;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#20010;&#35757;&#32451;&#26102;&#20195;&#25165;&#33021;&#23436;&#20840;&#25910;&#25947;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20851;&#38190;&#26159;&#22686;&#21152;&#27599;&#20010;&#22270;&#20687;&#23454;&#20363;&#30340;&#35009;&#21098;&#25968;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#39046;&#20808;&#30340;SSL&#26041;&#27861;&#20043;&#19968;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Extreme-Multi-Patch&#65288;EMP&#65289;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#35768;&#22810;&#29992;&#20110;SSL&#30340;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#20363;&#22914;&#20998;&#25903;&#20043;&#38388;&#30340;&#37325;&#37327;&#20849;&#20139;&#12289;&#29305;&#24449;&#24402;&#19968;&#21270;&#12289;&#36755;&#20986;&#37327;&#21270;&#21644;&#20572;&#27490;&#26799;&#24230;&#31561;&#65292;&#24182;&#23558;&#35757;&#32451;&#26102;&#20195;&#32553;&#30701;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20165;&#19968;&#20010;&#26102;&#20195;&#20869;&#25910;&#25947;&#21040;CIFAR-10&#19978;&#30340;85.1&#65285;&#65292;CIFAR-100&#19978;&#30340;58.5&#65285;&#65292;Tiny ImageNet&#19978;&#30340;38.1&#65285;&#21644;ImageNet-100&#19978;&#30340;58.5&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;SSL&#26041;&#27861;&#21644;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EMP-SSL&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather "inefficient" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermor
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#27169;&#22411;&#22312;&#21463;&#21040;&#21508;&#31181;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#65292;&#36739;&#20302;&#20301;&#30340;&#37327;&#21270;&#23545;&#25239;&#25915;&#20987;&#26356;&#20855;&#24377;&#24615;&#65292;&#20294;&#26356;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03968</link><description>&lt;p&gt;
&#37327;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Robustness of Quantized Models. (arXiv:2304.03968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03968
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#27169;&#22411;&#22312;&#21463;&#21040;&#21508;&#31181;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#65292;&#36739;&#20302;&#20301;&#30340;&#37327;&#21270;&#23545;&#25239;&#25915;&#20987;&#26356;&#20855;&#24377;&#24615;&#65292;&#20294;&#26356;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#27169;&#22411;&#22312;&#21463;&#21040;&#21508;&#31181;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#12290;&#23613;&#31649;&#35780;&#20272;&#37327;&#21270;&#23545;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#65292;&#20294;&#26159;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#29616;&#26377;&#30740;&#31350;&#26377;&#38480;&#19988;&#24120;&#24120;&#24573;&#30053;&#20102;&#24050;&#32463;&#24314;&#31435;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#21407;&#21017;&#65292;&#23548;&#33268;&#20102;&#19981;&#23436;&#25972;&#21644;&#26080;&#27861;&#19979;&#32467;&#35770;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;ImageNet&#19978;&#20805;&#20998;&#35780;&#20272;&#20102;&#37327;&#21270;&#27169;&#22411;&#23545;&#21508;&#31181;&#22122;&#22768;(&#23545;&#25239;&#25915;&#20987;&#12289;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;)&#30340;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36739;&#20302;&#20301;&#30340;&#37327;&#21270;&#23545;&#25239;&#25915;&#20987;&#26356;&#20855;&#24377;&#24615;&#65292;&#20294;&#26356;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#33033;&#20914;&#22122;&#22768;(&#22312;&#33258;&#28982;&#25200;&#21160;&#20013;)&#21644;&#26368;&#36817;&#37051;&#25554;&#20540;(&#22312;&#31995;&#32479;&#22122;&#22768;&#20013;)&#23545;&#37327;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. Extensive experiments demonstrate that lower-bit quantization is more resilient to adversarial attacks but is more susceptible to natural corruptions and systematic noises. Notably, our investigation reveals that impulse noise (in natural corruptions) and the nearest neighbor interpolation (in systematic noises) have the most significan
&lt;/p&gt;</description></item><item><title>MphayaNER&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#33576;&#25991;&#36798;&#35821;&#19982;&#20854;&#20182;&#30456;&#20851;&#29677;&#22270;&#35821;&#20043;&#38388;&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#12290;&#29992;chiShona&#25968;&#25454;&#25193;&#20805;MphayaNER&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03952</link><description>&lt;p&gt;
MphayaNER&#65306;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MphayaNER: Named Entity Recognition for Tshivenda. (arXiv:2304.03952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03952
&lt;/p&gt;
&lt;p&gt;
MphayaNER&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#33576;&#25991;&#36798;&#35821;&#19982;&#20854;&#20182;&#30456;&#20851;&#29677;&#22270;&#35821;&#20043;&#38388;&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#12290;&#29992;chiShona&#25968;&#25454;&#25193;&#20805;MphayaNER&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#32780;&#35328;&#65292;NER&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;MphayaNER&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#26032;&#38395;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;NER&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MphayaNER&#19978;\&#24494;&#35843;\&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#24314;&#31435;NER&#22522;&#32447;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#33576;&#25991;&#36798;&#35821;&#19982;&#20854;&#20182;&#30456;&#20851;&#29677;&#22270;&#35821;&#20043;&#38388;&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#65292;&#20854;&#20013;chiShona&#21644;Kiswahili&#34920;&#29616;&#26368;&#20339;&#12290;&#21457;&#29616;&#29992;chiShona&#25968;&#25454;&#25193;&#20805;MphayaNER&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;MphayaNER&#21644;&#22522;&#32447;&#27169;&#22411;&#37117;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) plays a vital role in various Natural Language Processing tasks such as information retrieval, text classification, and question answering. However, NER can be challenging, especially in low-resource languages with limited annotated datasets and tools. This paper adds to the effort of addressing these challenges by introducing MphayaNER, the first Tshivenda NER corpus in the news domain. We establish NER baselines by \textit{fine-tuning} state-of-the-art models on MphayaNER. The study also explores zero-shot transfer between Tshivenda and other related Bantu languages, with chiShona and Kiswahili showing the best results. Augmenting MphayaNER with chiShona data was also found to improve model performance significantly. Both MphayaNER and the baseline models are made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#21160;&#21306;&#20998;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#25968;&#25454;&#20013;&#24674;&#22797;&#26410;&#30693;&#21442;&#25968;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#24314;&#31435;&#21644;&#35757;&#32451;&#21487;&#21306;&#20998;&#27169;&#22411;&#20197;&#20998;&#26512;&#38598;&#20307;&#28608;&#21457;&#12290;</title><link>http://arxiv.org/abs/2304.03949</link><description>&lt;p&gt;
&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing dynamical correlations using implicit neural representations. (arXiv:2304.03949v1 [cond-mat.str-el])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#21160;&#21306;&#20998;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#25968;&#25454;&#20013;&#24674;&#22797;&#26410;&#30693;&#21442;&#25968;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#24314;&#31435;&#21644;&#35757;&#32451;&#21487;&#21306;&#20998;&#27169;&#22411;&#20197;&#20998;&#26512;&#38598;&#20307;&#28608;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#36136;&#20013;&#35266;&#27979;&#21644;&#25551;&#36848;&#38598;&#20307;&#28608;&#21457;&#26159;&#29702;&#35299;&#22810;&#20307;&#31995;&#32479;&#29289;&#29702;&#23398;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#36890;&#24120;&#36890;&#36807;&#24377;&#24615;&#25955;&#23556;&#25110;X&#23556;&#32447;&#25955;&#23556;&#25216;&#26415;&#27979;&#37327;&#21160;&#24577;&#32467;&#26500;&#22240;&#23376;S(Q&#65292;&#969;)&#24182;&#23558;&#20854;&#19982;&#35745;&#31639;&#30340;&#21160;&#24577;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20998;&#26512;&#36825;&#20123;&#28608;&#21457;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#25104;&#27169;&#20223;&#27169;&#22411;&#21704;&#23494;&#39039;&#37327;&#30340;&#27169;&#25311;&#25968;&#25454;&#65292;&#24182;&#33258;&#21160;&#21306;&#20998;&#26469;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#24674;&#22797;&#26410;&#30693;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#33258;&#26059;&#27874;&#29702;&#35770;&#65288;LSWT&#65289;&#27169;&#25311;&#22120;&#21644;&#26469;&#33258;&#26041;&#38453;&#33258;&#26059;-1&#21453;&#38081;&#30913;&#20307;La2NiO4&#30340;&#20808;&#36827;&#24377;&#24615;&#20013;&#23376;&#25955;&#23556;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#20998;&#26512;&#25311;&#21512;&#65292;&#27169;&#22411;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#30340;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#21563;&#21512;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24314;&#31435;&#21644;&#35757;&#32451;&#21487;&#21306;&#20998;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The observation and description of collective excitations in solids is a fundamental issue when seeking to understand the physics of a many-body system. Analysis of these excitations is usually carried out by measuring the dynamical structure factor, S(Q, $\omega$), with inelastic neutron or x-ray scattering techniques and comparing this against a calculated dynamical model. Here, we develop an artificial intelligence framework which combines a neural network trained to mimic simulated data from a model Hamiltonian with automatic differentiation to recover unknown parameters from experimental data. We benchmark this approach on a Linear Spin Wave Theory (LSWT) simulator and advanced inelastic neutron scattering data from the square-lattice spin-1 antiferromagnet La$_2$NiO$_4$. We find that the model predicts the unknown parameters with excellent agreement relative to analytical fitting. In doing so, we illustrate the ability to build and train a differentiable model only once, which th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21387;&#32553;&#22768;&#23398;&#19978;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.03940</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27744;&#21270;&#30340;&#30690;&#37327;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Speech Representation Pooling Using Vector Quantization. (arXiv:2304.03940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21387;&#32553;&#22768;&#23398;&#19978;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#65292;&#23558;&#19968;&#20010;&#27169;&#22411;&#24212;&#29992;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#31181;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#27744;&#21270;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65307;&#35821;&#38899;&#34920;&#31034;&#30340;&#38271;&#24230;&#22266;&#26377;&#22320;&#26159;&#21487;&#21464;&#30340;&#12290;&#23613;&#31649;&#24573;&#30053;&#20102;&#35821;&#38899;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#19981;&#21516;&#38271;&#24230;&#30340;&#38899;&#32032;&#65292;&#20294;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#24179;&#22343;&#27744;&#21270;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#26469;&#21387;&#32553;&#22768;&#23398;&#19978;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#19982;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26080;&#30417;&#30563;&#27744;&#21270;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#25955;&#33853;&#22312;&#35821;&#38899;&#21644;&#25991;&#26412;&#39046;&#22495;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65306;&#20851;&#38190;&#23383;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#24847;&#22270;&#20998;&#31867;&#21644;&#24773;&#24863;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#23558;&#20854;&#19982;&#26377;&#30417;&#30563;&#30340;&#27744;&#21270;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#30340;&#28508;&#21147;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#29702;&#35299;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03938</link><description>&lt;p&gt;
&#23398;&#29983;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#21019;&#24314;&#30340;&#20195;&#30721;&#35299;&#37322;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Code Explanations Created by Students and Large Language Models. (arXiv:2304.03938v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#30340;&#28508;&#21147;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#29702;&#35299;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20195;&#30721;&#24182;&#35299;&#37322;&#20854;&#29992;&#36884;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#39046;&#22495;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#23398;&#29983;&#35299;&#37322;&#20195;&#30721;&#33021;&#21147;&#19982;&#32534;&#20889;&#21644;&#36861;&#36394;&#20195;&#30721;&#31561;&#20854;&#20182;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#39640;&#25277;&#35937;&#32423;&#21035;&#25551;&#36848;&#20195;&#30721;&#22312;&#25152;&#26377;&#21487;&#33021;&#36755;&#20837;&#19979;&#30340;&#34892;&#20026;&#30340;&#33021;&#21147;&#24378;&#28872;&#20851;&#32852;&#30528;&#20195;&#30721;&#32534;&#20889;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#23398;&#29983;&#26469;&#35828;&#65292;&#24320;&#21457;&#29702;&#35299;&#21644;&#20934;&#30830;&#31616;&#27905;&#22320;&#35299;&#37322;&#20195;&#30721;&#30340;&#19987;&#19994;&#30693;&#35782;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25945;&#23398;&#26041;&#27861;&#24182;&#26410;&#23454;&#29616;&#29983;&#20135;&#21363;&#26102;&#33539;&#20363;&#20195;&#30721;&#35299;&#37322;&#20197;&#36827;&#34892;&#25351;&#23548;&#30340;&#22823;&#35268;&#27169;&#35838;&#22530;&#30340;&#27493;&#39588;&#12290;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20986;&#29616;&#36817;&#26399;&#21487;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102; LLMs &#29983;&#25104;&#21487;&#20197;&#20316;&#20026;&#31034;&#20363;&#26469;&#25903;&#25345;&#23398;&#29983;&#29702;&#35299;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To e
&lt;/p&gt;</description></item><item><title>3D GAN&#26159;&#29983;&#25104;&#19977;&#32500;&#37325;&#24314;&#12289;&#28857;&#20113;&#37325;&#24314;&#21644;3D&#35821;&#20041;&#22330;&#26223;&#23436;&#25104;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#36873;&#25321;&#22122;&#22768;&#20998;&#24067;&#23545;&#24212;&#30528;&#28508;&#31354;&#38388;&#65292;&#29702;&#35299;&#20854;&#32467;&#26500;&#26377;&#21161;&#20110;&#24494;&#35843;&#29983;&#25104;&#26679;&#26412;&#12290;&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;3D GAN&#21450;&#20854;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.03932</link><description>&lt;p&gt;
3D GAN&#19982;&#28508;&#31354;&#38388;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
3D GANs and Latent Space: A comprehensive survey. (arXiv:2304.03932v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03932
&lt;/p&gt;
&lt;p&gt;
3D GAN&#26159;&#29983;&#25104;&#19977;&#32500;&#37325;&#24314;&#12289;&#28857;&#20113;&#37325;&#24314;&#21644;3D&#35821;&#20041;&#22330;&#26223;&#23436;&#25104;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#36873;&#25321;&#22122;&#22768;&#20998;&#24067;&#23545;&#24212;&#30528;&#28508;&#31354;&#38388;&#65292;&#29702;&#35299;&#20854;&#32467;&#26500;&#26377;&#21161;&#20110;&#24494;&#35843;&#29983;&#25104;&#26679;&#26412;&#12290;&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;3D GAN&#21450;&#20854;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#23558;&#20302;&#32500;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#22312;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#32593;&#32476;&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#19977;&#32500;&#29289;&#20307;&#12290;&#22312;&#28216;&#25103;&#25110;&#27169;&#25311;&#31561;3D&#22270;&#24418;&#29615;&#22659;&#30340;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#39640;&#25928;&#24314;&#27169;3D&#23545;&#35937;&#21644;&#20154;&#33080;&#33267;&#20851;&#37325;&#35201;&#12290;3D GAN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#37325;&#24314;&#12289;&#28857;&#20113;&#37325;&#24314;&#21644;3D&#35821;&#20041;&#22330;&#26223;&#23436;&#25104;&#12290;&#22122;&#22768;&#20998;&#24067;&#30340;&#36873;&#25321;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#34920;&#31034;&#20102;&#28508;&#31354;&#38388;&#12290;&#20026;&#20102;&#24494;&#35843;&#29983;&#25104;&#30340;&#26679;&#26412;&#65292;&#29702;&#35299;GAN&#30340;&#28508;&#31354;&#38388;&#26159;&#24517;&#35201;&#30340;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23545;&#22270;&#20687;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#36827;&#34892;&#24418;&#24577;&#21464;&#25442;&#26469;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28508;&#31354;&#38388;&#21644;3D GAN&#65292;&#30740;&#31350;&#20102;&#20960;&#31181;GAN&#21464;&#20307;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#27934;&#23519;&#25552;&#39640;3D GAN&#35757;&#32451;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have emerged as a significant player in generative modeling by mapping lower-dimensional random noise to higher-dimensional spaces. These networks have been used to generate high-resolution images and 3D objects. The efficient modeling of 3D objects and human faces is crucial in the development process of 3D graphical environments such as games or simulations. 3D GANs are a new type of generative model used for 3D reconstruction, point cloud reconstruction, and 3D semantic scene completion. The choice of distribution for noise is critical as it represents the latent space. Understanding a GAN's latent space is essential for fine-tuning the generated samples, as demonstrated by the morphing of semantically meaningful parts of images. In this work, we explore the latent space and 3D GANs, examine several GAN variants and training methods to gain insights into improving 3D GAN training, and suggest potential future directions for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03916</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#26102;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#23475;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#25110;&#23548;&#33268;&#27169;&#22411;&#22522;&#20110;&#38169;&#35823;&#21407;&#22240;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#26159;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#30340;&#20027;&#35201;&#40065;&#26834;&#24615;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#26399;&#38388;&#32531;&#35299;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#20154;&#26469;&#35828;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#26399;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#38024;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#27169;&#24577;&#26469;&#26816;&#27979;&#24182;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#36890;&#36807;&#34920;&#36798;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#21644;&#28145;&#20837;&#30340;&#21487;&#35270;&#21270;&#26174;&#31034;&#65292;&#36825;&#31181;&#20171;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#65292;&#32780;&#19981;&#23384;&#22312;&#38169;&#35823;&#23646;&#24615;&#65292;&#24182;&#23558;&#27169;&#22411;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the 
&lt;/p&gt;</description></item><item><title>InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03906</link><description>&lt;p&gt;
InstructBio&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems. (arXiv:2304.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03906
&lt;/p&gt;
&lt;p&gt;
InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#22987;&#32456;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#24378;&#21147;&#30340;&#20219;&#21153;&#26080;&#20851;&#27169;&#22411;&#65292;&#20294;&#22312;&#21521;&#19979;&#28216;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructBio&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#20363;&#12290;&#23427;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#26469;&#25552;&#20379;&#20266;&#26631;&#31614;&#21487;&#38752;&#24615;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#12290;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#28982;&#21518;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#30340;&#20851;&#27880;&#65292;&#36991;&#20813;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InstructBio&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#65292;&#22312;&#27963;&#24615;&#24748;&#23830;&#20272;&#35745;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence for science, it is consistently an essential challenge to face a limited amount of labeled data for real-world problems. The prevailing approach is to pretrain a powerful task-agnostic model on a large unlabeled corpus but may struggle to transfer knowledge to downstream tasks. In this study, we propose InstructMol, a semi-supervised learning algorithm, to take better advantage of unlabeled examples. It introduces an instructor model to provide the confidence ratios as the measurement of pseudo-labels' reliability. These confidence scores then guide the target model to pay distinct attention to different data points, avoiding the over-reliance on labeled data and the negative influence of incorrect pseudo-annotations. Comprehensive experiments show that InstructBio substantially improves the generalization ability of molecular models, in not only molecular property predictions but also activity cliff estimations, demonstrating the superiority of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#19977;&#32500;&#30528;&#35013;&#20154;&#29289;&#37325;&#24314;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#19977;&#32500;&#20154;&#29289;&#12290;</title><link>http://arxiv.org/abs/2304.03903</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20687;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30528;&#35013;&#20154;&#29289;&#30340;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Clothed Avatar Reconstruction from a Single Image. (arXiv:2304.03903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#19977;&#32500;&#30528;&#35013;&#20154;&#29289;&#37325;&#24314;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#19977;&#32500;&#20154;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19977;&#32500;&#30528;&#35013;&#20154;&#29289;&#37325;&#24314;&#26694;&#26550;&#12290;&#23558;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#30340;&#39640;&#31934;&#24230;&#19982;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#39640;&#25928;&#29575;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#20687;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30528;&#35013;&#20154;&#29289;&#37325;&#24314;&#30340;&#31895;&#21040;&#31934;&#30340;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38544;&#24335;&#27169;&#22411;&#20197;&#23398;&#20064;&#30340;&#26041;&#24335;&#22312;&#20154;&#29289;&#30340;&#35268;&#33539;&#31354;&#38388;&#20013;&#23398;&#20064;&#19968;&#33324;&#24418;&#29366;&#65292;&#31532;&#20108;&#38454;&#27573;&#21017;&#36890;&#36807;&#22312;&#21464;&#24418;&#31354;&#38388;&#20013;&#20272;&#35745;&#38750;&#21018;&#24615;&#21464;&#24418;&#26469;&#32454;&#21270;&#34920;&#38754;&#32454;&#33410;&#12290;&#19968;&#20010;&#36229;&#32423;&#32593;&#32476;&#34987;&#29992;&#26469;&#29983;&#25104;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#21152;&#24555;&#20248;&#21270;&#36807;&#31243;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#20219;&#24847;&#30528;&#35013;&#20154;&#29289;&#20013;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#19977;&#32500;&#20154;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence o f the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03898</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#22312;&#24191;&#21578;&#25628;&#32034;&#21644;&#25512;&#33616;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#38271;&#24230;&#30701;&#65292;&#35821;&#20041;&#20449;&#24687;&#21294;&#20047;&#21644;&#21333;&#35789;&#27495;&#20041;&#38382;&#39064;&#25104;&#20026;&#27492;&#31867;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#25991;&#26412;&#34917;&#20805;&#21477;&#23376;&#25110;&#30693;&#35782;&#24211;&#26469;&#25552;&#20379;&#38468;&#21152;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#22320;&#20132;&#20114;&#21407;&#22987;&#21477;&#23376;&#21644;&#34917;&#20805;&#21477;&#23376;&#65292;&#20063;&#27809;&#26377;&#32771;&#34385;&#21040;&#22806;&#37096;&#30693;&#35782;&#24211;&#24341;&#20837;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#23545;&#24212;&#30340;&#34917;&#20805;&#21477;&#23376;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#33719;&#24471;&#26356;&#20855;&#35821;&#20041;&#21305;&#37197;&#24615;&#30340;&#21407;&#22987;&#21477;&#23376;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#22122;&#22768;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#38190;&#35789;&#20316;&#20026;&#21407;&#22987;&#21477;&#23376;&#30340;&#20027;&#35201;&#35821;&#20041;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, short Text Matching tasks have been widely applied in the fields ofadvertising search and recommendation. The difficulty lies in the lack of semantic information and word ambiguity caused by the short length of the text. Previous works have introduced complement sentences or knowledge bases to provide additional feature information. However, these methods have not fully interacted between the original sentence and the complement sentence, and have not considered the noise issue that may arise from the introduction of external knowledge bases. Therefore, this paper proposes a short Text Matching model that combines contrastive learning and external knowledge. The model uses a generative model to generate corresponding complement sentences and uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence. In addition, to avoid noise, we use keywords as the main semantics of the original sentence to retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22478;&#24066;&#35268;&#21010;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#21270;&#29992;&#22320;&#37197;&#32622;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#23545;&#35805;&#24335; AI &#21644;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#26426;&#22120;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;AI &#21487;&#20197;&#20026;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#24102;&#26469;&#19981;&#23569;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.03892</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#22478;&#24066;&#35268;&#21010;&#65306;&#29983;&#25104;&#24335;&#21644;&#32842;&#22825;&#24335; AI &#30456;&#32467;&#21512;&#30340;&#22478;&#24066;&#35268;&#21010;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning. (arXiv:2304.03892v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22478;&#24066;&#35268;&#21010;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#21270;&#29992;&#22320;&#37197;&#32622;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#23545;&#35805;&#24335; AI &#21644;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#26426;&#22120;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;AI &#21487;&#20197;&#20026;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#24102;&#26469;&#19981;&#23569;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26366;&#32463;&#26159;&#29420;&#31435;&#21457;&#23637;&#30340;&#65292;&#20294;&#29616;&#22312;&#20004;&#20010;&#39046;&#22495;&#24320;&#22987;&#20132;&#21449;&#27719;&#21512;&#65292;&#20114;&#30456;&#20511;&#37492;&#21644;&#21463;&#30410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22478;&#24066;&#35268;&#21010;&#20174;&#21487;&#25345;&#32493;&#24615;&#12289;&#29983;&#27963;&#12289;&#32463;&#27982;&#12289;&#28798;&#23475;&#21644;&#29615;&#22659;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#22238;&#39038;&#20102;&#22478;&#24066;&#35268;&#21010;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#24320;&#25918;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#21253;&#25324;&#23545;&#25239;&#23398;&#20064;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#23545;&#35805;&#24335; AI &#20197;&#21450;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#26426;&#22120;&#23398;&#20064;&#31561;&#65292;&#35780;&#20272;&#20102; AI &#22914;&#20309;&#20026;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#33258;&#21160;&#21270;&#29992;&#22320;&#37197;&#32622;&#65292;&#21363;&#20174;&#21608;&#22260;&#30340;&#22320;&#29702;&#31354;&#38388;&#12289;&#20154;&#31867;&#31227;&#21160;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#27963;&#21160;&#20013;&#20026;&#30446;&#26631;&#21306;&#22495;&#29983;&#25104;&#22303;&#22320;&#29992;&#36884;&#21644;&#24314;&#31569;&#37197;&#32622;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#21246;&#30011;&#20102;&#38598;&#25104; AI &#21644;&#22478;&#24066;&#35268;&#21010;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The two fields of urban planning and artificial intelligence (AI) arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we introduce the importance of urban planning from the sustainability, living, economic, disaster, and environmental perspectives. We review the fundamental concepts of urban planning and relate these concepts to crucial open problems of machine learning, including adversarial learning, generative neural networks, deep encoder-decoder networks, conversational AI, and geospatial and temporal machine learning, thereby assaying how AI can contribute to modern urban planning. Thus, a central problem is automated land-use configuration, which is formulated as the generation of land uses and building configuration for a target area from surrounding geospatial, human mobility, social media, environment, and economic activities. Finally, we delineate some 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#65292;&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#65288;COMs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;Langevin MCMC&#37319;&#26679;&#22120;&#26367;&#25442;&#26799;&#24230;&#19978;&#21319;&#37319;&#26679;&#22120;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.03866</link><description>&lt;p&gt;
&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conservative objective models are a special kind of contrastive divergence-based energy model. (arXiv:2304.03866v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#65292;&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#65288;COMs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;Langevin MCMC&#37319;&#26679;&#22120;&#26367;&#25442;&#26799;&#24230;&#19978;&#21319;&#37319;&#26679;&#22120;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#65288;COMs&#65289;&#29992;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#65288;MBO&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#33021;&#37327;&#20989;&#25968;&#26082;&#34920;&#31034;&#36755;&#20837;&#30340;&#26080;&#26465;&#20214;&#27010;&#29575;&#65292;&#20063;&#34920;&#31034;&#22870;&#21169;&#21464;&#37327;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#20844;&#24335;&#21482;&#20174;&#20854;&#23398;&#20064;&#20998;&#24067;&#20013;&#25277;&#26679;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#22797;&#26041;&#27861;&#65292;&#29992;Langevin MCMC&#37319;&#26679;&#22120;&#26367;&#25442;&#26799;&#24230;&#19978;&#21319;&#37319;&#26679;&#22120;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#37319;&#26679;&#36755;&#20837;&#30340;&#27010;&#29575;&#19982;&#20854;&#39044;&#27979;&#30340;&#22870;&#21169;&#25104;&#27604;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#27169;&#22411;&#20998;&#35299;&#65292;&#20351;&#26080;&#26465;&#20214;&#27010;&#29575;&#21644;&#26465;&#20214;&#27010;&#29575;&#20998;&#21035;&#24314;&#27169;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we theoretically show that conservative objective models (COMs) for offline model-based optimisation (MBO) are a special kind of contrastive divergence-based energy model, one where the energy function represents both the unconditional probability of the input and the conditional probability of the reward variable. While the initial formulation only samples modes from its learned distribution, we propose a simple fix that replaces its gradient ascent sampler with a Langevin MCMC sampler. This gives rise to a special probabilistic model where the probability of sampling an input is proportional to its predicted reward. Lastly, we show that better samples can be obtained if the model is decoupled so that the unconditional and conditional probabilities are modelled separately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03838</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#21040;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#26080;&#20851;&#28151;&#28102;&#22240;&#32032;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30452;&#25509;&#35757;&#32451;&#20110;&#20154;&#33080;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#25935;&#24863;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20154;&#30340;&#36523;&#20221;&#12290;&#35768;&#22810;&#19982;&#20154;&#33080;&#30456;&#20851;&#30340;&#20219;&#21153;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#26159;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#65292;&#24182;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#34920;&#29616;&#19968;&#33268;&#65288;&#21363;&#20844;&#24179;&#65289;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#22343;&#21248;&#24615;&#26159;&#24230;&#37327;&#21644;&#23454;&#26045;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#30340;&#25104;&#26412;&#65292;&#36825;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#65292;&#22823;&#22810;&#25968;&#20154;&#33080;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20219;&#21153;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26080;&#38656;&#27492;&#31867;&#27880;&#37322;&#21363;&#21487;&#25552;&#39640;&#36523;&#20221;&#30456;&#20851;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ChiroDiff"&#30340;&#27169;&#22411;&#31867;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;Denoising Diffusion Probabilistic Models&#65288;DDPMs&#65289;&#35299;&#20915;&#20102;&#25163;&#20889;&#25968;&#25454;&#24314;&#27169;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#23398;&#20064;&#25429;&#25417;&#25972;&#20307;&#27010;&#24565;&#65292;&#22312;&#26356;&#39640;&#30340;&#26102;&#38388;&#37319;&#26679;&#29575;&#19979;&#20445;&#25345;&#24377;&#24615;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#19979;&#28216;&#23454;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2304.03785</link><description>&lt;p&gt;
ChiroDiff: &#22522;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#25163;&#20889;&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ChiroDiff: Modelling chirographic data with Diffusion Models. (arXiv:2304.03785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ChiroDiff"&#30340;&#27169;&#22411;&#31867;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;Denoising Diffusion Probabilistic Models&#65288;DDPMs&#65289;&#35299;&#20915;&#20102;&#25163;&#20889;&#25968;&#25454;&#24314;&#27169;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#23398;&#20064;&#25429;&#25417;&#25972;&#20307;&#27010;&#24565;&#65292;&#22312;&#26356;&#39640;&#30340;&#26102;&#38388;&#37319;&#26679;&#29575;&#19979;&#20445;&#25345;&#24377;&#24615;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#19979;&#28216;&#23454;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#22238;&#24402;&#20998;&#24067;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#20110;&#36830;&#32493;&#26102;&#38388;&#20960;&#20309;&#32467;&#26500;&#65288;&#22914;&#25163;&#20889;&#12289;&#33609;&#22270;&#12289;&#22270;&#30011;&#31561;&#65289;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20005;&#26684;&#26377;&#24207;&#30340;&#31163;&#25955;&#20998;&#35299;&#26080;&#27861;&#25429;&#25417;&#25163;&#20889;&#25968;&#25454;&#30340;&#20851;&#38190;&#23646;&#24615;&#8212;&#8212;&#30001;&#20110;&#21333;&#21521;&#21487;&#35265;&#24615;&#65288;&#22240;&#26524;&#24615;&#65289;&#65292;&#23427;&#26080;&#27861;&#24314;&#31435;&#25972;&#20307;&#30340;&#26102;&#38388;&#27010;&#24565;&#29702;&#35299;&#65292;&#22240;&#27492;&#22312;&#24314;&#27169;&#26102;&#23558;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20026;&#22266;&#23450;&#37319;&#26679;&#29575;&#30340;&#31163;&#25955;&#26631;&#35760;&#24207;&#21015;&#65292;&#32780;&#26410;&#33021;&#25429;&#33719;&#30495;&#27491;&#30340;&#22522;&#30784;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21435;&#22122;&#24357;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;DDPMs&#65289;&#30340;&#24378;&#22823;&#27169;&#22411;&#31867;&#65292;&#19987;&#38376;&#20026;&#25163;&#20889;&#25968;&#25454;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21517;&#20026;&#8220;ChiroDiff&#8221;&#65292;&#26159;&#38750;&#33258;&#22238;&#24402;&#30340;&#65292;&#23398;&#20064;&#25429;&#25417;&#25972;&#20307;&#27010;&#24565;&#65292;&#22240;&#27492;&#22312;&#26356;&#39640;&#30340;&#26102;&#38388;&#37319;&#26679;&#29575;&#19979;&#20445;&#25345;&#24377;&#24615;&#33267;&#23569;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#19979;&#28216;&#23454;&#29992;&#31243;&#24207;&#65288;&#22914;&#26465;&#20214;&#37319;&#26679;&#12289;&#21019;&#24847;&#28151;&#21512;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modelling over continuous-time geometric constructs, a.k.a such as handwriting, sketches, drawings etc., have been accomplished through autoregressive distributions. Such strictly-ordered discrete factorization however falls short of capturing key properties of chirographic data -- it fails to build holistic understanding of the temporal concept due to one-way visibility (causality). Consequently, temporal data has been modelled as discrete token sequences of fixed sampling rate instead of capturing the true underlying concept. In this paper, we introduce a powerful model-class namely "Denoising Diffusion Probabilistic Models" or DDPMs for chirographic data that specifically addresses these flaws. Our model named "ChiroDiff", being non-autoregressive, learns to capture holistic concepts and therefore remains resilient to higher temporal sampling rate up to a good extent. Moreover, we show that many important downstream utilities (e.g. conditional sampling, creative mixing) c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03784</link><description>&lt;p&gt;
&#29983;&#25104;AI&#29992;&#20110;&#23398;&#20064;&#65306;&#30740;&#31350;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI for learning: Investigating the potential of synthetic learning videos. (arXiv:2304.03784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24341;&#36215;&#20102;&#20840;&#29699;&#30340;&#20851;&#27880;&#12290;&#20687;Dalle-2&#21644;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#34920;&#26126;&#65292;&#20197;&#21069;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;AI&#33021;&#21147;&#30340;&#20219;&#21153;&#29616;&#22312;&#21487;&#20197;&#20197;&#21508;&#31181;&#26032;&#26041;&#24335;&#22686;&#21152;&#21019;&#24847;&#23186;&#20307;&#30340;&#29983;&#20135;&#21147;&#65292;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#35270;&#39057;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#22312;&#32447;&#25945;&#32946;&#29615;&#22659;&#19979;&#21487;&#34892;&#30340;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#23186;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25945;&#32946;&#20215;&#20540;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#38543;&#26426;&#23558;&#25104;&#24180;&#23398;&#20064;&#32773;&#65288;n = 83&#65289;&#20998;&#37197;&#21040;&#20004;&#31181;&#24494;&#22411;&#23398;&#20064;&#26465;&#20214;&#20043;&#19968;&#65292;&#25910;&#38598;&#21069;&#21518;&#23398;&#20064;&#35780;&#20272;&#65292;&#24182;&#35843;&#26597;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25511;&#21046;&#32452;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative artificial intelligence (AI) have captured worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks previously thought to be beyond the capabilities of AI may now augment the productivity of creative media in various new ways, including through the generation of synthetic video. This research paper explores the utility of using AI-generated synthetic video to create viable educational content for online educational settings. To date, there is limited research investigating the real-world educational value of AI-generated synthetic media. To address this gap, we examined the impact of using AI-generated synthetic video in an online learning platform on both learners content acquisition and learning experience. We took a mixed-method approach, randomly assigning adult learners (n=83) into one of two micro-learning conditions, collecting pre- and post-learning assessments, and surveying participants on their learning experience. The control c
&lt;/p&gt;</description></item><item><title>AutoQNN&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#37327;&#21270;DNN&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#36866;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#28151;&#21512;&#31934;&#24230;&#31574;&#30053;&#30340;&#25628;&#32034;&#65292;&#24182;&#21487;&#26377;&#25928;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03782</link><description>&lt;p&gt;
AutoQNN: &#19968;&#31181;&#33258;&#21160;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks. (arXiv:2304.03782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03782
&lt;/p&gt;
&lt;p&gt;
AutoQNN&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#37327;&#21270;DNN&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#36866;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#28151;&#21512;&#31934;&#24230;&#31574;&#30053;&#30340;&#25628;&#32034;&#65292;&#24182;&#21487;&#26377;&#25928;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#36866;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21387;&#32553;&#30340;&#37327;&#21270;&#26041;&#26696;&#19982;&#28151;&#21512;&#31934;&#24230;&#31574;&#30053;&#26159;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21387;&#32553;&#26041;&#27861;&#30340;&#24040;&#22823;&#25628;&#32034;&#31354;&#38388;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#36807;&#31243;&#38590;&#20197;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoQNN&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#37327;&#21270;&#19981;&#21516;&#23618;&#27425;&#30340;DNN&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the expected quantizing scheme with suitable mixed-precision policy is the key point to compress deep neural networks (DNNs) in high efficiency and accuracy. This exploration implies heavy workloads for domain experts, and an automatic compression method is needed. However, the huge search space of the automatic method introduces plenty of computing budgets that make the automatic process challenging to be applied in real scenarios. In this paper, we propose an end-to-end framework named AutoQNN, for automatically quantizing different layers utilizing different schemes and bitwidths without any human labor. AutoQNN can seek desirable quantizing schemes and mixed-precision policies for mainstream DNN models efficiently by involving three techniques: quantizing scheme search (QSS), quantizing precision learning (QPL), and quantized architecture generation (QAG). QSS introduces five quantizing schemes and defines three new schemes as a candidate set for scheme search, and then u
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#39044;&#27979;&#27169;&#22411;&#30340;&#39564;&#35777;&#38382;&#39064;&#65292;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#20154;&#32676;&#30340;&#26032;&#25968;&#25454;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#30830;&#20445;&#39564;&#35777;&#24615;&#33021;&#23545;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#35748;&#30495;&#30740;&#31350;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#29615;&#22659;&#20013;&#30340;&#25299;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#65292;&#20197;&#20415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#38752;&#12289;&#20844;&#24179;&#12289;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03779</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#20844;&#24179;&#21644;&#21487;&#20449;&#39044;&#27979;&#27169;&#22411;&#39564;&#35777;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A roadmap to fair and trustworthy prediction model validation in healthcare. (arXiv:2304.03779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03779
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#39044;&#27979;&#27169;&#22411;&#30340;&#39564;&#35777;&#38382;&#39064;&#65292;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#20154;&#32676;&#30340;&#26032;&#25968;&#25454;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#30830;&#20445;&#39564;&#35777;&#24615;&#33021;&#23545;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#35748;&#30495;&#30740;&#31350;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#29615;&#22659;&#20013;&#30340;&#25299;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#65292;&#20197;&#20415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#38752;&#12289;&#20844;&#24179;&#12289;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#24320;&#21457;&#25968;&#25454;&#20197;&#22806;&#30340;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#37027;&#20040;&#23427;&#23601;&#26159;&#26368;&#26377;&#29992;&#30340;&#12290;&#20294;&#26159;&#65292;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23427;&#21487;&#20197;&#25512;&#24191;&#20173;&#19981;&#28165;&#26970;&#12290;&#23454;&#38469;&#19978;&#65292;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#21307;&#30103;&#31995;&#32479;&#25110;&#22269;&#23478;&#30340;&#20154;&#21475;&#31561;&#38750;&#24120;&#19981;&#21516;&#30340;&#25968;&#25454;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#39044;&#27979;&#32467;&#26524;&#36890;&#24120;&#24456;&#24046;&#12290;&#36825;&#21487;&#33021;&#19981;&#26159;&#23545;&#29305;&#23450;&#30446;&#26631;&#20154;&#32676;&#25110;&#29615;&#22659;&#35774;&#35745;&#30340;&#27169;&#22411;&#34920;&#29616;&#30340;&#20844;&#27491;&#21453;&#26144;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25289;&#20280;&#39044;&#26399;&#30340;&#27169;&#22411;&#25512;&#24191;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#20154;&#32676;&#30340;&#26032;&#25968;&#25454;&#26469;&#22806;&#37096;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#39564;&#35777;&#24615;&#33021;&#23545;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#28165;&#26224;&#24433;&#21709;&#65292;&#32780;&#27169;&#22411;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#29615;&#22659;&#24212;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#36827;&#34892;&#20180;&#32454;&#35843;&#26597;&#65292;&#32780;&#19981;&#26159;&#20107;&#21518;&#25506;&#35752;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#36335;&#32447;&#22270;&#65292;&#20197;&#20415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#38752;&#12289;&#20844;&#24179;&#12289;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20221;&#36335;&#32447;&#22270;&#24212;&#35813;&#28165;&#26224;&#23450;&#20041;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#32771;&#34385;&#27169;&#22411;&#20351;&#29992;&#30340;&#20262;&#29702;&#24433;&#21709;&#65292;&#36827;&#34892;&#20122;&#32452;&#20998;&#26512;&#65292;&#24182;&#37319;&#29992;&#20005;&#26684;&#30340;&#39564;&#35777;&#21327;&#35758;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prediction model is most useful if it generalizes beyond the development data with external validations, but to what extent should it generalize remains unclear. In practice, prediction models are externally validated using data from very different settings, including populations from other health systems or countries, with predictably poor results. This may not be a fair reflection of the performance of the model which was designed for a specific target population or setting, and may be stretching the expected model generalizability. To address this, we suggest to externally validate a model using new data from the target population to ensure clear implications of validation performance on model reliability, whereas model generalizability to broader settings should be carefully investigated during model development instead of explored post-hoc. Based on this perspective, we propose a roadmap that facilitates the development and application of reliable, fair, and trustworthy artifici
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03773</link><description>&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Explicable Robot Planning. (arXiv:2304.03773v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03773
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#30340;&#26399;&#26395;&#28304;&#33258;&#20110;&#20182;&#20204;&#23545;&#20854;&#20182;&#20154;&#21644;&#19990;&#30028;&#30340;&#20102;&#35299;&#12290;&#22312;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26426;&#22120;&#20154;&#30340;&#20102;&#35299;&#21487;&#33021;&#19982;&#29616;&#23454;&#19981;&#31526;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#19981;&#33021;&#28385;&#36275;&#20154;&#20204;&#30340;&#26399;&#26395;&#12290;&#21487;&#35299;&#37322;&#35268;&#21010;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#21327;&#35843;&#20154;&#31867;&#26399;&#26395;&#21644;&#26368;&#20248;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#36827;&#34892;&#26356;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#20915;&#31574;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22312;&#21487;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#21487;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#21487;&#35299;&#37322;&#35268;&#21010;&#65288;SEP&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#12290; SEP&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#31574;&#30053;&#65292;&#29983;&#25104;&#25509;&#36817;&#20110;&#20154;&#31867;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;&#36825;&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;SEP&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#19981;&#29306;&#29298;&#20219;&#20309;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20135;&#29983;&#20102;&#23433;&#20840;&#24615;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human expectations stem from their knowledge of the others and the world. Where human-robot interaction is concerned, such knowledge about the robot may be inconsistent with the ground truth, resulting in the robot not meeting its expectations. Explicable planning was previously introduced as a novel planning approach to reconciling human expectations and the optimal robot behavior for more interpretable robot decision-making. One critical issue that remains unaddressed is safety during explicable decision-making which can lead to explicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP), which extends explicable planning to support the specification of a safety bound. The objective of SEP is to find a policy that generates a behavior close to human expectations while satisfying the safety constraints introduced by the bound, which is a special case of multi-objective optimization where the solution to SEP lies on the Pareto frontier. Under such a formulation, we 
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03468</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03468
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#38656;&#35201;&#20174;&#21508;&#31181;&#26469;&#28304;&#25552;&#21462;&#30340;&#24322;&#26500;KG&#20043;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#36817;&#26469;&#65292;&#30001;&#20110;GNN&#30340;&#20986;&#33394;&#32467;&#26500;&#20449;&#24687;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312;EA&#20219;&#21153;&#20013;&#24191;&#27867;&#37319;&#29992;GNN&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#24120;&#35265;EA&#25968;&#25454;&#38598;&#30340;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#35774;&#32622;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#65292;&#36825;&#22952;&#30861;&#20102;&#23545;&#26368;&#36817;&#26041;&#27861;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#31181;&#29616;&#35937;&#20351;&#25105;&#20204;&#28145;&#24605;&#65306;&#29616;&#26377;&#22522;&#20110;GNN&#30340;EA&#26041;&#27861;&#26159;&#21542;&#30495;&#30340;&#21462;&#24471;&#20102;&#20255;&#22823;&#36827;&#23637;&#65311;&#20026;&#20102;&#30740;&#31350;EA&#26041;&#27861;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#39640;&#24230;&#24322;&#26500;&#30340;KG&#65288;HHKG&#65289;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;KG&#21644;&#36890;&#29992;KG&#65289;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;KG&#22312;&#35268;&#27169;&#21644;&#32467;&#26500;&#19978;&#19981;&#21516;&#65292;&#24182;&#20849;&#20139;&#26356;&#23569;&#30340;&#37325;&#21472;&#23454;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28165;&#29702;&#20102;&#19981;&#21512;&#29702;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HHKG&#25968;&#25454;&#38598;&#65292;&#20854;&#23494;&#20999;&#22320;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
&lt;/p&gt;</description></item><item><title>CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.03401</link><description>&lt;p&gt;
CAPOT: &#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#21019;&#24314;&#24378;&#20581;&#30340;&#23494;&#38598;&#26597;&#35810;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03401
&lt;/p&gt;
&lt;p&gt;
CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#25104;&#21151;&#21644;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#36827;&#27493;&#20351;&#24471;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#30340;&#26816;&#32034;&#25104;&#20026;&#27573;&#33853;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#21452;&#32534;&#30721;&#22120;&#34429;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#20294;&#23545;&#26597;&#35810;&#20998;&#24067;&#21644;&#22024;&#26434;&#26597;&#35810;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#21152;&#20581;&#22766;&#65292;&#20294;&#20250;&#24341;&#20837;&#35757;&#32451;&#38598;&#29983;&#25104;&#30340;&#24320;&#38144;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21644;&#32034;&#24341;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Contrastive Alignment POst Training (CAPOT)&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#35753;&#26597;&#35810;&#32534;&#30721;&#22120;&#23398;&#20064;&#23558;&#22024;&#26434;&#26597;&#35810;&#19982;&#20854;&#26410;&#26356;&#25913;&#30340;&#26681;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; CAPOT &#22312; MSMARCO&#12289;&#33258;&#28982;&#38382;&#39064;&#21644; Trivia QA &#27573;&#33853;&#26816;&#32034;&#30340;&#22024;&#26434;&#21464;&#20307;&#19978;&#65292;&#21457;&#29616; CAPOT &#20855;&#26377;&#19982;&#25968;&#25454;&#22686;&#24378;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#27809;&#26377;&#23427;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of contextual word representations and advances in neural information retrieval have made dense vector-based retrieval a standard approach for passage and document ranking. While effective and efficient, dual-encoders are brittle to variations in query distributions and noisy queries. Data augmentation can make models more robust but introduces overhead to training set generation and requires retraining and index regeneration. We present Contrastive Alignment POst Training (CAPOT), a highly efficient finetuning method that improves model robustness without requiring index regeneration, the training set optimization, or alteration. CAPOT enables robust retrieval by freezing the document encoder while the query encoder learns to align noisy queries with their unaltered root. We evaluate CAPOT noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval, finding CAPOT has a similar impact as data augmentation with none of its overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.03291</link><description>&lt;p&gt;
&#27604;&#36739;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#23545;ONA&#21644;$Q$-Learning&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms. (arXiv:2304.03291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;RL&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#21644;&#21019;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#38750;&#20844;&#29702;&#25512;&#29702;&#31995;&#32479;&#65288;NARS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35748;&#30693;&#25512;&#29702;&#26694;&#26550;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NARS&#20316;&#20026;RL&#26367;&#20195;&#26041;&#26696;&#22312;&#35299;&#20915;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;Open AI gym&#21019;&#24314;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23545;ONA&#20316;&#20026;NARS&#23454;&#29616;&#21644;$Q$-Learning&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#65292;NARS&#26159;&#19968;&#20010;&#26377;&#31454;&#20105;&#21147;&#30340;RL&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning (RL) has emerged as a popular approach for solving sequence-based tasks in machine learning. However, finding suitable alternatives to RL remains an exciting and innovative research area. One such alternative that has garnered attention is the Non-Axiomatic Reasoning System (NARS), which is a general-purpose cognitive reasoning framework. In this paper, we delve into the potential of NARS as a substitute for RL in solving sequence-based tasks. To investigate this, we conduct a comparative analysis of the performance of ONA as an implementation of NARS and $Q$-Learning in various environments that were created using the Open AI gym. The environments have different difficulty levels, ranging from simple to complex. Our results demonstrate that NARS is a promising alternative to RL, with competitive performance in diverse environments, particularly in non-deterministic ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;</title><link>http://arxiv.org/abs/2304.03031</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dense Retrieval with Unanswerable Counterfactuals. (arXiv:2304.03031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#24456;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#26816;&#32034;&#22120;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#20026;&#38405;&#35835;&#22120;&#25277;&#21462;&#19968;&#32452;&#30456;&#20851;&#30340;&#20505;&#36873;&#27573;&#33853;&#12290;&#36825;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#65292;&#20174;&#26816;&#32034;&#22120;&#24471;&#21040;&#30340;&#39640;&#30456;&#20851;&#24615;&#20998;&#25968;&#21487;&#33021;&#34920;&#26126;&#20174;&#38405;&#35835;&#22120;&#33719;&#21462;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#24456;&#39640;&#65292;&#36825;&#24847;&#21619;&#30528;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24456;&#21487;&#33021;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23454;&#35777;&#39539;&#26021;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#24182;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#23494;&#38598;&#26816;&#32034;&#20013;&#36825;&#31181;&#23545;&#31572;&#26696;&#26080;&#24863;&#30693;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23547;&#27714;&#20351;&#29992;&#21453;&#20107;&#23454;&#26679;&#26412;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#20197;&#26356;&#22909;&#22320;&#21516;&#27493;DPR&#30340;&#30456;&#20851;&#24615;&#27979;&#37327;&#21644;&#38382;&#39064;-&#27573;&#33853;&#23545;&#30340;&#21487;&#31572;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;Pivoting&#23545;&#27604;&#23398;&#20064;&#65288;PiCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retriever-reader framework is popular for open-domain question answering (ODQA), where a retriever samples for the reader a set of relevant candidate passages from a large corpus. A key assumption behind this method is that high relevance scores from the retriever likely indicate high answerability from the reader, which implies a high probability that the retrieved passages contain answers to a given question. In this work, we empirically dispel this belief and observe that recent dense retrieval models based on DPR often rank unanswerable counterfactual passages higher than their answerable original passages. To address such answer-unawareness in dense retrievers, we seek to use counterfactual samples as additional training resources to better synchronize the relevance measurement of DPR with the answerability of question-passage pairs. Specifically, we present counterfactually-Pivoting Contrastive Learning (PiCL), a novel representation learning approach for passage retrieval th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.02845</link><description>&lt;p&gt;
&#22362;&#38887;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Robust Neural Architecture Search. (arXiv:2304.02845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#24448;&#24448;&#26356;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#24378;&#20581;&#30340;NAS&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#24573;&#30053;&#20102;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#26412;&#36136;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21517;&#20026;Robust Neural Architecture Search&#65288;RNAS&#65289;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;RNAS&#29983;&#25104;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#32780;&#19981;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#20316;&#20026;&#25628;&#32034;&#26550;&#26500;&#30340;&#36755;&#20837;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RNAS&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RNAS&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architectures Search (NAS) becomes more and more popular over these years. However, NAS-generated models tends to suffer greater vulnerability to various malicious attacks. Lots of robust NAS methods leverage adversarial training to enhance the robustness of NAS-generated models, however, they neglected the nature accuracy of NAS-generated models. In our paper, we propose a novel NAS method, Robust Neural Architecture Search (RNAS). To design a regularization term to balance accuracy and robustness, RNAS generates architectures with both high accuracy and good robustness. To reduce search cost, we further propose to use noise examples instead adversarial examples as input to search architectures. Extensive experiments show that RNAS achieves state-of-the-art (SOTA) performance on both image classification and adversarial attacks, which illustrates the proposed RNAS achieves a good tradeoff between robustness and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2304.01508</link><description>&lt;p&gt;
EPVT: &#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#22312;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#39046;&#22495;&#19968;&#33324;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01508
&lt;/p&gt;
&lt;p&gt;
EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#19982;&#30142;&#30149;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#26263;&#35282;&#12289;&#27987;&#23494;&#27611;&#21457;&#65289;&#65292;&#23548;&#33268;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#19968;&#33324;&#21270;&#26041;&#27861;&#8212;&#8212;EPVT&#65292;&#23427;&#23558;&#25552;&#31034;&#23884;&#20837;&#21040;Vision Transformer&#20013;&#65292;&#20197;&#21327;&#21516;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPVT&#21033;&#29992;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#65292;&#27599;&#20010;&#39046;&#22495;&#25552;&#31034;&#37117;&#25198;&#28436;&#39046;&#22495;&#19987;&#23478;&#30340;&#35282;&#33394;&#65292;&#20197;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#21644;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#24471;&#39046;&#22495;&#25552;&#31034;&#19982;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#20302;&#31209;&#20056;&#24615;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
&lt;/p&gt;</description></item><item><title>TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.01433</link><description>&lt;p&gt;
TPU v4&#65306;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. (arXiv:2304.01433v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01433
&lt;/p&gt;
&lt;p&gt;
TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#65292;&#29983;&#20135;&#24037;&#20316;&#36127;&#36733;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#21644;&#36805;&#36895;&#30340;&#21464;&#21270;&#12290;TPU v4&#26159;&#35895;&#27468;&#30340;&#31532;&#20116;&#20195;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#26550;&#26500;&#65288;DSA&#65289;&#65292;&#26159;&#20854;&#31532;&#19977;&#20010;&#29992;&#20110;&#22788;&#29702;&#27492;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#65288;OCS&#65289;&#21160;&#24577;&#37325;&#26032;&#37197;&#32622;&#20854;&#20114;&#36830;&#25299;&#25169;&#65292;&#20197;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#12290;&#37096;&#32626;&#33258;2020&#24180;&#20197;&#26469;&#65292;TPU v4&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#34920;&#29616;&#20248;&#20110;TPU v3&#65292;&#21516;&#26102;&#24615;&#33021;/Watt&#25552;&#39640;&#20102;2.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5% of system cost and &lt;3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For sim
&lt;/p&gt;</description></item><item><title>RePAST&#26159;&#19968;&#31181;&#30456;&#23545;&#20301;&#23039;&#27880;&#24847;&#21147;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65292;&#20854;&#23558;&#25104;&#23545;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#30452;&#25509;&#27880;&#20837;&#36716;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#24103;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#26041;&#27861;&#30340;&#20840;&#37096;&#21151;&#33021;&#65292;&#21152;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#24182;&#19981;&#20250;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.00947</link><description>&lt;p&gt;
RePAST&#65306;&#30456;&#23545;&#20301;&#23039;&#27880;&#24847;&#21147;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
RePAST: Relative Pose Attention Scene Representation Transformer. (arXiv:2304.00947v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00947
&lt;/p&gt;
&lt;p&gt;
RePAST&#26159;&#19968;&#31181;&#30456;&#23545;&#20301;&#23039;&#27880;&#24847;&#21147;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65292;&#20854;&#23558;&#25104;&#23545;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#30452;&#25509;&#27880;&#20837;&#36716;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#24103;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#26041;&#27861;&#30340;&#20840;&#37096;&#21151;&#33021;&#65292;&#21152;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#24182;&#19981;&#20250;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65288;SRT&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#20132;&#20114;&#36895;&#29575;&#28210;&#26579;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;SRT&#20351;&#29992;&#30456;&#23545;&#20110;&#20219;&#24847;&#36873;&#25321;&#30340;&#21442;&#32771;&#25668;&#20687;&#26426;&#30340;&#30456;&#26426;&#23039;&#24577;&#65292;&#22240;&#27492;&#23427;&#23545;&#36755;&#20837;&#35270;&#22270;&#30340;&#39034;&#24207;&#19981;&#21464;&#12290;&#22240;&#27492;&#65292;SRT&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#38656;&#35201;&#23450;&#26399;&#26356;&#25913;&#21442;&#32771;&#24103;&#30340;&#22823;&#35268;&#27169;&#22330;&#26223;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#23039;&#24577;&#27880;&#24847;&#21147;SRT&#65288;RePAST&#65289;&#65306;&#25105;&#20204;&#23558;&#25104;&#23545;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#30452;&#25509;&#27880;&#20837;&#36716;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#26102;&#22266;&#23450;&#19968;&#20010;&#21442;&#32771;&#24103;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#23450;&#20041;&#19981;&#21464;&#20110;&#20219;&#20309;&#20840;&#23616;&#21442;&#32771;&#24103;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#21407;&#22987;&#26041;&#27861;&#30340;&#20840;&#37096;&#21151;&#33021;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#36825;&#31181;&#19981;&#21464;&#24615;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#24182;&#19981;&#20250;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#21521;&#24212;&#29992;&#23436;&#20840;&#28508;&#22312;&#30340;&#22522;&#20110;Transformer&#30340;&#28210;&#26579;&#26041;&#27861;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#36808;&#20986;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Scene Representation Transformer (SRT) is a recent method to render novel views at interactive rates. Since SRT uses camera poses with respect to an arbitrarily chosen reference camera, it is not invariant to the order of the input views. As a result, SRT is not directly applicable to large-scale scenes where the reference frame would need to be changed regularly. In this work, we propose Relative Pose Attention SRT (RePAST): Instead of fixing a reference frame at the input, we inject pairwise relative camera pose information directly into the attention mechanism of the Transformers. This leads to a model that is by definition invariant to the choice of any global reference frame, while still retaining the full capabilities of the original method. Empirical results show that adding this invariance to the model does not lead to a loss in quality. We believe that this is a step towards applying fully latent transformer-based rendering methods to large-scale scenes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;</title><link>http://arxiv.org/abs/2304.00252</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#20445;&#25252;&#65306;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#25915;&#20987;&#21487;&#20197;&#20351;&#24694;&#24847;&#29992;&#25143;&#25805;&#32437;&#29615;&#22659;&#25110;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#19968;&#20010;&#38544;&#34255;&#30340;&#21518;&#38376;&#25554;&#20837;&#21040;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#25915;&#20987;&#21361;&#21450;RL&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#22312;&#21508;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#23545;&#20110;RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#25252;&#21463;&#23475;&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290; RTS&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#20195;&#29702;&#20013;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#22312;&#35757;&#32451;&#26367;&#20195;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#26102;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#21160;&#20316;&#20449;&#24687;&#24182;&#20837;&#65292;&#20943;&#23569;&#20195;&#29702;&#22312;&#39044;&#27979;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#21644;&#23454;&#38469;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;&#22312;&#35757;&#32451;&#20013;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#20013;&#39044;&#27979;ICU&#24739;&#32773;&#19981;&#33391;&#20107;&#20214;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15354</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#21307;&#38498;&#21040;&#22810;&#20010;&#20013;&#24515;&#24212;&#29992;&#65306;&#22686;&#24378;ICU&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
From Single-Hospital to Multi-Centre Applications: Enhancing the Generalisability of Deep Learning Models for Adverse Event Prediction in the ICU. (arXiv:2303.15354v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;&#22312;&#35757;&#32451;&#20013;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#20013;&#39044;&#27979;ICU&#24739;&#32773;&#19981;&#33391;&#20107;&#20214;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#21450;&#26089;&#26816;&#27979;&#24739;&#32773;&#24694;&#21270;&#29366;&#24577;&#65292;&#20026;&#20182;&#20204;&#25552;&#20379;&#21453;&#24212;&#26102;&#38388;&#24182;&#38450;&#27490;&#19981;&#33391;&#32467;&#26524;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26089;&#26399;&#39044;&#35686;&#27169;&#22411;&#36890;&#24120;&#22312;&#23427;&#20204;&#21463;&#36807;&#35757;&#32451;&#30340;&#21307;&#38498;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#26032;&#21307;&#38498;&#24212;&#29992;&#26102;&#23427;&#20204;&#24448;&#24448;&#19981;&#22826;&#21487;&#38752;&#12290;&#36825;&#20351;&#24471;&#22312;&#35268;&#27169;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#27431;&#27954;&#21644;&#32654;&#22269;&#22235;&#20010;&#25968;&#25454;&#28304;&#30340;&#31934;&#24515;&#21327;&#35843;&#30340;&#37325;&#30151;&#30417;&#25252;&#25968;&#25454;&#65288;&#24635;&#35745;334,812&#20010;&#20572;&#30041;&#26102;&#38388;&#65289;&#65292;&#31995;&#32479;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#19977;&#31181;&#24120;&#35265;&#19981;&#33391;&#20107;&#20214;&#30340;&#21487;&#38752;&#24615;&#65306;&#27515;&#20129;&#12289;&#24613;&#24615;&#32958;&#25439;&#20260;&#65288;AKI&#65289;&#21644;&#33043;&#27602;&#30151;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;/&#25110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#33021;&#21147;&#26159;&#21542;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#35757;&#32451;&#21307;&#38498;&#23545;&#20110;&#27515;&#20129;&#29575;&#65288;0.838-0.869&#65289;&#12289;AKI&#65288;0.823-0.866&#65289;&#21644;&#33043;&#27602;&#30151;&#65288;0.749-0.824&#65289;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;AUROC&#12290;&#39044;&#26399;&#22320;&#65292;&#24615;&#33021;&#22312;&#26032;&#21307;&#38498;&#19979;&#38477;&#65292;&#26377;&#26102;&#19979;&#38477;&#20102;-0.200&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#36229;&#36807;&#19968;&#20010;&#25968;&#25454;&#28304;&#24182;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#24615;&#33021;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#37096;&#32626;&#20581;&#22766;&#19988;&#26222;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#20013;&#30340;&#19981;&#33391;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) can aid doctors in detecting worsening patient states early, affording them time to react and prevent bad outcomes. While DL-based early warning models usually work well in the hospitals they were trained for, they tend to be less reliable when applied at new hospitals. This makes it difficult to deploy them at scale. Using carefully harmonised intensive care data from four data sources across Europe and the US (totalling 334,812 stays), we systematically assessed the reliability of DL models for three common adverse events: death, acute kidney injury (AKI), and sepsis. We tested whether using more than one data source and/or explicitly optimising for generalisability during training improves model performance at new hospitals. We found that models achieved high AUROC for mortality (0.838-0.869), AKI (0.823-0.866), and sepsis (0.749-0.824) at the training hospital. As expected, performance dropped at new hospitals, sometimes by as much as -0.200. Using more than one 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.11470</link><description>&lt;p&gt;
&#20320;&#26377;&#22312;&#20351;&#29992;&#25105;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21527;&#65311;&#20351;&#29992;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#23454;&#29616;&#20844;&#20849;&#25968;&#25454;&#38598;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (arXiv:2303.11470v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#28304;&#28304;&#19981;&#26029;&#30340;&#25903;&#25345;&#35757;&#32451;&#25968;&#25454;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22823;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#20063;&#24341;&#36215;&#20102;&#23545;&#25968;&#25454;&#38598;&#34987;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#20110;&#21830;&#19994;&#30446;&#30340;&#30340;&#25285;&#24551;&#65292;&#36825;&#26159;&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#25152;&#31105;&#27490;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#20445;&#25252;&#20844;&#20849;&#25968;&#25454;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#38544;&#24335;&#23398;&#20064;&#30001;&#38450;&#24481;&#32773;&#35774;&#32622;&#30340;&#31192;&#23494;&#20989;&#25968;&#12290;&#36825;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#21487;&#20197;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#29992;&#20110;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#32972;&#38376;&#25554;&#20837;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#21521;&#35757;&#32451;&#38598;&#20013;&#28155;&#21152;&#20219;&#24847;&#30340;&#12289;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#23481;&#26131;&#34987;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28165;&#27905;&#26631;&#35760;&#32972;&#38376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#32780;&#19981;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26816;&#27979;&#38750;&#27861;&#25968;&#25454;&#38598;&#20351;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoo
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#34892;&#20026;&#21463;&#32422;&#26463;&#35268;&#33539;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#38024;&#23545;&#20110;&#26500;&#24314;&#36981;&#23432;&#36825;&#20123;&#32422;&#26463;&#30340;&#36890;&#29992;&#20195;&#29702;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35745;&#31639;&#27700;&#24179;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.04352</link><description>&lt;p&gt;
&#35745;&#31639;&#27700;&#24179;&#20998;&#26512;&#36890;&#29992;&#26234;&#33021;&#30340;&#32422;&#26463;&#36981;&#20174;&#24615;
&lt;/p&gt;
&lt;p&gt;
Computational-level Analysis of Constraint Compliance for General Intelligence. (arXiv:2303.04352v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04352
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#34892;&#20026;&#21463;&#32422;&#26463;&#35268;&#33539;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#38024;&#23545;&#20110;&#26500;&#24314;&#36981;&#23432;&#36825;&#20123;&#32422;&#26463;&#30340;&#36890;&#29992;&#20195;&#29702;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35745;&#31639;&#27700;&#24179;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34892;&#20026;&#21463;&#21040;&#38480;&#21046;&#34892;&#20026;&#30340;&#35268;&#33539;&#21644;&#35268;&#24459;&#30340;&#21046;&#32422;&#12290;&#35268;&#21017;&#12289;&#31036;&#20202;&#12289;&#27861;&#24459;&#21644;&#36947;&#24503;&#20041;&#21153;&#31561;&#32422;&#26463;&#34892;&#20026;&#30340;&#24418;&#24335;&#37117;&#26159;&#23545;&#20154;&#31867;&#34892;&#20026;&#36827;&#34892;&#35268;&#33539;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#32422;&#26463;&#31995;&#32479;&#26159;&#8220;&#22797;&#26434;&#30340;&#8221;&#65306;&#20010;&#20307;&#32422;&#26463;&#36890;&#24120;&#23450;&#20041;&#19981;&#26126;&#30830;&#65292;&#29305;&#23450;&#24773;&#20917;&#19979;&#21738;&#20123;&#32422;&#26463;&#26159;&#30456;&#20851;&#30340;&#21487;&#33021;&#26410;&#30693;&#25110;&#23384;&#22312;&#27495;&#20041;&#65292;&#32422;&#26463;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#20914;&#31361;&#65292;&#30830;&#23450;&#22914;&#20309;&#22312;&#30456;&#20851;&#32422;&#26463;&#30340;&#33539;&#22260;&#20869;&#34892;&#20107;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#24555;&#36895;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#26679;&#30340;&#28151;&#20081;&#65292;&#20154;&#31867;&#20173;&#28982;&#33021;&#22815;&#31283;&#20581;&#12289;&#24555;&#36895;&#22320;&#23558;&#32422;&#26463;&#34701;&#20837;&#20854;&#20915;&#31574;&#20013;&#12290;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20063;&#24517;&#39035;&#33021;&#22815;&#22312;&#29616;&#23454;&#32422;&#26463;&#31995;&#32479;&#30340;&#28151;&#20081;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20197;&#20415;&#34892;&#20026;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#20195;&#29702;&#32422;&#26463;&#22788;&#29702;&#30340;&#22797;&#26434;&#24615;&#26469;&#28304;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#24605;&#24819;&#25551;&#36848;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#32422;&#26463;&#35745;&#31639;&#27700;&#24179;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#20960;&#20010;&#24314;&#35774;&#36890;&#29992;&#30340;&#12289;&#36981;&#23432;&#32422;&#26463;&#30340;&#20195;&#29702;&#30340;&#38556;&#30861;&#65292;&#24182;&#25551;&#36848;&#20102;&#20195;&#29702;&#22914;&#20309;&#21487;&#20197;&#20197;&#26368;&#20248;&#26041;&#24335;&#32771;&#34385;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#35745;&#31639;&#27700;&#24179;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#36890;&#29992;&#26234;&#33021;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human behavior is conditioned by codes and norms that constrain action. Rules, ``manners,'' laws, and moral imperatives are examples of classes of constraints that govern human behavior. These systems of constraints are ``messy:'' individual constraints are often poorly defined, what constraints are relevant in a particular situation may be unknown or ambiguous, constraints interact and conflict with one another, and determining how to act within the bounds of the relevant constraints may be a significant challenge, especially when rapid decisions are needed. Despite such messiness, humans incorporate constraints in their decisions robustly and rapidly. General, artificially-intelligent agents must also be able to navigate the messiness of systems of real-world constraints in order to behave predictability and reliably. In this paper, we characterize sources of complexity in constraint processing for general agents and describe a computational-level analysis for such \textit{constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03388</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22810;&#26680;&#22270;&#23398;&#20064;&#30340;&#33258;&#38381;&#30151;&#39044;&#27979;&#19982;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Multi-kernel Graph Learning for Autism Prediction and Biomarker Discovery. (arXiv:2303.03388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#21644;&#20998;&#31867;&#26159;&#30142;&#30149;&#39044;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#26469;&#26377;&#25928;&#25269;&#28040;&#22810;&#27169;&#24577;&#38598;&#25104;&#36807;&#31243;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22270;&#23884;&#20837;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#22270;&#65292;&#28982;&#21518;&#25552;&#20986;&#22810;&#26680;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#20174;&#22810;&#27169;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#32858;&#21512;&#22810;&#27169;&#24577;&#22270;&#20013;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its complexity, graph learning-based multi-modal integration and classification is one of the most challenging obstacles for disease prediction. To effectively offset the negative impact between modalities in the process of multi-modal integration and extract heterogeneous information from graphs, we propose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning). For the problem of negative impact between modalities, we propose a multi-modal graph embedding module to construct a multi-modal graph. Different from conventional methods that manually construct static graphs for all modalities, each modality generates a separate graph by adaptive learning, where a function graph and a supervision graph are introduced for optimization during the multi-graph fusion embedding process. We then propose a multi-kernel graph learning module to extract heterogeneous information from the multi-modal graph. The information in the multi-modal graph at different levels is aggregat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GAN&#35757;&#32451;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#21363;&#22312;&#37492;&#21035;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#25910;&#32553;&#35757;&#32451;&#25968;&#25454;&#30340;&#21306;&#22495;&#65292;&#26500;&#24314;&#30828;&#26679;&#26412;&#24182;&#32553;&#23567;&#30828;&#26679;&#26412;&#19982;&#26131;&#26679;&#26412;&#20043;&#38388;&#30340;&#29305;&#24449;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2303.01559</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#25910;&#32553;&#25913;&#21892;GAN&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improving GAN Training via Feature Space Shrinkage. (arXiv:2303.01559v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GAN&#35757;&#32451;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#21363;&#22312;&#37492;&#21035;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#25910;&#32553;&#35757;&#32451;&#25968;&#25454;&#30340;&#21306;&#22495;&#65292;&#26500;&#24314;&#30828;&#26679;&#26412;&#24182;&#32553;&#23567;&#30828;&#26679;&#26412;&#19982;&#26131;&#26679;&#26412;&#20043;&#38388;&#30340;&#29305;&#24449;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#24322;&#33021;&#21147;&#65292;&#23427;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;GAN&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#37492;&#21035;&#22120;&#30340;&#35757;&#32451;&#20998;&#24067;&#26159;&#21160;&#24577;&#30340;&#65292;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#35757;&#32451;GAN&#30340;&#38382;&#39064;&#65292;&#21363;&#40065;&#26834;&#22270;&#20687;&#20998;&#31867;&#12290;&#21463;&#21040;&#40065;&#26834;&#22270;&#20687;&#34920;&#31034;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;GAN&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#65292;&#21363;AdaptiveMix&#65292;&#23427;&#21487;&#20197;&#22312;&#37492;&#21035;&#22120;&#30340;&#22270;&#20687;&#34920;&#31034;&#31354;&#38388;&#20013;&#25910;&#32553;&#35757;&#32451;&#25968;&#25454;&#30340;&#21306;&#22495;&#12290;&#32771;&#34385;&#21040;&#30452;&#25509;&#38480;&#21046;&#29305;&#24449;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#26500;&#24314;&#30828;&#26679;&#26412;&#65292;&#24182;&#32553;&#23567;&#30828;&#26679;&#26412;&#19982;&#26131;&#26679;&#26412;&#20043;&#38388;&#30340;&#29305;&#24449;&#36317;&#31163;&#12290;&#30828;&#26679;&#26412;&#26159;&#36890;&#36807;&#28151;&#21512;&#19968;&#23545;&#35757;&#32451;&#22270;&#20687;&#26469;&#26500;&#24314;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;AdaptiveMix&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21644;&#26368;&#20808;&#36827;&#30340;GAN&#26550;&#26500;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the outstanding capability for data generation, Generative Adversarial Networks (GANs) have attracted considerable attention in unsupervised learning. However, training GANs is difficult, since the training distribution is dynamic for the discriminator, leading to unstable image representation. In this paper, we address the problem of training GANs from a novel perspective, \emph{i.e.,} robust image classification. Motivated by studies on robust image representation, we propose a simple yet effective module, namely AdaptiveMix, for GANs, which shrinks the regions of training data in the image representation space of the discriminator. Considering it is intractable to directly bound feature space, we propose to construct hard samples and narrow down the feature distance between hard and easy samples. The hard samples are constructed by mixing a pair of training images. We evaluate the effectiveness of our AdaptiveMix with widely-used and state-of-the-art GAN architectures. The ev
&lt;/p&gt;</description></item><item><title>&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.01513</link><description>&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#22312;&#21307;&#30103;&#21450;&#20854;&#20182;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning machines for health and beyond. (arXiv:2303.01513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01513
&lt;/p&gt;
&lt;p&gt;
&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#25928;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#25797;&#38271;&#35782;&#21035;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24320;&#21457;&#24448;&#24448;&#20572;&#30041;&#22312;&#21457;&#34920;&#35770;&#25991;&#12289;&#27010;&#24565;&#39564;&#35777;&#25110;&#36890;&#36807;&#26576;&#31181;&#37096;&#32626;&#27169;&#24335;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#39046;&#22495;&#37324;&#65292;&#27169;&#22411;&#30340;&#24739;&#32773;&#20154;&#21475;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#26159;&#30830;&#20445;&#20854;&#38271;&#26399;&#23433;&#20840;&#26377;&#25928;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26159;&#26377;&#25928;&#22320;&#35757;&#32451;&#20197;&#22312;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#27169;&#24335;&#30340;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20250;&#22312;&#21457;&#34920;&#25110;&#37096;&#32626;&#26102;&#36798;&#21040;&#23792;&#20540;&#21518;&#22266;&#23450;&#19981;&#21464;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#21464;&#21270;&#32780;&#20135;&#29983;&#21464;&#21270;&#65292;&#32780;&#24403;&#27169;&#22411;&#34987;&#36816;&#24448;&#26032;&#30340;&#22320;&#26041;&#20379;&#26032;&#30340;&#20154;&#32676;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are effective for building predictive models because they are good at identifying patterns in large datasets. Development of a model for complex real life problems often stops at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as soon as patient demographic changes. The maintenance and monitoring of predictive models post-publication is crucial to guarantee their safe and effective long term use. As machine learning techniques are effectively trained to look for patterns in available datasets, the performance of a model for complex real life problems will not peak and remain fixed at the point of publication or even point of deployment. Rather, data changes over time, and they also changed when models are transported to new places to be used by new demography.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>SloshNet&#26159;&#19968;&#20010;&#37325;&#26032;&#23457;&#35270;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#24314;&#27169;&#26041;&#38754;&#36827;&#34892;&#20102;&#31934;&#32454;&#21270;&#30340;&#25506;&#32034;&#65292;&#21033;&#29992;&#29305;&#24449;&#34701;&#21512;&#21644;&#38271;&#26399;&#26102;&#38388;&#24314;&#27169;&#27169;&#22359;&#26469;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.07944</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition. (arXiv:2301.07944v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07944
&lt;/p&gt;
&lt;p&gt;
SloshNet&#26159;&#19968;&#20010;&#37325;&#26032;&#23457;&#35270;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#24314;&#27169;&#26041;&#38754;&#36827;&#34892;&#20102;&#31934;&#32454;&#21270;&#30340;&#25506;&#32034;&#65292;&#21033;&#29992;&#29305;&#24449;&#34701;&#21512;&#21644;&#38271;&#26399;&#26102;&#38388;&#24314;&#27169;&#27169;&#22359;&#26469;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#21644;&#26102;&#38388;&#24314;&#27169;&#26159;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#20013;&#26368;&#26680;&#24515;&#30340;&#26041;&#38754;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#39640;&#23618;&#27425;&#31354;&#38388;&#34920;&#31034;&#30340;&#38271;&#26399;&#26102;&#38388;&#20851;&#31995;&#24314;&#27169;&#19978;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#33267;&#20851;&#37325;&#35201;&#30340;&#20302;&#32423;&#31354;&#38388;&#29305;&#24449;&#21644;&#30701;&#26399;&#26102;&#38388;&#20851;&#31995;&#12290;&#23454;&#38469;&#19978;&#65292;&#21069;&#32773;&#21487;&#20197;&#24102;&#26469;&#20016;&#23500;&#30340;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#21518;&#32773;&#21487;&#20197;&#20998;&#21035;&#34920;&#31034;&#30456;&#37051;&#24103;&#30340;&#36816;&#21160;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SloshNet&#65292;&#19968;&#31181;&#37325;&#26032;&#23457;&#35270;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#20013;&#31354;&#38388;&#21644;&#26102;&#38388;&#24314;&#27169;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21033;&#29992;&#20302;&#32423;&#31354;&#38388;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#24449;&#34701;&#21512;&#26550;&#26500;&#25628;&#32034;&#27169;&#22359;&#65292;&#33258;&#21160;&#25628;&#32034;&#20302;&#32423;&#21644;&#39640;&#32423;&#31354;&#38388;&#29305;&#24449;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#21463;&#26368;&#36817;&#30340;Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#26399;&#26102;&#38388;&#24314;&#27169;&#27169;&#22359;&#65292;&#22522;&#20110;&#25552;&#21462;&#30340;&#31354;&#38388;&#22806;&#35266;&#24314;&#27169;&#20840;&#23616;&#26102;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial and temporal modeling is one of the most core aspects of few-shot action recognition. Most previous works mainly focus on long-term temporal relation modeling based on high-level spatial representations, without considering the crucial low-level spatial features and short-term temporal relations. Actually, the former feature could bring rich local semantic information, and the latter feature could represent motion characteristics of adjacent frames, respectively. In this paper, we propose SloshNet, a new framework that revisits the spatial and temporal modeling for few-shot action recognition in a finer manner. First, to exploit the low-level spatial features, we design a feature fusion architecture search module to automatically search for the best combination of the low-level and high-level spatial features. Next, inspired by the recent transformer, we introduce a long-term temporal modeling module to model the global temporal relations based on the extracted spatial appearan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20998;&#20915;&#31574;&#26694;&#26550;&#19979;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#29983;&#30340;&#20027;&#35266;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#24471;&#20986;&#25490;&#21517;&#21015;&#34920;&#21644;&#26435;&#37325;&#65292;&#24182;&#23558;&#30142;&#30149;&#36827;&#34892;&#27604;&#36739;&#20998;&#31867;&#20026;&#19977;&#32452;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34917;&#20805;&#24037;&#20855;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.03351</link><description>&lt;p&gt;
&#22522;&#20110;&#19977;&#20998;&#20915;&#31574;&#30340;&#20020;&#24202;&#21307;&#29983;&#20027;&#35266;&#26041;&#27861;&#29992;&#20110;&#31934;&#31070;&#38556;&#30861;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Mental-Disorders through Clinicians Subjective Approach based on Three-way Decision. (arXiv:2301.03351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20998;&#20915;&#31574;&#26694;&#26550;&#19979;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#29983;&#30340;&#20027;&#35266;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#24471;&#20986;&#25490;&#21517;&#21015;&#34920;&#21644;&#26435;&#37325;&#65292;&#24182;&#23558;&#30142;&#30149;&#36827;&#34892;&#27604;&#36739;&#20998;&#31867;&#20026;&#19977;&#32452;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34917;&#20805;&#24037;&#20855;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#31070;&#35786;&#26029;&#20013;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25163;&#21160;&#26041;&#27861;&#34987;&#29992;&#20110;&#31934;&#31070;&#38556;&#30861;&#20998;&#31867;&#65292;&#20294;&#26159;&#23427;&#23384;&#22312;&#19968;&#20123;&#19981;&#21487;&#36991;&#20813;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20998;&#20915;&#31574;&#26694;&#26550;&#19979;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#29983;&#30340;&#20027;&#35266;&#26041;&#27861;&#65292;&#21253;&#21547;&#23450;&#37327;&#20998;&#26512;&#12289;&#23450;&#37327;&#20998;&#26512;&#20197;&#21450;&#22522;&#20110;&#35780;&#20272;&#30340;&#20998;&#26512;&#12290;&#22522;&#20110;&#20020;&#24202;&#21307;&#29983;&#26368;&#22823;&#31243;&#24230;&#30340;&#20551;&#35774;&#65292;&#23450;&#24615;&#21644;&#23450;&#37327;&#30740;&#31350;&#24471;&#20986;&#20102;&#25490;&#21517;&#21015;&#34920;&#21644;&#19968;&#32452;&#25968;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#30142;&#30149;&#36827;&#34892;&#27604;&#36739;&#20998;&#31867;&#20026;&#19977;&#32452;&#65292;&#37319;&#29992;&#19977;&#20998;&#22522;&#20110;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#29702;&#35299;&#21644;&#26356;&#28165;&#26224;&#22320;&#25551;&#36848;&#36825;&#20123;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34917;&#20805;&#24037;&#20855;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In psychiatric diagnosis, a contemporary data-driven, manual-based method for mental disorders classification is the most popular technique; however, it has several inevitable flaws. Using the three-way decision as a framework, we propose a unified model that stands for clinicians' subjective approach (CSA) analysis consisting of three parts: quantitative analysis, quantitative analysis, and evaluation-based analysis. A ranking list and a set of numerical weights based on illness magnitude levels according to the clinician's greatest degree of assumptions are the findings of the qualitative and quantitative investigation. We further create a comparative classification of illnesses into three groups with varying important levels; a three-way evaluation-based model is utilized in this study for the aim of understanding and portraying these results in a more clear way. This proposed method might be integrated with the manual-based process as a complementary tool to improve precision while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MobileTL&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#26631;&#20934;&#21270;&#23618;&#20855;&#26377;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;IRBs&#27169;&#22411;&#12290;MobileTL&#36890;&#36807;&#35757;&#32451;&#20869;&#37096;&#35268;&#33539;&#21270;&#23618;&#30340;&#31227;&#20301;&#26469;&#36991;&#20813;&#23384;&#20648;&#21521;&#21518;&#20256;&#36882;&#30340;&#28608;&#27963;&#22270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#24182;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03246</link><description>&lt;p&gt;
MobileTL: &#22522;&#20110;Inverted Residual Blocks&#30340;&#35774;&#22791;&#26412;&#22320;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MobileTL: On-device Transfer Learning with Inverted Residual Blocks. (arXiv:2212.03246v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MobileTL&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#26631;&#20934;&#21270;&#23618;&#20855;&#26377;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;IRBs&#27169;&#22411;&#12290;MobileTL&#36890;&#36807;&#35757;&#32451;&#20869;&#37096;&#35268;&#33539;&#21270;&#23618;&#30340;&#31227;&#20301;&#26469;&#36991;&#20813;&#23384;&#20648;&#21521;&#21518;&#20256;&#36882;&#30340;&#28608;&#27963;&#22270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#24182;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#26412;&#22320;&#36801;&#31227;&#23398;&#20064;&#38754;&#20020;&#26377;&#38480;&#30340;&#35774;&#22791;&#36164;&#28304;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#21442;&#25968;&#30340;&#23376;&#38598;&#25110;&#21152;&#20837;&#27169;&#22411;&#34917;&#19969;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;Inverted Residual Blocks&#65288;IRBs&#65289;&#23558;&#21367;&#31215;&#23618;&#20998;&#20026;&#36880;&#23618;&#28145;&#24230;&#21644;&#36880;&#28857;&#21367;&#31215;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22810;&#21367;&#31215;&#12289;&#26631;&#20934;&#21270;&#21644;&#28608;&#27963;&#23618;&#30340;&#22534;&#21472;&#12290;&#34429;&#28982;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;IRBs&#38656;&#35201;&#22312;&#20869;&#23384;&#20013;&#23384;&#20648;&#39069;&#22806;&#30340;&#28608;&#27963;&#26144;&#23556;&#26469;&#35757;&#32451;&#21367;&#31215;&#23618;&#30340;&#26435;&#37325;&#21644;&#26631;&#20934;&#21270;&#23618;&#30340;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#39640;&#20869;&#23384;&#25104;&#26412;&#38459;&#30861;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;IRBs&#65292;&#20351;&#20854;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MobileTL&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#26631;&#20934;&#21270;&#23618;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;IRBs&#27169;&#22411;&#12290;MobileTL&#35757;&#32451;&#20869;&#37096;&#35268;&#33539;&#21270;&#23618;&#30340;&#31227;&#20301;&#65292;&#20197;&#36991;&#20813;&#23384;&#20648;&#21521;&#21518;&#20256;&#36882;&#30340;&#28608;&#27963;&#22270;&#12290;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;MobileTL&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#24182;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#30340;&#36801;&#31227;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning on edge is challenging due to on-device limited resources. Existing work addresses this issue by training a subset of parameters or adding model patches. Developed with inference in mind, Inverted Residual Blocks (IRBs) split a convolutional layer into depthwise and pointwise convolutions, leading to more stacking layers, e.g., convolution, normalization, and activation layers. Though they are efficient for inference, IRBs require that additional activation maps are stored in memory for training weights for convolution layers and scales for normalization layers. As a result, their high memory cost prohibits training IRBs on resource-limited edge devices, and making them unsuitable in the context of transfer learning. To address this issue, we present MobileTL, a memory and computationally efficient on-device transfer learning method for models built with IRBs. MobileTL trains the shifts for internal normalization layers to avoid storing activation maps for the backwar
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#26550;&#26500;&#65292;&#23427;&#21487;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32452;&#21512;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#32467;&#26524;&#26469;&#24314;&#31435;&#23436;&#25972;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#26159;&#23545;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#20854;&#30740;&#31350;&#29616;&#29366;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.00622</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#39033;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning: A Structured Literature Review. (arXiv:2212.00622v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00622
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#26550;&#26500;&#65292;&#23427;&#21487;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32452;&#21512;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#32467;&#26524;&#26469;&#24314;&#31435;&#23436;&#25972;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#26159;&#23545;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#20854;&#30740;&#31350;&#29616;&#29366;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#12290;&#38543;&#30528;&#23545;&#25968;&#25454;&#25152;&#26377;&#32773;&#20043;&#38388;&#21512;&#20316;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;FL&#24050;&#24341;&#36215;&#32452;&#32455;&#30340;&#37325;&#35270;&#12290;FL&#30340;&#24819;&#27861;&#26159;&#20351;&#21512;&#20316;&#21442;&#19982;&#32773;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#32852;&#37030;&#23398;&#20064;&#26159;&#8220;&#23558;&#27169;&#22411;&#24102;&#21040;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#23558;&#25968;&#25454;&#24102;&#21040;&#27169;&#22411;&#8221;&#30340;&#26041;&#27861;&#12290;&#24403;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#22402;&#30452;&#20998;&#21306;&#30340;&#25968;&#25454;&#26102;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#20351;&#29992;&#21508;&#22320;&#28857;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#24314;&#31435;&#23436;&#25972;&#30340;ML&#27169;&#22411;&#12290;&#36825;&#31181;FL&#30340;&#26550;&#26500;&#34987;&#31216;&#20026;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#65292;&#23427;&#19981;&#21516;&#20110;&#27700;&#24179;&#20998;&#21306;&#30340;&#20256;&#32479;FL&#12290;&#30001;&#20110;VFL&#19982;&#20256;&#32479;FL&#19981;&#21516;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#33258;&#36523;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30740;&#31350;&#20013;&#20851;&#20110;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;VFL&#30340;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising distributed learning paradigm with an added advantage of data privacy. With the growing interest in having collaboration among data owners, FL has gained significant attention of organizations. The idea of FL is to enable collaborating participants train machine learning (ML) models on decentralized data without breaching privacy. In simpler words, federated learning is the approach of ``bringing the model to the data, instead of bringing the data to the mode''. Federated learning, when applied to data which is partitioned vertically across participants, is able to build a complete ML model by combining local models trained only using the data with distinct features at the local sites. This architecture of FL is referred to as vertical federated learning (VFL), which differs from the conventional FL on horizontally partitioned data. As VFL is different from conventional FL, it comes with its own issues and challenges. In this paper, we
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KG&#19978;&#65292;&#26080;&#27861;&#38543;&#30528;KG&#19981;&#26029;&#22686;&#38271;&#32780;&#21450;&#26102;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#32456;&#36523;KG&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#32456;&#36523;&#23884;&#20837;&#23398;&#20064;&#19982;&#36716;&#31227;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#31227;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2211.15845</link><description>&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#32456;&#36523;&#23884;&#20837;&#23398;&#20064;&#19982;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15845
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KG&#19978;&#65292;&#26080;&#27861;&#38543;&#30528;KG&#19981;&#26029;&#22686;&#38271;&#32780;&#21450;&#26102;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#32456;&#36523;KG&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#32456;&#36523;&#23884;&#20837;&#23398;&#20064;&#19982;&#36716;&#31227;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#31227;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KG&#19978;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;KG&#24182;&#19981;&#20445;&#25345;&#38745;&#24577;&#65292;&#32780;&#26159;&#38543;&#30528;KG&#24212;&#29992;&#30340;&#21457;&#23637;&#32780;&#21457;&#23637;&#21644;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#26032;&#20107;&#23454;&#21644;&#20197;&#21069;&#26410;&#35265;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#19981;&#26029;&#20986;&#29616;&#65292;&#38656;&#35201;&#19968;&#31181;&#23884;&#20837;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22686;&#38271;&#24555;&#36895;&#23398;&#20064;&#21644;&#36716;&#31227;&#26032;&#30693;&#35782;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;KG&#23884;&#20837;&#30340;&#19968;&#20010;&#25193;&#23637;&#39046;&#22495;&#65292;&#21363;&#32456;&#36523;KG&#23884;&#20837;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#24517;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25345;&#23545;KG&#22686;&#38271;&#24555;&#29031;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20445;&#30041;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#29992;&#20110;&#23884;&#20837;&#23398;&#20064;&#21644;&#26356;&#26032;&#30340;&#25513;&#30721;KG&#33258;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;&#23884;&#20837;&#36716;&#31227;&#31574;&#30053;&#65292;&#23558;&#23398;&#20064;&#30340;&#30693;&#35782;&#27880;&#20837;&#26032;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#65292;&#20197;&#21450;&#23884;&#20837;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#30740;&#31350;KG&#22686;&#38271;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Existing knowledge graph (KG) embedding models have primarily focused on static KGs. However, real-world KGs do not remain static, but rather evolve and grow in tandem with the development of KG applications. Consequently, new facts and previously unseen entities and relations continually emerge, necessitating an embedding model that can quickly learn and transfer new knowledge through growth. Motivated by this, we delve into an expanding field of KG embedding in this paper, i.e., lifelong KG embedding. We consider knowledge transfer and retention of the learning on growing snapshots of a KG without having to learn embeddings from scratch. The proposed model includes a masked KG autoencoder for embedding learning and update, with an embedding transfer strategy to inject the learned knowledge into the new entity and relation embeddings, and an embedding regularization method to avoid catastrophic forgetting. To investigate the impacts of different aspects of KG growth, we construct four
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#38598;&#25104;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#20801;&#35768;&#25968;&#25454;&#30456;&#20851;&#30340;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#21407;&#26377;&#30340;&#26550;&#26500;&#65292;&#20854;&#22312; ImageNet-1K &#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; DINO &#21644; MSN &#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2211.09981</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#38598;&#25104;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Self-Supervised Learning. (arXiv:2211.09981v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#38598;&#25104;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#20801;&#35768;&#25968;&#25454;&#30456;&#20851;&#30340;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#21407;&#26377;&#30340;&#26550;&#26500;&#65292;&#20854;&#22312; ImageNet-1K &#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; DINO &#21644; MSN &#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20581;&#22766;&#24615;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#36827;&#34892;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#21644;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#25968;&#25454;&#30456;&#20851;&#30340;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26694;&#26550;&#26469;&#25913;&#36827;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#36991;&#20813;&#23545;&#34920;&#31034;&#39592;&#24178;&#36827;&#34892;&#38598;&#25104;&#65307;&#36825;&#20010;&#36873;&#25321;&#20135;&#29983;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#24456;&#23567;&#65292;&#23545;&#19979;&#28216;&#35780;&#20272;&#19981;&#38656;&#35201;&#36827;&#34892;&#26550;&#26500;&#25913;&#21464;&#25110;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; ImageNet-1K &#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861; DINO (Caron &#31561;&#20154;&#65292;2021) &#21644; MSN (Assran &#31561;&#20154;&#65292;2022)&#65292;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#23427;&#20204;&#65292;&#23588;&#20854;&#22312;&#23567;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#21152;&#26435;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#8230;&#65288;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QSL&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12289;&#20943;&#23569;&#36890;&#35759;&#25104;&#26412;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2211.06524</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#30340;&#37327;&#23376;&#20998;&#35010;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum Split Neural Network Learning using Cross-Channel Pooling. (arXiv:2211.06524v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QSL&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12289;&#20943;&#23569;&#36890;&#35759;&#25104;&#26412;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#31185;&#23398;&#39046;&#22495;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#12289;&#37327;&#23376;&#36890;&#20449;&#21644;&#37327;&#23376;&#35745;&#31639;&#31561;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#26032;&#20852;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#19982;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;(FL)&#30456;&#32467;&#21512;&#65292;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;(QFL)&#24341;&#36215;&#20102;&#29305;&#21035;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#20998;&#35010;&#23398;&#20064;(QSL)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#32463;&#20856;&#20998;&#35010;&#23398;&#20064;&#30340;&#20808;&#36827;&#24310;&#20280;&#12290;&#22312;&#20256;&#32479;&#35745;&#31639;&#26426;&#39046;&#22495;&#20013;&#65292;&#20998;&#35010;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#21152;&#36895;&#25910;&#25947;&#65292;&#20943;&#23569;&#36890;&#35759;&#25104;&#26412;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;QSL&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#21033;&#29992;&#20102;QNNs&#25152;&#23454;&#29616;&#30340;&#37327;&#23376;&#24577;&#37325;&#26500;&#30340;&#29420;&#29305;&#24615;&#36136;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#25968;&#20540;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;QSL&#19981;&#20165;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#20063;&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of quantum science has attracted significant interest across various disciplines, including quantum machine learning, quantum communication, and quantum computing. Among these emerging areas, quantum federated learning (QFL) has gained particular attention due to the integration of quantum neural networks (QNNs) with traditional federated learning (FL) techniques. In this study, a novel approach entitled quantum split learning (QSL) is presented, which represents an advanced extension of classical split learning. Previous research in classical computing has demonstrated numerous advantages of split learning, such as accelerated convergence, reduced communication costs, and enhanced privacy protection. To maximize the potential of QSL, cross-channel pooling is introduced, a technique that capitalizes on the distinctive properties of quantum state tomography facilitated by QNNs. Through rigorous numerical analysis, evidence is provided that QSL not only achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.02533</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26367;&#20195;&#21697;&#25512;&#33616;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#24369;&#30417;&#30563;&#30340;&#39038;&#23458;&#34892;&#20026;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#21697;&#30340;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#26367;&#20195;&#21697;&#32473;&#39038;&#23458;&#12290;&#20294;&#26159;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#39038;&#23458;&#30340;&#34892;&#20026;&#20449;&#21495;&#65288;&#22914;&#20849;&#21516;&#27983;&#35272;&#21644;&#27983;&#35272;&#20294;&#36141;&#20080;&#21478;&#19968;&#20010;&#20135;&#21697;&#65289;&#26469;&#25429;&#25417;&#26367;&#20195;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20010;&#26041;&#27861;&#21548;&#36215;&#26469;&#24456;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20570;&#27861;&#21487;&#33021;&#20250;&#24573;&#30053;&#20135;&#21697;&#30340;&#21151;&#33021;&#21644;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#20135;&#21697;&#26631;&#39064;&#25551;&#36848;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#65292;&#24182;&#32771;&#34385;&#20135;&#21697;&#21151;&#33021;&#65292;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#26469;&#21435;&#38500;&#20174;&#29983;&#20135;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20449;&#21495;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#24037;&#31243;&#35282;&#24230;&#32771;&#34385;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#37096;&#32626;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00173</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#31435;&#26377;&#25928;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#20154;&#20204;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#31526;&#21512;&#39044;&#27979;&#65292;&#20294;&#36825;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#23545;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#30340;&#33539;&#22260;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#21487;&#20197;&#35777;&#26126;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#19982;&#26222;&#36890;&#31526;&#21512;&#39044;&#27979;&#32467;&#21512;&#20351;&#29992;&#65292;&#32780;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#33258;&#36866;&#24212;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#38500;&#20102;&#29616;&#26377;&#39044;&#27979;&#25512;&#26029;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#65288;&#22914;ImageNet&#20998;&#31867;&#21644;Cityscapes&#22270;&#20687;&#20998;&#21106;&#65289;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on large-scale tasks such as ImageNet classification and Cityscapes image segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#23558;&#21028;&#21035;&#22120;&#20316;&#20026;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#27867;&#21270;&#24182;&#35774;&#35745;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29109;&#20197;&#35299;&#20915;GAN&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.12055</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#24418;&#29109;&#20272;&#35745;&#35299;&#20915;GAN&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Mode Collapse in GANs via Manifold Entropy Estimation. (arXiv:2208.12055v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#23558;&#21028;&#21035;&#22120;&#20316;&#20026;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#27867;&#21270;&#24182;&#35774;&#35745;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29109;&#20197;&#35299;&#20915;GAN&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;GAN&#20013;&#65292;&#27169;&#24335;&#23849;&#28291;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#27969;&#31243;&#26469;&#35299;&#20915;GAN&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21028;&#21035;&#22120;&#20316;&#20026;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#27867;&#21270;&#65292;&#26368;&#22823;&#21270;&#21028;&#21035;&#22120;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29109;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21363;&#28145;&#24230;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;DLLE&#65289;&#21644;&#28145;&#24230;&#31561;&#36317;&#29305;&#24449;&#26144;&#23556;&#65288;DIsoMap&#65289;&#65292;&#20197;&#40723;&#21169;&#21028;&#21035;&#22120;&#23398;&#20064;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#24471;&#21028;&#21035;&#22120;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#21487;&#20197;&#34987;&#24456;&#22909;&#22320;&#24418;&#25104;&#12290;&#22522;&#20110;&#30001;&#21028;&#21035;&#22120;&#25903;&#25345;&#30340;&#23398;&#20064;&#33391;&#22909;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#29109;&#20272;&#35745;&#22120;&#65292;&#20197;&#39640;&#25928;&#22320;&#26368;&#22823;&#21270;&#23884;&#20837;&#21521;&#37327;&#30340;&#29109;&#65292;&#20316;&#20026;&#26368;&#22823;&#21270;&#25968;&#25454;&#20998;&#24067;&#29109;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have shown compelling results in various tasks and applications in recent years. However, mode collapse remains a critical problem in GANs. In this paper, we propose a novel training pipeline to address the mode collapse issue of GANs. Different from existing methods, we propose to generalize the discriminator as feature embedding and maximize the entropy of distributions in the embedding space learned by the discriminator. Specifically, two regularization terms, i.e., Deep Local Linear Embedding (DLLE) and Deep Isometric feature Mapping (DIsoMap), are designed to encourage the discriminator to learn the structural information embedded in the data, such that the embedding space learned by the discriminator can be well-formed. Based on the well-learned embedding space supported by the discriminator, a non-parametric entropy estimator is designed to efficiently maximize the entropy of embedding vectors, playing as an approximation of maximizing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340;&#25345;&#20037;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#33021;&#22815;&#36827;&#34892;&#35270;&#35282;&#21512;&#25104;&#12289;&#22330;&#26223;&#35299;&#32806;&#34920;&#31034;&#21644;&#21487;&#31227;&#21160;&#32452;&#20214;&#30340;&#20998;&#31163;&#65292;&#37325;&#26500;&#21487;&#31227;&#21160;&#29289;&#20307;&#33021;&#22815;&#36827;&#34892;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2207.11232</link><description>&lt;p&gt;
&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#65306;&#22522;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#25345;&#20037;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Groundplans: Persistent Neural Scene Representations from a Single Image. (arXiv:2207.11232v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340;&#25345;&#20037;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#33021;&#22815;&#36827;&#34892;&#35270;&#35282;&#21512;&#25104;&#12289;&#22330;&#26223;&#35299;&#32806;&#34920;&#31034;&#21644;&#21487;&#31227;&#21160;&#32452;&#20214;&#30340;&#20998;&#31163;&#65292;&#37325;&#26500;&#21487;&#31227;&#21160;&#29289;&#20307;&#33021;&#22815;&#36827;&#34892;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22330;&#26223;&#30340;2D&#22270;&#20687;&#35266;&#27979;&#26144;&#23556;&#21040;&#25345;&#20037;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#20013;&#65292;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;&#21644;&#22330;&#26223;&#21487;&#31227;&#21160;&#21644;&#19981;&#21487;&#31227;&#21160;&#32452;&#20214;&#30340;&#35299;&#32806;&#34920;&#31034;&#12290;&#21463;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#24120;&#29992;&#30340;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#65292;&#21363;&#22320;&#38754;&#23545;&#40784;&#30340;2D&#29305;&#24449;&#32593;&#26684;&#65292;&#20316;&#20026;&#25345;&#20037;&#19988;&#21344;&#29992;&#20869;&#23384;&#23569;&#30340;&#22330;&#26223;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26080;&#26631;&#31614;&#30340;&#22810;&#35270;&#35282;&#35266;&#27979;&#30340;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#23398;&#20064;&#23436;&#25104;&#36974;&#25377;&#21306;&#22495;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#21487;&#20197;&#21033;&#29992;&#22810;&#35270;&#35282;&#35270;&#39057;&#22312;&#35757;&#32451;&#26102;&#26469;&#23398;&#20064;&#20998;&#21035;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#37325;&#26500;&#22330;&#26223;&#30340;&#38745;&#24577;&#21644;&#21487;&#31227;&#21160;&#32452;&#20214;&#12290;&#20998;&#21035;&#37325;&#26500;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#20854;&#21487;&#20197;&#36827;&#34892;&#35832;&#22810;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#25552;&#21462;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;3D&#34920;&#31034;&#12289;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;&#21644;&#29289;&#20307;&#25805;&#20316;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird's-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36882;&#24402;&#25991;&#26412;&#26465;&#20214;&#19979;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19968;&#31181;&#36880;&#27493;&#28155;&#21152;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#19968;&#33268;&#30340;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2207.09446</link><description>&lt;p&gt;
ShapeCrafter&#65306;&#19968;&#31181;&#36882;&#24402;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model. (arXiv:2207.09446v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36882;&#24402;&#25991;&#26412;&#26465;&#20214;&#19979;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19968;&#31181;&#36880;&#27493;&#28155;&#21152;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#19968;&#33268;&#30340;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ShapeCrafter&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36882;&#24402;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#19977;&#32500;&#24418;&#29366;&#30340;&#26041;&#27861;&#20250;&#21033;&#29992;&#25972;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#19968;&#27425;&#24615;&#29983;&#25104;&#19968;&#20010;&#19977;&#32500;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20542;&#21521;&#20110;&#36882;&#24402;&#22320;&#25551;&#36848;&#24418;&#29366;&#8212;&#8212;&#25105;&#20204;&#21487;&#33021;&#20250;&#20174;&#19968;&#20010;&#21021;&#22987;&#25551;&#36848;&#24320;&#22987;&#65292;&#24182;&#26681;&#25454;&#20013;&#38388;&#32467;&#26524;&#36880;&#28176;&#28155;&#21152;&#32454;&#33410;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20010;&#36882;&#24402;&#36807;&#31243;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#19968;&#20010;&#19977;&#32500;&#24418;&#29366;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#20197;&#21021;&#22987;&#30701;&#35821;&#20026;&#26465;&#20214;&#65292;&#24182;&#38543;&#30528;&#28155;&#21152;&#26356;&#22810;&#30340;&#30701;&#35821;&#32780;&#36880;&#28176;&#28436;&#21464;&#12290;&#30001;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#20197;&#35757;&#32451;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Text2Shape++&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;369K&#20010;&#24418;&#29366;-&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#36882;&#24402;&#24418;&#29366;&#29983;&#25104;&#12290;&#20026;&#20102;&#25429;&#25417;&#32463;&#24120;&#29992;&#20110;&#31934;&#32454;&#24418;&#29366;&#25551;&#36848;&#30340;&#23616;&#37096;&#32454;&#33410;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#24418;&#29366;&#30340;&#20998;&#24067;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#19968;&#33268;&#30340;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ShapeCrafter, a neural network for recursive text-conditioned 3D shape generation. Existing methods to generate text-conditioned 3D shapes consume an entire text prompt to generate a 3D shape in a single step. However, humans tend to describe shapes recursively-we may start with an initial description and progressively add details based on intermediate results. To capture this recursive process, we introduce a method to generate a 3D shape distribution, conditioned on an initial phrase, that gradually evolves as more phrases are added. Since existing datasets are insufficient for training this approach, we present Text2Shape++, a large dataset of 369K shape-text pairs that supports recursive shape generation. To capture local details that are often used to refine shape descriptions, we build on top of vector-quantized deep implicit functions that generate a distribution of high-quality shapes. Results show that our method can generate shapes consistent with text descriptions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.10498</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#26080;&#27861;&#35268;&#21010;&#65288;LLM&#22312;&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#20013;&#30340;&#22522;&#20934;&#65289;&#12290;&#65288;arXiv:2206.10498v3 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#20174;GPT-3&#21040;PaLM&#65292;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#27491;&#22312;&#38543;&#30528;&#27599;&#20010;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#20986;&#19981;&#26029;&#25552;&#39640;&#12290;&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#22806;&#65292;&#20154;&#20204;&#23545;&#20110;&#29702;&#35299;&#27492;&#31867;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#37319;&#29992;&#20102;&#25512;&#29702;&#22522;&#20934;&#26469;&#36827;&#34892;&#27979;&#35780;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32467;&#26524;&#30475;&#20284;&#31215;&#26497;&#65292;&#36825;&#20123;&#22522;&#20934;&#22312;&#26412;&#36136;&#19978;&#26159;&#31616;&#21333;&#30340;&#65292;LLMs&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#33021;&#20316;&#20026;&#25903;&#25345;LLMs&#25512;&#29702;&#33021;&#21147;&#65288;&#26377;&#26102;&#26159;&#33618;&#35884;&#30340;&#65289;&#22768;&#31216;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21482;&#20195;&#34920;&#20102;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#38598;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#34913;&#37327;&#27492;&#31867;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#30340;&#30495;&#27491;&#38480;&#21046;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31995;&#21015;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#21629;&#39064;&#36923;&#36753;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#24120;&#35782;&#25512;&#29702;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#23637;&#32780;&#36880;&#28176;&#22686;&#21152;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;GPT-3&#21644;GShard&#65289;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29978;&#33267;&#26080;&#27861;&#22788;&#29702;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#24403;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#21487;&#20197;&#35268;&#21010;&#21644;&#25512;&#29702;&#21464;&#21270;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616; SHapley Additive exPlanations (SHAP) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#32972;&#26223;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#32780;&#36873;&#25321;&#21512;&#36866;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#33021;&#22815;&#30830;&#20445; SHAP &#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.11351</link><description>&lt;p&gt;
&#32972;&#26223;&#25968;&#25454;&#35268;&#27169;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411; SHAP &#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models. (arXiv:2204.11351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616; SHapley Additive exPlanations (SHAP) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#32972;&#26223;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#32780;&#36873;&#25321;&#21512;&#36866;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#33021;&#22815;&#30830;&#20445; SHAP &#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25512;&#26029;&#32467;&#26524;&#20934;&#30830;&#24615;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22914;&#20915;&#31574;&#26641;&#25317;&#26377;&#22825;&#28982;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21017;&#38656;&#35201;&#22806;&#37096;&#26041;&#27861;&#25581;&#31034;&#20854;&#25512;&#26029;&#26426;&#21046;&#12290;SHapley Additive exPlanations (SHAP)&#23601;&#26159;&#19968;&#31181;&#22806;&#37096;&#35299;&#37322;&#26041;&#24335;&#65292;&#23427;&#38656;&#35201;&#19968;&#20010;&#32972;&#26223;&#25968;&#25454;&#38598;&#23545;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#12290;&#36890;&#24120;&#65292;&#32972;&#26223;&#25968;&#25454;&#38598;&#30001;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#23454;&#20363;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#32972;&#26223;&#25968;&#25454;&#38598;&#30340;&#25277;&#26679;&#35268;&#27169;&#21450;&#20854;&#23545; SHAP &#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#25506;&#35752;&#12290;&#22312;&#25105;&#20204;&#23545; MIMIC-III &#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#25277;&#26679;&#24471;&#21040;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#26680;&#24515;&#35299;&#37322;&#8212;&#8212; SHAP &#20540;&#21644;&#21464;&#37327;&#25490;&#24207;&#20540;&#30340;&#27874;&#21160;&#65292;&#36825;&#34920;&#26126;&#29992;&#25143;&#19981;&#33021;&#36731;&#20449; SHAP &#25552;&#20379;&#30340;&#19968;&#27425;&#24615;&#35299;&#37322;&#32467;&#26524;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#27874;&#21160;&#24182;&#19981;&#24847;&#21619;&#30528; SHAP &#22833;&#36133;&#65292;&#32780;&#26159;&#34920;&#26126;&#36873;&#25321;&#21512;&#36866;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#23545;&#20110;&#30830;&#20445; SHAP &#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the interpretation of why a machine learning (ML) model makes certain inferences is as crucial as the accuracy of such inferences. Some ML models like the decision tree possess inherent interpretability that can be directly comprehended by humans. Others like artificial neural networks (ANN), however, rely on external methods to uncover the deduction mechanism. SHapley Additive exPlanations (SHAP) is one of such external methods, which requires a background dataset when interpreting ANNs. Generally, a background dataset consists of instances randomly sampled from the training dataset. However, the sampling size and its effect on SHAP remain to be unexplored. In our empirical study on the MIMIC-III dataset, we show that the two core explanations - SHAP values and variable rankings fluctuate when using different background datasets acquired from random sampling, indicating that users cannot unquestioningly trust the one-shot interpretation from SHAP. Luckily, such fluctuation d
&lt;/p&gt;</description></item><item><title>FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10581</link><description>&lt;p&gt;
FAIR4Cov&#65306;&#29992;&#20110; COVID-19 &#26816;&#27979;&#30340;&#34701;&#21512;&#38899;&#39057;&#23454;&#20363;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10581
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#22768;&#38899;&#30340;&#20998;&#31867;&#25216;&#26415;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#30740;&#31350;&#29992;&#20110;&#25903;&#25345;&#35786;&#26029;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#12290;&#38024;&#23545; COVID-19 &#30123;&#24773;&#30340;&#32039;&#36843;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#34987;&#24320;&#21457;&#26469;&#22522;&#20110;&#22768;&#23398;&#36755;&#20837;&#35782;&#21035; COVID-19 &#24739;&#32773;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#20391;&#37325;&#20110;&#21683;&#22013;&#65292;&#22240;&#20026;&#24178;&#21683;&#26159; COVID-19 &#26368;&#20026;&#20154;&#25152;&#30693;&#30340;&#30151;&#29366;&#12290;&#28982;&#32780;&#65292;&#21628;&#21560;&#21644;&#35328;&#35821;&#31561;&#20854;&#20182;&#36523;&#20307;&#22768;&#38899;&#20063;&#34987;&#21457;&#29616;&#19982; COVID-19 &#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FAIR4Cov&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36523;&#20307;&#22768;&#38899;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#12290;FAIR4Cov &#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#34701;&#21512;&#21333;&#20803;&#65292;&#23427;&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#24314;&#31435;&#22810;&#20010;&#36523;&#20307;&#22768;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#20851;&#31995;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#65292;&#21253;&#25324;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#26089;&#26399;&#26816;&#27979;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAIR4Cov &#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#21508;&#31181;&#36523;&#20307;&#22768;&#38899;&#26816;&#27979; COVID-19 &#24739;&#32773;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SynDD2&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;&#39550;&#39542;&#21592;&#30340;&#20998;&#24515;&#34892;&#20026;&#21644;&#19981;&#21516;&#20957;&#35270;&#21306;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.08096</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#26512;&#39550;&#39542;&#21592;&#20998;&#24515;&#34892;&#20026;&#21644;&#19981;&#21516;&#20957;&#35270;&#21306;&#22495;&#30340;&#21512;&#25104;&#20998;&#24515;&#39550;&#39542;&#25968;&#25454;&#38598;&#65288;SynDD2&#65289;
&lt;/p&gt;
&lt;p&gt;
Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver. (arXiv:2204.08096v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SynDD2&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;&#39550;&#39542;&#21592;&#30340;&#20998;&#24515;&#34892;&#20026;&#21644;&#19981;&#21516;&#20957;&#35270;&#21306;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21644;&#20998;&#26512;&#39550;&#39542;&#21592;&#21508;&#31181;&#20998;&#24515;&#34892;&#20026;&#21644;&#19981;&#21516;&#20957;&#35270;&#21306;&#22495;&#30340;&#21512;&#25104;&#20998;&#24515;&#39550;&#39542;&#65288;SynDD2&#65289;&#25968;&#25454;&#38598;&#65288;SynDD1&#30340;&#24310;&#32493;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#36710;&#20869;&#25668;&#20687;&#22836;&#22312;&#22266;&#23450;&#30340;&#36710;&#36742;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#23427;&#20204;&#20998;&#21035;&#20301;&#20110;&#20202;&#34920;&#30424;&#12289;&#38752;&#36817;&#21518;&#35270;&#38236;&#21644;&#21491;&#20391;&#19978;&#35282;&#30340;&#31383;&#25143;&#20301;&#32622;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#27963;&#21160;&#31867;&#22411;&#65306;&#20998;&#24515;&#27963;&#21160;&#21644;&#20957;&#35270;&#21306;&#22495;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#26377;&#20004;&#20010;&#27963;&#21160;&#38598;&#65306;&#27809;&#26377;&#20986;&#29616;&#22359;&#21644;&#26377;&#20986;&#29616;&#22359;&#65292;&#20363;&#22914;&#25140;&#24125;&#23376;&#25110;&#22826;&#38451;&#38236;&#12290;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#27599;&#20010;&#27963;&#21160;&#39034;&#24207;&#21644;&#25345;&#32493;&#26102;&#38388;&#37117;&#26159;&#38543;&#26426;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#21253;&#21547;&#27599;&#20010;&#27963;&#21160;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#26631;&#35760;&#20854;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#31867;&#39550;&#39542;&#21592;&#21508;&#31181;&#20998;&#24515;&#27963;&#21160;&#21644;&#20957;&#35270;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a synthetic distracted driving (SynDD2 - a continuum of SynDD1) dataset for machine learning models to detect and analyze drivers' various distracted behavior and different gaze zones. We collected the data in a stationary vehicle using three in-vehicle cameras positioned at locations: on the dashboard, near the rearview mirror, and on the top right-side window corner. The dataset contains two activity types: distracted activities and gaze zones for each participant, and each activity type has two sets: without appearance blocks and with appearance blocks such as wearing a hat or sunglasses. The order and duration of each activity for each participant are random. In addition, the dataset contains manual annotations for each activity, having its start and end time annotated. Researchers could use this dataset to evaluate the performance of machine learning algorithms to classify various distracting activities and gaze zones of drivers.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20445;&#30041;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#20998;&#31867;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.04213</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structure-aware Protein Self-supervised Learning. (arXiv:2204.04213v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04213
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20445;&#30041;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#20998;&#31867;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#36824;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#34507;&#30333;&#36136;&#26631;&#31614;&#25968;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#25429;&#33719;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#31168;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27531;&#22522;&#38388;&#36317;&#21644;&#20108;&#38754;&#35282;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26469;&#20445;&#30041;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21033;&#29992;&#24050;&#26377;&#30340;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550; GP-SSL&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#22522;&#20110;GNN&#30340;&#32467;&#26500;&#27169;&#22411;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;fine-tuning&#23558;&#20174;GNN&#27169;&#22411;&#23398;&#21040;&#30340;&#26377;&#29992;&#30340;&#32467;&#26500;&#24863;&#30693;&#34920;&#31034;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GP-SSL&#22312;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#20445;&#30041;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/GP-SSL&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein representation learning methods have shown great potential to yield useful representation for many downstream tasks, especially on protein classification. Moreover, a few recent studies have shown great promise in addressing insufficient labels of proteins with self-supervised learning methods. However, existing protein language models are usually pretrained on protein sequences without considering the important protein structural information. To this end, we propose a novel structure-aware protein self-supervised learning method to effectively capture structural information of proteins. In particular, a well-designed graph neural network (GNN) model is pretrained to preserve the protein structural information with self-supervised tasks from a pairwise residue distance perspective and a dihedral angle perspective, respectively. Furthermore, we propose to leverage the available protein language model pretrained on protein sequences to enhance the self-supervised learning. Specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#65288;RST&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;SGD&#21644;SAM&#20043;&#38388;&#38543;&#26426;&#36873;&#25321;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#35843;&#24230;&#20989;&#25968;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25104;&#26412;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2203.09962</link><description>&lt;p&gt;
&#38754;&#21521;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#25552;&#21319;&#30340;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning. (arXiv:2203.09962v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#65288;RST&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;SGD&#21644;SAM&#20043;&#38388;&#38543;&#26426;&#36873;&#25321;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#35843;&#24230;&#20989;&#25968;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25104;&#26412;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#27169;&#22411;&#25910;&#25947;&#20110;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#65292;SAM&#31561;&#38160;&#24230;&#24863;&#30693;&#30340;&#23398;&#20064;&#31639;&#27861;&#24050;&#26174;&#31034;&#20986;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20250;&#22312;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#20013;&#22810;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;-&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#25193;&#23637;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;(RST)&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;RST&#20013;&#30340;&#20248;&#21270;&#22120;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#36827;&#34892;&#20271;&#21162;&#21033;&#23454;&#39564;&#65292;&#20197;&#30001;&#39044;&#23450;&#20041;&#30340;&#35843;&#24230;&#20989;&#25968;&#23433;&#25490;&#30340;&#27010;&#29575;&#38543;&#26426;&#36873;&#25321;&#22522;&#26412;&#31639;&#27861;&#65288;SGD&#65289;&#21644;&#38160;&#24230;&#24863;&#30693;&#31639;&#27861;&#65288;SAM&#65289;&#20043;&#19968;&#12290;&#30001;&#20110;&#22522;&#26412;&#31639;&#27861;&#30340;&#28151;&#21512;&#65292;&#20256;&#25773;&#23545;&#30340;&#24635;&#25968;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;RST&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#21508;&#31181;&#35843;&#24230;&#20989;&#25968;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#32622;&#36866;&#24403;&#35843;&#24230;&#20989;&#25968;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
By driving models to converge to flat minima, sharpness-aware learning algorithms (such as SAM) have shown the power to achieve state-of-the-art performances. However, these algorithms will generally incur one extra forward-backward propagation at each training iteration, which largely burdens the computation especially for scalable models. To this end, we propose a simple yet efficient training scheme, called Randomized Sharpness-Aware Training (RST). Optimizers in RST would perform a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and sharpness-aware algorithms (SAM) with a probability arranged by a predefined scheduling function. Due to the mixture of base algorithms, the overall count of propagation pairs could be largely reduced. Also, we give theoretical analysis on the convergence of RST. Then, we empirically study the computation cost and effect of various types of scheduling functions, and give directions on setting appropriate scheduling functi
&lt;/p&gt;</description></item></channel></rss>