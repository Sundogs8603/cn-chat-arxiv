<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04413</link><description>&lt;p&gt;
&#36229;&#36234;&#22343;&#21248;&#37319;&#26679;&#65306;&#20351;&#29992;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#20915;&#31574;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#31561;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#25214;&#21040;&#19968;&#20010;&#27604;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#36798;&#21040;&#26356;&#39640;&#24179;&#22343;&#25910;&#30410;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#24403;&#19968;&#20010;&#25968;&#25454;&#38598;&#34987;&#27425;&#20248;&#36712;&#36857;&#25152;&#20027;&#23548;&#26102;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#22312;&#24179;&#22343;&#25910;&#30410;&#19978;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#24403;&#21069;&#31163;&#32447;RL&#31639;&#27861;&#20551;&#35774;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#20445;&#25345;&#25509;&#36817;&#12290;&#22914;&#26524;&#25968;&#25454;&#38598;&#20027;&#35201;&#30001;&#27425;&#20248;&#36712;&#36857;&#32452;&#25104;&#65292;&#36825;&#20010;&#20551;&#35774;&#23558;&#24378;&#21046;&#31574;&#30053;&#27169;&#20223;&#27425;&#20248;&#21160;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.04407</link><description>&lt;p&gt;
&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26816;&#32034;&#22312;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#21040;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32842;&#22825;&#24335;&#32593;&#39029;&#25628;&#32034;&#21040;&#38382;&#31572;&#31995;&#32479;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#36807;&#20856;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#22522;&#20110;LLM&#30340;&#26816;&#32034;&#22120;&#38656;&#35201;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21253;&#25324;&#36873;&#25321;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#21644;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#20316;&#20026;&#23398;&#20064;&#20449;&#21495;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#21407;&#22240;&#26159;&#23545;&#27604;&#25439;&#22833;&#26412;&#36523;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#20248;&#21270;&#22788;&#29702;&#27969;&#31243;&#26411;&#31471;&#20915;&#31574;&#36136;&#37327;&#30340;&#19979;&#28216;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;PG-RANK&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23398;&#20064;&#25490;&#24207;&#12290;&#31070;&#32463;PG-RANK&#20026;&#26816;&#32034;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20316;&#20026;&#26356;&#22823;&#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems vi
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04381</link><description>&lt;p&gt;
Hermes&#65306;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21512;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04381
&lt;/p&gt;
&lt;p&gt;
Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Hermes&#65292;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31227;&#21160;&#35268;&#33539;&#30340;&#24418;&#24335;&#34920;&#36798;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#31070;&#32463;&#32452;&#25104;&#20998;&#26512;&#22120;NEUTREX&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#36716;&#25442;&#30456;&#20851;&#30340;&#25991;&#26412;&#24182;&#25552;&#21462;&#36716;&#25442;&#32452;&#20214;&#65288;&#21363;&#29366;&#24577;&#12289;&#26465;&#20214;&#21644;&#21160;&#20316;&#65289;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#35299;&#26512;&#26641;&#23558;&#36825;&#20123;&#36716;&#25442;&#32452;&#20214;&#36716;&#21270;&#25104;&#36923;&#36753;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36923;&#36753;&#20844;&#24335;&#32534;&#35793;&#25104;&#36716;&#25442;&#21644;&#21019;&#24314;&#24418;&#24335;&#27169;&#22411;&#20316;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#20026;&#20102;&#35777;&#26126;Hermes&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;4G NAS&#12289;5G NAS&#21644;5G RRC&#35268;&#33539;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;81-87%&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#25552;&#21462;&#30340;&#27169;&#22411;&#36827;&#34892;&#30340;&#23433;&#20840;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;3&#20010;&#26032;&#30340;&#28431;&#27934;&#12289;&#21457;&#29616;&#20102;19&#20010;&#20043;&#21069;&#30340;&#25915;&#20987;4G&#21644;5G&#35268;&#33539;&#65292;&#20197;&#21450;7&#20010;&#21830;&#19994;4G&#22522;&#24102;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04373</link><description>&lt;p&gt;
&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#23545;&#25239;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#36866;&#24212;&#20154;&#31867;&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20174;&#20960;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#27966;&#29983;&#20986;&#22870;&#21169;&#65292;&#27599;&#20010;&#27169;&#22411;&#25429;&#25417;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#32452;&#21512;&#36825;&#20123;&#32452;&#25104;&#30340;&#22870;&#21169;&#27169;&#22411;&#26102;&#65292;&#36866;&#24403;&#22320;&#21152;&#26435;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#21152;&#22256;&#38590;&#30340;&#26159;&#65292;&#30001;&#20110;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#21482;&#26159;&#20154;&#31867;&#35780;&#20215;&#30340;&#20195;&#29702;&#65292;&#36825;&#19968;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#36229;&#36807;&#26576;&#19968;&#28857;&#21518;&#65292;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#19982;&#26356;&#24046;&#30340;&#20154;&#31867;&#35780;&#20215;&#30456;&#20851;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#36825;&#20123;&#28857;&#30340;&#20301;&#32622;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04345</link><description>&lt;p&gt;
Neur2RO: &#31070;&#32463;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04345
&lt;/p&gt;
&lt;p&gt;
Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#24314;&#27169;&#21644;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#65288;&#20063;&#31216;&#20026;&#21487;&#35843;&#25972;&#40065;&#26834;&#20248;&#21270;&#65289;&#38382;&#39064;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20043;&#21069;&#21644;&#20043;&#21518;&#36827;&#34892;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20915;&#31574;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#35745;&#31639;&#19978;&#26469;&#35828;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#20915;&#31574;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Neur2RO&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#65288;CCG&#65289;&#30340;&#23454;&#20363;&#31639;&#27861;&#65292;CCG&#26159;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#30340;&#32463;&#20856;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#31181;&#26550;&#26500;&#26131;&#20110;&#20248;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;CCG&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#36825;&#22312;&#20004;&#20010;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#32972;&#21253;&#38382;&#39064;&#21644;&#36164;&#26412;&#39044;&#31639;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;AR2L&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#35843;&#33410;&#40065;&#26834;&#24615;&#26435;&#37325;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04323</link><description>&lt;p&gt;
&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#22312;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adjustable Robust Reinforcement Learning for Online 3D Bin Packing. (arXiv:2310.04323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04323
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;AR2L&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#35843;&#33410;&#40065;&#26834;&#24615;&#26435;&#37325;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20256;&#20837;&#31665;&#23376;&#24207;&#21015;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#20005;&#26684;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#23613;&#31649;&#24403;&#21069;&#29992;&#20110;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#22312;&#20248;&#21270;&#28508;&#22312;&#30340;&#31665;&#23376;&#24207;&#21015;&#20998;&#24067;&#30340;&#24179;&#22343;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24448;&#24448;&#26080;&#27861;&#22788;&#29702;&#19968;&#20123;&#26368;&#22351;&#24773;&#20917;&#12290;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;DRL&#31639;&#27861;&#24448;&#24448;&#36807;&#20998;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#29306;&#29298;&#20102;&#22312;&#27491;&#24120;&#38382;&#39064;&#23454;&#20363;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21015;&#30340;&#25915;&#20987;&#32773;&#26469;&#30740;&#31350;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#30340;DRL&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#23454;&#38469;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;Adjustable Robust Reinforcement Learning&#65292;AR2L&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#35843;&#33410;&#40065;&#26834;&#24615;&#26435;&#37325;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#27169;&#22411;&#21270;&#27599;&#20010;&#20010;&#20307;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21033;&#29992;&#38543;&#26426;&#23884;&#20837;&#26469;&#20195;&#26367;&#30830;&#23450;&#24615;&#30340;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#27010;&#29575;&#21644;&#22312;&#25512;&#26029;&#38454;&#27573;&#20135;&#29983;&#22810;&#26679;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.04306</link><description>&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#30340;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning. (arXiv:2310.04306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#27169;&#22411;&#21270;&#27599;&#20010;&#20010;&#20307;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21033;&#29992;&#38543;&#26426;&#23884;&#20837;&#26469;&#20195;&#26367;&#30830;&#23450;&#24615;&#30340;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#27010;&#29575;&#21644;&#22312;&#25512;&#26029;&#38454;&#27573;&#20135;&#29983;&#22810;&#26679;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#26159;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#20013;&#19981;&#21487;&#20998;&#21106;&#30340;&#19968;&#37096;&#20998;&#65292;&#26088;&#22312;&#35782;&#21035;&#22810;&#20154;&#22330;&#26223;&#20013;&#30340;&#25972;&#20307;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#25972;&#21512;&#19981;&#21516;&#30340;&#24773;&#32490;&#32447;&#32034;&#65292;&#32780;&#24573;&#35270;&#20102;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#19979;&#23384;&#22312;&#30340;&#22242;&#20307;&#20869;&#25317;&#25380;&#21644;&#36974;&#25377;&#31561;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20165;&#26377;&#32676;&#20307;&#32423;&#26631;&#31614;&#21487;&#29992;&#65292;&#22312;&#19968;&#20010;&#32676;&#20307;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24773;&#32490;&#39044;&#27979;&#20250;&#28151;&#28102;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#25552;&#21462;&#26356;&#21152;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#27599;&#20010;&#20010;&#20307;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#20013;&#30340;&#38543;&#26426;&#23884;&#20837;&#26469;&#20195;&#26367;&#30830;&#23450;&#24615;&#30340;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#25429;&#25417;&#20102;&#19981;&#21516;&#24773;&#32490;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#36825;&#31181;&#38543;&#26426;&#24615;&#22312;&#25512;&#26029;&#38454;&#27573;&#20135;&#29983;&#22810;&#26679;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty of each individual, we utilize stochastic embedding drawn from a Gaussian distribution instead of deterministic point embedding. This representation captures the probabilities of different emotions and generates diverse predictions through this stochasticity during the inference stage. Furthermore, uncertainty-sensitive scores are a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#25935;&#25463;&#27169;&#22411;&#39537;&#21160;&#24320;&#21457;&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#31995;&#32479;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#24314;&#27169;&#35821;&#35328;&#65288;UML&#65289;&#22270;&#24418;&#21644;&#32422;&#26463;&#35821;&#35328;&#65288;OCL&#65289;&#65292;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#27495;&#20041;&#24615;&#23545;&#22797;&#26434;&#36719;&#20214;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.04304</link><description>&lt;p&gt;
&#32534;&#30721;&#35774;&#35745;&#65306;GPT-4&#36171;&#20104;&#25935;&#25463;&#27169;&#22411;&#39537;&#21160;&#24320;&#21457;&#20197;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Coding by Design: GPT-4 empowers Agile Model Driven Development. (arXiv:2310.04304v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#25935;&#25463;&#27169;&#22411;&#39537;&#21160;&#24320;&#21457;&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#31995;&#32479;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#24314;&#27169;&#35821;&#35328;&#65288;UML&#65289;&#22270;&#24418;&#21644;&#32422;&#26463;&#35821;&#35328;&#65288;OCL&#65289;&#65292;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#27495;&#20041;&#24615;&#23545;&#22797;&#26434;&#36719;&#20214;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20195;&#30721;&#20284;&#20046;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26356;&#24191;&#27867;&#30340;&#20351;&#29992;&#65292;&#26174;&#28982;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20854;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#12290;&#33258;&#28982;&#35821;&#35328;&#30340;&#22266;&#26377;&#27495;&#20041;&#32473;&#22797;&#26434;&#36719;&#20214;&#35774;&#35745;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#30340;&#25935;&#25463;&#27169;&#22411;&#39537;&#21160;&#24320;&#21457;&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#20351;&#29992;OpenAI&#30340;GPT-4&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#8220;&#25935;&#25463;&#24615;&#8221;&#20316;&#20026;&#24403;&#21069;MDD&#26041;&#27861;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#29305;&#21035;&#26159;&#24403;&#27169;&#22411;&#21457;&#29983;&#21464;&#26356;&#25110;&#38656;&#35201;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#37096;&#32626;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26080;&#20154;&#36710;&#38431;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31532;&#19968;&#23618;&#21644;&#31532;&#20108;&#23618;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#19968;&#24314;&#27169;&#35821;&#35328;&#65288;UML&#65289;&#22270;&#24418;&#26500;&#24314;&#20102;&#26696;&#20363;&#30740;&#31350;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#25509;&#19979;&#26469;&#30340;&#19968;&#23618;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#32452;&#32422;&#26463;&#26469;&#20943;&#23569;&#27169;&#22411;&#30340;&#27495;&#20041;&#12290;&#23545;&#35937;&#32422;&#26463;&#35821;&#35328;&#65288;OCL&#65289;&#26159;&#20854;&#20013;&#19968;&#32452;&#32422;&#26463;&#65292;&#21478;&#19968;&#32452;&#26159;&#22522;&#20110;UML&#24207;&#21015;&#22270;&#30340;&#35821;&#20041;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating code from a natural language using Large Language Models (LLMs) such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's evident that this approach has its own limitations. The inherent ambiguity of natural language presents challenges for complex software designs. Accordingly, our research offers an Agile Model-Driven Development (MDD) approach that enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes "Agility" as a significant contribution to the current MDD method, particularly when the model undergoes changes or needs deployment in a different programming language. Thus, we present a case-study showcasing a multi-agent simulation system of an Unmanned Vehicle Fleet. In the first and second layer of our approach, we constructed a textual representation of the case-study using Unified Model Language (UML) diagrams. In the next layer, we introduced two sets of constraints that minimize model ambiguity. Object Constraints Language (OCL) is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.04295</link><description>&lt;p&gt;
&#35782;&#21035;&#24178;&#39044;&#22806;&#25512;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35782;&#21035;&#21644;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#23398;&#20064;&#30340;&#21069;&#25552;&#26159;&#25913;&#36827;&#24403;&#21069;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#22312;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20855;&#20307;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#21363;&#20351;&#36825;&#20123;&#24178;&#39044;&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#33021;&#22815;&#20026;&#36825;&#20010;&#20219;&#21153;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#32467;&#26524;Y&#65292;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;X&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#28508;&#22312;&#29305;&#24449;Z&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#20197;&#21450;&#24433;&#21709;Z&#30340;&#22806;&#29983;&#34892;&#20026;&#21464;&#37327;A&#12290;&#24178;&#39044;&#22806;&#25512;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#20301;&#20110;&#35757;&#32451;&#25903;&#25345;&#20043;&#22806;&#30340;A&#19978;&#30340;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;Y&#12290;&#22312;&#36825;&#37324;&#65292;&#22806;&#25512;&#21464;&#24471;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36816;&#34892;&#26102;&#20445;&#35777;&#31995;&#32479;&#65288;RTA&#65289;&#35774;&#35745;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20445;&#35777;&#23433;&#20840;&#24182;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04288</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#36798;&#24615;&#21644;&#24378;&#21270;&#23398;&#20064;&#23547;&#25214;&#26368;&#20339;&#36816;&#34892;&#26102;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Searching for Optimal Runtime Assurance via Reachability and Reinforcement Learning. (arXiv:2310.04288v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36816;&#34892;&#26102;&#20445;&#35777;&#31995;&#32479;&#65288;RTA&#65289;&#35774;&#35745;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20445;&#35777;&#23433;&#20840;&#24182;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32473;&#23450;&#30340;&#31995;&#32479;&#65292;&#36816;&#34892;&#26102;&#20445;&#35777;&#31995;&#32479;&#65288;RTA&#65289;&#20801;&#35768;&#20351;&#29992;&#19981;&#21487;&#20449;&#25110;&#23454;&#39564;&#24615;&#30340;&#25511;&#21046;&#22120;&#65292;&#21516;&#26102;&#36890;&#36807;&#22791;&#20221;&#65288;&#25110;&#23433;&#20840;&#65289;&#25511;&#21046;&#22120;&#26469;&#30830;&#20445;&#23433;&#20840;&#12290;&#30456;&#20851;&#30340;&#35745;&#31639;&#35774;&#35745;&#38382;&#39064;&#26159;&#36890;&#36807;&#26681;&#25454;&#38656;&#35201;&#20999;&#25442;&#21040;&#23433;&#20840;&#25511;&#21046;&#22120;&#26469;&#30830;&#20445;&#23433;&#20840;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#19968;&#20123;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#19981;&#21487;&#20449;&#25511;&#21046;&#22120;&#30340;&#21033;&#29992;&#29575;&#65289;&#30340;&#36923;&#36753;&#12290;&#29616;&#26377;&#30340;RTA&#35774;&#35745;&#31574;&#30053;&#34987;&#24191;&#27867;&#35748;&#20026;&#36807;&#20110;&#20445;&#23432;&#65292;&#21407;&#21017;&#19978;&#21487;&#33021;&#23548;&#33268;&#23433;&#20840;&#36829;&#35268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#20339;RTA&#35774;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22870;&#21169;&#22609;&#24418;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20445;&#35777;&#23433;&#20840;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#29616;&#20102;&#36825;&#20010;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20123;&#20351;&#29992;3D&#31354;&#38388;&#39134;&#26426;&#27169;&#22411;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#21487;&#36798;&#24615;&#21644;&#22522;&#20110;&#27169;&#25311;&#30340;RTA&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
A runtime assurance system (RTA) for a given plant enables the exercise of an untrusted or experimental controller while assuring safety with a backup (or safety) controller. The relevant computational design problem is to create a logic that assures safety by switching to the safety controller as needed, while maximizing some performance criteria, such as the utilization of the untrusted controller. Existing RTA design strategies are well-known to be overly conservative and, in principle, can lead to safety violations. In this paper, we formulate the optimal RTA design problem and present a new approach for solving it. Our approach relies on reward shaping and reinforcement learning. It can guarantee safety and leverage machine learning technologies for scalability. We have implemented this algorithm and present experimental results comparing our approach with state-of-the-art reachability and simulation-based RTA approaches in a number of scenarios using aircraft models in 3D space w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65288;ScoreAG&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#25110;&#26032;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04285</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#22270;&#20687;&#29983;&#25104;&#35780;&#20272;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65288;ScoreAG&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#25110;&#26032;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#37117;&#38598;&#20013;&#22312;&#23567;&#30340;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#20869;&#30340;&#25200;&#21160;&#19978;&#12290;&#28982;&#32780;&#65292;$\ell_p$&#23041;&#32961;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#25200;&#21160;&#65292;&#22240;&#27492;&#65292;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#33539;&#22260;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#65288;ScoreAG&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ScoreAG&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26102;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#29616;&#26377;&#22270;&#20687;&#25110;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#21512;&#25104;&#26032;&#22270;&#20687;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;ScoreAG&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#20928;&#21270;&#22270;&#20687;&#65292;&#20174;&#32463;&#39564;&#19978;&#22686;&#24378;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;ScoreAG&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most adversarial attacks and defenses focus on perturbations within small $\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond $\ell_p$-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art atta
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#24182;&#19982;&#20154;&#31867;&#23637;&#31034;&#30340;&#19990;&#20439;&#30693;&#35782;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#20855;&#22791;&#30340;&#26159;&#8220;&#24037;&#20855;&#24615;&#30693;&#35782;&#8221;&#65292;&#32780;&#24674;&#22797;&#19990;&#20439;&#30693;&#35782;&#23558;&#21463;&#21040;&#19990;&#30028;&#27169;&#22411;&#21644;&#20219;&#21153;&#38656;&#27714;&#20043;&#38388;&#30340;&#26435;&#34913;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.04276</link><description>&lt;p&gt;
&#20174;&#20219;&#21153;&#32467;&#26500;&#21040;&#19990;&#30028;&#27169;&#22411;&#65306;LLMs&#30693;&#36947;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
From task structures to world models: What do LLMs know?. (arXiv:2310.04276v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#24182;&#19982;&#20154;&#31867;&#23637;&#31034;&#30340;&#19990;&#20439;&#30693;&#35782;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#20855;&#22791;&#30340;&#26159;&#8220;&#24037;&#20855;&#24615;&#30693;&#35782;&#8221;&#65292;&#32780;&#24674;&#22797;&#19990;&#20439;&#30693;&#35782;&#23558;&#21463;&#21040;&#19990;&#30028;&#27169;&#22411;&#21644;&#20219;&#21153;&#38656;&#27714;&#20043;&#38388;&#30340;&#26435;&#34913;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#21738;&#31181;&#30693;&#35782;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#36229;&#36234;&#20102;&#29305;&#23450;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#24182;&#25361;&#25112;&#20102;&#25105;&#20204;&#23545;&#30693;&#35782;&#21644;&#26234;&#33021;&#26412;&#36136;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#36171;&#20104;LLMs&#8220;&#24037;&#20855;&#24615;&#30693;&#35782;&#8221;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65307;&#36825;&#31181;&#30693;&#35782;&#30001;&#19968;&#23450;&#30340;&#33021;&#21147;&#38598;&#21512;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#36825;&#31181;&#30693;&#35782;&#22914;&#20309;&#19982;&#20154;&#31867;&#20195;&#29702;&#25152;&#23637;&#31034;&#30340;&#26356;&#26222;&#36890;&#30340;&#8220;&#19990;&#20439;&#8221;&#30693;&#35782;&#20851;&#32852;&#65292;&#24182;&#20174;&#35748;&#30693;&#31185;&#23398;&#20013;&#32467;&#26500;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#31243;&#24230;&#19978;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#22914;&#20309;&#24674;&#22797;&#19990;&#20439;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#24182;&#24314;&#35758;&#36825;&#31181;&#24674;&#22797;&#23558;&#21463;&#21040;&#19990;&#30028;&#27169;&#22411;&#21644;&#20219;&#21153;&#38656;&#27714;&#20043;&#38388;&#30340;&#38544;&#21547;&#36164;&#28304;&#21512;&#29702;&#21270;&#26435;&#34913;&#30340;&#21046;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
In what sense does a large language model have knowledge? The answer to this question extends beyond the capabilities of a particular AI system, and challenges our assumptions about the nature of knowledge and intelligence. We answer by granting LLMs "instrumental knowledge"; knowledge defined by a certain set of abilities. We then ask how such knowledge is related to the more ordinary, "worldly" knowledge exhibited by human agents, and explore this in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. We discuss ways LLMs could recover degrees of worldly knowledge, and suggest such recovery will be governed by an implicit, resource-rational tradeoff between world models and task demands.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04270</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;6&#20010;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;26&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#28909;&#38376;LLMs&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#38646;&#26679;&#26412;LLMs&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#27809;&#26377;&#19968;&#20010;LLM&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#35757;&#32451;&#31934;&#30830;&#25805;&#20316;&#31574;&#30053;&#20197;&#24212;&#23545;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#22871;&#20214;&#20855;&#26377;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04266</link><description>&lt;p&gt;
DRIFT: &#26234;&#33021;&#28014;&#21160;&#24179;&#21488;&#36712;&#36857;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories. (arXiv:2310.04266v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#35757;&#32451;&#31934;&#30830;&#25805;&#20316;&#31574;&#30053;&#20197;&#24212;&#23545;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#22871;&#20214;&#20855;&#26377;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#28014;&#21160;&#24179;&#21488;&#12290;&#28014;&#21160;&#24179;&#21488;&#21487;&#20316;&#20026;&#22810;&#21151;&#33021;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#22320;&#29699;&#19978;&#27169;&#25311;&#24494;&#37325;&#21147;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#27492;&#31867;&#24179;&#21488;&#20013;&#30340;&#31995;&#32479;&#21644;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#22871;&#20214;&#23454;&#29616;&#20102;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#33391;&#22909;&#21487;&#20256;&#36882;&#24615;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#25552;&#20379;&#20102;&#24555;&#36895;&#35757;&#32451;&#26102;&#38388;&#12289;&#22823;&#35268;&#27169;&#27979;&#35797;&#33021;&#21147;&#12289;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#20197;&#21450;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;ROS&#32465;&#23450;&#12290;&#38500;&#20102;&#31574;&#30053;&#24320;&#21457;&#65292;&#25105;&#20204;&#30340;&#22871;&#20214;&#36824;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/elharirymatteo/RANS/tree/ICRA24&#12290;
&lt;/p&gt;
&lt;p&gt;
This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate microgravity environments on Earth. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging state-of-the-art deep reinforcement learning techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our Deep Reinforcement Learning (DRL) framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Beyond policy development, our suite provides a comprehensive platform for researchers, offering open-access at https://github.com/elharirymatteo/RANS/tree/ICRA24.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.04241</link><description>&lt;p&gt;
&#27604;&#36739;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#20219;&#21153;&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning. (arXiv:2310.04241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#29615;&#22659;&#22238;&#25253;&#65292;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#31181;&#30452;&#25509;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#20010;&#19982;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#34920;&#31034;&#12290;&#34429;&#28982;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20294;&#22312;&#20856;&#22411;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#36827;&#34892;&#27604;&#36739;&#35745;&#31639;&#37327;&#22823;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#20197;&#21069;&#26410;&#36827;&#34892;&#36807;&#12290;&#26412;&#25991;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#19978;&#36827;&#34892;&#20102;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#27604;&#36739;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#31616;&#21333;&#25670;&#32447;&#21040;&#22797;&#26434;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning state representations has gained steady popularity in reinforcement learning (RL) due to its potential to improve both sample efficiency and returns on many environments. A straightforward and efficient method is to generate representations with a distinct neural network trained on an auxiliary task, i.e. a task that differs from the actual RL task. While a whole range of such auxiliary tasks has been proposed in the literature, a comparison on typical continuous control benchmark environments is computationally expensive and has, to the best of our knowledge, not been performed before. This paper presents such a comparison of common auxiliary tasks, based on hundreds of agents trained with state-of-the-art off-policy RL algorithms. We compare possible improvements in both sample efficiency and returns for environments ranging from simple pendulum to a complex simulated robotics task. Our findings show that representation learning with auxiliary tasks is beneficial for environ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#36890;&#36807;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#28909;&#22270;&#24182;&#21033;&#29992;&#25277;&#26679;&#31639;&#27861;&#25552;&#21462;&#26368;&#21487;&#33021;&#30340;&#26410;&#26469;&#20301;&#32622;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#32553;&#25918;&#25216;&#26415;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04232</link><description>&lt;p&gt;
WayHome&#65306;&#21160;&#24577;&#20280;&#32553;&#19979;&#30340;&#38271;&#26399;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
The WayHome: Long-term Motion Prediction on Dynamically Scaled. (arXiv:2310.04232v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#36890;&#36807;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#28909;&#22270;&#24182;&#21033;&#29992;&#25277;&#26679;&#31639;&#27861;&#25552;&#21462;&#26368;&#21487;&#33021;&#30340;&#26410;&#26469;&#20301;&#32622;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#32553;&#25918;&#25216;&#26415;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20934;&#30830;&#39044;&#27979;&#21608;&#22260;&#29615;&#22659;&#20013;&#20854;&#20182;&#29289;&#20307;&#30340;&#36816;&#21160;&#65292;&#22914;&#34892;&#20154;&#25110;&#20854;&#20182;&#36710;&#36742;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#30340;&#26032;&#22411;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#21040;Gilles&#31561;&#20154;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#20026;&#33258;&#20027;&#36710;&#36742;&#38468;&#36817;&#30340;&#27599;&#20010;&#20132;&#36890;&#21442;&#19982;&#32773;&#39044;&#27979;&#22810;&#20010;&#28909;&#22270;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39044;&#27979;&#19968;&#20010;&#28909;&#22270;&#12290;&#36825;&#20123;&#28909;&#22270;&#34987;&#29992;&#20316;&#25552;&#21462;&#26368;&#21487;&#33021;&#30340;&#26410;&#26469;&#20301;&#32622;&#23545;&#24212;&#22352;&#26631;&#30340;&#19968;&#31181;&#26032;&#22411;&#25277;&#26679;&#31639;&#27861;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#20197;&#21450;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#32553;&#25918;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;3&#31186;&#20989;&#25968;&#30456;&#20851;&#39044;&#27979;&#38388;&#38548;&#30340;&#35823;&#24046;&#29575;&#24615;&#33021;&#19978;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#21516;&#26102;&#22312;&#26356;&#38271;&#30340;&#39044;&#27979;&#38388;&#38548;&#19979;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key challenges for autonomous vehicles is the ability to accurately predict the motion of other objects in the surrounding environment, such as pedestrians or other vehicles. In this contribution, a novel motion forecasting approach for autonomous vehicles is developed, inspired by the work of Gilles et al. [1]. We predict multiple heatmaps with a neuralnetwork-based model for every traffic participant in the vicinity of the autonomous vehicle; with one heatmap per timestep. The heatmaps are used as input to a novel sampling algorithm that extracts coordinates corresponding to the most likely future positions. We experiment with different encoders and decoders, as well as a comparison of two loss functions. Additionally, a new grid-scaling technique is introduced, showing further improved performance. Overall, our approach improves stateof-the-art miss rate performance for the function-relevant prediction interval of 3 seconds while being competitive in longer prediction int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#22686;&#24378;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#35789;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20302;&#25104;&#26412;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#38899;&#25509;&#21475;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.04205</link><description>&lt;p&gt;
&#20851;&#38190;&#35789;&#22686;&#24378;&#26816;&#32034;: &#38598;&#25104;&#35821;&#38899;&#25509;&#21475;&#30340;&#20449;&#24687;&#26816;&#32034;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface. (arXiv:2310.04205v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04205
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#22686;&#24378;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#35789;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20302;&#25104;&#26412;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#38899;&#25509;&#21475;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32452;&#21512;&#20013;&#24555;&#36895;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#32034;&#31572;&#26696;&#65292;&#32780;&#19981;&#20135;&#29983;&#24187;&#35273;&#65292;&#26159;&#38459;&#27490;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#33258;&#21160;&#21270;&#20013;&#24212;&#29992;&#30340;&#19968;&#22823;&#38556;&#30861;&#12290;&#24403;&#24819;&#35201;&#38598;&#25104;&#35821;&#38899;&#25509;&#21475;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#21830;&#19994;&#25628;&#32034;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#26469;&#35828;&#65292;&#23436;&#20840;&#20381;&#36182;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT 3.5&#31561;&#65289;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#20316;&#32773;&#36890;&#36807;&#39318;&#20808;&#24320;&#21457;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#23545;&#35201;&#25552;&#20379;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#30340;&#21457;&#29616;&#12290;&#20851;&#38190;&#35789;&#21453;&#36807;&#26469;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#32531;&#23384;&#65292;&#20197;&#20415;&#19982;&#26597;&#35810;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#22312;&#25991;&#26723;&#20013;&#26597;&#25214;&#19978;&#19979;&#25991;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#19968;&#26086;&#19978;&#19979;&#25991;&#35774;&#32622;&#22909;&#20102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23601;&#21487;&#20197;&#26681;&#25454;&#20026;&#38382;&#31572;&#23450;&#21046;&#30340;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle which prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface. Besides, for commercial search and chatbot applications, complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly. In this work, authors have addressed this problem by first developing a keyword based search framework which augments discovery of the context to be provided to the large language model. The keywords in turn are generated by LLM and cached for comparison with keywords generated by LLM against the query raised. This significantly reduces time and cost to find the context within documents. Once the context is set, LLM uses that to provide answers based on a prompt tailored for Q&amp;A. This research work demonstrates that u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;(ASI)&#65292;&#29992;&#20110;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;XAI&#24402;&#22240;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#24402;&#22240;&#21644;ASI&#20998;&#25968;&#20998;&#24067;&#26469;&#35777;&#26126;&#35813;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04178</link><description>&lt;p&gt;
&#24341;&#20837;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;XAI&#24402;&#22240;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions. (arXiv:2310.04178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;(ASI)&#65292;&#29992;&#20110;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;XAI&#24402;&#22240;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#24402;&#22240;&#21644;ASI&#20998;&#25968;&#20998;&#24067;&#26469;&#35777;&#26126;&#35813;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#34701;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#20013;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#21152;&#21644;&#26222;&#36941;&#22797;&#26434;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#24615;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#20851;&#32852;&#24615;&#30340;&#21487;&#35299;&#37322;&#27934;&#23519;&#12290; &#24402;&#22240;&#25216;&#26415;&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#35299;&#37322;&#20197;&#33719;&#24471;&#27934;&#23519;&#65292;&#20294;&#24456;&#38590;&#35780;&#20272;&#20854;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;&#65288;ASI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#24402;&#22240;&#25216;&#26415;&#23646;&#24615;&#32771;&#34385;&#36827;&#21435;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290; &#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#25200;&#21160;&#23454;&#20363;&#21644;&#24402;&#22240;&#19982;&#30456;&#20851;&#24615;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#22312;&#24230;&#37327;&#26041;&#27861;&#20013;&#21152;&#20837;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290; &#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#32452;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#24402;&#22240;&#21450;ASI&#20998;&#25968;&#20998;&#24067;&#30340;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#38656;&#30340;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#20102;&#19977;&#20010;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the increasing amount and general complexity of time series data in domains such as finance, weather forecasting, and healthcare, there is a growing need for state-of-the-art performance models that can provide interpretable insights into underlying patterns and relationships. Attribution techniques enable the extraction of explanations from time series models to gain insights but are hard to evaluate for their robustness and trustworthiness. We propose the Attribution Stability Indicator (ASI), a measure to incorporate robustness and trustworthiness as properties of attribution techniques for time series into account. We extend a perturbation analysis with correlations of the original time series to the perturbed instance and the attributions to include wanted properties in the measure. We demonstrate the wanted properties based on an analysis of the attributions in a dimension-reduced space and the ASI scores distribution over three whole time series classification datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04171</link><description>&lt;p&gt;
&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26088;&#22312;&#21457;&#29616;&#27450;&#35784;&#32773;&#36890;&#36807;&#30041;&#19979;&#20551;&#35780;&#35770;&#25110;&#36827;&#34892;&#24322;&#24120;&#20132;&#26131;&#27450;&#39575;&#20854;&#20182;&#29992;&#25143;&#12290;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#31867;&#21035;&#65288;&#27450;&#35784;&#25110;&#27491;&#24120;&#65289;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#23454;&#38469;&#19990;&#30028;&#22270;&#34920;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#32858;&#21512;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#20989;&#25968;&#20026;&#27599;&#20010;&#20851;&#31995;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#32858;&#21512;&#36807;&#31243;&#20013;&#37319;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#20803;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#26368;&#20339;&#22270;&#20687;&#25513;&#34109;&#27604;&#29575;&#21644;&#25513;&#34109;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#20803;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04148</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#20803;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning. (arXiv:2310.04148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#20803;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#26368;&#20339;&#22270;&#20687;&#25513;&#34109;&#27604;&#29575;&#21644;&#25513;&#34109;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#20803;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30417;&#30563;&#31070;&#32463;&#20803;&#20998;&#21106;&#26041;&#27861;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#20934;&#30830;&#26631;&#27880;&#30340;&#25968;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;EM&#65289;&#25968;&#25454;&#26102;&#12290;&#36890;&#36807;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#25513;&#34109;&#22270;&#20687;&#27169;&#22411;&#65288;MIM&#65289;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#20174;&#25513;&#34109;&#22270;&#20687;&#20013;&#24674;&#22797;&#21407;&#22987;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#32780;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;EM&#22270;&#20687;&#20013;&#23384;&#22312;&#39640;&#24230;&#30340;&#32467;&#26500;&#23616;&#37096;&#24615;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#22122;&#22768;&#65292;&#35768;&#22810;&#20307;&#20803;&#32032;&#21253;&#21547;&#24456;&#23569;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#24471;MIM&#39044;&#35757;&#32451;&#22312;&#31070;&#32463;&#20803;&#20998;&#21106;&#20219;&#21153;&#20013;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#30340;MIM&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#26368;&#20339;&#22270;&#20687;&#25513;&#34109;&#27604;&#29575;&#21644;&#25513;&#34109;&#31574;&#30053;&#12290;&#30001;&#20110;&#25506;&#32034;&#31354;&#38388;&#24040;&#22823;&#65292;&#20351;&#29992;&#21333;&#26234;&#33021;&#20307;RL&#36827;&#34892;&#20307;&#20803;&#32032;&#39044;&#27979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Theref
&lt;/p&gt;</description></item><item><title>&#20197;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#20026;&#29305;&#28857;&#30340;&#31639;&#27861;&#26080;&#20851;&#30340;&#35760;&#24518;&#27169;&#22411;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24378;&#32467;&#26500;&#20808;&#39564;&#38480;&#21046;&#27169;&#22411;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#27604;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#30340;&#22870;&#21169;&#24182;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04128</link><description>&lt;p&gt;
&#20197;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#20026;&#29305;&#28857;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04128
&lt;/p&gt;
&lt;p&gt;
&#20197;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#20026;&#29305;&#28857;&#30340;&#31639;&#27861;&#26080;&#20851;&#30340;&#35760;&#24518;&#27169;&#22411;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24378;&#32467;&#26500;&#20808;&#39564;&#38480;&#21046;&#27169;&#22411;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#27604;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#30340;&#22870;&#21169;&#24182;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#25152;&#26377;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#37117;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#65292;&#24517;&#39035;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#20351;&#29992;&#35760;&#24518;&#12290;&#22823;&#22810;&#25968;&#26080;&#27169;&#22411;&#26041;&#27861;&#20351;&#29992;&#20174;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#20511;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23558;&#36712;&#36857;&#27719;&#24635;&#20026;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#65292;&#23613;&#31649;RL&#24448;&#24448;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#35757;&#32451;&#21644;&#25928;&#29575;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#24322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;RL&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#26080;&#20851;&#30340;&#35760;&#24518;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21463;&#35745;&#31639;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#24378;&#32467;&#26500;&#20808;&#39564;&#26469;&#38480;&#21046;&#27169;&#22411;&#25628;&#32034;&#31354;&#38388;&#12290;&#23427;&#22312;&#36882;&#24402;RL&#31639;&#27861;&#20013;&#21487;&#20197;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#22312;&#21508;&#31181;&#36882;&#24402;&#22522;&#20934;&#21644;&#31639;&#27861;&#20013;&#23454;&#29616;&#20102;&#27604;RNN&#26356;&#39640;&#30340;&#22870;&#21169;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#20219;&#20309;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;RNN&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#30340;&#23545;&#25968;&#26102;&#38388;&#21644;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#32435;&#20160;&#31119;&#21033;&#30446;&#26631;&#20989;&#25968;&#24212;&#29992;&#20110;&#35774;&#26045;&#20301;&#32622;&#38382;&#39064;&#65292;&#36890;&#36807;&#36716;&#21270;&#20010;&#20307;&#25104;&#26412;&#20026;&#25928;&#29992;&#65292;&#24182;&#20998;&#26512;&#26368;&#22823;&#21270;&#32435;&#20160;&#31119;&#21033;&#30340;&#35774;&#26045;&#20301;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#26469;&#35745;&#31639;&#35774;&#26045;&#20301;&#32622;&#12290;&#35777;&#26126;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#36798;&#21040;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#65292;&#24182;&#20174;&#26426;&#21046;&#35774;&#35745;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#32435;&#20160;&#31119;&#21033;&#30340;&#36817;&#20284;&#27604;&#20363;&#26377;&#30028;&#30340;&#31574;&#30053;&#35777;&#26126;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04102</link><description>&lt;p&gt;
Nash&#31119;&#21033;&#19982;&#35774;&#26045;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Nash Welfare and Facility Location. (arXiv:2310.04102v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#32435;&#20160;&#31119;&#21033;&#30446;&#26631;&#20989;&#25968;&#24212;&#29992;&#20110;&#35774;&#26045;&#20301;&#32622;&#38382;&#39064;&#65292;&#36890;&#36807;&#36716;&#21270;&#20010;&#20307;&#25104;&#26412;&#20026;&#25928;&#29992;&#65292;&#24182;&#20998;&#26512;&#26368;&#22823;&#21270;&#32435;&#20160;&#31119;&#21033;&#30340;&#35774;&#26045;&#20301;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#26469;&#35745;&#31639;&#35774;&#26045;&#20301;&#32622;&#12290;&#35777;&#26126;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#36798;&#21040;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#65292;&#24182;&#20174;&#26426;&#21046;&#35774;&#35745;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#32435;&#20160;&#31119;&#21033;&#30340;&#36817;&#20284;&#27604;&#20363;&#26377;&#30028;&#30340;&#31574;&#30053;&#35777;&#26126;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19968;&#26465;&#32447;&#19978;&#20026;&#19968;&#32452;&#20195;&#29702;&#20154;&#30830;&#23450;&#20301;&#32622;&#30340;&#38382;&#39064;&#12290;&#32435;&#20160;&#31119;&#21033;&#30446;&#26631;&#20989;&#25968;&#65292;&#23450;&#20041;&#20026;&#20195;&#29702;&#20154;&#25928;&#29992;&#30340;&#20056;&#31215;&#65292;&#22312;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#20013;&#24050;&#30693;&#25552;&#20379;&#20102;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#24615;&#20043;&#38388;&#30340;&#25240;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31119;&#21033;&#27010;&#24565;&#24212;&#29992;&#20110;&#35774;&#26045;&#20301;&#32622;&#38382;&#39064;&#65292;&#23558;&#20010;&#20307;&#25104;&#26412;&#36716;&#21270;&#20026;&#25928;&#29992;&#65292;&#24182;&#20998;&#26512;&#26368;&#22823;&#21270;&#32435;&#20160;&#31119;&#21033;&#30340;&#35774;&#26045;&#20301;&#32622;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#26469;&#35745;&#31639;&#36825;&#20010;&#35774;&#26045;&#20301;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;&#32467;&#26524;&#34920;&#26126;&#23427;&#36798;&#21040;&#20102;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#24615;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#26426;&#21046;&#35774;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#35777;&#26126;&#26426;&#21046;&#65292;&#20854;&#32435;&#20160;&#31119;&#21033;&#30340;&#36817;&#20284;&#27604;&#20363;&#26377;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of locating a facility to serve a set of agents located along a line. The Nash welfare objective function, defined as the product of the agents' utilities, is known to provide a compromise between fairness and efficiency in resource allocation problems. We apply this welfare notion to the facility location problem, converting individual costs to utilities and analyzing the facility placement that maximizes the Nash welfare. We give a polynomial-time approximation algorithm to compute this facility location, and prove results suggesting that it achieves a good balance of fairness and efficiency. Finally, we take a mechanism design perspective and propose a strategy-proof mechanism with a bounded approximation ratio for Nash welfare.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#20132;&#36890;&#22270;&#20687;&#20013;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.04081</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#30340;&#28145;&#24230;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deeply Supervised Semantic Segmentation Method Based on GAN. (arXiv:2310.04081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#20132;&#36890;&#22270;&#20687;&#20013;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#21463;&#21040;&#20102;&#33258;&#21160;&#21270;&#21644;&#20132;&#36890;&#31995;&#32479;&#25928;&#29575;&#19981;&#26029;&#25552;&#39640;&#30340;&#38656;&#27714;&#30340;&#25512;&#21160;&#12290;&#20132;&#36890;&#23433;&#20840;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#20043;&#19968;&#65292;&#38656;&#35201;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#21508;&#31181;&#36947;&#36335;&#20803;&#32032;&#65292;&#22914;&#36947;&#36335;&#35010;&#32541;&#12289;&#36710;&#36947;&#21644;&#20132;&#36890;&#26631;&#24535;&#12290;&#35821;&#20041;&#20998;&#21106;&#22312;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#19978;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#20855;&#26377;&#20934;&#30830;&#36793;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#23558;&#23545;&#25239;&#23398;&#20064;&#30340;&#20248;&#21183;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26694;&#26550;&#38598;&#25104;&#21040;&#20256;&#32479;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#25429;&#25417;&#20132;&#36890;&#22270;&#20687;&#20013;&#22797;&#26434;&#21644;&#24494;&#22937;&#29305;&#24449;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of intelligent transportation has witnessed rapid advancements, driven by the increasing demand for automation and efficiency in transportation systems. Traffic safety, one of the tasks integral to intelligent transport systems, requires accurately identifying and locating various road elements, such as road cracks, lanes, and traffic signs. Semantic segmentation plays a pivotal role in achieving this task, as it enables the partition of images into meaningful regions with accurate boundaries. In this study, we propose an improved semantic segmentation model that combines the strengths of adversarial learning with state-of-the-art semantic segmentation techniques. The proposed model integrates a generative adversarial network (GAN) framework into the traditional semantic segmentation model, enhancing the model's performance in capturing complex and subtle features in transportation images. The effectiveness of our approach is demonstrated by a significant boo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#36890;&#36807;&#36328;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.04074</link><description>&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Automatic Aspect Extraction from Scientific Texts. (arXiv:2310.04074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#36890;&#36807;&#36328;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#20986;&#20027;&#35201;&#35266;&#28857;&#12289;&#20851;&#38190;&#35265;&#35299;&#21644;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#65288;&#22312;&#27492;&#31216;&#20026;&#26041;&#38754;&#65289;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#36827;&#34892;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#30340;&#26159;&#21019;&#24314;&#19968;&#20010;&#29992;&#20110;&#20174;&#20219;&#20309;&#39046;&#22495;&#30340;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#27880;&#37322;&#26377;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#22522;&#20934;&#31639;&#27861;&#36827;&#34892;&#26041;&#38754;&#25552;&#21462;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#39046;&#22495;&#20013;&#26041;&#38754;&#30340;&#34920;&#31034;&#23384;&#22312;&#19968;&#20123;&#24046;&#24322;&#65292;&#20294;&#21363;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#20173;&#28982;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#36825;&#22312;&#36328;&#39046;&#22495;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to extract from scientific papers their main points, key insights, and other important information, referred to here as aspects, might facilitate the process of conducting a scientific literature review. Therefore, the aim of our research is to create a tool for automatic aspect extraction from Russian-language scientific texts of any domain. In this paper, we present a cross-domain dataset of scientific texts in Russian, annotated with such aspects as Task, Contribution, Method, and Conclusion, as well as a baseline algorithm for aspect extraction, based on the multilingual BERT model fine-tuned on our data. We show that there are some differences in aspect representation in different domains, but even though our model was trained on a limited number of scientific domains, it is still able to generalize to new domains, as was proved by cross-domain experiments. The code and the dataset are available at \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#27431;&#30431;&#30340;AI&#30417;&#31649;&#20197;&#21450;&#19982;&#33521;&#22269;&#19981;&#21516;&#30340;&#37096;&#38376;&#21270;&#21644;&#33258;&#24459;&#26041;&#27861;&#12290;&#25991;&#31456;&#20027;&#24352;&#37319;&#21462;&#28151;&#21512;&#30340;&#30417;&#31649;&#31574;&#30053;&#65292;&#24378;&#35843;&#25935;&#25463;&#24615;&#21644;&#23433;&#20840;&#28207;&#21475;&#12290;AI&#27861;&#26696;&#26159;&#35299;&#20915;AI&#25361;&#25112;&#30340;&#24320;&#21019;&#24615;&#31435;&#27861;&#21162;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#23436;&#21892;&#21644;&#20840;&#29699;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.04072</link><description>&lt;p&gt;
&#27431;&#27954;&#30340;AI&#30417;&#31649;&#65306;&#20174;AI&#27861;&#26696;&#21040;&#26410;&#26469;&#30340;&#30417;&#31649;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
AI Regulation in Europe: From the AI Act to Future Regulatory Challenges. (arXiv:2310.04072v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27431;&#30431;&#30340;AI&#30417;&#31649;&#20197;&#21450;&#19982;&#33521;&#22269;&#19981;&#21516;&#30340;&#37096;&#38376;&#21270;&#21644;&#33258;&#24459;&#26041;&#27861;&#12290;&#25991;&#31456;&#20027;&#24352;&#37319;&#21462;&#28151;&#21512;&#30340;&#30417;&#31649;&#31574;&#30053;&#65292;&#24378;&#35843;&#25935;&#25463;&#24615;&#21644;&#23433;&#20840;&#28207;&#21475;&#12290;AI&#27861;&#26696;&#26159;&#35299;&#20915;AI&#25361;&#25112;&#30340;&#24320;&#21019;&#24615;&#31435;&#27861;&#21162;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#23436;&#21892;&#21644;&#20840;&#29699;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#23545;&#27431;&#30431;&#30340;AI&#30417;&#31649;&#36827;&#34892;&#20102;&#20840;&#38754;&#35752;&#35770;&#65292;&#24182;&#23558;&#20854;&#19982;&#33521;&#22269;&#26356;&#21152;&#37096;&#38376;&#21270;&#21644;&#33258;&#24459;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25991;&#31456;&#20027;&#24352;&#37319;&#21462;&#19968;&#31181;&#28151;&#21512;&#30340;&#30417;&#31649;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#21746;&#23398;&#30340;&#35201;&#32032;&#65292;&#24378;&#35843;&#38656;&#35201;&#25935;&#25463;&#24615;&#21644;&#23433;&#20840;&#28207;&#21475;&#20197;&#20415;&#20110;&#21512;&#35268;&#12290;&#25991;&#31456;&#35748;&#20026;AI&#27861;&#26696;&#26159;&#35299;&#20915;AI&#25552;&#20986;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#30340;&#24320;&#21019;&#24615;&#31435;&#27861;&#21162;&#21147;&#65292;&#20294;&#20063;&#25351;&#20986;&#27861;&#26696;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#21487;&#33021;&#20250;&#38459;&#30861;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#25991;&#31456;&#36824;&#39044;&#27979;&#20102;&#21363;&#23558;&#38754;&#20020;&#30340;&#30417;&#31649;&#25361;&#25112;&#65292;&#22914;&#26377;&#23475;&#20869;&#23481;&#31649;&#29702;&#12289;&#29615;&#22659;&#20851;&#27880;&#21644;&#28151;&#21512;&#23041;&#32961;&#12290;&#25991;&#31456;&#21628;&#21505;&#31435;&#21363;&#37319;&#21462;&#34892;&#21160;&#65292;&#21046;&#23450;&#23545;&#39640;&#24615;&#33021;&#12289;&#28508;&#22312;&#24320;&#28304;&#30340;AI&#31995;&#32479;&#36827;&#34892;&#21463;&#31649;&#21046;&#35775;&#38382;&#30340;&#21327;&#35758;&#12290;&#34429;&#28982;AI&#27861;&#26696;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#31435;&#27861;&#37324;&#31243;&#30865;&#65292;&#20294;&#23427;&#38656;&#35201;&#36827;&#19968;&#27493;&#23436;&#21892;&#21644;&#20840;&#29699;&#21512;&#20316;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter provides a comprehensive discussion on AI regulation in the European Union, contrasting it with the more sectoral and self-regulatory approach in the UK. It argues for a hybrid regulatory strategy that combines elements from both philosophies, emphasizing the need for agility and safe harbors to ease compliance. The paper examines the AI Act as a pioneering legislative effort to address the multifaceted challenges posed by AI, asserting that, while the Act is a step in the right direction, it has shortcomings that could hinder the advancement of AI technologies. The paper also anticipates upcoming regulatory challenges, such as the management of toxic content, environmental concerns, and hybrid threats. It advocates for immediate action to create protocols for regulated access to high-performance, potentially open-source AI systems. Although the AI Act is a significant legislative milestone, it needs additional refinement and global collaboration for the effective governan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04055</link><description>&lt;p&gt;
&#25226;&#22351;&#20154;&#36386;&#20986;&#21435;&#65281;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#20182;&#20204;&#36890;&#36807;&#25552;&#20132;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#36798;&#21040;&#23545;&#25239;&#30446;&#26631;&#65292;&#27604;&#22914;&#38459;&#27490;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25110;&#32773;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20381;&#36182;&#37325;&#26032;&#21152;&#26435;&#25110;&#20462;&#25913;&#25552;&#20132;&#30340;&#26041;&#24335;&#12290;&#36825;&#26159;&#22240;&#20026;&#25915;&#20987;&#32773;&#36890;&#24120;&#19981;&#20250;&#22312;&#25915;&#20987;&#20043;&#21069;&#23459;&#24067;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#32780;&#37325;&#26032;&#21152;&#26435;&#21487;&#33021;&#20250;&#25913;&#21464;&#32858;&#21512;&#32467;&#26524;&#65292;&#21363;&#20351;&#27809;&#26377;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23574;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#20165;&#22312;&#21457;&#29983;&#25915;&#20987;&#26102;&#26816;&#27979;&#25915;&#20987;&#30340;&#21457;&#29983;&#24182;&#36827;&#34892;&#38450;&#24481;&#25805;&#20316;&#65307;ii&#65289;&#19968;&#26086;&#21457;&#29983;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#24182;&#23558;&#20854;&#28040;&#38500;&#65292;&#32780;&#19981;&#20250;&#23545;&#27491;&#24120;&#27169;&#22411;&#36896;&#25104;&#20260;&#23475;&#65307;iii&#65289;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04041</link><description>&lt;p&gt;
&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36136;&#37327;&#25511;&#21046;&#21644;&#24555;&#36895;&#37319;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#23558;&#35266;&#27979;&#36807;&#31243;&#30340;&#24341;&#23548;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#24314;&#31435;&#20102;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#25105;&#20204;&#20351;&#24471;&#20248;&#21270;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#25104;&#20026;&#21487;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#21363;&#20351;&#21482;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#20063;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#24555;&#36895;&#25512;&#29702;&#31574;&#30053;&#20860;&#23481;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23436;&#20840;&#30456;&#21516;&#30340;&#25512;&#29702;&#36807;&#31243;&#20135;&#29983;&#26356;&#22909;&#30340;&#21435;&#22122;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Excision and Recovery (EAR)&#30340;&#26032;&#22411;&#37325;&#24314;-&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#26469;&#22686;&#24378;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04010</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#22686;&#24378;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking. (arXiv:2310.04010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Excision and Recovery (EAR)&#30340;&#26032;&#22411;&#37325;&#24314;-&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#26469;&#22686;&#24378;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#21046;&#36896;&#19994;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#31232;&#32570;&#24322;&#24120;&#25968;&#25454;&#30340;&#25968;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#24191;&#27867;&#37319;&#29992;&#20165;&#20351;&#29992;&#26080;&#24322;&#24120;&#26679;&#26412;&#35757;&#32451;&#30340;&#37325;&#24314;&#32534;&#30721;-&#35299;&#30721;&#22120;&#65288;ED&#65289;&#65292;&#20363;&#22914;&#33258;&#32534;&#30721;&#22120;&#25110;U-Net&#65292;&#24076;&#26395;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#24212;&#35813;&#20135;&#29983;&#27604;&#27491;&#24120;&#24773;&#20917;&#26356;&#22823;&#30340;&#37325;&#24314;&#35823;&#24046;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#26377;&#20851;&#33258;&#25105;&#30417;&#30563;&#37325;&#24314;-&#20462;&#22797;&#30340;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#12290;&#20182;&#20204;&#36974;&#25377;&#20102;&#21487;&#30097;&#30340;&#32570;&#38519;&#21306;&#22495;&#20197;&#36827;&#34892;&#20462;&#22797;&#65292;&#20197;&#20351;&#23427;&#20204;&#23545;&#37325;&#24314;ED&#19981;&#21487;&#35265;&#65292;&#20174;&#32780;&#25925;&#24847;&#23548;&#33268;&#24322;&#24120;&#30340;&#19981;&#20934;&#30830;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#22810;&#27425;&#38543;&#26426;&#36974;&#32617;&#20197;&#35206;&#30422;&#25972;&#20010;&#36755;&#20837;&#22270;&#20687;&#65292;&#22240;&#20026;&#19981;&#20250;&#20107;&#20808;&#30693;&#36947;&#32570;&#38519;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Excision and Recovery (EAR)&#30340;&#26032;&#22411;&#37325;&#24314;-&#20462;&#22797;&#26041;&#27861;&#65292;&#20855;&#26377;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#30340;&#29305;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) in surface inspection is an essential yet challenging task in manufacturing due to the quantity imbalance problem of scarce abnormal data. To overcome the above, a reconstruction encoder-decoder (ED) such as autoencoder or U-Net which is trained with only anomaly-free samples is widely adopted, in the hope that unseen abnormals should yield a larger reconstruction error than normal. Over the past years, researches on self-supervised reconstruction-by-inpainting have been reported. They mask out suspected defective regions for inpainting in order to make them invisible to the reconstruction ED to deliberately cause inaccurate reconstruction for abnormals. However, their limitation is multiple random masking to cover the whole input image due to defective regions not being known in advance. We propose a novel reconstruction-by-inpainting method dubbed Excision and Recovery (EAR) that features single deterministic masking. For this, we exploit a pre-trained spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CUPre&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#23558;&#24120;&#35265;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#30340;&#33021;&#21147;&#24212;&#29992;&#20110;&#32454;&#32990;&#22270;&#20687;&#39046;&#22495;&#65292;&#20026;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03981</link><description>&lt;p&gt;
CUPre: &#36328;&#39046;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation. (arXiv:2310.03981v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CUPre&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#23558;&#24120;&#35265;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#30340;&#33021;&#21147;&#24212;&#29992;&#20110;&#32454;&#32990;&#22270;&#20687;&#39046;&#22495;&#65292;&#20026;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#20013;&#65292;&#20363;&#22914;&#22312;&#24120;&#35265;&#29289;&#20307;&#19978;&#19979;&#25991;&#65288;COCO&#65289;[1]&#19978;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32454;&#32990;&#20998;&#21106;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#31934;&#32454;&#27880;&#37322;&#30340;&#32454;&#32990;&#22270;&#20687;[2]&#65292;&#20854;&#20013;&#21253;&#25324;&#27599;&#20010;&#22270;&#20687;&#20013;&#27599;&#20010;&#32454;&#32990;&#30340;&#36793;&#30028;&#26694;&#12289;&#25513;&#33180;&#21644;&#32454;&#32990;&#31867;&#22411;&#65292;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;&#30340;&#39044;&#35757;&#32451;DNN&#27169;&#22411;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#32454;&#32990;&#22270;&#20687;&#21487;&#29992;&#65292;&#20294;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#34987;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;CUPre&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26410;&#26631;&#27880;&#22270;&#20687;&#23558;&#23545;&#35937;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#30340;&#33021;&#21147;&#65288;&#20174;COCO&#23398;&#20064;&#65289;&#36716;&#31227;&#21040;&#32454;&#32990;&#30340;&#35270;&#35273;&#39046;&#22495;&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#20027;&#24178;&#12289;&#33046;&#23376;&#21644;&#22836;&#37096;&#27169;&#22359;&#30340;&#26631;&#20934;COCO&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;CUPre&#37319;&#29992;&#20132;&#26367;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#65288;AMT2&#65289;&#27969;&#31243;&#24182;&#36827;&#34892;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-training on object detection tasks, such as Common Objects in Contexts (COCO) [1], could significantly boost the performance of cell segmentation, it still consumes on massive fine-annotated cell images [2] with bounding boxes, masks, and cell types for every cell in every image, to fine-tune the pre-trained model. To lower the cost of annotation, this work considers the problem of pre-training DNN models for few-shot cell segmentation, where massive unlabeled cell images are available but only a small proportion is annotated. Hereby, we propose Cross-domain Unsupervised Pre-training, namely CUPre, transferring the capability of object detection and instance segmentation for common visual objects (learned from COCO) to the visual domain of cells using unlabeled images. Given a standard COCO pre-trained network with backbone, neck, and head modules, CUPre adopts an alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in every iteration of pre-training, AMT2
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#20195;&#24065;&#31354;&#38388;&#24179;&#31227;&#38598;&#21512;&#20197;&#35299;&#20915;ViTs&#37327;&#21270;&#20266;&#24433;&#38382;&#39064;&#30340;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#36229;&#20998;&#36776;&#29575;&#39044;&#35757;&#32451;&#30340;ViTs&#29305;&#24449;&#65292;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.03967</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#30340;&#23376;&#20195;&#24065;ViT&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Sub-token ViT Embedding via Stochastic Resonance Transformers. (arXiv:2310.03967v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#20195;&#24065;&#31354;&#38388;&#24179;&#31227;&#38598;&#21512;&#20197;&#35299;&#20915;ViTs&#37327;&#21270;&#20266;&#24433;&#38382;&#39064;&#30340;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#36229;&#20998;&#36776;&#29575;&#39044;&#35757;&#32451;&#30340;ViTs&#29305;&#24449;&#65292;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;Vision Transformers&#65288;ViTs&#65289;&#20013;&#23384;&#22312;&#37327;&#21270;&#20266;&#24433;&#65292;&#36825;&#26159;&#30001;&#20110;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#22270;&#20687;&#26631;&#35760;&#27493;&#39588;&#24341;&#36215;&#30340;&#12290;&#36825;&#20123;&#20266;&#24433;&#23548;&#33268;&#20102;&#31895;&#31961;&#30340;&#37327;&#21270;&#29305;&#24449;&#65292;&#23545;&#19979;&#28216;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#29305;&#21035;&#26159;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;shot&#26041;&#27861;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;ViTs&#22788;&#29702;&#31354;&#38388;&#37327;&#21270;&#30340;&#26041;&#24335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23376;&#20195;&#24065;&#31354;&#38388;&#24179;&#31227;&#26469;&#38598;&#21512;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#22270;&#20687;&#33719;&#24471;&#30340;&#29305;&#24449;&#65292;&#36825;&#21463;&#21040;&#20102;&#38543;&#26426;&#20849;&#25391;&#30340;&#21551;&#21457;&#65292;&#38543;&#26426;&#20849;&#25391;&#26159;&#20256;&#32479;&#19978;&#24212;&#29992;&#20110;&#27668;&#20505;&#21160;&#21147;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#38543;&#26426;&#20849;&#25391;&#21464;&#21387;&#22120;&#8221;&#65288;SRT&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SRT&#33021;&#22815;&#26377;&#25928;&#22320;&#36229;&#20998;&#36776;&#29575;&#39044;&#35757;&#32451;&#30340;ViTs&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#21040;&#20102;&#20316;&#20026;&#26631;&#35760;&#32467;&#26524;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26356;&#22810;&#23616;&#37096;&#32454;&#31890;&#24230;&#32467;&#26500;&#12290;SRT&#21487;&#20197;&#22312;&#20219;&#20309;&#23618;&#38754;&#12289;&#20219;&#20309;&#20219;&#21153;&#19978;&#24212;&#29992;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#21069;&#32773;&#30340;&#20248;&#21183;&#26159;&#26126;&#26174;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discover the presence of quantization artifacts in Vision Transformers (ViTs), which arise due to the image tokenization step inherent in these architectures. These artifacts result in coarsely quantized features, which negatively impact performance, especially on downstream dense prediction tasks. We present a zero-shot method to improve how pre-trained ViTs handle spatial quantization. In particular, we propose to ensemble the features obtained from perturbing input images via sub-token spatial translations, inspired by Stochastic Resonance, a method traditionally applied to climate dynamics and signal processing. We term our method ``Stochastic Resonance Transformer" (SRT), which we show can effectively super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might otherwise be neglected as a result of tokenization. SRT can be applied at any layer, on any task, and does not require any fine-tuning. The advantage of the former is evident
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03965</link><description>&lt;p&gt;
&#24605;&#32500;&#20256;&#25773;&#65306;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#26041;&#27861;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#27861;&#37325;&#29992;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#32047;&#31215;&#20102;&#38169;&#35823;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#20174;&#38646;&#24320;&#22987;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#20256;&#25773;&#8221;&#65288;TP&#65289;&#65292;&#23427;&#25506;&#32034;&#31867;&#20284;&#38382;&#39064;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#27604;&#38382;&#39064;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#65292;&#20855;&#26377;&#21487;&#37325;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23558;&#35299;&#20915;&#20808;&#21069;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#20256;&#25773;&#20197;&#28608;&#21457;&#26032;&#30340;&#38382;&#39064;&#35299;&#20915;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;TP&#39318;&#20808;&#25552;&#31034;LLMs&#25552;&#20986;&#24182;&#35299;&#20915;&#19968;&#32452;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;TP&#37325;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#32467;&#26524;&#30452;&#25509;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32773;&#25512;&#23548;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#31070;&#32463;&#38556;&#30861;&#35786;&#26029;&#30340;&#21453;&#26465;&#20214;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35786;&#26029;&#21644;&#35299;&#37322;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#21508;&#38454;&#27573;&#32467;&#26524;&#21487;&#38752;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03964</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#31070;&#32463;&#38556;&#30861;&#35786;&#26029;&#30340;&#21487;&#23398;&#20064;&#30340;&#21453;&#26465;&#20214;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis. (arXiv:2310.03964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#31070;&#32463;&#38556;&#30861;&#35786;&#26029;&#30340;&#21453;&#26465;&#20214;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35786;&#26029;&#21644;&#35299;&#37322;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#21508;&#38454;&#27573;&#32467;&#26524;&#21487;&#38752;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#21151;&#33021;&#36830;&#25509;&#24615;(FC)&#19982;&#31070;&#32463;&#38556;&#30861;&#30340;&#29983;&#29289;&#29305;&#24449;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#35782;&#21035;&#30142;&#30149;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#36827;&#34892;&#20107;&#21518;&#20998;&#26512;&#20197;&#21457;&#29616;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26694;&#26550;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21363;&#29305;&#24449;&#36873;&#25321;&#12289;&#20998;&#31867;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#26512;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#26159;&#20998;&#21035;&#23454;&#26045;&#30340;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27599;&#20010;&#38454;&#27573;&#30340;&#32467;&#26524;&#32570;&#20047;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#35786;&#21644;&#21518;&#32493;&#38454;&#27573;&#30340;&#38169;&#35823;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#38598;&#25104;&#20102;&#35786;&#26029;(&#21363;&#29305;&#24449;&#36873;&#25321;&#21644;&#29305;&#24449;&#25552;&#21462;)&#21644;&#35299;&#37322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#20010;&#20307;&#29305;&#23450;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#36830;&#25509;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21151;&#33021;&#32593;&#32476;&#20851;&#31995;&#32534;&#30721;&#22120;&#65292;&#24635;&#32467;&#20102;FC&#30340;&#20840;&#23616;&#25299;&#25169;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand the biological characteristics of neurological disorders with functional connectivity (FC), recent studies have widely utilized deep learning-based models to identify the disease and conducted post-hoc analyses via explainable models to discover disease-related biomarkers. Most existing frameworks consist of three stages, namely, feature selection, feature extraction for classification, and analysis, where each stage is implemented separately. However, if the results at each stage lack reliability, it can cause misdiagnosis and incorrect analysis in afterward stages. In this study, we propose a novel unified framework that systemically integrates diagnoses (i.e., feature selection and feature extraction) and explanations. Notably, we devised an adaptive attention network as a feature selection approach to identify individual-specific disease-related connections. We also propose a functional network relational encoder that summarizes the global topological properties of FC
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.03951</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#26681;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03951
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#23450;&#30456;&#20851;&#25991;&#26723;&#20316;&#20026;&#32972;&#26223;&#19978;&#19979;&#25991;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29983;&#25104;&#27969;&#21033;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#36825;&#31181;&#33021;&#21147;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;LLMs&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#27809;&#26377;&#25552;&#20379;&#26469;&#28304;&#25903;&#25345;&#30340;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#36825;&#31181;&#26080;&#26681;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#21518;&#26399;&#32534;&#36753;&#36827;&#34892;&#24187;&#35273;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#37325;&#20889;&#22686;&#24378;&#25991;&#26412;&#36136;&#37327;&#65292;&#20351;&#29992;LLMs&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#30340;&#26377;&#25928;&#36873;&#25321;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03940</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#38590;&#35270;&#22270;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#23545;&#22270;&#20687;&#36755;&#20837;&#30340;&#19981;&#21516;&#8220;&#35270;&#22270;&#8221;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#32780;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#23545;&#27492;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#22686;&#24378;&#27969;&#31243;&#20013;&#30340;&#25805;&#20316;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#65292;&#22914;&#38543;&#26426;&#35009;&#21098;&#25110;&#39068;&#33394;&#25197;&#26354;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#35270;&#22270;&#29983;&#25104;&#21450;&#20854;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22312;&#30446;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#20294;&#24378;&#22823;&#30340;&#8220;&#38590;&#35270;&#22270;&#36873;&#25321;&#8221;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#35270;&#22270;&#29983;&#25104;&#25193;&#23637;&#21040;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31574;&#30053;&#21253;&#25324;&#20197;&#19979;&#36845;&#20195;&#27493;&#39588;&#65306;1&#65289;&#38543;&#26426;&#36873;&#25321;&#22810;&#20010;&#35270;&#22270;&#24182;&#21019;&#24314;&#20004;&#20010;&#35270;&#22270;&#30340;&#37197;&#23545;&#65292;2&#65289;&#36827;&#34892;&#21521;&#21069;&#20256;&#36882;...
&lt;/p&gt;
&lt;p&gt;
Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.03925</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#23494;&#20999;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#21270;&#27169;&#22411;&#65292;MTL&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#38750;MTL&#27169;&#22411;&#12290;&#23613;&#31649;MTL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;MTL&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#19968;&#32500;&#21367;&#31215;&#30340;TSC&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#65292;TSC&#27169;&#22411;&#30340;&#24615;&#33021;&#23454;&#38469;&#19978;&#20250;&#19979;&#38477;&#12290;&#36890;&#36807;&#23558;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#20989;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#30475;&#20986;&#20302;&#19979;&#30340;&#32467;&#26524;&#26159;&#30001;&#20110;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#19982;&#31995;&#32479;&#23454;&#26102;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#24230;&#37327;&#21644;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#28385;&#36275;&#29992;&#25143;&#20174;&#22810;&#20010;&#39046;&#22495;&#33719;&#21462;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03919</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Efficient Content-based Time Series Retrieval System. (arXiv:2310.03919v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#19982;&#31995;&#32479;&#23454;&#26102;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#24230;&#37327;&#21644;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#28385;&#36275;&#29992;&#25143;&#20174;&#22810;&#20010;&#39046;&#22495;&#33719;&#21462;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;(CTSR)&#31995;&#32479;&#26159;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#19982;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;(&#22914;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#21046;&#36896;&#19994;)&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20132;&#20114;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#24819;&#35201;&#20102;&#35299;&#26102;&#38388;&#24207;&#21015;&#30340;&#26469;&#28304;&#65292;&#21487;&#20197;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#26597;&#35810;&#25552;&#20132;&#32473;CTSR&#31995;&#32479;&#65292;&#24182;&#26816;&#32034;&#19982;&#20043;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#21015;&#34920;&#21450;&#30456;&#20851;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20998;&#26512;&#26816;&#32034;&#21040;&#30340;&#20803;&#25968;&#25454;&#65292;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#26102;&#38388;&#24207;&#21015;&#26469;&#28304;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#30001;&#20110;CTSR&#31995;&#32479;&#38656;&#35201;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20010;&#39640;&#23481;&#37327;&#27169;&#22411;&#26469;&#26377;&#25928;&#22320;&#24230;&#37327;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#27492;&#22806;&#65292;CTSR&#31995;&#32479;&#20869;&#30340;&#27169;&#22411;&#36824;&#38656;&#35201;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;&#30456;&#20284;&#24230;&#24471;&#20998;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#22312;&#23454;&#26102;&#20132;&#20114;&#20013;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;CTSR&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#27169;&#22411;&#65292;&#21516;&#26102;&#20173;&#28982;&#25552;&#20379;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Content-based Time Series Retrieval (CTSR) system is an information retrieval system for users to interact with time series emerged from multiple domains, such as finance, healthcare, and manufacturing. For example, users seeking to learn more about the source of a time series can submit the time series as a query to the CTSR system and retrieve a list of relevant time series with associated metadata. By analyzing the retrieved metadata, users can gather more information about the source of the time series. Because the CTSR system is required to work with time series data from diverse domains, it needs a high-capacity model to effectively measure the similarity between different time series. On top of that, the model within the CTSR system has to compute the similarity scores in an efficient manner as the users interact with the system in real-time. In this paper, we propose an effective and efficient CTSR model that outperforms alternative models, while still providing reasonable in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#26469;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03916</link><description>&lt;p&gt;
&#26397;&#30528;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#26469;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#20351;&#29992;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#65292;&#23548;&#33268;&#23545;&#20854;&#20182;&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#30340;&#30693;&#35782;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#26469;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;UCR&#23384;&#26723;&#65292;&#24182;&#35780;&#20272;&#20102;&#22235;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has mostly focused on models pre-trained solely on data from a single domain, resulting in a lack of knowledge about other types of time series. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#26681;&#21270;&#36923;&#36753;&#30446;&#26631;&#20989;&#25968;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#23567;&#33539;&#25968;&#35299;&#30456;&#21516;&#30340;&#26368;&#23567;&#21270;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26368;&#20248;&#32467;&#26524;&#30340;&#36798;&#25104;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03890</link><description>&lt;p&gt;
&#22522;&#20110;&#26681;&#21270;&#36923;&#36753;&#30446;&#26631;&#20989;&#25968;&#30340;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#26681;&#21270;&#36923;&#36753;&#30446;&#26631;&#20989;&#25968;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#23567;&#33539;&#25968;&#35299;&#30456;&#21516;&#30340;&#26368;&#23567;&#21270;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26368;&#20248;&#32467;&#26524;&#30340;&#36798;&#25104;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#20351;&#29992;&#22522;&#20110;&#20132;&#21449;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30693;&#36947;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;&#26799;&#24230;&#19979;&#38477;&#65289;&#30340;&#34892;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#21487;&#20998;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#21363;&#20351;&#22312;&#26368;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#36895;&#24230;&#21462;&#20915;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;1&#65289;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#65292;&#21644;&#65288;2&#65289;&#25968;&#25454;&#38598;&#30340;&#21487;&#20998;&#24615;&#12290;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#39044;&#22788;&#29702;&#25216;&#26415;&#65288;&#22914;&#36229;&#21442;&#25968;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#31561;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20998;&#24615;&#26159;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#20998;&#24067;&#22266;&#26377;&#30340;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36923;&#36753;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#33267;&#23569;&#21644;&#36923;&#36753;&#25439;&#22833;&#19968;&#26679;&#20005;&#26684;&#12290;&#36825;&#20123;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#28857;&#19982;&#26368;&#23567;&#33539;&#25968;&#35299;&#30340;&#26368;&#23567;&#21270;&#28857;&#19968;&#33268;&#65292;&#20197;&#23613;&#21487;&#33021;&#20351;&#29992;&#12290;&#36825;&#20010;&#25512;&#23548;&#30340;&#20989;&#25968;&#30340;&#20005;&#26684;&#20984;&#24615;&#21487;&#20197;&#25193;&#23637;&#21040;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techniques such as over-parametrization, data augmentation etc., separability is an intrinsic quantity of the data distribution under consideration. We focus on the landscape design of the logistic function and derive a novel sequence of {\em strictly} convex functions that are at least as strict as logistic loss. The minimizers of these functions coincide with those of the minimum norm solution wherever possible. The strict convexity of the derived function can be extended to finetune state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#23567;&#25209;&#37327;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#21457;&#29616;&#23558;&#25209;&#37327;&#22823;&#23567;&#20943;&#23567;&#21487;&#20197;&#20135;&#29983;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.03882</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Small batch deep reinforcement learning. (arXiv:2310.03882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03882
&lt;/p&gt;
&lt;p&gt;
&#23567;&#25209;&#37327;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#21457;&#29616;&#23558;&#25209;&#37327;&#22823;&#23567;&#20943;&#23567;&#21487;&#20197;&#20135;&#29983;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25209;&#37327;&#22823;&#23567;&#21442;&#25968;&#25351;&#23450;&#27599;&#27425;&#26799;&#24230;&#26356;&#26032;&#35201;&#37319;&#26679;&#30340;&#36716;&#25442;&#25968;&#37327;&#12290;&#34429;&#28982;&#36825;&#20010;&#20540;&#23545;&#23398;&#20064;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#22312;&#25552;&#20986;&#26032;&#31639;&#27861;&#26102;&#19981;&#20250;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25209;&#37327;&#22823;&#23567;&#20943;&#23567;&#21487;&#20197;&#20135;&#29983;&#22810;&#20010;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65307;&#36825;&#19968;&#28857;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36890;&#24120;&#20542;&#21521;&#20110;&#20351;&#29992;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#35777;&#20998;&#26512;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests {\em reducing} the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaKERMap&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;transformer&#20013;&#65292;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#24182;&#24212;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.03840</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#32467;&#26500;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Contextualized Structural Self-supervised Learning for Ontology Matching. (arXiv:2310.03840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaKERMap&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;transformer&#20013;&#65292;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#24182;&#24212;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#28041;&#21450;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#30693;&#35782;&#22270;&#20013;&#35782;&#21035;&#27010;&#24565;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20316;&#20026;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#30693;&#35782;&#22270;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26368;&#36817;&#28145;&#24230;OM&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#20102;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;OM&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#21442;&#32771;&#23545;&#40784;&#12289;&#36816;&#34892;&#26102;&#24310;&#36831;&#21644;&#26410;&#24320;&#21457;&#30340;&#20869;&#37096;&#19981;&#21516;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#21517;&#20026;LaKERMap&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#27010;&#24565;&#30340;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#23558;&#38544;&#24335;&#30693;&#35782;&#25972;&#21512;&#21040;transformer&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#23616;&#37096;&#21644;&#20840;&#23616;&#20132;&#20114;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Bio-ML&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology matching (OM) entails the identification of semantic relationships between concepts within two or more knowledge graphs (KGs) and serves as a critical step in integrating KGs from various sources. Recent advancements in deep OM models have harnessed the power of transformer-based language models and the advantages of knowledge graph embedding. Nevertheless, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML 
&lt;/p&gt;</description></item><item><title>ECAvg&#26159;&#19968;&#31181;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#22343;&#26435;&#37325;&#23454;&#29616;&#36793;&#32536;&#35774;&#22791;&#21644;&#20113;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#21327;&#20316;&#19982;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.03823</link><description>&lt;p&gt;
ECAvg&#65306;&#19968;&#31181;&#20351;&#29992;&#24179;&#22343;&#26435;&#37325;&#30340;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights. (arXiv:2310.03823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03823
&lt;/p&gt;
&lt;p&gt;
ECAvg&#26159;&#19968;&#31181;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#22343;&#26435;&#37325;&#23454;&#29616;&#36793;&#32536;&#35774;&#22791;&#21644;&#20113;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#21327;&#20316;&#19982;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#19982;&#20113;&#30340;&#32467;&#21512;&#25552;&#20379;&#20102;&#20004;&#31867;&#35774;&#22791;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#65292;&#24444;&#27492;&#20043;&#38388;&#20114;&#34917;&#20248;&#21183;&#12290;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#21368;&#36733;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#20174;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;&#20016;&#23500;&#35745;&#31639;&#33021;&#21147;&#20013;&#33719;&#30410;&#12290;&#21516;&#26102;&#65292;&#36793;&#32536;&#35774;&#22791;&#21487;&#20197;&#21033;&#29992;&#20854;&#25509;&#36817;&#25968;&#25454;&#28304;&#30340;&#20248;&#21183;&#65292;&#22312;&#25968;&#25454;&#19978;&#25191;&#34892;&#36739;&#23569;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECAvg&#30340;&#21327;&#21516;&#36793;&#32536;-&#20113;&#33539;&#24335;&#65292;&#22312;&#35813;&#33539;&#24335;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#22312;&#21508;&#33258;&#30340;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#26381;&#21153;&#22120;&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20026;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#36793;&#32536;&#35774;&#22791;&#30340;&#32452;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20840;&#23616;&#27169;&#22411;&#30340;&#26435;&#37325;&#26356;&#26032;&#26412;&#22320;&#65288;&#36793;&#32536;&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;MobileNetV2&#23454;&#29616;&#20102;CIFAR-10&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;ResN&#65288;&#24453;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
The use of edge devices together with cloud provides a collaborative relationship between both classes of devices where one complements the shortcomings of the other. Resource-constraint edge devices can benefit from the abundant computing power provided by servers by offloading computationally intensive tasks to the server. Meanwhile, edge devices can leverage their close proximity to the data source to perform less computationally intensive tasks on the data. In this paper, we propose a collaborative edge-cloud paradigm called ECAvg in which edge devices pre-train local models on their respective datasets and transfer the models to the server for fine-tuning. The server averages the pre-trained weights into a global model, which is fine-tuned on the combined data from the various edge devices. The local (edge) models are then updated with the weights of the global (server) model. We implement a CIFAR-10 classification task using MobileNetV2, a CIFAR-100 classification task using ResN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03813</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#65306;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;
&lt;/p&gt;
&lt;p&gt;
Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating. (arXiv:2310.03813v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20919;&#21551;&#21160;&#25414;&#32465;&#65311;&#25414;&#32465;&#25512;&#33616;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26032;&#24314;&#25414;&#32465;&#19981;&#26029;&#20986;&#29616;&#20197;&#28385;&#36275;&#21508;&#31181;&#33829;&#38144;&#30446;&#30340;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#28041;&#21450;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#21382;&#21490;&#20449;&#24687;&#65292;&#21363;&#20351;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#25414;&#32465;&#20063;&#26159;&#22914;&#27492;&#65292;&#26080;&#27861;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#39640;&#24230;&#20542;&#26012;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoHeat&#65288;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;CoHeat&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#20851;&#32852;&#20449;&#24687;&#26469;&#20272;&#35745;&#29992;&#25143;&#19982;&#25414;&#32465;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#39640;&#24230;&#20542;&#26012;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;CoHeat&#36824;&#36890;&#36807;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#32858;&#21512;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#21033;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#20449;&#24687;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03780</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20154;&#24037;&#23548;&#24072;&#24335;&#32534;&#31243;&#21453;&#39304;: &#21033;&#29992;GPT-4&#23548;&#24072;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#21644;GPT-3.5&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. (arXiv:2310.03780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#21033;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#20449;&#24687;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25552;&#20379;&#20154;&#24037;&#23548;&#24072;&#24335;&#32534;&#31243;&#25552;&#31034;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#35299;&#20915;&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#24037;&#20316;&#34429;&#28982;&#23545;&#21508;&#31181;&#21453;&#39304;&#29983;&#25104;&#22330;&#26223;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#25972;&#20307;&#36136;&#37327;&#20173;&#36828;&#19981;&#21450;&#20154;&#24037;&#23548;&#24072;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#20934;&#22791;&#22909;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25237;&#20837;&#20351;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#39640;&#36136;&#37327;&#32534;&#31243;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21517;&#20026;GPT4Hints-GPT3.5Val&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#31526;&#21495;&#20449;&#24687;&#21644;&#25552;&#31034;&#20013;&#30340;&#20462;&#22797;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint 
&lt;/p&gt;</description></item><item><title>HandMeThat&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.03779</link><description>&lt;p&gt;
HandMeThat: &#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
HandMeThat: Human-Robot Communication in Physical and Social Environments. (arXiv:2310.03779v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03779
&lt;/p&gt;
&lt;p&gt;
HandMeThat&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HandMeThat&#65292;&#19968;&#20010;&#29992;&#20110;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#25351;&#20196;&#29702;&#35299;&#21644;&#36981;&#24490;&#30340;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20851;&#27880;&#35821;&#35328;&#20381;&#23384;&#21644;&#35268;&#21010;&#19981;&#21516;&#65292;HandMeThat&#32771;&#34385;&#20102;&#22522;&#20110;&#29289;&#29702;&#65288;&#29289;&#20307;&#29366;&#24577;&#21644;&#20851;&#31995;&#65289;&#21644;&#31038;&#20132;&#65288;&#20154;&#31867;&#34892;&#21160;&#21644;&#30446;&#26631;&#65289;&#20449;&#24687;&#30340;&#21547;&#26377;&#27495;&#20041;&#30340;&#20154;&#31867;&#25351;&#20196;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HandMeThat&#21253;&#21547;&#20102;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;&#22312;&#27599;&#20010;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#39318;&#20808;&#35266;&#23519;&#21040;&#20154;&#31867;&#34892;&#21160;&#30340;&#36712;&#36857;&#20197;&#36798;&#21040;&#20869;&#37096;&#30446;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#26426;&#22120;&#20154;&#25509;&#25910;&#21040;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#25351;&#20196;&#37319;&#21462;&#34892;&#21160;&#20197;&#23436;&#25104;&#23376;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#25991;&#26412;&#30028;&#38754;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#25991;&#26412;&#21629;&#20196;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;HandMeThat&#19978;&#30340;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HandMeThat&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#26126;&#20854;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting signif
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#26469;&#28040;&#38500;&#38750;&#20449;&#24687;&#24615;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#22788;&#29702;&#26377;&#22122;&#22768;&#30340;&#36830;&#32493;&#29305;&#24449;&#21644;&#20855;&#26377;&#22823;&#37327;&#21807;&#19968;&#20540;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ACM RecSys Challenge 2023&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03778</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#30340;&#36731;&#37327;&#32423;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lightweight Boosting Models for User Response Prediction Using Adversarial Validation. (arXiv:2310.03778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#26469;&#28040;&#38500;&#38750;&#20449;&#24687;&#24615;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#22788;&#29702;&#26377;&#22122;&#22768;&#30340;&#36830;&#32493;&#29305;&#24449;&#21644;&#20855;&#26377;&#22823;&#37327;&#21807;&#19968;&#20540;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ACM RecSys Challenge 2023&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ShareChat&#32452;&#32455;&#30340;ACM RecSys Challenge 2023&#26088;&#22312;&#39044;&#27979;&#24212;&#29992;&#34987;&#23433;&#35013;&#30340;&#27010;&#29575;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#36825;&#20010;&#25361;&#25112;&#30340;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#35813;&#20219;&#21153;&#23450;&#20041;&#20026;&#29992;&#25143;&#21709;&#24212;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#30340;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65306;1&#65289;&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#65292;&#26377;&#25928;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#28040;&#38500;&#38750;&#20449;&#24687;&#24615;&#29305;&#24449;&#65307;2&#65289;&#20026;&#20102;&#22788;&#29702;&#26377;&#22122;&#22768;&#30340;&#36830;&#32493;&#29305;&#24449;&#21644;&#20855;&#26377;&#22823;&#37327;&#21807;&#19968;&#20540;&#30340;&#20998;&#31867;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#65307;3&#65289;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;LightGBM&#27169;&#22411;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#29616;&#24471;&#24456;&#22909;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#20061;&#21517;&#65292;&#26368;&#32456;&#25490;&#34892;&#27036;&#24471;&#20998;&#20026;6.059065&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#65306;https://github.com/choco9966/recsys-challenge-2023&#12290;
&lt;/p&gt;
&lt;p&gt;
The ACM RecSys Challenge 2023, organized by ShareChat, aims to predict the probability of the app being installed. This paper describes the lightweight solution to this challenge. We formulate the task as a user response prediction task. For rapid prototyping for the task, we propose a lightweight solution including the following steps: 1) using adversarial validation, we effectively eliminate uninformative features from a dataset; 2) to address noisy continuous features and categorical features with a large number of unique values, we employ feature engineering techniques.; 3) we leverage Gradient Boosted Decision Trees (GBDT) for their exceptional performance and scalability. The experiments show that a single LightGBM model, without additional ensembling, performs quite well. Our team achieved ninth place in the challenge with the final leaderboard score of 6.059065. Code for our approach can be found here: https://github.com/choco9966/recsys-challenge-2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20174;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#65292;&#20943;&#23569;&#25968;&#25454;&#38656;&#27714;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#23454;&#29992;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#65292;&#20445;&#30041;&#20808;&#21069;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#21033;&#29992;&#20854;&#20013;&#26377;&#20215;&#20540;&#30340;&#37096;&#20998;&#21487;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03770</link><description>&lt;p&gt;
&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#65306;&#29992;&#36873;&#25321;&#24615;&#30693;&#35782;&#20256;&#36882;&#22686;&#24378;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer. (arXiv:2310.03770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20174;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#65292;&#20943;&#23569;&#25968;&#25454;&#38656;&#27714;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#23454;&#29992;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#65292;&#20445;&#30041;&#20808;&#21069;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#21033;&#29992;&#20854;&#20013;&#26377;&#20215;&#20540;&#30340;&#37096;&#20998;&#21487;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#21487;&#33021;&#38754;&#20020;&#23545;&#25968;&#25454;&#30340;&#19981;&#26029;&#38656;&#27714;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#24182;&#19988;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#12289;&#20449;&#24687;&#31232;&#32570;&#65292;&#23545;&#24037;&#31243;&#24212;&#29992;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#24182;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31867;&#20284;&#20110;&#20154;&#31867;&#36873;&#25321;&#24615;&#20351;&#29992;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#32780;&#24573;&#30053;&#26080;&#29992;&#20449;&#24687;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#26377;&#36873;&#25321;&#24615;&#22320;&#20174;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#20808;&#21069;&#27169;&#22411;&#20013;&#31579;&#36873;&#20986;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#36716;&#25442;&#26102;&#38388;&#21644;&#36739;&#23567;&#35757;&#32451;&#38598;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#20256;&#36755;&#12289;&#37325;&#21147;&#39537;&#21160;&#27969;&#21160;&#21644;&#36229;&#24377;&#26448;&#26009;&#30340;&#26377;&#38480;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20445;&#30041;&#20808;&#21069;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#21033;&#29992;&#20854;&#20013;&#30340;&#26377;&#20215;&#20540;&#37096;&#20998;&#21487;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven modeling can suffer from a constant demand for data, leading to reduced accuracy and impractical for engineering applications due to the high cost and scarcity of information. To address this challenge, we propose a progressive reduced order modeling framework that minimizes data cravings and enhances data-driven modeling's practicality. Our approach selectively transfers knowledge from previously trained models through gates, similar to how humans selectively use valuable knowledge while ignoring unuseful information. By filtering relevant information from previous models, we can create a surrogate model with minimal turnaround time and a smaller training set that can still achieve high accuracy. We have tested our framework in several cases, including transport in porous media, gravity-driven flow, and finite deformation in hyperelastic materials. Our results illustrate that retaining information from previous models and utilizing a valuable portion of that knowledge can 
&lt;/p&gt;</description></item><item><title>LBD&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#20013;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21307;&#23398;&#26415;&#35821;&#20043;&#38388;&#30340;&#26032;&#20851;&#32852;&#26469;&#32553;&#30701;&#21457;&#29616;&#28508;&#22312;&#20851;&#32852;&#30340;&#26102;&#38388;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03766</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#65306;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#20013;&#23454;&#29616;&#20551;&#35774;&#29983;&#25104;&#21644;&#30693;&#35782;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining. (arXiv:2310.03766v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03766
&lt;/p&gt;
&lt;p&gt;
LBD&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#20013;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21307;&#23398;&#26415;&#35821;&#20043;&#38388;&#30340;&#26032;&#20851;&#32852;&#26469;&#32553;&#30701;&#21457;&#29616;&#28508;&#22312;&#20851;&#32852;&#30340;&#26102;&#38388;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20197;&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#24418;&#24335;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#25991;&#26412;&#25366;&#25496;&#24037;&#20855;&#21644;&#26041;&#27861;&#20195;&#34920;&#20102;&#20174;&#36825;&#20123;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#38544;&#34255;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#20013;&#65292;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26159;&#33258;&#21160;&#21457;&#29616;&#19981;&#21516;&#25991;&#29486;&#38598;&#20013;&#25552;&#21040;&#30340;&#21307;&#23398;&#26415;&#35821;&#20043;&#38388;&#30340;&#26032;&#20851;&#32852;&#30340;&#36807;&#31243;&#12290;LBD&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25104;&#21151;&#32553;&#30701;&#22312;&#22823;&#37327;&#31185;&#23398;&#25991;&#29486;&#20013;&#38544;&#34255;&#30340;&#28508;&#22312;&#20851;&#32852;&#30340;&#21457;&#29616;&#26102;&#38388;&#12290;&#35813;&#36807;&#31243;&#20391;&#37325;&#20110;&#20026;&#30142;&#30149;&#25110;&#30151;&#29366;&#31561;&#21307;&#23398;&#26415;&#35821;&#21019;&#24314;&#27010;&#24565;&#26723;&#26696;&#65292;&#24182;&#26681;&#25454;&#20849;&#20139;&#26723;&#26696;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#23558;&#20854;&#19982;&#33647;&#29289;&#21644;&#27835;&#30103;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#31181;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#22312;1989&#24180;&#24341;&#20837;&#21518;&#20173;&#28982;&#26159;&#25991;&#26412;&#25366;&#25496;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical knowledge is growing in an astounding pace with a majority of this knowledge is represented as scientific publications. Text mining tools and methods represents automatic approaches for extracting hidden patterns and trends from this semi structured and unstructured data. In Biomedical Text mining, Literature Based Discovery (LBD) is the process of automatically discovering novel associations between medical terms otherwise mentioned in disjoint literature sets. LBD approaches proven to be successfully reducing the discovery time of potential associations that are hidden in the vast amount of scientific literature. The process focuses on creating concept profiles for medical terms such as a disease or symptom and connecting it with a drug and treatment based on the statistical significance of the shared profiles. This knowledge discovery approach introduced in 1989 still remains as a core task in text mining. Currently the ABC principle based two approaches namely open disco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#30456;&#20301;&#19981;&#25935;&#24863;&#36317;&#31163;&#22312;LoS&#20449;&#36947;&#24314;&#27169;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2310.03762</link><description>&lt;p&gt;
&#20248;&#21270;LoS&#20449;&#36947;&#24314;&#27169;&#20013;&#30340;&#22810;&#36733;&#27874;&#22810;&#22825;&#32447;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Optimizing Multicarrier Multiantenna Systems for LoS Channel Charting. (arXiv:2310.03762v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#30456;&#20301;&#19981;&#25935;&#24863;&#36317;&#31163;&#22312;LoS&#20449;&#36947;&#24314;&#27169;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#36947;&#24314;&#27169;&#36890;&#36807;&#23398;&#20064;&#23558;&#22810;&#36733;&#27874;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#22522;&#20110;&#23548;&#39057;&#20449;&#36947;&#20272;&#35745;&#24471;&#21040;&#30340;&#21407;&#22987;&#20449;&#36947;&#35266;&#27979;&#31354;&#38388;&#19982;&#20302;&#32500;&#31354;&#38388;&#36827;&#34892;&#26144;&#23556;&#65292;&#20854;&#20013;&#25509;&#36817;&#30340;&#28857;&#23545;&#24212;&#20110;&#31354;&#38388;&#19978;&#38752;&#36817;&#30340;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#30340;&#20449;&#36947;&#12290;&#22312;&#23398;&#20064;&#36825;&#31181;&#26144;&#23556;&#30340;&#19981;&#21516;&#26041;&#27861;&#20013;&#65292;&#26377;&#20123;&#20381;&#36182;&#20110;&#20449;&#36947;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#36825;&#26679;&#30340;&#36317;&#31163;&#24212;&#35813;&#21487;&#38752;&#22320;&#21453;&#26144;UE&#30340;&#23616;&#37096;&#31354;&#38388;&#37051;&#22495;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#30456;&#20301;&#19981;&#25935;&#24863;&#65288;PI&#65289;&#36317;&#31163;&#22312;&#36825;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#20854;&#21608;&#26399;&#24615;&#21644;&#25391;&#33633;&#24615;&#32780;&#20135;&#29983;&#27495;&#20041;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20351;&#36828;&#31163;&#24444;&#27492;&#30340;&#29992;&#25143;&#30475;&#36215;&#26469;&#26356;&#25509;&#36817;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#35813;&#36317;&#31163;&#21450;&#20854;&#23616;&#38480;&#24615;&#30340;&#28145;&#20837;&#29702;&#35770;&#20998;&#26512;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#24471;&#20986;&#20102;&#35774;&#35745;&#33021;&#22815;&#23398;&#20064;&#20248;&#36136;&#20449;&#36947;&#22270;&#30340;&#31995;&#32479;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
Channel charting (CC) consists in learning a mapping between the space of raw channel observations, made available from pilot-based channel estimation in multicarrier multiantenna system, and a low-dimensional space where close points correspond to channels of user equipments (UEs) close spatially. Among the different methods of learning this mapping, some rely on a distance measure between channel vectors. Such a distance should reliably reflect the local spatial neighborhoods of the UEs. The recently proposed phase-insensitive (PI) distance exhibits good properties in this regards, but suffers from ambiguities due to both its periodic and oscillatory aspects, making users far away from each other appear closer in some cases. In this paper, a thorough theoretical analysis of the said distance and its limitations is provided, giving insights on how they can be mitigated. Guidelines for designing systems capable of learning quality charts are consequently derived. Experimental validatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29305;&#24449;&#25552;&#21462;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27963;&#21160;&#35782;&#21035;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#26426;&#21046;&#21644;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03760</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29305;&#24449;&#25552;&#21462;&#35774;&#35745;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Deep Neural Network Architecture and Feature Extraction Designs for Sensor-based Human Activity Recognition. (arXiv:2310.03760v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29305;&#24449;&#25552;&#21462;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27963;&#21160;&#35782;&#21035;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#26426;&#21046;&#21644;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#20013;&#20256;&#24863;&#22120;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#23454;&#29616;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#22788;&#29702;&#21644;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#30456;&#21453;&#65292;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#35777;&#26126;&#26377;&#25928;&#24615;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#35768;&#22810;&#28145;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#35757;&#32451;&#26426;&#21046;&#65288;&#22914;&#23545;&#27604;&#23398;&#20064;&#65289;&#21644;&#20174;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#21508;&#31181;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive ubiquitous availability of sensors in smart devices and the Internet of Things (IoT) has opened up the possibilities for implementing sensor-based activity recognition. As opposed to traditional sensor time-series processing and hand-engineered feature extraction, in light of deep learning's proven effectiveness across various domains, numerous deep methods have been explored to tackle the challenges in activity recognition, outperforming the traditional signal processing and traditional machine learning approaches. In this work, by performing extensive experimental studies on two human activity recognition datasets, we investigate the performance of common deep learning and machine learning approaches as well as different training mechanisms (such as contrastive learning), and various feature representations extracted from the sensor time-series data and measure their effectiveness for the human activity recognition task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;1D-CycleGAN&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#27597;&#20307;&#24515;&#30005;&#22270;&#20013;&#25552;&#21462;&#32974;&#20799;&#24515;&#30005;&#22270;&#65292;&#20445;&#25345;&#20854;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03759</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20445;&#25345;&#32974;&#20799;&#24515;&#30005;&#22270;&#24418;&#24577;&#30340;&#20174;&#27597;&#20307;&#24515;&#30005;&#22270;&#20013;&#25552;&#21462;&#32974;&#20799;&#24515;&#30005;&#22270;&#30340;1D-CycleGAN
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN. (arXiv:2310.03759v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;1D-CycleGAN&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#27597;&#20307;&#24515;&#30005;&#22270;&#20013;&#25552;&#21462;&#32974;&#20799;&#24515;&#30005;&#22270;&#65292;&#20445;&#25345;&#20854;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24335;&#30340;&#32974;&#20799;&#24515;&#30005;&#22270;&#65288;fECG&#65289;&#30417;&#27979;&#32974;&#20799;&#24515;&#33039;&#30340;&#30005;&#33033;&#20914;&#21487;&#20197;&#36731;&#26494;&#26816;&#27979;&#21457;&#32946;&#20013;&#24515;&#33039;&#30340;&#24322;&#24120;&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#23156;&#20799;&#27515;&#20129;&#29575;&#21644;&#20135;&#21518;&#24182;&#21457;&#30151;&#12290;&#30001;&#20110;&#27597;&#20307;&#21644;&#32974;&#20799;R&#23792;&#30340;&#37325;&#21472;&#65292;fECG&#20449;&#21495;&#30340;&#20302;&#24133;&#24230;&#65292;&#20197;&#21450;&#31995;&#32479;&#21644;&#29615;&#22659;&#22122;&#22768;&#65292;&#20256;&#32479;&#30340;&#20449;&#21495;&#25552;&#21462;&#26041;&#27861;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;fECG&#12290;&#34429;&#28982;&#19968;&#20123;&#25216;&#26415;&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#30340;QRS&#27874;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;ECG&#30340;&#20854;&#20182;&#37325;&#35201;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;1D CycleGAN&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#39044;&#22788;&#29702;&#21644;&#36866;&#24403;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;mECG&#20449;&#21495;&#37325;&#26500;fECG&#20449;&#21495;&#65292;&#24182;&#20445;&#25345;&#20854;&#24418;&#24577;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#36890;&#36807;&#32467;&#21512;Physionet&#30340;&#20004;&#20010;&#21487;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;"Abdominal and Direct Fetal ECG Database"&#21644;"Fetal electrocardiograms, direct an"
&lt;/p&gt;
&lt;p&gt;
Monitoring the electrical pulse of fetal heart through a non-invasive fetal electrocardiogram (fECG) can easily detect abnormalities in the developing heart to significantly reduce the infant mortality rate and post-natal complications. Due to the overlapping of maternal and fetal R-peaks, the low amplitude of the fECG, systematic and ambient noises, typical signal extraction methods, such as adaptive filters, independent component analysis, empirical mode decomposition, etc., are unable to produce satisfactory fECG. While some techniques can produce accurate QRS waves, they often ignore other important aspects of the ECG. Our approach, which is based on 1D CycleGAN, can reconstruct the fECG signal from the mECG signal while maintaining the morphology due to extensive preprocessing and appropriate framework. The performance of our solution was evaluated by combining two available datasets from Physionet, "Abdominal and Direct Fetal ECG Database" and "Fetal electrocardiograms, direct an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#26127;&#36855;&#24739;&#32773;&#30340;&#20302;&#30149;&#29702;&#24615;&#31070;&#32463;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#26497;EEG&#35760;&#24405;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.03756</link><description>&lt;p&gt;
&#33258;&#25105;&#21644;&#36328;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#19979;&#65292;&#29992;&#20110;&#26127;&#36855;&#24739;&#32773;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#23500;&#36139;&#39044;&#27979;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism. (arXiv:2310.03756v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#26127;&#36855;&#24739;&#32773;&#30340;&#20302;&#30149;&#29702;&#24615;&#31070;&#32463;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#26497;EEG&#35760;&#24405;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21452;&#26497;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#23545;&#39044;&#27979;&#20302;&#30149;&#29702;&#24615;&#31070;&#32463;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#37319;&#29992;&#22238;&#39038;&#24615;&#35774;&#35745;&#21644;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#23454;&#29616;&#39640;&#29305;&#24322;&#24615;&#65288;&#21363;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#65289;&#21644;&#38477;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;&lt; 0.05&#65289;&#12290;&#36873;&#21462;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#20010;&#23567;&#26102;&#20869;&#30340;5&#20998;&#38047;&#27573;&#33853;&#30340;18&#20010;&#21452;&#26497;&#36890;&#36947;&#23545;&#30340;&#22810;&#36890;&#36947;EEG&#38453;&#21015;&#12290;&#20026;&#20102;&#30830;&#23450;&#32467;&#26524;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#29305;&#24449;&#32534;&#30721;&#22120;&#19982;1-D&#21367;&#31215;&#23618;&#12289;&#21487;&#23398;&#20064;&#30340;&#20301;&#32622;&#32534;&#30721;&#12289;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#19978;&#19979;&#25991;&#32593;&#32476;&#65292;&#20197;&#21450;&#22238;&#24402;&#22120;&#21644;&#20998;&#31867;&#22120;&#27169;&#22359;&#30340;&#32452;&#21512;&#12290;&#29305;&#24449;&#32534;&#30721;&#22120;&#25552;&#21462;&#23616;&#37096;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#32780;&#21518;&#32493;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#27880;&#24847;&#26426;&#21046;&#35797;&#22270;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22242;&#38431;&#25552;&#20986;&#30340;OUS IVS&#26694;&#26550;&#65292;&#22312;&#39564;&#35777;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the predictive potential of bipolar electroencephalogram (EEG) recordings towards efficient prediction of poor neurological outcomes. A retrospective design using a hybrid deep learning approach is utilized to optimize an objective function aiming for high specificity, i.e., true positive rate (TPR) with reduced false positives (&lt; 0.05). A multi-channel EEG array of 18 bipolar channel pairs from a randomly selected 5-minute segment in an hour is kept. In order to determine the outcome prediction, a combination of a feature encoder with 1-D convolutional layers, learnable position encoding, a context network with attention mechanisms, and finally, a regressor and classifier blocks are used. The feature encoder extricates local temporal and spatial features, while the following position encoding and attention mechanisms attempt to capture global temporal dependencies. Results: The proposed framework by our team, OUS IVS, when validated on the challenge hidden valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMGTFNet&#30340;&#22522;&#20110;&#27169;&#31946;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#65292;&#21487;&#20197;&#20934;&#30830;&#20998;&#31867;&#21508;&#31181;&#25163;&#21183;&#65292;&#26080;&#38656;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25110;&#32593;&#32476;&#21442;&#25968;&#30340;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2310.03754</link><description>&lt;p&gt;
EMGTFNet&#65306;&#29992;&#20110;&#35299;&#30721;&#19978;&#32930;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#27169;&#31946;&#35270;&#35273;&#21464;&#21387;&#22120;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EMGTFNet: Fuzzy Vision Transformer to decode Upperlimb sEMG signals for Hand Gestures Recognition. (arXiv:2310.03754v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMGTFNet&#30340;&#22522;&#20110;&#27169;&#31946;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#65292;&#21487;&#20197;&#20934;&#30830;&#20998;&#31867;&#21508;&#31181;&#25163;&#21183;&#65292;&#26080;&#38656;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25110;&#32593;&#32476;&#21442;&#25968;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#30005;&#25511;&#21046;&#26159;&#30005;&#32908;&#22270;&#30340;&#19968;&#20010;&#26085;&#30410;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20223;&#29983;&#20551;&#32930;&#30340;&#25163;&#21183;&#35782;&#21035;&#31561;&#24212;&#29992;&#20013;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27169;&#24335;&#35782;&#21035;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#31232;&#30095;&#30340;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38543;&#26426;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#29305;&#24615;&#65292;&#20256;&#32479;&#27169;&#22411;&#26080;&#27861;&#23558;&#26679;&#26412;&#25512;&#24191;&#21040;&#38750;&#20856;&#22411;&#25110;&#22122;&#22768;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;(ViT)&#21644;&#27169;&#31946;&#31070;&#32463;&#22359;(FNB)&#30340;EMGTFNet&#35774;&#35745;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;EMGTFNet&#26550;&#26500;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#31867;&#21508;&#31181;&#25163;&#21183;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12289;&#36801;&#31227;&#23398;&#20064;&#25110;&#32593;&#32476;&#21442;&#25968;&#30340;&#22823;&#24133;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myoelectric control is an area of electromyography of increasing interest nowadays, particularly in applications such as Hand Gesture Recognition (HGR) for bionic prostheses. Today's focus is on pattern recognition using Machine Learning and, more recently, Deep Learning methods. Despite achieving good results on sparse sEMG signals, the latter models typically require large datasets and training times. Furthermore, due to the nature of stochastic sEMG signals, traditional models fail to generalize samples for atypical or noisy values. In this paper, we propose the design of a Vision Transformer (ViT) based architecture with a Fuzzy Neural Block (FNB) called EMGTFNet to perform Hand Gesture Recognition from surface electromyography (sEMG) signals. The proposed EMGTFNet architecture can accurately classify a variety of hand gestures without any need for data augmentation techniques, transfer learning or a significant increase in the number of parameters in the network. The accuracy of t
&lt;/p&gt;</description></item><item><title>ECGNet&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#20174;&#21333;&#23548;&#32852;&#36755;&#20837;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#26512;&#35782;&#21035;&#20986;&#21487;&#29992;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.03753</link><description>&lt;p&gt;
ECGNet&#65306;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#20174;&#21333;&#23548;&#32852;&#36755;&#20837;&#20013;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ECGNet: A generative adversarial network (GAN) approach to the synthesis of 12-lead ECG signals from single lead inputs. (arXiv:2310.03753v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03753
&lt;/p&gt;
&lt;p&gt;
ECGNet&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#20174;&#21333;&#23548;&#32852;&#36755;&#20837;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#26512;&#35782;&#21035;&#20986;&#21487;&#29992;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#36827;&#34892;&#24515;&#30005;&#22270; (ECG) &#20449;&#21495;&#21512;&#25104;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23454;&#29616;12&#23548;&#32852;&#24515;&#30005;&#22270;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;GAN&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#20165;&#38024;&#23545;&#22810;&#23548;&#32852;&#36755;&#20837;&#36827;&#34892;&#35774;&#35745;&#65292;&#24182;&#19988;&#23578;&#26410;&#30830;&#23450;GAN&#27169;&#22411;&#25152;&#20445;&#30041;&#30340;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#29983;&#25104;&#20449;&#21495;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ECGNet&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;GAN&#26694;&#26550;&#12289;&#20855;&#26377;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#29983;&#25104;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#21028;&#21035;&#22120;&#30340;&#26041;&#27861;&#65292;&#20174;&#20219;&#20309;&#21333;&#23548;&#32852;&#36755;&#20837;&#29983;&#25104;&#23436;&#25972;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#12290;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;&#20132;&#21449;&#21644;&#33258;&#30456;&#20851;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#20449;&#21495;&#29983;&#25104;&#36807;&#31243;&#20013;&#20445;&#30041;&#30340;&#29305;&#24449;&#65292;&#21363;&#33021;&#22815;&#34920;&#24449;&#27599;&#20010;&#20449;&#21495;&#29420;&#29305;&#24615;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#24456;&#21487;&#33021;&#26159;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#27880;&#26377;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;ECGNet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiography (ECG) signal generation has been heavily explored using generative adversarial networks (GAN) because the implementation of 12-lead ECGs is not always feasible. The GAN models have achieved remarkable results in reproducing ECG signals but are only designed for multiple lead inputs and the features the GAN model preserves have not been identified-limiting the generated signals use in cardiovascular disease (CVD)-predictive models. This paper presents ECGNet which is a procedure that generates a complete set of 12-lead ECG signals from any single lead input using a GAN framework with a bidirectional long short-term memory (LSTM) generator and a convolutional neural network (CNN) discriminator. Cross and auto-correlation analysis performed on the generated signals identifies features conserved during the signal generation-i.e., features that can characterize the unique-nature of each signal and thus likely indicators of CVD. Finally, by using ECG signals annotated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCVCNet&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20998;&#26512;&#33041;&#30005;&#22270;&#20013;&#30340;&#32454;&#31890;&#24230;&#39057;&#29575;&#32467;&#26500;&#26469;&#28040;&#38500;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#30456;&#20851;&#30340;&#24178;&#25200;&#65292;&#23454;&#29616;&#20102;&#36328;&#20219;&#21153;&#21644;&#20010;&#20307;&#38388;&#30340;&#33041;&#30005;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.03749</link><description>&lt;p&gt;
SCVCNet: &#29992;&#20110;&#36328;&#20219;&#21153;&#21644;&#20010;&#20307;&#38388;&#33041;&#30005;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#30340;&#28369;&#21160;&#20132;&#21449;&#21521;&#37327;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SCVCNet: Sliding cross-vector convolution network for cross-task and inter-individual-set EEG-based cognitive workload recognition. (arXiv:2310.03749v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCVCNet&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20998;&#26512;&#33041;&#30005;&#22270;&#20013;&#30340;&#32454;&#31890;&#24230;&#39057;&#29575;&#32467;&#26500;&#26469;&#28040;&#38500;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#30456;&#20851;&#30340;&#24178;&#25200;&#65292;&#23454;&#29616;&#20102;&#36328;&#20219;&#21153;&#21644;&#20010;&#20307;&#38388;&#30340;&#33041;&#30005;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#20154;&#26426;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#19978;&#30340;&#24120;&#35265;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#27169;&#24335;&#26469;&#24212;&#29992;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCVCNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20998;&#26512;&#21151;&#29575;&#35889;&#23494;&#24230;&#20013;&#26356;&#31934;&#32454;&#30340;&#39057;&#29575;&#32467;&#26500;&#26469;&#28040;&#38500;&#33041;&#30005;&#22270;&#20013;&#30340;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#30456;&#20851;&#24178;&#25200;&#12290;SCVCNet&#21033;&#29992;&#28369;&#21160;&#20132;&#21449;&#21521;&#37327;&#21367;&#31215;&#65288;SCVC&#65289;&#25805;&#20316;&#65292;&#20854;&#20013;&#20351;&#29992;&#20195;&#34920;theta&#21644;alpha&#21151;&#29575;&#30340;&#37197;&#23545;&#36755;&#20837;&#23618;&#12290;&#36890;&#36807;&#25552;&#21462;&#26680;&#30697;&#38453;&#30340;&#20013;&#22830;&#34892;&#21644;&#21015;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#35745;&#31639;&#25351;&#23450;&#22836;&#30382;&#20301;&#32622;&#21608;&#22260;&#20004;&#20010;&#21521;&#37327;&#30340;&#21152;&#26435;&#21644;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39057;&#29575;&#28857;&#38388;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#26469;&#34701;&#21512;SCVC&#29305;&#24449;&#22270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#27169;&#22359;&#19982;&#36755;&#20986;&#36890;&#36947;&#27744;&#21270;&#21644;&#20998;&#31867;&#23618;&#32452;&#21512;&#36215;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#20026;&#20102;&#35757;&#32451;SCVCNet&#65292;&#25105;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a generic approach for applying the cognitive workload recognizer by exploiting common electroencephalogram (EEG) patterns across different human-machine tasks and individual sets. We propose a neural network called SCVCNet, which eliminates task- and individual-set-related interferences in EEGs by analyzing finer-grained frequency structures in the power spectral densities. The SCVCNet utilizes a sliding cross-vector convolution (SCVC) operation, where paired input layers representing the theta and alpha power are employed. By extracting the weights from a kernel matrix's central row and column, we compute the weighted sum of the two vectors around a specified scalp location. Next, we introduce an inter-frequency-point feature integration module to fuse the SCVC feature maps. Finally, we combined the two modules with the output-channel pooling and classification layers to construct the model. To train the SCVCNet, we employ the regularized least-square method with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03605</link><description>&lt;p&gt;
FASER: &#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#36328;&#26550;&#26500;&#36719;&#20214;&#20013;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#23545;&#20110;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12289;&#20445;&#25252;&#36719;&#20214;&#20379;&#24212;&#38142;&#25110;&#36827;&#34892;&#28431;&#27934;&#30740;&#31350;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36328;&#26550;&#26500;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#24050;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#25506;&#32034;&#65292;&#24182;&#20351;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#20174;&#20108;&#36827;&#21046;&#25991;&#20214;&#20013;&#25552;&#21462;&#30340;&#24120;&#35265;&#32467;&#26500;&#65292;&#22914;&#20989;&#25968;&#25511;&#21046;&#27969;&#22270;&#25110;&#20108;&#36827;&#21046;&#32423;&#35843;&#29992;&#22270;&#65292;&#21453;&#27719;&#32534;&#36807;&#31243;&#30340;&#36755;&#20986;&#25110;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#30340;&#36755;&#20986;&#12290;&#20854;&#20013;&#19968;&#31181;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#12290;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#20004;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#23427;&#20204;&#30340;&#36328;&#26550;&#26500;&#24615;&#36136;&#20197;&#21450;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#20989;&#25968;&#23383;&#31526;&#20018;&#32534;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#25991;&#26723;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03281</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#30721;mRNA&#30340;5' UTR&#35821;&#35328;&#27169;&#22411;&#21644;&#21151;&#33021;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5' UTR&#26159;mRNA&#20998;&#23376;&#24320;&#31471;&#30340;&#35843;&#25511;&#21306;&#22495;&#65292;&#22312;&#35843;&#25511;&#32763;&#35793;&#36807;&#31243;&#21644;&#24433;&#21709;&#34507;&#30333;&#34920;&#36798;&#27700;&#24179;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#35299;&#30721;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#32452;&#24207;&#21015;&#21151;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;5' UTR&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;UTR-LM&#12290;UTR-LM&#22312;&#22810;&#20010;&#29289;&#31181;&#30340;&#20869;&#28304;&#24615;5' UTR&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#21253;&#25324;&#20108;&#32423;&#32467;&#26500;&#21644;&#26368;&#23567;&#33258;&#30001;&#33021;&#22312;&#20869;&#30340;&#26377;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;UTR-LM&#36827;&#34892;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#27169;&#22411;&#22312;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#24050;&#30693;&#30340;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#26368;&#22810;42%&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#32763;&#35793;&#25928;&#29575;&#21644;mRNA&#34920;&#36798;&#27700;&#24179;&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;&#26368;&#22810;60%&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26410;&#27880;&#37322;&#30340;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#65292;&#24182;&#23558;AUPR&#19982;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20174;0.37&#25552;&#39640;&#33267;0.52&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;NeuFace&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35270;&#39057;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#20154;&#33080;&#32593;&#26684;&#26631;&#27880;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;3D&#20154;&#33080;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25913;&#21892;&#29616;&#26377;&#30340;3D&#20154;&#33080;&#37325;&#24314;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;3D&#38754;&#37096;&#36816;&#21160;&#20808;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.03205</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#23454;&#29616;&#30340;&#22823;&#35268;&#27169;3D&#20154;&#33080;&#32593;&#26684;&#35270;&#39057;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeuFace&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35270;&#39057;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#20154;&#33080;&#32593;&#26684;&#26631;&#27880;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;3D&#20154;&#33080;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25913;&#21892;&#29616;&#26377;&#30340;3D&#20154;&#33080;&#37325;&#24314;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;3D&#38754;&#37096;&#36816;&#21160;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuFace&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#20248;&#21270;&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;3D&#20154;&#33080;&#32593;&#26684;&#30340;&#20266;&#26631;&#27880;&#12290;&#23613;&#31649;&#22312;3D&#20154;&#33080;&#37325;&#24314;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#37326;&#22806;&#21160;&#24577;&#35270;&#39057;&#20013;&#29983;&#25104;&#21487;&#38752;&#30340;3D&#20154;&#33080;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;NeuFace&#20248;&#21270;&#65292;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#20154;&#33080;&#35270;&#39057;&#36827;&#34892;&#27599;&#20010;&#35270;&#35282;/&#24103;&#20934;&#30830;&#32780;&#19968;&#33268;&#30340;&#20154;&#33080;&#32593;&#26684;&#26631;&#27880;&#65292;&#31216;&#20026;NeuFace&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#20877;&#21442;&#25968;&#21270;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#20998;&#26512;&#23558;&#22270;&#20687;&#23545;&#40784;&#30340;&#38754;&#37096;&#32454;&#33410;&#37325;&#24314;&#21040;3D&#32593;&#26684;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;3D&#20154;&#33080;&#30340;&#33258;&#28982;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#22312;3D&#20154;&#33080;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#29992;&#22788;&#65306;&#25552;&#39640;&#29616;&#26377;3D&#20154;&#33080;&#37325;&#24314;&#27169;&#22411;&#30340;&#37325;&#24314;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;3D&#38754;&#37096;&#36816;&#21160;&#20808;&#39564;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312;https://neuface-dataset.github&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neural re-parameterized optimization. Despite the huge progress in 3D face reconstruction methods, generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. Using NeuFace optimization, we annotate the per-view/-frame accurate and consistent face meshes on large-scale face videos, called the NeuFace-dataset. We investigate how neural re-parameterization helps to reconstruct image-aligned facial details on 3D meshes via gradient analysis. By exploiting the naturalness and diversity of 3D faces in our dataset, we demonstrate the usefulness of our dataset for 3D face-related tasks: improving the reconstruction accuracy of an existing 3D face reconstruction model and learning 3D facial motion prior. Code and datasets will be available at https://neuface-dataset.github.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02842</link><description>&lt;p&gt;
&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#25195;&#25551;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26377;&#33021;&#21147;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#26159;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20351;&#29992;&#25552;&#31034;&#25351;&#23548;&#35843;&#33410;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#20197;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25193;&#23637;&#25552;&#31034;&#35843;&#33410;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26159;&#19968;&#20010;&#24191;&#27867;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;"&#28151;&#21512;&#25552;&#31034;"&#25110;MoPs&#65292;&#24182;&#32467;&#21512;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65306;&#21518;&#32773;&#30340;&#35774;&#35745;&#26159;&#26412;&#25991;&#30340;&#36129;&#29486;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;(&#21363;&#19968;&#32452;&#25552;&#31034;)&#12290;&#27492;&#22806;&#65292;MoPs&#22312;&#24212;&#29992;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26102;&#37117;&#19981;&#21463;&#24433;&#21709;&#8212;&#8212;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
&lt;/p&gt;</description></item><item><title>USB-NeRF&#26159;&#19968;&#31181;&#35299;&#20915;&#28378;&#21160;&#24555;&#38376;&#30456;&#26426;&#38382;&#39064;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#33021;&#22815;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02687</link><description>&lt;p&gt;
USB-NeRF: &#35299;&#21367;&#26354;&#24555;&#38376;&#26463;&#35843;&#25972;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields. (arXiv:2310.02687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02687
&lt;/p&gt;
&lt;p&gt;
USB-NeRF&#26159;&#19968;&#31181;&#35299;&#20915;&#28378;&#21160;&#24555;&#38376;&#30456;&#26426;&#38382;&#39064;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#33021;&#22815;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#26469;&#34920;&#31034;3D&#22330;&#26223;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#36755;&#20837;&#22270;&#20687;&#26159;&#30001;&#20840;&#23616;&#24555;&#38376;&#30456;&#26426;&#25293;&#25668;&#30340;&#12290;&#22240;&#27492;&#65292;&#28378;&#21160;&#24555;&#38376;&#65288;RS&#65289;&#22270;&#20687;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;NeRF&#31639;&#27861;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#28378;&#21160;&#24555;&#38376;&#25928;&#24212;&#36824;&#20250;&#24433;&#21709;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#65288;&#20363;&#22914;&#36890;&#36807;COLMAP&#65289;&#65292;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#20351;&#29992;RS&#22270;&#20687;&#30340;NeRF&#31639;&#27861;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21367;&#26354;&#24555;&#38376;&#26463;&#35843;&#25972;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;USB-NeRF&#65289;&#12290;USB-NeRF&#33021;&#22815;&#22312;NeRF&#26694;&#26550;&#19979;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#21516;&#26102;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#36890;&#36807;&#23545;RS&#30456;&#26426;&#30340;&#29289;&#29702;&#22270;&#20687;&#24418;&#25104;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;USB-NeRF&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02520</link><description>&lt;p&gt;
MedDiffusion: &#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#20854;&#24207;&#21015;&#29305;&#24615;&#65292;&#39640;&#32500;&#24230;&#21644;&#22266;&#26377;&#22122;&#38899;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24433;&#21709;&#23427;&#20204;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#24448;&#24448;&#21463;&#21040;&#20219;&#21153;&#26080;&#20851;&#35774;&#35745;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;MedDiffusion&#65292;&#26469;&#22686;&#24378;&#39118;&#38505;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk predicti
&lt;/p&gt;</description></item><item><title>MiniGPT-5&#20351;&#29992;&#29983;&#25104;&#20973;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#23454;&#29616;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02239</link><description>&lt;p&gt;
MiniGPT-5: &#36890;&#36807;&#29983;&#25104;&#20973;&#25454;&#23454;&#29616;&#20132;&#38169;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. (arXiv:2310.02239v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02239
&lt;/p&gt;
&lt;p&gt;
MiniGPT-5&#20351;&#29992;&#29983;&#25104;&#20973;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#23454;&#29616;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#25991;&#26412;&#21465;&#36848;&#30340;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#21069;&#27839;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#20197;"&#29983;&#25104;&#20973;&#25454;"&#30340;&#27010;&#24565;&#20026;&#22522;&#30784;&#65292;&#20316;&#20026;&#21327;&#35843;&#22270;&#20687;&#25991;&#26412;&#36755;&#20986;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#28857;&#26159;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#65292;&#35757;&#32451;&#36807;&#31243;&#19981;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#20840;&#38754;&#30340;&#25551;&#36848;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#20973;&#25454;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MiniGPT-5&#22312;MMDialog&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#32447;Divter&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22987;&#32456;&#25552;&#20379;&#20248;&#36234;&#25110;&#21487;&#27604;&#30340;&#22810;&#27169;&#24577;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of "generative vokens," acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01320</link><description>&lt;p&gt;
Avalon&#30340;&#24605;&#32771;&#28216;&#25103;&#65306;&#36890;&#36807;&#36882;&#24402;&#24605;&#32771;&#23545;&#25239;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#22312;LLM&#20316;&#20026;&#26234;&#33021;&#20307;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;LLM&#22788;&#29702;&#30340;&#20449;&#24687;&#22987;&#32456;&#26159;&#35802;&#23454;&#30340;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#31038;&#20250;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27450;&#39575;&#25110;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#20010;&#30095;&#24573;&#20351;&#24471;LLM&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25506;&#32034;LLM&#22312;&#27450;&#39575;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#20805;&#28385;&#20102;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#65292;&#34920;&#29616;&#20026;&#8220;&#24605;&#32771;&#30340;&#28216;&#25103;&#8221;&#12290;&#21463;&#21040;&#20154;&#31867;&#22312;Avalon&#28216;&#25103;&#20013;&#36882;&#24402;&#24605;&#32771;&#21644;&#36879;&#35270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#65292;&#20197;&#22686;&#24378;LLM&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;ReCon&#32467;&#21512;&#20102;&#20844;&#24335;&#21270;&#24605;&#32771;&#21644;&#23436;&#21892;&#24605;&#32771;&#30340;&#36807;&#31243;&#65307;&#20844;&#24335;&#21270;&#24605;&#32771;&#20135;&#29983;&#21021;&#22987;&#24605;&#32771;&#65292;&#23436;&#21892;&#24605;&#32771;&#23545;&#21021;&#22987;&#24605;&#32771;&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00771</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;Decision Transformer&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#21482;&#33021;&#36890;&#36807;&#35821;&#35328;&#39044;&#35757;&#32451;&#23454;&#29616;&#65292;&#36824;&#26159;&#21487;&#20197;&#36890;&#36807;&#19981;&#28041;&#21450;&#35821;&#35328;&#30340;&#26356;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35821;&#35328;&#23545;&#20110;&#25913;&#21892;&#24615;&#33021;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#21512;&#25104;&#30340;IID&#25968;&#25454;&#36827;&#34892;&#23569;&#37327;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#25552;&#21319;&#65307;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#39044;&#35757;&#32451;Conservative Q-Learning(CQL)&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;Q-learning&#65292;&#24182;&#36890;&#24120;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#39592;&#24178;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#22312;CQL&#31639;&#27861;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>Suspicion-Agent&#26159;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;&#65292;&#21033;&#29992;&#20855;&#22791;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17277</link><description>&lt;p&gt;
Suspicion-Agent: &#20351;&#29992;&#20855;&#22791;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT4. (arXiv:2309.17277v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17277
&lt;/p&gt;
&lt;p&gt;
Suspicion-Agent&#26159;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;&#65292;&#21033;&#29992;&#20855;&#22791;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#65292;&#20854;&#20013;&#27599;&#20010;&#29609;&#23478;&#37117;&#30693;&#36947;&#25152;&#26377;&#20803;&#32032;&#65292;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#27169;&#25311;&#20102;&#22312;&#19981;&#30830;&#23450;&#25110;&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#36827;&#34892;&#20915;&#31574;&#30340;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;GPT-4&#20197;&#20854;&#30693;&#35782;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;GPT-4&#30340;&#23398;&#20064;&#30693;&#35782;&#24212;&#29992;&#20110;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;\textbf{Suspicion-Agent}&#65292;&#35813;&#20195;&#29702;&#31243;&#24207;&#21033;&#29992;GPT-4&#30340;&#33021;&#21147;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#23545;&#23616;&#12290;&#36890;&#36807;&#21512;&#36866;&#30340;&#25552;&#31034;&#24037;&#31243;&#26469;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#65292;&#22522;&#20110;GPT-4&#30340;Suspicion-Agent&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#19981;&#23436;&#20840;&#20449;&#24687;&#32440;&#29260;&#28216;&#25103;&#20013;&#30340;&#26174;&#33879;&#36866;&#24212;&#33021;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-4&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#65288;Theory of Mind&#65289;&#33021;&#21147;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#33021;&#22815;&#29702;&#35299;&#20182;&#20154;&#24182;&#26377;&#24847;&#35782;&#22320;&#24433;&#21709;&#20182;&#20154;&#30340;&#34892;&#20026;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a plann
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17255</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#65306;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#26159;&#30740;&#31350;&#29983;&#29289;&#21644;&#29983;&#21629;&#36807;&#31243;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#30456;&#20851;&#23398;&#31185;&#12290;&#29983;&#21629;&#31185;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#20135;&#29983;&#21644;&#28040;&#36153;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20855;&#26377;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#12290;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20854;&#20013;&#28041;&#21450;&#30340;&#31185;&#23398;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#25512;&#21160;&#20102;&#24212;&#29992;&#20808;&#36827;&#30340;&#30693;&#35782;&#39537;&#21160;&#25216;&#26415;&#26469;&#31649;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25512;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#21644;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#31649;&#29702;&#65292;&#20197;&#21450;&#22312;&#26032;&#21457;&#29616;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14674</link><description>&lt;p&gt;
&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#22522;&#20110;UPTST&#30340;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#36275;&#21475;&#30149;&#65288;HFMD&#65289;&#29190;&#21457;&#19982;&#20005;&#37325;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#20799;&#31185;HFMD&#24739;&#32773;&#30340;&#27599;&#26085;&#20303;&#38498;&#20154;&#25968;&#23545;&#20110;&#21327;&#21161;&#21307;&#38498;&#24212;&#23545;&#28508;&#22312;&#30340;&#29190;&#21457;&#21644;&#20943;&#23569;&#21307;&#38498;&#20869;&#20256;&#25773;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;U-net&#24418;&#29366;&#65292;&#24182;&#21033;&#29992;&#20102;&#19982;HFMD&#23494;&#20999;&#30456;&#20851;&#30340;&#33133;&#21693;&#21475;&#28814;&#30340;&#35265;&#35299;&#12290;&#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#24341;&#20837;&#37325;&#26500;&#25439;&#22833;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#26469;&#25972;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;UPTST&#27169;&#22411;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;HFMD&#38271;&#30701;&#33218;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#24615;&#30340;&#25193;&#23637;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#36229;&#20986;&#20102;&#20256;&#26579;&#30149;&#30340;&#39044;&#27979;&#65292;&#25552;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with significant morbidity and, in severe cases, mortality. Accurate forecasting of daily admissions of pediatric HFMD patients is therefore crucial for aiding the hospital in preparing for potential outbreaks and mitigating nosocomial transmissions. To address this pressing need, we propose a novel transformer-based model with a U-net shape, utilizing the patching strategy and the joint prediction strategy that capitalizes on insights from herpangina, a disease closely correlated with HFMD. This model also integrates representation learning by introducing reconstruction loss as an auxiliary loss. The results show that our U-net Patching Time Series Transformer (UPTST) model outperforms existing approaches in both long- and short-arm prediction accuracy of HFMD at hospital-level. Furthermore, the exploratory extension experiments show that the model's capabilities extend beyond prediction of infectious disease, suggest
&lt;/p&gt;</description></item><item><title>NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.14293</link><description>&lt;p&gt;
NAS-NeRF: &#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14293
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#21487;&#37096;&#32626;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#21162;&#21147;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#19981;&#32771;&#34385;&#22330;&#26223;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#36890;&#29992;&#26550;&#26500;&#12290;&#21516;&#19968;&#20010;&#26550;&#26500;&#21487;&#33021;&#23545;&#31616;&#21333;&#22330;&#26223;&#26469;&#35828;&#36807;&#20110;&#24222;&#22823;&#65292;&#23545;&#22797;&#26434;&#22330;&#26223;&#21017;&#19981;&#36275;&#22815;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21160;&#24577;&#20248;&#21270;NeRF&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#21512;&#25104;&#36136;&#37327;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NAS-NeRF&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#30446;&#26631;&#24230;&#37327;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#25351;&#23548;&#25628;&#32034;&#20197;&#33719;&#24471;&#36866;&#21512;&#27599;&#20010;&#22330;&#26223;&#30340;&#26550;&#26500;&#12290;&#22312;Blender&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;NAS-NeRF&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;5&#20010;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38459;&#27490;&#29983;&#25104;&#28431;&#27934;&#26631;&#35760;&#65292;&#20197;&#39640;&#25928;&#20943;&#23569;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#20197;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09826</link><description>&lt;p&gt;
&#20351;&#29992;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26469;&#39640;&#25928;&#36991;&#20813;&#33258;&#21160;&#23436;&#25104;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding. (arXiv:2309.09826v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38459;&#27490;&#29983;&#25104;&#28431;&#27934;&#26631;&#35760;&#65292;&#20197;&#39640;&#25928;&#20943;&#23569;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#20197;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#24320;&#21457;&#36895;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#24050;&#34987;&#24212;&#29992;&#20110;&#20195;&#30721;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#35768;&#22810;&#36825;&#31181;&#21512;&#25104;&#30340;&#20195;&#30721;&#23384;&#22312;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#36825;&#31867;&#27169;&#22411;&#29983;&#25104;&#30340;&#28431;&#27934;&#20195;&#30721;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26631;&#35760;&#30340;&#28431;&#27934;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#21253;&#21547;&#28431;&#27934;&#26631;&#35760;&#65292;&#24182;&#20805;&#24403;&#20869;&#23884;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38459;&#27490;&#27169;&#22411;&#29983;&#25104;&#36825;&#20123;&#26631;&#35760;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#28431;&#27934;&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#36873;&#25321;&#33258;&#21160;&#23436;&#25104;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#26234;&#33021;&#21512;&#32422;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#22240;&#20026;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#20855;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;186,397&#20010;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#65288;&#32463;&#21435;&#37325;&#21518;&#21097;&#19979;2,217,692&#20010;SC&#65289;&#23545;60&#20159;&#21442;&#25968;&#30340;GPT-J&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-completing code enables developers to speed up coding significantly. Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities. We propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. Using a small dataset of labeled vulnerable lines of code, we fine-tune an LLM to include vulnerability labels when generating code, acting as an embedded classifier. Then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. To evaluate the method, we chose to automatically complete Ethereum Blockchain smart contracts (SCs) as the case study due to the strict requirements of SC security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397 Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning took more than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.05938</link><description>&lt;p&gt;
&#36890;&#36807;&#24635;&#32467;&#22810;&#28304;&#22810;&#35270;&#35282;&#30693;&#35782;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#30340;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#20294;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#26469;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#8220;&#25163;&#26426;&#26159;&#21542;&#37325;&#8221;&#30340;&#31572;&#26696;&#26377;&#22810;&#31181;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#19968;&#20010;&#28385;&#24847;&#30340;&#31572;&#26696;&#24212;&#35813;&#33021;&#22815;&#24635;&#32467;&#36825;&#20123;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#65292;&#24182;&#25552;&#20379;&#23458;&#35266;&#30693;&#35782;&#65292;&#27604;&#22914;&#25163;&#26426;&#30340;&#37325;&#37327;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#65292;&#20256;&#32479;QA&#20219;&#21153;&#20013;&#23545;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21333;&#20010;&#25968;&#25454;&#28304;&#20013;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#30693;&#35782;&#28304;&#20013;&#26816;&#32034;&#25152;&#26377;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#32447;&#32034;&#65292;&#21253;&#25324;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#36824;&#25910;&#38598;&#20102;&#38544;&#21547;&#30340;&#24120;&#35782;&#20107;&#23454;&#26469;&#34917;&#20805;&#24517;&#35201;&#20294;&#32570;&#22833;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#20114;&#24335;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#23427;&#20204;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#26469;&#32858;&#21512;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new task in the field of Answering Subjective Induction Question on Products (SUBJPQA). The answer to this kind of question is non-unique, but can be interpreted from many perspectives. For example, the answer to 'whether the phone is heavy' has a variety of different viewpoints. A satisfied answer should be able to summarize these subjective opinions from multiple sources and provide objective knowledge, such as the weight of a phone. That is quite different from the traditional QA task, in which the answer to a factoid question is unique and can be found from a single data source. To address this new task, we propose a three-steps method. We first retrieve all answer-related clues from multiple knowledge sources on facts and opinions. The implicit commonsense facts are also collected to supplement the necessary but missing contexts. We then capture their relevance with the questions by interactive attention. Next, we design a reinforcement-based summarizer to ag
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2306.11695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#33258;&#28982;&#20505;&#36873;&#23545;&#35937;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#21162;&#21147;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20002;&#24323;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;LLMs&#26469;&#35828;&#24456;&#23569;&#21487;&#34892;&#65292;&#35201;&#20040;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20381;&#36182;&#20108;&#38454;&#20449;&#24687;&#30340;&#26435;&#37325;&#37325;&#26500;&#38382;&#39064;&#65292;&#36825;&#20063;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Wanda&#65288;&#22522;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#21098;&#26525;&#65289;&#65292;&#26088;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;LLMs&#20013;&#20986;&#29616;&#30340;&#22823;&#24133;&#29305;&#24449;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#36755;&#20986;&#19978;&#25353;&#29031;&#26435;&#37325;&#21644;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#30456;&#20056;&#30340;&#26368;&#23567;&#24133;&#24230;&#26469;&#21098;&#26525;&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wanda&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#65292;&#21098;&#26525;&#21518;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;LLaMA&#21644;LLaMA-2&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;Wanda&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.11363</link><description>&lt;p&gt;
&#21463;&#36974;&#34109;&#25193;&#25955;&#27169;&#22411;&#26159;&#24555;&#36895;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#20107;&#23454;&#19978;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#35813;&#25216;&#26415;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#36974;&#34109;&#36755;&#20837;&#22270;&#20687;&#30340;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;&#39640;&#36798;90&#65285;&#65289;&#65292;&#24182;&#21033;&#29992;&#36974;&#34109;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#26469;&#21435;&#22122;&#21487;&#35265;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26174;&#33879;&#30340;&#29305;&#24449;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#36974;&#34109;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;CelebA-HQ $256 \times 256$&#20687;&#32032;&#31354;&#38388;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#20102;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4&#20493;&#21152;&#36895;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#19982;&#21435;&#22122;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#65307;&#36890;&#36807;&#20998;&#35299;&#28436;&#31034;&#12289;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#20219;&#21153;&#25551;&#36848;&#65292;&#31034;&#20363;&#26816;&#32034;&#31561;&#27493;&#39588;&#65292;Synapse &#20855;&#22791;&#20102;&#36866;&#24212;&#22810;&#20219;&#21153;&#12289;&#27867;&#21270;&#22810;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07863</link><description>&lt;p&gt;
Synapse&#65306;&#21033;&#29992;&#23569;&#37327;&#31034;&#20363;&#20026;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#25171;&#19979;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control. (arXiv:2306.07863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#65307;&#36890;&#36807;&#20998;&#35299;&#28436;&#31034;&#12289;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#20219;&#21153;&#25551;&#36848;&#65292;&#31034;&#20363;&#26816;&#32034;&#31561;&#27493;&#39588;&#65292;Synapse &#20855;&#22791;&#20102;&#36866;&#24212;&#22810;&#20219;&#21153;&#12289;&#27867;&#21270;&#22810;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#36827;&#34892;&#35745;&#31639;&#26426;&#33258;&#21160;&#21270;&#30340;&#35774;&#35745;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#30528;&#37325;&#20110;&#33258;&#25105;&#32416;&#27491;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#26377;&#33391;&#22909;&#32467;&#26500;&#30340;&#31034;&#20363;&#23601;&#36275;&#20197;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Synapse&#65292;&#19968;&#31181;&#19978;&#19979;&#25991;&#35745;&#31639;&#26426;&#25511;&#21046;&#20195;&#29702;&#65292;&#22312; MiniWob++ &#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#20154;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;Synapse &#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;1&#65289;&#29366;&#24577;&#26465;&#20214;&#20998;&#35299;&#65292;&#26681;&#25454;&#20195;&#29702;&#38656;&#35201;&#26032;&#29615;&#22659;&#29366;&#24577;&#23558;&#28436;&#31034;&#20998;&#20026;&#31034;&#20363;&#38598;&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#25277;&#35937;&#65307;2&#65289;&#32467;&#26500;&#21270;&#25552;&#31034;&#65292;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#27599;&#20010;&#38598;&#21512;&#30340;&#20219;&#21153;&#25551;&#36848;&#20197;&#25913;&#21892;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#65307;3&#65289;&#31034;&#20363;&#26816;&#32034;&#65292;&#23558;&#20256;&#20837;&#30340;&#20219;&#21153;&#19982;&#31034;&#20363;&#25968;&#25454;&#24211;&#20013;&#30340;&#23545;&#24212;&#31034;&#20363;&#30456;&#20851;&#32852;&#65292;&#20197;&#23454;&#29616;&#22810;&#20219;&#21153;&#36866;&#24212;&#21644;&#27867;&#21270;&#12290;Synapse &#20811;&#26381;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#65292;&#20943;&#23569;&#20102;&#22810;&#27493;&#25511;&#21046;&#20013;&#30340;&#38169;&#35823;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#28789;&#27963;&#22320;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35745;&#31639;&#26426;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04959</link><description>&lt;p&gt;
FedMLSecurity&#65306;&#32852;&#37030;&#23398;&#20064;&#19982;LLMs&#20013;&#25915;&#20987;&#19982;&#38450;&#24481;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FedMLSecurity&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#27169;&#25311;&#23545;&#25239;&#25915;&#20987;&#21644;&#30456;&#24212;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#24320;&#28304;&#24211;FedML&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22359;&#65292;FedMLSecurity&#22686;&#24378;&#20102;FedML&#30340;&#23433;&#20840;&#35780;&#20272;&#33021;&#21147;&#12290;FedMLSecurity&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;FedMLAttacker&#27169;&#25311;&#22312;FL&#35757;&#32451;&#20013;&#27880;&#20837;&#30340;&#25915;&#20987;&#65292;&#32780;FedMLDefender&#21017;&#27169;&#25311;&#26088;&#22312;&#20943;&#36731;&#25915;&#20987;&#24433;&#21709;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;FedMLSecurity&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;ResNet&#65292;GAN&#31561;&#65289;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;FedAVG&#65292;FedOPT&#65292;FedNOVA&#31561;&#65289;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#35780;&#20272;&#36824;&#23637;&#31034;&#20102;&#23558;FedMLSecurity&#36731;&#26494;&#24212;&#29992;&#20110;LLMs&#30340;&#20415;&#21033;&#24615;&#65292;&#36827;&#19968;&#27493;&#24378;&#21270;&#20102;&#20854;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
&lt;/p&gt;</description></item><item><title>Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.</title><link>http://arxiv.org/abs/2305.18030</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18030
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20107;&#20808;&#25163;&#24037;&#21019;&#24314;&#25628;&#32034;&#31354;&#38388;&#26469;&#25628;&#32034;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#26368;&#20248;&#23376;&#32593;&#32476;&#12290;&#36825;&#26679;&#30340;&#35201;&#27714;&#20351;&#24471;&#22312;&#27809;&#26377;&#26174;&#33879;&#30340;&#20154;&#24037;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#36890;&#29992;&#22330;&#26223;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Automated Search-Space Generation Neural Architecture Search&#65288;ASGNAS&#65289;&#65292;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20197;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#35757;&#32451;&#35206;&#30422;&#25152;&#26377;&#20505;&#36873;&#36830;&#25509;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;DNN&#65292;&#24182;&#20135;&#29983;&#39640;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#12290;&#25216;&#26415;&#19978;&#65292;ASGNAS&#20855;&#26377;&#19977;&#20010;&#26174;&#33879;&#30340;&#36129;&#29486;&#20197;&#20943;&#23569;&#20154;&#21147;&#24037;&#20316;&#65306;&#65288;i&#65289;&#36890;&#29992;DNN&#30340;&#33258;&#21160;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#20869;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;Hierarchical Half-Space Projected Gradient&#65288;H2SPG&#65289;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30830;&#20445;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#38752;&#22320;&#20135;&#29983;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#31232;&#30095;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
To search an optimal sub-network within a general deep neural network (DNN), existing neural architecture search (NAS) methods typically rely on handcrafting a search space beforehand. Such requirements make it challenging to extend them onto general scenarios without significant human expertise and manual intervention. To overcome the limitations, we propose Automated Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first automated system to train general DNNs that cover all candidate connections and operations and produce high-performing sub-networks in the one shot manner. Technologically, ASGNAS delivers three noticeable contributions to minimize human efforts: (i) automated search space generation for general DNNs; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy and dependency within generated search space to ensure the network validity during optimization, and reliably produces a solution with both high performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;COWP&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#24773;&#22659;&#22788;&#29702;&#12290;COWP&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#34892;&#21160;&#30693;&#35782;&#65292;&#21160;&#24577;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#26032;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.17590</link><description>&lt;p&gt;
&#25972;&#21512;&#34892;&#21160;&#30693;&#35782;&#21644;LLM&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#24773;&#22659;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds. (arXiv:2305.17590v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;COWP&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#24773;&#22659;&#22788;&#29702;&#12290;COWP&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#34892;&#21160;&#30693;&#35782;&#65292;&#21160;&#24577;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#26032;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35268;&#21010;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#21033;&#29992;&#20154;&#31867;&#20851;&#20110;&#34892;&#21160;&#30340;&#30693;&#35782;&#26469;&#23436;&#25104;&#38271;&#26102;&#31243;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#31995;&#32479;&#37117;&#26159;&#20026;&#8220;&#23553;&#38381;&#19990;&#30028;&#8221;&#32780;&#35774;&#35745;&#30340;&#65292;&#21516;&#26102;&#20551;&#35774;&#26426;&#22120;&#20154;&#25552;&#20379;&#23436;&#25972;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#36890;&#24120;&#26159;&#24320;&#25918;&#30340;&#65292;&#26426;&#22120;&#20154;&#32463;&#24120;&#36935;&#21040;&#26080;&#27861;&#39044;&#26009;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#25171;&#30772;&#35268;&#21010;&#22120;&#30340;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#32463;&#20856;&#35268;&#21010;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#26032;&#30340;&#24773;&#20917;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;COWP&#30340;&#24320;&#25918;&#19990;&#30028;&#20219;&#21153;&#35268;&#21010;&#21644;&#24773;&#22659;&#22788;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;COWP&#21160;&#24577;&#22320;&#22686;&#21152;&#20102;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#30693;&#35782;&#65292;&#21253;&#25324;&#34892;&#21160;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#25928;&#26524;&#65292;&#20197;&#21450;&#38754;&#21521;&#20219;&#21153;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;COWP&#20511;&#37492;&#20102;LLMs&#30340;&#24320;&#25918;&#24615;&#65292;&#24182;&#36890;&#36807;&#34892;&#21160;&#30693;&#35782;&#19982;&#29305;&#23450;&#39046;&#22495;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#21253;&#21547;1085&#20010;&#25191;&#34892;&#26102;&#24773;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task planning systems have been developed to help robots use human knowledge (about actions) to complete long-horizon tasks. Most of them have been developed for "closed worlds" while assuming the robot is provided with complete world knowledge. However, the real world is generally open, and the robots frequently encounter unforeseen situations that can potentially break the planner's completeness. Could we leverage the recent advances on pre-trained Large Language Models (LLMs) to enable classical planning systems to deal with novel situations?  This paper introduces a novel framework, called COWP, for open-world task planning and situation handling. COWP dynamically augments the robot's action knowledge, including the preconditions and effects of actions, with task-oriented commonsense knowledge. COWP embraces the openness from LLMs, and is grounded to specific domains via action knowledge. For systematic evaluations, we collected a dataset that includes 1,085 execution-time situatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.17154</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On convex conceptual regions in deep network representations. (arXiv:2305.17154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#23545;&#40784;&#30340;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;Gardenfors&#30340;&#27010;&#24565;&#31354;&#38388;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#20010;&#37325;&#35201;&#26694;&#26550;&#12290;&#22312;&#27010;&#24565;&#31354;&#38388;&#20013;&#65292;&#23545;&#35937;&#21306;&#22495;&#30340;&#20984;&#24615;&#34987;&#35748;&#20026;&#26159;&#20419;&#36827;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27010;&#24565;&#21306;&#22495;&#30340;&#20984;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#32452;&#29992;&#20110;&#27979;&#37327;&#37319;&#26679;&#25968;&#25454;&#20013;&#20984;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#23618;&#34920;&#31034;&#20013;&#30340;&#20984;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20984;&#24615;&#23545;&#20110;&#22522;&#26412;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26159;&#31283;&#20581;&#30340;&#65292;&#22240;&#27492;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#36136;&#37327;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36817;&#20284;&#20984;&#24615;&#22312;&#31070;&#32463;&#34920;&#31034;&#20013;&#24191;&#27867;&#23384;&#22312;&#20110;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#20154;&#31867;&#27963;&#21160;&#12289;&#25991;&#26412;&#21644;&#33041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current study of human-machine alignment aims at understanding the geometry of latent spaces and the correspondence to human representations. G\"ardenfors' conceptual spaces is a prominent framework for understanding human representations. Convexity of object regions in conceptual spaces is argued to promote generalizability, few-shot learning, and intersubject alignment. Based on these insights, we investigate the notion of convexity of concept regions in machine-learned latent spaces. We develop a set of tools for measuring convexity in sampled data and evaluate emergent convexity in layered representations of state-of-the-art deep networks. We show that convexity is robust to basic re-parametrization, hence, meaningful as a quality of machine-learned latent spaces. We find that approximate convexity is pervasive in neural representations in multiple application domains, including models of images, audio, human activity, text, and brain data. We measure convexity separately for l
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03584</link><description>&lt;p&gt;
&#29616;&#22312;&#23427;&#21548;&#36215;&#26469;&#20687;&#20320;&#20102;&#65306;&#22312;&#35774;&#22791;&#19978;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#36827;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#24212;&#29992;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#31471;&#35821;&#35328;&#24314;&#27169;&#12290;&#30001;&#20110;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25903;&#25345;&#23376;&#21333;&#35789;&#26631;&#35760;&#25110;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20915;&#23450;&#37096;&#32626;&#23553;&#38381;&#35789;&#27719;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23553;&#38381;&#35789;&#27719;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#29305;&#23450;&#29992;&#25143;&#30340;&#29983;&#35789;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#65292;&#26377;&#25928;&#22320;&#20174;&#20013;&#22830;&#27169;&#22411;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#35789;&#27719;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#12290;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#29983;&#35789;&#25193;&#23637;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.14993</link><description>&lt;p&gt;
ChatGPT - &#23545;&#20110;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#21644;&#25945;&#24072;&#26159;&#31119;&#26159;&#31096;?
&lt;/p&gt;
&lt;p&gt;
ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?. (arXiv:2304.14993v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#12290;&#23427;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#24320;&#21457;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20869;&#23481;&#21019;&#20316;&#12289;&#20010;&#24615;&#21270;&#12289;&#25991;&#26412;&#23436;&#25104;&#21644;&#25925;&#20107;&#21465;&#36848;&#31561;&#22810;&#31181;&#29992;&#36884;&#12290;&#34429;&#28982;ChatGPT&#21463;&#21040;&#20102;&#30456;&#24403;&#31215;&#26497;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23398;&#26415;&#30028;&#20063;&#24341;&#36215;&#20102;&#19968;&#31181;&#25285;&#24551;&#21644;&#19981;&#30830;&#23450;&#24863;&#12290;&#23384;&#22312;&#25285;&#24551;&#23398;&#29983;&#21487;&#33021;&#20250;&#21033;&#29992;ChatGPT&#23436;&#25104;&#35838;&#22806;&#20316;&#19994;&#21644;&#32771;&#35797;&#65292;&#24182;&#33719;&#24471;&#26377;&#21033;&#30340;&#25104;&#32489;&#65292;&#32780;&#19981;&#30495;&#27491;&#33719;&#24471;&#30693;&#35782;&#12290;&#26412;&#25991;&#37319;&#29992;&#23450;&#37327;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#29983;&#30450;&#30446;&#20381;&#36182;ChatGPT&#23436;&#25104;&#20316;&#19994;&#21644;&#32771;&#35797;&#21487;&#33021;&#20250;&#33258;&#27585;&#21069;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#20998;&#26512;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#22914;&#20309;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text. It can be used for a variety of use cases such as language generation, question answering, text summarization, chatbot development, language translation, sentiment analysis, content creation, personalization, text completion, and storytelling. While ChatGPT has garnered significant positive attention, it has also generated a sense of apprehension and uncertainty in academic circles. There is concern that students may leverage ChatGPT to complete take-home assignments and exams and obtain favorable grades without genuinely acquiring knowledge. This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science. Our analysis shows that students may risk self-sabotage by blindly depending on ChatGPT to complete assignments and exams. We build upon this analysis to p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#24182;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#65292;&#21487;&#29992;&#20110;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;</title><link>http://arxiv.org/abs/2304.10888</link><description>&lt;p&gt;
&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;AMP&#65306;&#23398;&#20064;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#26377;&#33151;&#31227;&#21160;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
AMP in the wild: Learning robust, agile, natural legged locomotion skills. (arXiv:2304.10888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#24182;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#65292;&#21487;&#29992;&#20110;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#20010;&#23398;&#20064;&#25511;&#21046;&#22120;&#20174;&#20223;&#30495;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#38656;&#35201;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#31995;&#32479;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#20934;&#30830;&#22320;&#20272;&#35745;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#65292;&#36824;&#21487;&#20197;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#22312;&#20223;&#30495;&#21644;&#22312;Unitree A1&#22235;&#36275;&#26426;&#22120;&#20154;&#30495;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#20102;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#21151;&#32791;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successful transfer of a learned controller from simulation to the real world for a legged robot requires not only the ability to identify the system, but also accurate estimation of the robot's state. In this paper, we propose a novel algorithm that can infer not only information about the parameters of the dynamic system, but also estimate important information about the robot's state from previous observations. We integrate our algorithm with Adversarial Motion Priors and achieve a robust, agile, and natural gait in both simulation and on a Unitree A1 quadruped robot in the real world. Empirical results demonstrate that our proposed algorithm enables traversing challenging terrains with lower power consumption compared to the baselines. Both qualitative and quantitative results are presented in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#36890;&#36807;&#20511;&#21161;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20132;&#20114;&#24335;&#30340;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#23618;&#26469;&#36807;&#28388;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05608</link><description>&lt;p&gt;
&#21487;&#24494;&#24322;&#24120;&#26816;&#27979;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#36890;&#36807;&#20511;&#21161;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20132;&#20114;&#24335;&#30340;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#23618;&#26469;&#36807;&#28388;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#24402;&#32435;&#24335;&#30340;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#24403;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26102;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#23545;&#35937;&#65288;&#25110;&#27010;&#24565;&#65289;&#20043;&#38388;&#22312;&#32676;&#20307;&#23618;&#38754;&#19978;&#23384;&#22312;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#38544;&#21547;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#22914;&#20309;&#20197;&#21453;&#21521;&#20256;&#25773;&#21451;&#22909;&#30340;&#26041;&#24335;&#25351;&#23450;&#39046;&#22495;&#25110;&#20808;&#39564;&#27169;&#24577;&#30693;&#35782;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#38544;&#24335;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#20132;&#20114;&#24335;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#30340;&#23618;&#12290;&#35813;&#23618;&#29992;&#20110;&#36807;&#28388;&#30001;&#22806;&#37096;&#30693;&#35782;&#24211;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12289;&#35270;&#35273;&#25512;&#29702;&#21644;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21435;&#38500;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often, deep network models are purely inductive during training and while performing inference on unseen data. Thus, when such models are used for predictions, it is well known that they often fail to capture the semantic information and implicit dependencies that exist among objects (or concepts) on a population level. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. In this work, we propose an end-to-end vision and language model incorporating explicit knowledge graphs. We also introduce an interactive out-of-distribution (OOD) layer using implicit network operator. The layer is used to filter noise that is brought by external knowledge base. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#31995;&#32479;&#20013;&#25490;&#22806;&#20027;&#20041;&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20419;&#36827;&#26410;&#26469;AI&#31995;&#32479;&#21253;&#23481;&#21644;&#21451;&#21892;&#35774;&#35745;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2212.07877</link><description>&lt;p&gt;
AI&#31995;&#32479;&#20013;&#30340;&#25490;&#22806;&#20027;&#20041;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Manifestations of Xenophobia in AI Systems. (arXiv:2212.07877v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#31995;&#32479;&#20013;&#25490;&#22806;&#20027;&#20041;&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20419;&#36827;&#26410;&#26469;AI&#31995;&#32479;&#21253;&#23481;&#21644;&#21451;&#21892;&#35774;&#35745;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#22806;&#20027;&#20041;&#26159;&#36793;&#32536;&#21270;&#12289;&#27495;&#35270;&#21644;&#20914;&#31361;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#20043;&#19968;&#65292;&#28982;&#32780;&#35768;&#22810;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20844;&#24179;&#24615;&#26694;&#26550;&#26410;&#33021;&#20840;&#38754;&#34913;&#37327;&#25110;&#20943;&#36731;&#30001;&#27492;&#23548;&#33268;&#30340;&#25490;&#22806;&#20027;&#20041;&#21361;&#23475;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#27010;&#24565;&#19978;&#30340;&#24046;&#36317;&#65292;&#24182;&#24110;&#21161;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#21644;&#36947;&#24503;&#35774;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#30830;&#23450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25490;&#22806;&#20027;&#20041;&#21361;&#23475;&#26469;&#22522;&#20110;&#23454;&#38469;&#24773;&#20917;&#20998;&#26512;&#25490;&#22806;&#20027;&#20041;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;AI&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#23457;&#26597;&#31038;&#20132;&#23186;&#20307;&#21644;&#25512;&#33616;&#31995;&#32479;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#31227;&#27665;&#12289;&#23601;&#19994;&#20197;&#21450;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;AI&#19982;&#25490;&#22806;&#20027;&#20041;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#26377;&#21161;&#20110;&#20026;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#21253;&#23481;&#12289;&#21451;&#21892;&#35774;&#35745;&#25552;&#20379;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Xenophobia is one of the key drivers of marginalisation, discrimination, and conflict, yet many prominent machine learning (ML) fairness frameworks fail to comprehensively measure or mitigate the resulting xenophobic harms. Here we aim to bridge this conceptual gap and help facilitate safe and ethical design of artificial intelligence (AI) solutions. We ground our analysis of the impact of xenophobia by first identifying distinct types of xenophobic harms, and then applying this framework across a number of prominent AI application domains, reviewing the potential interplay between AI and xenophobia on social media and recommendation systems, healthcare, immigration, employment, as well as biases in large pre-trained models. These help inform our recommendations towards an inclusive, xenophilic design of future AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;</title><link>http://arxiv.org/abs/2212.06096</link><description>&lt;p&gt;
&#38544;&#24335;&#21367;&#31215;&#26680;&#29992;&#20110;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26500;&#24314;&#19982;&#24179;&#31227;&#21644;&#20854;&#20182;&#21464;&#25442;&#31561;&#21516;&#21464;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36825;&#20123;&#21464;&#25442;&#23646;&#20110;&#22522;&#20110;&#21407;&#28857;&#20445;&#25345;&#30340;&#32676;G&#65292;&#20363;&#22914;&#21453;&#23556;&#21644;&#26059;&#36716;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#36890;&#36807;&#22312;&#26680;&#31354;&#38388;&#19978;&#24378;&#21152;&#29305;&#23450;&#20110;&#32676;G&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#26469;&#35299;&#26512;&#27714;&#35299;&#24471;&#21040;&#30340;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#26631;&#20934;&#21367;&#31215;&#12290;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#23545;&#29305;&#23450;&#30340;&#32676;G&#23450;&#21046;&#65292;&#26680;&#22522;&#30784;&#30340;&#23454;&#29616;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#23545;&#31216;&#21464;&#25442;&#65292;&#36825;&#23548;&#33268;&#20102;&#36890;&#29992;&#32676;&#31561;&#21464;&#27169;&#22411;&#30340;&#24320;&#21457;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21442;&#25968;&#21270;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#23454;&#29616;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#20219;&#20309;&#21487;&#20197;&#26500;&#24314;G-&#31561;&#21464;MLP&#30340;&#32676;G&#37117;&#21487;&#20197;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;N&#20307;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;SPE&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.04205</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Preserving Semantics in Textual Adversarial Attacks. (arXiv:2211.04205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;SPE&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#24615;&#22312;&#32447;&#20869;&#23481;&#30340;&#22686;&#38271;&#19982;&#20840;&#29699;&#23545;&#23569;&#25968;&#32676;&#20307;&#30340;&#26292;&#21147;&#29359;&#32618;&#22686;&#21152;&#26377;&#20851;&#12290;&#26377;&#23475;&#30340;&#22312;&#32447;&#20869;&#23481;&#21487;&#20197;&#36731;&#26494;&#12289;&#33258;&#21160;&#21644;&#21311;&#21517;&#22320;&#20135;&#29983;&#12290;&#34429;&#28982;&#36890;&#36807;NLP&#20013;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#24050;&#32463;&#23454;&#29616;&#26576;&#31181;&#24418;&#24335;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#34987;&#23545;&#25239;&#25915;&#20987;&#25152;&#24858;&#24324;&#12290;&#20026;&#20102;&#21152;&#24378;&#29616;&#26377;&#31995;&#32479;&#24182;&#36214;&#22312;&#25915;&#20987;&#32773;&#21069;&#38754;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22909;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#20013;&#39640;&#36798;70%&#30340;&#31034;&#20363;&#24212;&#35813;&#34987;&#20002;&#24323;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#20445;&#25345;&#35821;&#20041;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#26680;&#24515;&#24369;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#23436;&#20840;&#30417;&#30563;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;&#65292;&#31216;&#20026;&#35821;&#20041;&#20445;&#25345;&#32534;&#30721;&#22120;&#65288;SPE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;1.2&#20493;&#33267;5.1&#20493;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#20316;&#20026;&#25554;&#20214;&#21457;&#24067;&#65292;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#20854;&#36136;&#37327;&#21644;&#21152;&#24555;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of hateful online content, or hate speech, has been associated with a global increase in violent crimes against minorities [23]. Harmful online content can be produced easily, automatically and anonymously. Even though, some form of auto-detection is already achieved through text classifiers in NLP, they can be fooled by adversarial attacks. To strengthen existing systems and stay ahead of attackers, we need better adversarial attacks. In this paper, we show that up to 70% of adversarial examples generated by adversarial attacks should be discarded because they do not preserve semantics. We address this core weakness and propose a new, fully supervised sentence embedding technique called Semantics-Preserving-Encoder (SPE). Our method outperforms existing sentence encoders used in adversarial attacks by achieving 1.2x - 5.1x better real attack success rate. We release our code as a plugin that can be used in any existing adversarial attack to improve its quality and speed up 
&lt;/p&gt;</description></item><item><title>TwiRGCN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#21152;&#26435;&#22270;&#21367;&#31215;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20851;&#32852;&#38382;&#39064;&#30340;&#26102;&#38388;&#27573;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#36793;&#26469;&#20256;&#36882;&#20449;&#24687;&#65292;&#20351;&#29992;&#38376;&#25511;&#35013;&#32622;&#26469;&#39044;&#27979;&#31572;&#26696;&#30340;&#31867;&#22411;&#65292;&#24182;&#22312;&#22810;&#36339;&#22797;&#26434;&#26102;&#38388;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.06281</link><description>&lt;p&gt;
TwiRGCN: &#22522;&#20110;&#26102;&#38388;&#21152;&#26435;&#22270;&#21367;&#31215;&#30340;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs. (arXiv:2210.06281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06281
&lt;/p&gt;
&lt;p&gt;
TwiRGCN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#21152;&#26435;&#22270;&#21367;&#31215;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20851;&#32852;&#38382;&#39064;&#30340;&#26102;&#38388;&#27573;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#36793;&#26469;&#20256;&#36882;&#20449;&#24687;&#65292;&#20351;&#29992;&#38376;&#25511;&#35013;&#32622;&#26469;&#39044;&#27979;&#31572;&#26696;&#30340;&#31867;&#22411;&#65292;&#24182;&#22312;&#22810;&#36339;&#22797;&#26434;&#26102;&#38388;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#26159;&#20154;&#31867;&#30340;&#33021;&#21147;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RGCN&#65289;&#25512;&#24191;&#21040;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#26696;&#65292;&#22312;&#21367;&#31215;&#36807;&#31243;&#20013;&#36890;&#36807;&#35843;&#33410;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#26102;&#38388;&#27573;&#19982;KG&#36793;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#26469;&#20256;&#36882;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#38376;&#25511;&#35013;&#32622;&#65292;&#29992;&#26469;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#21542;&#21487;&#33021;&#26159;&#19968;&#20010;KG&#23454;&#20307;&#25110;&#26102;&#38388;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#39044;&#27979;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;TimeQuestions&#19978;&#35780;&#20272;&#20102;&#24471;&#21040;&#30340;&#31995;&#32479;&#65292;&#31216;&#20026;TwiRGCN&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;TwiRGCN&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;TwiRGCN&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;9-10&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed much interest in temporal reasoning over knowledge graphs (KG) for complex question answering (QA), but there remains a substantial gap in human capabilities. We explore how to generalize relational graph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose a novel, intuitive and interpretable scheme to modulate the messages passed through a KG edge during convolution, based on the relevance of its associated time period to the question. We also introduce a gating device to predict if the answer to a complex temporal question is likely to be a KG entity or time and use this prediction to guide our scoring mechanism. We evaluate the resulting system, which we call TwiRGCN, on TimeQuestions, a recently released, challenging dataset for multi-hop complex temporal QA. We show that TwiRGCN significantly outperforms state-of-the-art systems on this dataset across diverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#30340;&#27450;&#39575;&#24615;&#35268;&#21010;&#31574;&#30053;&#12290;&#36890;&#36807;&#25511;&#21046;&#33258;&#20027;&#22242;&#38431;&#30340;&#23494;&#24230;&#21644;&#34892;&#20026;&#65292;&#21487;&#20197;&#27450;&#39575;&#23545;&#25163;&#65292;&#24182;&#23454;&#29616;&#25152;&#38656;&#30340;&#26368;&#32456;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2206.01306</link><description>&lt;p&gt;
&#36164;&#28304;&#20998;&#37197;&#30340;&#27450;&#39575;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deceptive Planning for Resource Allocation. (arXiv:2206.01306v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#30340;&#27450;&#39575;&#24615;&#35268;&#21010;&#31574;&#30053;&#12290;&#36890;&#36807;&#25511;&#21046;&#33258;&#20027;&#22242;&#38431;&#30340;&#23494;&#24230;&#21644;&#34892;&#20026;&#65292;&#21487;&#20197;&#27450;&#39575;&#23545;&#25163;&#65292;&#24182;&#23454;&#29616;&#25152;&#38656;&#30340;&#26368;&#32456;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#33258;&#20027;&#20195;&#29702;&#22242;&#38431;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#36164;&#28304;&#20998;&#37197;&#21040;&#19968;&#32452;&#30446;&#26631;&#20301;&#32622;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#29615;&#22659;&#20013;&#30340;&#23545;&#25163;&#35266;&#23519;&#33258;&#20027;&#22242;&#38431;&#30340;&#34892;&#20026;&#20197;&#25512;&#26029;&#20182;&#20204;&#30340;&#30446;&#26631;&#65292;&#24182;&#23545;&#22242;&#38431;&#20570;&#20986;&#21453;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25511;&#21046;&#33258;&#20027;&#22242;&#38431;&#23494;&#24230;&#30340;&#31574;&#30053;&#65292;&#20197;&#27450;&#39575;&#23545;&#25163;&#20851;&#20110;&#20182;&#20204;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#23454;&#29616;&#25152;&#38656;&#30340;&#26368;&#32456;&#36164;&#28304;&#20998;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#65292;&#26469;&#34920;&#36798;&#23545;&#25163;&#39044;&#26399;&#30340;&#22242;&#38431;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;Kullback-Leibler&#25955;&#24230;&#26469;&#34913;&#37327;&#27450;&#39575;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#20984;&#20248;&#21270;&#30340;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#22840;&#22823;&#34892;&#20026;&#26397;&#30528;&#24178;&#25200;&#20998;&#37197;&#31574;&#30053;&#25110;&#21019;&#36896;&#26368;&#32456;&#20998;&#37197;&#31574;&#30053;&#30340;&#27169;&#31946;&#24615;&#26469;&#27450;&#39575;&#23545;&#25163;&#12290;&#19968;&#39033;&#28041;&#21450;320&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a team of autonomous agents that navigate in an adversarial environment and aim to achieve a task by allocating their resources over a set of target locations. An adversary in the environment observes the autonomous team's behavior to infer their objective and responds against the team. In this setting, we propose strategies for controlling the density of the autonomous team so that they can deceive the adversary regarding their objective while achieving the desired final resource allocation. We first develop a prediction algorithm based on the principle of maximum entropy to express the team's behavior expected by the adversary. Then, by measuring the deceptiveness via Kullback-Leibler divergence, we devise convex optimization-based planning algorithms that deceive the adversary by either exaggerating the behavior towards a decoy allocation strategy or creating ambiguity regarding the final allocation strategy. A user study with $320$ participants demonstrates that the pro
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#22806;&#37096;&#32456;&#27490;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#23450;&#20041;&#32456;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;TerMDP&#65289;&#24182;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#32456;&#27490;&#24773;&#20917;&#24182;&#38480;&#21046;&#36951;&#25022;&#20540;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#39550;&#39542;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.15376</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32456;&#27490;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with a Terminator. (arXiv:2205.15376v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15376
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#22806;&#37096;&#32456;&#27490;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#23450;&#20041;&#32456;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;TerMDP&#65289;&#24182;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#32456;&#27490;&#24773;&#20917;&#24182;&#38480;&#21046;&#36951;&#25022;&#20540;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#39550;&#39542;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#37096;&#32456;&#27490;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#32456;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;TerMDP&#65289;&#65292;&#23427;&#26159;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;episode&#21487;&#33021;&#20250;&#34987;&#22806;&#37096;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#35266;&#23519;&#32773;&#20013;&#26029;&#12290;&#36825;&#20010;&#23450;&#20041;&#32771;&#34385;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24773;&#20917;&#65292;&#27604;&#22914;&#20154;&#31867;&#20986;&#20110;&#19981;&#36866;&#22240;&#32032;&#20013;&#26029;&#33258;&#20027;&#39550;&#39542;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;TerMDP&#30340;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#29366;&#24577;&#32622;&#20449;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#30028;&#38480;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#35777;&#26126;&#26377;&#25928;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#32456;&#27490;&#24773;&#20917;&#65292;&#24182;&#38480;&#21046;&#20102;&#36951;&#25022;&#20540;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#23558;&#20048;&#35266;&#24615;&#65288;&#30456;&#23545;&#20110;&#32456;&#27490;&#65289;&#19982;&#21160;&#24577;&#25240;&#25187;&#22240;&#23376;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32456;&#27490;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#32500;&#39550;&#39542;&#21644;MinAtar&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#39550;&#39542;&#29615;&#22659;&#20013;&#23545;&#20154;&#31867;&#25968;&#25454;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
We present the problem of reinforcement learning with exogenous termination. We define the Termination Markov Decision Process (TerMDP), an extension of the MDP framework, in which episodes may be interrupted by an external non-Markovian observer. This formulation accounts for numerous real-world situations, such as a human interrupting an autonomous driving agent for reasons of discomfort. We learn the parameters of the TerMDP and leverage the structure of the estimation problem to provide state-wise confidence bounds. We use these to construct a provably-efficient algorithm, which accounts for termination, and bound its regret. Motivated by our theoretical analysis, we design and implement a scalable approach, which combines optimism (w.r.t. termination) and a dynamic discount factor, incorporating the termination probability. We deploy our method on high-dimensional driving and MinAtar benchmarks. Additionally, we test our approach on human data in a driving setting. Our results dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.06865</link><description>&lt;p&gt;
&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v3 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#37329;&#34701;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23384;&#22312;&#36866;&#21512;&#32473;&#23450;&#19968;&#32452;&#26399;&#26435;&#24066;&#22330;&#20215;&#26684;&#30340;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20351;&#29992;&#30452;&#35273;&#12289;&#29702;&#35770;&#21644;&#32463;&#39564;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#23547;&#25214;&#23454;&#29616;&#31934;&#30830;&#25110;&#36817;&#20284;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#21338;&#24328;&#29702;&#35770;&#24418;&#24335;&#21270;&#38382;&#39064;&#65292;&#20511;&#21161;&#29616;&#20195;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#26377;&#36827;&#23637;&#26469;&#25628;&#32034;&#38543;&#26426;&#36807;&#31243;&#31354;&#38388;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#34987;&#31038;&#21306;&#21033;&#29992;&#21644;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#32852;&#21512;SPX-VIX&#26657;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#22312;&#27874;&#21160;&#29575;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#65292;&#20197;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#31890;&#23376;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;Guyon et Henry-Labordere&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. More importantly, we hope that our techniques can be leveraged and extended by the community to solve important problems in that field, such as the joint SPX-VIX calibration problem. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \`{a} la Guyon et Henry-Labordere where partic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20301;&#32622;&#24863;&#30693;&#27169;&#24335;&#26680;&#20989;&#25968;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23376;&#31354;&#38388;&#20869;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#35299;&#37322;&#21644;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.02272</link><description>&lt;p&gt;
&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Motif Kernel Networks. (arXiv:2111.02272v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20301;&#32622;&#24863;&#30693;&#27169;&#24335;&#26680;&#20989;&#25968;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23376;&#31354;&#38388;&#20869;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#35299;&#37322;&#21644;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26816;&#27979;&#19982;&#29305;&#23450;&#32467;&#26524;&#30456;&#20851;&#30340;&#25968;&#25454;&#20869;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#21487;&#33021;&#20250;&#38459;&#30861;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#36827;&#27493;&#65292;&#22240;&#20026;&#23427;&#20250;&#25513;&#30422;&#20915;&#31574;&#36807;&#31243;&#24182;&#38459;&#27490;&#31185;&#23398;&#23478;&#23436;&#20840;&#29702;&#35299;&#39044;&#27979;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20687;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#36825;&#26679;&#30340;&#39046;&#22495;&#19987;&#23478;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#22312;&#39640;&#39118;&#38505;&#24773;&#26223;&#20013;&#26159;&#21542;&#21487;&#20197;&#20449;&#20219;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#23558;&#27169;&#22411;&#25972;&#21512;&#21040;&#33258;&#24049;&#30340;&#26085;&#24120;&#24037;&#20316;&#20013;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#24773;&#22659;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;&#65292;&#19968;&#31181;&#28041;&#21450;&#22312;&#26680;&#20989;&#25968;&#30340;&#20301;&#32622;&#24863;&#30693;&#27169;&#24335;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#35299;&#37322;&#21644;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks show promising performance in detecting correlations within data that are associated with specific outcomes. However, the black-box nature of such models can hinder the knowledge advancement in research fields by obscuring the decision process and preventing scientist to fully conceptualize predicted outcomes. Furthermore, domain experts like healthcare providers need explainable predictions to assess whether a predicted outcome can be trusted in high stakes scenarios and to help them integrating a model into their own routine. Therefore, interpretable models play a crucial role for the incorporation of machine learning into high stakes scenarios like healthcare. In this paper we introduce Convolutional Motif Kernel Networks, a neural network architecture that involves learning a feature representation within a subspace of the reproducing kernel Hilbert space of the position-aware motif kernel function. The resulting model enables to directly interpret and ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30524;&#30555;&#35270;&#22270;&#35270;&#39057;&#12289;&#27880;&#35270;&#30524;&#21160;&#21644;&#32908;&#30005;&#36827;&#34892;&#25569;&#25345;&#24847;&#22270;&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#35777;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#20551;&#32930;&#25163;&#25511;&#21046;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2104.03893</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20551;&#32930;&#25163;&#25511;&#21046;&#20013;&#30340;&#32908;&#30005;&#21644;&#35270;&#35273;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.03893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30524;&#30555;&#35270;&#22270;&#35270;&#39057;&#12289;&#27880;&#35270;&#30524;&#21160;&#21644;&#32908;&#30005;&#36827;&#34892;&#25569;&#25345;&#24847;&#22270;&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#35777;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#20551;&#32930;&#25163;&#25511;&#21046;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#23545;&#20110;&#19979;&#32930;&#25130;&#32930;&#32773;&#65292;&#20351;&#29992;&#26426;&#22120;&#20154;&#20551;&#32930;&#25163;&#21487;&#20197;&#24674;&#22797;&#36827;&#34892;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#65288;&#22914;&#32908;&#30005;&#65289;&#30340;&#25511;&#21046;&#26041;&#27861;&#23481;&#26131;&#22240;&#20026;&#36816;&#21160;&#20266;&#36857;&#12289;&#32908;&#32905;&#30130;&#21171;&#31561;&#21407;&#22240;&#23548;&#33268;&#25512;&#29702;&#32467;&#26524;&#19981;&#20339;&#12290;&#35270;&#35273;&#20256;&#24863;&#22120;&#26159;&#20851;&#20110;&#29615;&#22659;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#25512;&#26029;&#21487;&#34892;&#21644;&#39044;&#26399;&#25163;&#21183;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35777;&#25454;&#20063;&#23481;&#26131;&#21463;&#21040;&#33258;&#36523;&#20266;&#36857;&#30340;&#24433;&#21709;&#65292;&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#26159;&#29289;&#20307;&#36974;&#25377;&#12289;&#20809;&#29031;&#21464;&#21270;&#31561;&#12290;&#20351;&#29992;&#29983;&#29702;&#21644;&#35270;&#35273;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22810;&#27169;&#24577;&#35777;&#25454;&#34701;&#21512;&#26159;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#24577;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;&#26041;&#27861;&#65306;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#35777;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#30524;&#30555;&#35270;&#22270;&#35270;&#39057;&#12289;&#27880;&#35270;&#30524;&#21160;&#21644;&#32908;&#30005;&#20174;&#21069;&#33218;&#36827;&#34892;&#25569;&#25345;&#24847;&#22270;&#25512;&#29702;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20010;&#20307;&#21644;&#34701;&#21512;&#24615;&#33021;&#19982;&#26576;&#20123;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: For lower arm amputees, robotic prosthetic hands promise to regain the capability to perform daily living activities. Current control methods based on physiological signals such as electromyography (EMG) are prone to yielding poor inference outcomes due to motion artifacts, muscle fatigue, and many more. Vision sensors are a major source of information about the environment state and can play a vital role in inferring feasible and intended gestures. However, visual evidence is also susceptible to its own artifacts, most often due to object occlusion, lighting changes, etc. Multimodal evidence fusion using physiological and vision sensor measurements is a natural approach due to the complementary strengths of these modalities. Methods: In this paper, we present a Bayesian evidence fusion framework for grasp intent inference using eye-view video, eye-gaze, and EMG from the forearm processed by neural network models. We analyze individual and fused performance as a function of 
&lt;/p&gt;</description></item><item><title>&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#32593;&#32476;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#32806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#29983;&#25104;&#21464;&#24418;&#22330;&#23454;&#29616;&#20960;&#20309;&#21464;&#24418;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/1806.06298</link><description>&lt;p&gt;
&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#32593;&#32476;&#65306;&#26080;&#30417;&#30563;&#35299;&#32806;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry. (arXiv:1806.06298v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1806.06298
&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#32593;&#32476;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#32806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#29983;&#25104;&#21464;&#24418;&#22330;&#23454;&#29616;&#20960;&#20309;&#21464;&#24418;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#27169;&#22411;&#65292;&#20197;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#32806;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;&#22806;&#35266;&#29983;&#25104;&#22120;&#32593;&#32476;&#27169;&#25311;&#19982;&#22806;&#35266;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#39068;&#33394;&#12289;&#29031;&#26126;&#12289;&#36523;&#20221;&#25110;&#31867;&#21035;&#65292;&#32780;&#20960;&#20309;&#29983;&#25104;&#22120;&#36890;&#36807;&#29983;&#25104;&#21464;&#24418;&#22330;&#26469;&#25191;&#34892;&#20960;&#20309;&#21464;&#24418;&#65292;&#22914;&#26059;&#36716;&#21644;&#25289;&#20280;&#65292;&#36890;&#36807;&#25197;&#26354;&#29983;&#25104;&#30340;&#22806;&#35266;&#26469;&#33719;&#21462;&#26368;&#32456;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#24207;&#21015;&#12290;&#20004;&#20010;&#29983;&#25104;&#22120;&#25509;&#25910;&#29420;&#31435;&#30340;&#28508;&#22312;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#22270;&#20687;&#25110;&#35270;&#39057;&#24207;&#21015;&#20013;&#35299;&#32806;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;&#23545;&#20110;&#35270;&#39057;&#25968;&#25454;&#65292;&#24341;&#20837;&#38750;&#32447;&#24615;&#36716;&#25442;&#27169;&#22411;&#21040;&#22806;&#35266;&#21644;&#20960;&#20309;&#29983;&#25104;&#22120;&#20013;&#65292;&#20197;&#25429;&#25417;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#22823;&#37327;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#34920;&#26126;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#21487;&#20197;&#25104;&#21151;&#35299;&#32806;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deformable generator model to disentangle the appearance and geometric information for both image and video data in a purely unsupervised manner. The appearance generator network models the information related to appearance, including color, illumination, identity or category, while the geometric generator performs geometric warping, such as rotation and stretching, through generating deformation field which is used to warp the generated appearance to obtain the final image or video sequences. Two generators take independent latent vectors as input to disentangle the appearance and geometric information from image or video sequences. For video data, a nonlinear transition model is introduced to both the appearance and geometric generators to capture the dynamics over time. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric informat
&lt;/p&gt;</description></item></channel></rss>