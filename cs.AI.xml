<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;</title><link>https://arxiv.org/abs/2404.02090</link><description>&lt;p&gt;
&#24050;&#32463;&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#35777;&#26126;&#23545;&#22122;&#22768;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Already Moderate Population Sizes Provably Yield Strong Robustness to Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02090
&lt;/p&gt;
&lt;p&gt;
&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#34920;&#26126;&#65292;&#20856;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#24212;&#23545;&#35832;&#22914;&#22024;&#26434;&#30340;&#20989;&#25968;&#35780;&#20272;&#31561;&#38543;&#26426;&#24178;&#25200;&#12290;&#22312;&#31532;&#19968;&#27425;&#38024;&#23545;$(1+\lambda)$&#21644;$(1,\lambda)$&#36827;&#21270;&#31639;&#27861;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20004;&#31181;&#31639;&#27861;&#37117;&#33021;&#23481;&#24525;&#24658;&#23450;&#30340;&#22122;&#22768;&#27010;&#29575;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#31181;&#32676;&#35268;&#27169;$\lambda$&#24212;&#33267;&#23569;&#20026;&#38382;&#39064;&#35268;&#27169;$n$&#30340;&#23545;&#25968;&#12290;&#22312;&#36825;&#26041;&#21521;&#19978;&#30340;&#21807;&#19968;&#20808;&#21069;&#32467;&#26524;&#28041;&#21450;&#19981;&#22826;&#29616;&#23454;&#30340;&#19968;&#20301;&#22122;&#22768;&#27169;&#22411;&#65292;&#38656;&#35201;&#36229;&#32447;&#24615;&#30340;&#38382;&#39064;&#35268;&#27169;&#31181;&#32676;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20110;OneMax&#22522;&#20934;&#35777;&#26126;&#20102;&#22823;&#33268;&#26159;&#26080;&#22122;&#22768;&#36816;&#34892;&#26102;&#38388;&#30340;&#19977;&#27425;&#26041;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26174;&#30528;&#26356;&#24378;&#32467;&#26524;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#21363;&#26080;&#22122;&#22768;&#21518;&#20195;&#21487;&#20197;&#30475;&#20316;&#26159;&#29238;&#20195;&#21644;&#26377;&#22122;&#22768;&#30340;&#21518;&#20195;&#20043;&#38388;&#30340;&#26377;&#20559;&#32479;&#19968;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02090v1 Announce Type: cross  Abstract: Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy o
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#36825;&#20123;&#26041;&#38754;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01030</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#35843;&#26597;&#65306;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#36825;&#20123;&#26041;&#38754;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;OpenAI&#30340;DALLE-3&#21644;Google&#30340;Gemini&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#25552;&#31034;&#20063;&#21487;&#33021;&#23548;&#33268;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23637;&#29616;&#26126;&#26174;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;&#31038;&#20250;&#20013;&#30340;&#20998;&#37197;&#21644;&#20195;&#34920;&#24615;&#20260;&#23475;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#23569;&#25968;&#32676;&#20307;&#12290;&#37492;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#35843;&#26597;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#20013;&#20559;&#35265;&#30340;&#19981;&#21516;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#20840;&#38754;&#22238;&#39038;&#20173;&#28982;&#32570;&#20047;&#65292;&#38459;&#30861;&#20102;&#23545;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#30340;&#31995;&#32479;&#24615;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#31532;&#19968;&#27425;&#24191;&#27867;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20808;&#21069;&#20851;&#20110;&#20559;&#35265;&#32500;&#24230;&#30340;&#30740;&#31350;&#65306;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01030v1 Announce Type: cross  Abstract: The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how 
&lt;/p&gt;</description></item><item><title>HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.20183</link><description>&lt;p&gt;
HARMamba: &#22522;&#20110;&#21452;&#21521;&#36873;&#25321;&#24615;SSM&#30340;&#39640;&#25928;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20183
&lt;/p&gt;
&lt;p&gt;
HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#27963;&#21160;&#24863;&#30693;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#30828;&#20214;&#24863;&#30693;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;Mamba&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#12290;HARMamba&#24341;&#20837;&#20102;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#26412;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#31995;&#32479;&#35745;&#31639;&#36127;&#36733;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20183v1 Announce Type: cross  Abstract: Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resourc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19561</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Improved Learning for Scalable Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
end-to-end&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#19968;&#31181;&#21019;&#26032;&#30340;&#23616;&#37096;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36523;&#36845;&#20195;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19050</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26816;&#27979;&#29983;&#25104;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Detecting Generative Parroting through Overfitting Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#20869;&#23481;&#21019;&#24314;&#30340;&#26041;&#24335;&#65292;&#28982;&#32780;&#30001;&#20110;&#29983;&#25104;&#24615;&#27169;&#20223;&#38382;&#39064;&#65292;&#27169;&#22411;&#36807;&#20110;&#27169;&#20223;&#20854;&#35757;&#32451;&#25968;&#25454;&#32780;&#32473;&#29256;&#26435;&#23436;&#25972;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#36825;&#31181;&#27169;&#20223;&#26679;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#24314;&#31435;&#19968;&#20010;&#26816;&#27979;&#38408;&#20540;&#65292;&#20174;&#32780;&#31934;&#30830;&#23450;&#20301;&#20462;&#25913;&#21518;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#20869;&#23481;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#24182;&#21152;&#24378;&#27861;&#24459;&#21512;&#35268;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16854</link><description>&lt;p&gt;
&#19968;&#20010;&#19987;&#23478;&#20215;&#20540;&#19968;&#20010;&#20195;&#24065;&#65306;&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#65288;Expert-Token-Routing&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36890;&#29992;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLM&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#19987;&#23478;LLMs&#34920;&#31034;&#20026;&#20803;LLM&#35789;&#27719;&#20013;&#30340;&#29305;&#27530;&#19987;&#23478;&#20195;&#24065;&#12290;&#20803;LLM&#21487;&#20197;&#36335;&#30001;&#21040;&#19987;&#23478;LLM&#65292;&#23601;&#20687;&#29983;&#25104;&#26032;&#20195;&#24065;&#19968;&#26679;&#12290;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#19981;&#20165;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19987;&#23478;LLMs&#30340;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#12290;&#23427;&#36824;&#21487;&#20197;&#38544;&#34255;&#29992;&#25143;&#35270;&#35282;&#20013;&#30340;&#35814;&#32454;&#21327;&#20316;&#36807;&#31243;&#65292;&#20419;&#36827;&#20132;&#20114;&#23601;&#20687;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#19968;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#28085;&#30422;&#20845;&#20010;&#19981;&#21516;&#19987;&#23478;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#65292;&#23637;&#29616;&#20102;&#36890;&#36807;&#21327;&#21516;&#22810;&#20010;&#19987;&#23478;LLM&#26469;&#26500;&#24314;&#36890;&#29992;&#22411;LLM&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21767;&#37096;&#22320;&#26631;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#35828;&#35805;&#20154;&#30340;&#21767;&#35835;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.16071</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#36827;&#34892;&#22522;&#20934;&#24341;&#23548;&#30340;&#36328;&#35828;&#35805;&#20154;&#21767;&#35835;
&lt;/p&gt;
&lt;p&gt;
Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16071
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21767;&#37096;&#22320;&#26631;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#35828;&#35805;&#20154;&#30340;&#21767;&#35835;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lip reading&#65292;&#21363;&#36890;&#36807;&#35270;&#35273;&#21767;&#37096;&#36816;&#21160;&#35299;&#37322;&#26080;&#22768;&#35821;&#38899;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#24341;&#36215;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#24403;&#21069;&#30340;&#21767;&#35835;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#35828;&#35805;&#20154;&#21464;&#21270;&#30340;&#20132;&#21449;&#35828;&#35805;&#20154;&#22330;&#26223;&#20013;&#36827;&#34892;&#21767;&#35835;&#65292;&#30001;&#20110;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#65292;&#23384;&#22312;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#21767;&#35835;&#31995;&#32479;&#22312;&#22788;&#29702;&#20840;&#26032;&#30340;&#35828;&#35805;&#20154;&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#35828;&#35805;&#20154;&#30340;&#21767;&#35835;&#27169;&#22411;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#35265;&#35299;&#26159;&#20943;&#23569;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#35270;&#35273;&#21464;&#21270;&#65292;&#36991;&#20813;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#35828;&#35805;&#20154;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#28151;&#21512;CTC/attention&#26550;&#26500;&#30340;&#36755;&#20837;&#35270;&#35273;&#32447;&#32034;&#21644;&#22522;&#20110;&#38544;&#21464;&#37327;&#34920;&#31034;&#65292;&#25552;&#20986;&#21033;&#29992;&#21767;&#37096;&#22320;&#26631;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#32447;&#32034;&#65292;&#32780;&#19981;&#26159;&#39057;&#32321;&#20351;&#29992;&#30340;&#35009;&#21098;&#22068;&#24052;&#22270;&#29255;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#20943;&#23569;&#35828;&#35805;&#20154;&#29305;&#23450;&#30340;&#22806;&#35266;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16071v1 Announce Type: new  Abstract: Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15989</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge-guided Machine Learning: Current Trends and Future Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15989
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31185;&#23398;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#24314;&#27169;&#20013;&#30340;&#20114;&#34917;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#26032;&#20852;&#39046;&#22495;&#31185;&#23398;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26088;&#22312;&#21033;&#29992;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20174;&#20351;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#31867;&#22411;&#12289;&#25506;&#35752;&#30340;&#30693;&#35782;-ML&#38598;&#25104;&#24418;&#24335;&#20197;&#21450;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25972;&#21512;&#31185;&#23398;&#30693;&#35782;&#30340;&#26041;&#27861;&#31561;&#26041;&#38754;&#35752;&#35770;&#20102;KGML&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#29615;&#22659;&#31185;&#23398;&#20013;&#21457;&#23637;&#30340;KGML&#26041;&#27861;&#30340;&#19968;&#20123;&#24120;&#35265;&#29992;&#20363;&#31867;&#21035;&#65292;&#20197;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20363;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15989v1 Announce Type: cross  Abstract: This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;</title><link>https://arxiv.org/abs/2403.12237</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#30340;&#39640;&#25928;&#22522;&#20110;Transformer&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#36807;&#31243;&#23545;&#20110;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;HPO&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#20197;&#20854;&#21487;&#35266;&#30340;&#35745;&#31639;&#21344;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#32780;&#38395;&#21517;&#65307;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;TRL-HPO&#37197;&#22791;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#21270;&#21644;&#28176;&#36827;&#29983;&#25104;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;TRL-HPO&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;CNN&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30456;&#21516;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;TRL-HPO&#30340;&#20998;&#31867;&#32467;&#26524;&#20248;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;6.8%&#65292;&#35777;&#26126;&#20102;TRL-HPO&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for 
&lt;/p&gt;</description></item><item><title>Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07815</link><description>&lt;p&gt;
Chronos: &#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Chronos: Learning the Language of Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07815
&lt;/p&gt;
&lt;p&gt;
Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Chronos&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26694;&#26550;&#12290;Chronos&#20351;&#29992;&#32553;&#25918;&#21644;&#37327;&#21270;&#23558;&#26102;&#38388;&#24207;&#21015;&#20540;&#26631;&#35760;&#21270;&#20026;&#22266;&#23450;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#36825;&#20123;&#26631;&#35760;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;T5&#31995;&#21015;&#65288;&#21442;&#25968;&#33539;&#22260;&#20174;20M&#21040;710M&#65289;&#23545;Chronos&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#21547;42&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Chronos&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;b&#65289;&#30456;&#23545;&#20110;&#19987;&#38376;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26469;&#37327;&#21270;&#22810;&#38454;&#27573;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07741</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#36827;&#34892;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26469;&#37327;&#21270;&#22810;&#38454;&#27573;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
6D&#29289;&#20307;&#23039;&#24577;&#30340;&#20272;&#35745;&#26159;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#22312;&#20154;&#26426;&#20132;&#20114;&#12289;&#24037;&#19994;&#26816;&#39564;&#21644;&#33258;&#21160;&#21270;&#31561;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#21487;&#38752;&#30340;&#23039;&#24577;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#31934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;&#35768;&#22810;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#24182;&#38750;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#32780;&#26159;&#30001;&#22810;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#22312;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#38598;&#25104;&#34987;&#35748;&#20026;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#26657;&#20934;&#21644;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#38598;&#25104;&#21482;&#33021;&#24212;&#29992;&#20110;&#21487;&#20197;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26469;&#37327;&#21270;&#22810;&#38454;&#27573;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#36873;&#25321;SurfEmb&#20316;&#20026;&#20195;&#34920;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07741v1 Announce Type: cross  Abstract: The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;HE-Diffusion&#65292;&#36890;&#36807;&#26368;&#23567;&#22833;&#30495;&#26041;&#27861;&#21644;&#31232;&#30095;&#24352;&#37327;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;500&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.05794</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Diffusion Model Using Homomorphic Encryption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;HE-Diffusion&#65292;&#36890;&#36807;&#26368;&#23567;&#22833;&#30495;&#26041;&#27861;&#21644;&#31232;&#30095;&#24352;&#37327;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;500&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#31283;&#23450;&#25193;&#25955;&#26694;&#26550;&#65292;&#31216;&#20026;HE-Diffusion&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#20445;&#25252;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#21435;&#22122;&#38454;&#27573;&#12290;HE-Diffusion&#26159;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#21152;&#23494;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#20197;&#19982;&#31283;&#23450;&#25193;&#25955;&#30340;&#29420;&#29305;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#30830;&#20445;&#38544;&#31169;&#21644;&#21151;&#33021;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#22266;&#26377;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#23567;&#22833;&#30495;&#26041;&#27861;&#65292;&#20351;&#24471;&#37096;&#20998;&#22270;&#20687;&#21152;&#23494;&#26356;&#21152;&#39640;&#25928;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24320;&#38144;&#32780;&#19981;&#25439;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31232;&#30095;&#24352;&#37327;&#34920;&#31034;&#26469;&#21152;&#36895;&#35745;&#31639;&#25805;&#20316;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#36807;&#31243;&#30340;&#25972;&#20307;&#25928;&#29575;&#12290;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#22522;&#20110;HE&#30340;&#38544;&#31169;&#20445;&#25252;&#31283;&#23450;&#25193;&#25955;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HE-Diffusion&#30456;&#27604;&#20043;&#19979;&#23454;&#29616;&#20102;500&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05794v1 Announce Type: cross  Abstract: In this paper, we introduce a privacy-preserving stable diffusion framework leveraging homomorphic encryption, called HE-Diffusion, which primarily focuses on protecting the denoising phase of the diffusion process. HE-Diffusion is a tailored encryption framework specifically designed to align with the unique architecture of stable diffusion, ensuring both privacy and functionality. To address the inherent computational challenges, we propose a novel min-distortion method that enables efficient partial image encryption, significantly reducing the overhead without compromising the model's output quality. Furthermore, we adopt a sparse tensor representation to expedite computational operations, enhancing the overall efficiency of the privacy-preserving diffusion process. We successfully implement HE-based privacy-preserving stable diffusion inference. The experimental results show that HE-Diffusion achieves 500 times speedup compared wit
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.19471</link><description>&lt;p&gt;
&#20005;&#26684;&#30340;LIPS&#27785;&#27809;&#33328;&#33337;&#65306;&#22312;Battleship&#20013;&#20351;&#29992;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#25552;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19471
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#32467;&#21512;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#30340;&#25484;&#25569;&#21644;&#25105;&#20204;&#23545;&#20110;&#22312;&#26377;&#38480;&#35748;&#30693;&#36164;&#28304;&#24773;&#20917;&#19979;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20154;&#20204;&#22914;&#20309;&#22312;&#24040;&#22823;&#20551;&#35774;&#31354;&#38388;&#20013;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#22312;&#22522;&#20110;&#25112;&#33328;&#28216;&#25103;Battleship&#30340;&#32463;&#20856;&#25552;&#38382;&#20219;&#21153;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#65288;LIPS&#65289;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#26399;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#33945;&#29305;&#21345;&#32599;&#20248;&#21270;&#31574;&#30053;&#20063;&#33021;&#20135;&#29983;&#21453;&#26144;&#20154;&#31867;&#22312;&#21508;&#31181;Battleship&#26827;&#30424;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#20016;&#23500;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#20351;&#29992;LLM&#30340;&#22522;&#32447;&#22312;&#23558;&#38382;&#39064;&#19982;&#26827;&#30424;&#29366;&#24577;&#32852;&#31995;&#36215;&#26469;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4V&#24182;&#27809;&#26377;&#27604;&#26080;&#35270;&#35273;&#22522;&#32447;&#25552;&#20379;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#25552;&#38382;&#27169;&#22411;&#22914;&#20309;&#21487;&#33021;&#27169;&#25311;&#21644;&#25351;&#23548;&#20154;&#31867;&#30340;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.07031</link><description>&lt;p&gt;
&#23454;&#20363;&#32423;&#21035;&#30340;&#23433;&#20840;&#24863;&#30693;&#19982;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#21450;&#20854;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#26657;&#20934;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#22609;&#36896;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#21462;&#20195;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#20851;&#27880;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#36229;&#36234;&#32431;&#35270;&#35273;&#36755;&#20837;&#29305;&#24449;&#30340;&#22235;&#31181;&#23454;&#20363;&#32423;&#21035;&#36136;&#37327;&#65292;&#26088;&#22312;&#20351;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38382;&#39064;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#20943;&#23569;&#30001;&#22522;&#20110;DNN&#30340;&#32452;&#20214;&#35782;&#21035;&#20986;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35843;&#20248;&#21487;&#20197;&#22686;&#24378;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20013;&#23433;&#20840;&#20851;&#38190;&#38169;&#35823;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Verilog&#29983;&#25104;&#26694;&#26550;BetterV&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#21028;&#21035;&#22120;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;Verilog&#29983;&#25104;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#30340;Verilog&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03375</link><description>&lt;p&gt;
BetterV: &#36890;&#36807;&#26377;&#21306;&#20998;&#24230;&#30340;&#24341;&#23548;&#23454;&#29616;&#21487;&#25511;&#30340;Verilog&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BetterV: Controlled Verilog Generation with Discriminative Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Verilog&#29983;&#25104;&#26694;&#26550;BetterV&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#21028;&#21035;&#22120;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;Verilog&#29983;&#25104;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#30340;Verilog&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#20195;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#30005;&#36335;&#35774;&#35745;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#30828;&#20214;&#35774;&#35745;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#19981;&#26029;&#22686;&#22810;&#65292;&#20197;&#20415;&#20419;&#36827;&#35774;&#35745;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Verilog&#29983;&#25104;&#26694;&#26550;BetterV&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#32467;&#21512;&#29983;&#25104;&#21028;&#21035;&#22120;&#20197;&#25351;&#23548;&#29305;&#23450;&#35774;&#35745;&#38656;&#27714;&#12290;Verilog&#27169;&#22359;&#26159;&#20174;&#20114;&#32852;&#32593;&#20013;&#25910;&#38598;&#12289;&#36807;&#28388;&#21644;&#22788;&#29702;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#24178;&#20928;&#32780;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;Instruct-tuning&#26041;&#27861;&#65292;&#23545;LLMs&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20197;&#20102;&#35299;&#20851;&#20110;Verilog&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20016;&#23500;&#35757;&#32451;&#38598;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#21028;&#21035;&#22120;&#65292;&#20026;LLMs&#20248;&#21270;Verilog&#23454;&#29616;&#25552;&#20379;&#25351;&#23548;&#12290;BetterV&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#21644;&#21151;&#33021;&#19978;&#27491;&#30830;&#30340;Verilog&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen rising research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes the large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. The Verilog modules are collected, filtered and processed from internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tuned the LLMs to understand the knowledge about Verilog. Furthermore, data are augmented to enrich the training set and also used to train a generative discriminator on particular downstream task, which leads a guidance for the LLMs to optimize the Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, whic
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2310.18948</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36335;&#24452;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#19978;&#20132;&#36890;&#23545;&#20110;&#23454;&#29616;&#20840;&#29699;&#32463;&#27982;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#21487;&#25345;&#32493;&#24615;&#21644;&#20445;&#25252;&#28626;&#21361;&#28023;&#27915;&#29289;&#31181;&#26041;&#38754;&#23653;&#34892;&#29983;&#24577;&#20041;&#21153;&#65292;&#23588;&#20854;&#26159;&#20445;&#25252;&#22823;&#22411;&#40120;&#31867;&#31181;&#32676;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;(AIS)&#25968;&#25454;&#36890;&#36807;&#25552;&#20379;&#33337;&#33334;&#36816;&#21160;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#21270;&#30340;&#20132;&#36890;&#30417;&#25511;&#65292;&#20174;&#32780;&#36991;&#20813;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#65292;&#20174;&#32780;&#39044;&#38450;&#33337;&#33334;&#19982;&#40120;&#40060;&#30340;&#30896;&#25758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(Bi-LSTM)&#26500;&#24314;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;1&#21040;3&#23567;&#26102;&#30340;AIS&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#25509;&#19979;&#26469;12&#23567;&#26102;&#30340;&#33337;&#33334;&#36712;&#36857;&#12290;&#25105;&#20204;&#20174;&#21382;&#21490;AIS&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#27010;&#29575;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#27169;&#22411;&#38543;&#21518;&#39044;&#27979;&#33337;&#33334;&#30340;&#36712;&#36857;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
&lt;/p&gt;</description></item><item><title>MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2312.06786</link><description>&lt;p&gt;
&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06786
&lt;/p&gt;
&lt;p&gt;
MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(LTSF)&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#36807;&#21435;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#26410;&#26469;&#20540;&#12290;&#24403;&#21069;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;(SOTA)&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#30001;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20855;&#26377;&#32447;&#24615;&#26144;&#23556;&#23618;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#39118;&#26684;&#30340;&#22686;&#24378;&#32447;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;(MoLE)&#12290;MoLE&#19981;&#26159;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#35757;&#32451;&#22810;&#20010;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;(&#21363;&#19987;&#23478;)&#21644;&#19968;&#20010;&#26435;&#34913;&#21644;&#28151;&#21512;&#20854;&#36755;&#20986;&#30340;&#36335;&#30001;&#27169;&#22411;&#12290;&#34429;&#28982;&#25972;&#20010;&#26694;&#26550;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#20294;&#27599;&#20010;&#19987;&#23478;&#37117;&#23398;&#20250;&#19987;&#38376;&#22788;&#29702;&#29305;&#23450;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#32780;&#36335;&#30001;&#27169;&#22411;&#21017;&#23398;&#20250;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#19987;&#23478;&#20204;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MoLE&#38477;&#20302;&#20102;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;(DLinear&#65292;RLinear&#21644;RMLP)&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23383;&#20856;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#31639;&#27861;&#65288;PSQD&#65289;&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#20855;&#26377;&#23383;&#20856;&#22411;&#20248;&#20808;&#32423;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20808;&#21069;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38646;-shot&#32452;&#25104;&#21644;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.02360</link><description>&lt;p&gt;
&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#29992;&#20110;&#23383;&#20856;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning. (arXiv:2310.02360v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23383;&#20856;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#31639;&#27861;&#65288;PSQD&#65289;&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#20855;&#26377;&#23383;&#20856;&#22411;&#20248;&#20808;&#32423;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20808;&#21069;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38646;-shot&#32452;&#25104;&#21644;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35774;&#35745;&#26631;&#37327;&#22870;&#21169;&#20989;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#20174;&#22836;&#24320;&#21457;&#27169;&#22411;&#30340;&#22266;&#26377;&#20302;&#25928;&#24615;&#12290;&#30456;&#21453;&#65292;&#26368;&#22909;&#26159;&#23558;&#22797;&#26434;&#20219;&#21153;&#20197;&#22522;&#26412;&#23376;&#20219;&#21153;&#30340;&#24418;&#24335;&#25351;&#23450;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#37325;&#22797;&#20351;&#29992;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36830;&#32493;&#31354;&#38388;&#30340;&#23383;&#20856;&#22411;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20248;&#20808;&#32423;&#23376;&#20219;&#21153;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#35299;&#20915;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23376;&#20219;&#21153;&#36716;&#25442;&#36827;&#34892;&#26631;&#37327;&#21270;&#65292;&#24182;&#20351;&#29992;&#20215;&#20540;&#20998;&#35299;&#36880;&#27493;&#35299;&#20915;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#65288;PSQD&#65289;&#65292;&#19968;&#31181;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#20855;&#26377;&#23383;&#20856;&#22411;&#20248;&#20808;&#32423;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#31639;&#27861;&#12290;PSQD&#33021;&#22815;&#22312;&#38646;-shot&#32452;&#25104;&#20043;&#21518;&#37325;&#22797;&#20351;&#29992;&#20808;&#21069;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#36866;&#24212;&#27493;&#39588;&#12290;&#23427;&#20855;&#22791;&#20445;&#30041;&#23376;&#20219;&#21153;&#35757;&#32451;&#20449;&#24687;&#24182;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19968;&#31995;&#21015;&#32447;&#24615;&#31995;&#32479;&#23454;&#20363;&#20013;&#35774;&#32622;&#27714;&#35299;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#21442;&#25968;&#65292;&#21487;&#20197;&#25509;&#36817;&#26368;&#20339;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30697;&#38453;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2310.02246</link><description>&lt;p&gt;
&#23398;&#20064;&#25918;&#26494;&#65306;&#22312;&#19968;&#31995;&#21015;&#32447;&#24615;&#31995;&#32479;&#23454;&#20363;&#20013;&#35774;&#32622;&#27714;&#35299;&#22120;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances. (arXiv:2310.02246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19968;&#31995;&#21015;&#32447;&#24615;&#31995;&#32479;&#23454;&#20363;&#20013;&#35774;&#32622;&#27714;&#35299;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#21442;&#25968;&#65292;&#21487;&#20197;&#25509;&#36817;&#26368;&#20339;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30697;&#38453;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;$Ax=b$&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#31185;&#23398;&#35745;&#31639;&#21407;&#29702;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#27714;&#35299;&#22120;&#21644;&#39044;&#22788;&#29702;&#22120;&#12290;&#23427;&#20204;&#24102;&#26377;&#21442;&#25968;&#65292;&#20854;&#26368;&#20339;&#20540;&#21462;&#20915;&#20110;&#35201;&#35299;&#20915;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#30830;&#23450;&#65307;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#27425;&#20248;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#38656;&#35201;&#35299;&#20915;&#35768;&#22810;&#30456;&#20851;&#32447;&#24615;&#31995;&#32479;&#30340;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#22312;&#21333;&#20010;&#25968;&#20540;&#27169;&#25311;&#26399;&#38388;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#39034;&#24207;&#36873;&#25321;&#21442;&#25968;&#65292;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20339;&#24635;&#36845;&#20195;&#27425;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30697;&#38453;&#35745;&#31639;&#65311;&#23545;&#20110;&#36807;&#24230;&#36731;&#26494;&#65288;SOR&#65289;&#36825;&#31181;&#26631;&#20934;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#22238;&#31572;&#32943;&#23450;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#20165;&#36845;&#20195;&#27425;&#25968;&#20316;&#20026;&#21453;&#39304;&#30340;&#36172;&#24466;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36873;&#25321;&#24207;&#21015;&#23454;&#20363;&#30340;&#21442;&#25968;&#65292;&#20351;&#24471;&#24635;&#25104;&#26412;&#25509;&#36817;&#26368;&#20339;&#22266;&#23450;&#30340;&#969;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving a linear system $Ax=b$ is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter $\omega$ has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm -- using only the number of iterations as feedback -- can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed $\omega$ as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02067</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65306;&#26397;&#30528;&#26356;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability. (arXiv:2310.02067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#22270;&#20687;&#21462;&#35777;&#30340;&#32972;&#26223;&#19979;&#65292;&#24456;&#38590;&#30830;&#23450;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20165;&#20165;&#21033;&#29992;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#24120;&#65292;&#26102;&#38388;&#30456;&#36817;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#23646;&#20110;&#21516;&#19968;&#24180;&#40836;&#31867;&#21035;&#30340;&#65289;&#20855;&#26377;&#19968;&#20123;&#20849;&#21516;&#30340;&#20869;&#23481;&#23646;&#24615;&#12290;&#36825;&#31181;&#20869;&#23481;&#20559;&#24046;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#20869;&#23481;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#24102;&#26377;&#23884;&#20837;&#24335;&#24180;&#40836;&#20449;&#21495;&#30340;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#24180;&#40836;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#35757;&#32451;&#30340;&#8220;&#26631;&#20934;&#8221;&#31070;&#32463;&#32593;&#32476;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#20316;&#20026;&#28508;&#22312;&#30340;&#23545;&#31574;&#65292;&#26412;&#25991;&#24212;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#20943;&#36731;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of temporal image forensics, it is not evident that a neural network, trained on images from different time-slots (classes), exploit solely age related features. Usually, images taken in close temporal proximity (e.g., belonging to the same age class) share some common content properties. Such content bias can be exploited by a neural network. In this work, a novel approach that evaluates the influence of image content is proposed. This approach is verified using synthetic images (where content bias can be ruled out) with an age signal embedded. Based on the proposed approach, it is shown that a `standard' neural network trained in the context of age classification is strongly dependent on image content. As a potential countermeasure, two different techniques are applied to mitigate the influence of the image content during training, and they are also evaluated by the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12481</link><description>&lt;p&gt;
HANS&#65292;&#20320;&#32874;&#26126;&#21527;&#65311;&#31070;&#32463;&#31995;&#32479;&#30340;Clever Hans&#25928;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(It-LLMs)&#23637;&#31034;&#20986;&#20102;&#22312;&#35748;&#30693;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#26041;&#38754;&#25512;&#29702;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21487;&#20197;&#35753;&#20154;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#29702;&#35299;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#20107;&#23454;&#19978;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;(MCQ)&#22522;&#20934;&#26469;&#26500;&#24314;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#30830;&#20999;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;It-LLMs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#39034;&#24207;&#20559;&#35265;&#8221;&#65292;&#32473;&#36866;&#24403;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;MCQ&#22522;&#20934;&#23545;It-LLMs&#30340;&#25269;&#25239;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#65292;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#24182;&#24341;&#21457;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#36890;&#36807;&#31532;&#19968;&#20301;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#27169;&#22411;&#20013;&#23384;&#22312;&#32467;&#26500;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (It-LLMs) have been exhibiting outstanding abilities to reason around cognitive states, intentions, and reactions of all people involved, letting humans guide and comprehend day-to-day social interactions effectively. In fact, several multiple-choice questions (MCQ) benchmarks have been proposed to construct solid assessments of the models' abilities. However, earlier works are demonstrating the presence of inherent "order bias" in It-LLMs, posing challenges to the appropriate evaluation. In this paper, we investigate It-LLMs' resilience abilities towards a series of probing tests using four MCQ benchmarks. Introducing adversarial examples, we show a significant performance gap, mainly when varying the order of the choices, which reveals a selection bias and brings into discussion reasoning abilities. Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in 
&lt;/p&gt;</description></item><item><title>CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11295</link><description>&lt;p&gt;
CPLLM: &#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11295
&lt;/p&gt;
&lt;p&gt;
CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;LLM&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#24739;&#32773;&#22312;&#19979;&#19968;&#27425;&#23601;&#35786;&#25110;&#38543;&#21518;&#30340;&#35786;&#26029;&#20013;&#26159;&#21542;&#20250;&#34987;&#35786;&#26029;&#20026;&#30446;&#26631;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;RETAIN&#21644;Med-BERT&#65292;&#21518;&#32773;&#26159;&#20351;&#29992;&#32467;&#26500;&#21270;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CPLLM&#22312;PR-AUC&#21644;ROC-AUC&#25351;&#26631;&#19978;&#22343;&#36229;&#36807;&#20102;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.08793</link><description>&lt;p&gt;
Fin-Fact:&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08793
&lt;/p&gt;
&lt;p&gt;
Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#20107;&#23454;&#26680;&#26597;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fin-Fact&#65292;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21253;&#25324;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#25552;&#20379;&#19987;&#19994;&#30693;&#35782;&#21644;&#21487;&#20449;&#24230;&#12290;&#30001;&#20110;&#20854;&#22810;&#27169;&#24577;&#24615;&#36136;&#28085;&#30422;&#20102;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#65292;Fin-Fact&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#28304;&#65292;&#20197;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#37329;&#34701;&#39046;&#22495;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#22312;&#36130;&#21153;&#25253;&#21578;&#21644;&#26032;&#38395;&#20256;&#25773;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#36890;&#36807;&#25552;&#20379;&#26377;&#28145;&#24230;&#30340;&#35299;&#37322;&#65292;Fin-Fact&#20351;&#29992;&#25143;&#65292;&#21253;&#25324;&#39046;&#22495;&#19987;&#23478;&#21644;&#32456;&#31471;&#29992;&#25143;&#65292;&#33021;&#22815;&#29702;&#35299;&#20107;&#23454;&#26680;&#26597;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#39564;&#35777;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#20419;&#36827;&#23545;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#30340;&#20449;&#20219;&#12290;Fin-Fact&#25968;&#25454;&#38598;&#20197;&#21450;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;https://github.com/IIT-DM/Fin-Fact/ &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
&lt;/p&gt;</description></item><item><title>APLA&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#29983;&#25104;&#20013;&#19968;&#33268;&#24615;&#32454;&#33410;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12605</link><description>&lt;p&gt;
APLA: &#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#20351;&#19968;&#33268;&#24615;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12605
&lt;/p&gt;
&lt;p&gt;
APLA&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#29983;&#25104;&#20013;&#19968;&#33268;&#24615;&#32454;&#33410;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#22312;&#24103;&#20043;&#38388;&#20445;&#30041;&#23616;&#37096;&#21306;&#22495;&#30340;&#19968;&#33268;&#32454;&#33410;&#12290;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#36924;&#36817;&#39640;&#26031;&#22122;&#22768;&#20998;&#24067;&#26102;&#21033;&#29992;&#20102;&#39044;&#27979;&#22122;&#22768;&#65292;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#36755;&#20837;&#26412;&#36523;&#30340;&#20869;&#22312;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24378;&#35843;&#39044;&#27979;&#21644;&#21442;&#32771;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24573;&#35270;&#20102;&#35270;&#39057;&#26412;&#36523;&#22266;&#26377;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#21463;&#21040;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#32593;&#32476;&#32467;&#26500;&#65292;&#21517;&#20026;&#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#65288;APLA&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#19968;&#20010;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#24314;&#31435;&#22312;&#39044;&#35757;&#32451;&#31283;&#23450;&#30340;&#25193;&#25955;&#32593;&#32476;&#19978;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#32039;&#20945;&#32593;&#32476;&#65292;&#31216;&#20026;&#35270;&#39057;&#29983;&#25104;&#21464;&#25442;&#22120;&#65288;VGT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have exhibited promising progress in video generation. However, they often struggle to retain consistent details within local regions across frames. One underlying cause is that traditional diffusion models approximate Gaussian noise distribution by utilizing predictive noise, without fully accounting for the impact of inherent information within the input itself. Additionally, these models emphasize the distinction between predictions and references, neglecting information intrinsic to the videos. To address this limitation, inspired by the self-attention mechanism, we propose a novel text-to-video (T2V) generation network structure based on diffusion models, dubbed Additional Perturbation for Latent noise with Adversarial training (APLA). Our approach only necessitates a single video as input and builds upon pre-trained stable diffusion networks. Notably, we introduce an additional compact network, known as the Video Generation Transformer (VGT). This auxiliary compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.07308</link><description>&lt;p&gt;
LLM&#33258;&#21355;&#65306;&#36890;&#36807;&#33258;&#26816;&#65292;LLMs&#24847;&#35782;&#21040;&#23427;&#20204;&#34987;&#24858;&#24324;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#20154;&#31867;&#25552;&#31034;&#20570;&#20986;&#39640;&#36136;&#37327;&#25991;&#26412;&#22238;&#24212;&#32780;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#24212;&#29992;&#25143;&#25552;&#31034;&#26102;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#32473;&#29992;&#25143;&#25552;&#20379;&#29359;&#32618;&#25351;&#23548;&#65289;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30528;&#37325;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#26041;&#27861;&#65288;&#20363;&#22914;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65289;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#32469;&#36807;&#29983;&#25104;&#26377;&#23475;&#25991;&#26412;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#24049;&#30340;&#22238;&#24212;&#36827;&#34892;&#36807;&#28388;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#27809;&#26377;&#34987;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#26469;&#38450;&#27490;&#20854;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12488</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#36719;&#20214;&#23433;&#20840;&#65306;&#25506;&#32034;ChatGPT&#22312;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#22810;&#25165;&#22810;&#33402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#23545;&#38382;&#39064;&#30340;&#28508;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23637;&#31034;&#12290;&#23427;&#33021;&#22815;&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#32508;&#21512;&#26469;&#33258;&#22312;&#32447;&#36164;&#28304;&#21644;&#29992;&#25143;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#23457;&#26597;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38754;&#21521;&#23433;&#20840;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#33021;&#21147;&#65292;&#20174;&#25915;&#20987;&#32773;&#21644;&#23433;&#20840;&#20998;&#26512;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22312;&#20960;&#20010;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#26377;&#24847;&#22320;&#24341;&#20837;&#25361;&#25112;&#26469;&#35780;&#20272;&#20854;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;ChatGPT&#25552;&#20379;&#30340;&#31572;&#26696;&#36136;&#37327;&#30340;&#32771;&#23519;&#65292;&#25105;&#20204;&#23545;&#20854;&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#39046;&#22495;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#26377;&#20102;&#26356;&#28165;&#26224;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#22320;&#21306;&#23545;&#29615;&#22659;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#20174;&#32780;&#25512;&#36827;&#29615;&#22659;&#20844;&#24179;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.05494</link><description>&lt;p&gt;
&#36890;&#36807;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#29615;&#22659;&#20844;&#24179;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Environmentally Equitable AI via Geographical Load Balancing. (arXiv:2307.05494v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#22320;&#21306;&#23545;&#29615;&#22659;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#20174;&#32780;&#25512;&#36827;&#29615;&#22659;&#20844;&#24179;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#39129;&#21319;&#20154;&#27668;&#25512;&#21160;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24040;&#22823;&#29615;&#22659;&#36275;&#36857;&#30340;&#24555;&#36895;&#22686;&#38271;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#20351;AI&#26356;&#33410;&#33021;&#29615;&#20445;&#65292;&#20294;&#29615;&#22659;&#19981;&#24179;&#31561;&#8212;&#8212;&#21363;AI&#30340;&#29615;&#22659;&#36275;&#36857;&#22312;&#26576;&#20123;&#22320;&#21306;&#21487;&#33021;&#19981;&#25104;&#27604;&#20363;&#22320;&#26356;&#39640;&#8212;&#8212;&#24050;&#32463;&#20986;&#29616;&#65292;&#24182;&#24341;&#21457;&#20102;&#31038;&#20250;&#29983;&#24577;&#27491;&#20041;&#30340;&#20851;&#20999;&#12290;&#26412;&#25991;&#36890;&#36807;&#24179;&#34913;AI&#30340;&#21306;&#22495;&#36127;&#38754;&#29615;&#22659;&#24433;&#21709;&#26469;&#39318;&#27425;&#35299;&#20915;AI&#30340;&#29615;&#22659;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;AI&#27169;&#22411;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#21644;&#27700;&#36275;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#37325;&#20844;&#24179;&#30340;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#65288;GLB&#65289;&#26469;&#26126;&#30830;&#35299;&#20915;AI&#23545;&#26368;&#24369;&#21183;&#22320;&#21306;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#20998;&#24067;&#22312;&#22320;&#29702;&#19978;&#30340;10&#20010;&#25968;&#25454;&#20013;&#24515;&#26469;&#36816;&#34892;&#22522;&#20110;&#36319;&#36394;&#30340;&#20223;&#30495;&#65292;&#36825;&#20123;&#25968;&#25454;&#20013;&#24515;&#20026;&#22823;&#22411;L
&lt;/p&gt;
&lt;p&gt;
Fueled by the soaring popularity of large language and foundation models, the accelerated growth of artificial intelligence (AI) models' enormous environmental footprint has come under increased scrutiny. While many approaches have been proposed to make AI more energy-efficient and environmentally friendly, environmental inequity -- the fact that AI's environmental footprint can be disproportionately higher in certain regions than in others -- has emerged, raising social-ecological justice concerns. This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact. Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions. We run trace-based simulations by considering a set of 10 geographically-distributed data centers that serve inference requests for a large l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#20013;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20256;&#36882;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07662</link><description>&lt;p&gt;
&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#30340;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries. (arXiv:2306.07662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#20013;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20256;&#36882;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#31034;&#20363;&#26469;&#30740;&#31350;&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#24050;&#32463;&#25193;&#23637;&#21040;&#20102;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33719;&#24471;&#30340;&#32467;&#26524;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#25552;&#21319;&#21040;&#26102;&#38388;&#21270;&#30340;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38750;&#26102;&#38388;&#21270;&#24773;&#20917;&#19979;&#30456;&#20851;&#26041;&#27861;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#36890;&#29992;&#30340;&#20256;&#36882;&#32467;&#26524;&#65292;&#21487;&#20197;&#30830;&#23450;&#29616;&#26377;&#32467;&#26524;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#21487;&#20197;&#25512;&#24191;&#21040;&#26102;&#38388;&#21270;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the study of the unique characterisability and learnability of database queries by means of examples has been extended to ontology-mediated queries. Here, we study in how far the obtained results can be lifted to temporalised ontology-mediated queries. We provide a systematic introduction to the relevant approaches in the non-temporal case and then show general transfer results pinpointing under which conditions existing results can be lifted to temporalised queries.
&lt;/p&gt;</description></item><item><title>MULTIGAIN 2.0&#26159;&#19968;&#20010;&#22522;&#20110;PRISM&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;MultiGain&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#21487;&#23545;&#20855;&#26377;&#22810;&#32500;&#32422;&#26463;&#30340;&#27010;&#29575;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#39564;&#35777;&#21644;&#32508;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20108;&#32500;&#21644;&#19977;&#32500; Pareto &#26354;&#32447;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16752</link><description>&lt;p&gt;
MULTIGAIN 2.0&#65306;&#22810;&#20010;&#24179;&#22343;&#22238;&#25253;&#12289;LTL&#21644;&#31283;&#24577;&#32422;&#26463;&#30340;MDP&#25511;&#21046;&#22120;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL and steady-state constraints. (arXiv:2305.16752v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16752
&lt;/p&gt;
&lt;p&gt;
MULTIGAIN 2.0&#26159;&#19968;&#20010;&#22522;&#20110;PRISM&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;MultiGain&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#21487;&#23545;&#20855;&#26377;&#22810;&#32500;&#32422;&#26463;&#30340;&#27010;&#29575;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#39564;&#35777;&#21644;&#32508;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20108;&#32500;&#21644;&#19977;&#32500; Pareto &#26354;&#32447;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MULTIGAIN 2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26500;&#24314;&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#24037;&#20855;MultiGain&#30340;&#19968;&#20010;&#37325;&#22823;&#25193;&#23637;&#12290;&#36825;&#20010;&#26032;&#29256;&#26412;&#25193;&#23637;&#20102;MultiGain&#30340;&#22810;&#30446;&#26631;&#33021;&#21147;&#65292;&#20801;&#35768;&#23545;&#20855;&#26377;&#22810;&#32500;&#38271;&#26399;&#24179;&#22343;&#22238;&#25253;&#32467;&#26500;&#12289;&#31283;&#24577;&#32422;&#26463;&#21644;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#23646;&#24615;&#30340;&#27010;&#29575;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#39564;&#35777;&#21644;&#32508;&#21512;&#12290;&#27492;&#22806;&#65292;MULTIGAIN 2.0&#25552;&#20379;&#20102;&#19968;&#31181;&#23547;&#25214;&#26377;&#38480;&#20869;&#23384;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20108;&#32500;&#21644;&#19977;&#32500; Pareto &#26354;&#32447;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#22810;&#30446;&#26631;&#24773;&#20917;&#19979;&#36827;&#34892;&#26435;&#34913;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MULTIGAIN 2.0, a major extension to the controller synthesis tool MultiGain, built on top of the probabilistic model checker PRISM. This new version extends MultiGain's multi-objective capabilities, by allowing for the formal verification and synthesis of controllers for probabilistic systems with multi-dimensional long-run average reward structures, steady-state constraints, and linear temporal logic properties. Additionally, MULTIGAIN 2.0 provides an approach for finding finite memory solutions and the capability for two- and three-dimensional visualization of Pareto curves to facilitate trade-off analysis in multi-objective scenarios
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06027</link><description>&lt;p&gt;
&#25345;&#32493;&#25193;&#25955;&#65306;&#20351;&#29992;C-LoRA&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#25345;&#32493;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21482;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#23450;&#20041;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22810;&#20010;&#32454;&#31890;&#24230;&#27010;&#24565;&#20197;&#36830;&#32493;&#26041;&#24335;&#65288;&#21363;&#25345;&#32493;&#24615;&#22320;&#65289;&#33258;&#23450;&#20041;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#65292;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#25216;&#26415;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;C-LoRA&#65292;&#37319;&#29992;&#27969;&#34892;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36328;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#36830;&#32493;&#33258;&#25105;&#27491;&#21017;&#21270;&#20302;&#31209;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21253;&#25324;&#33258;&#23450;&#20041;&#23545;&#35937;&#30340;&#21333;&#35789;&#65288;&#21363;&#8220;&#20154;&#8221;&#29992;&#20110;&#20154;&#33080;&#25968;&#25454;&#38598;&#65289;&#24182;&#21021;&#22987;&#21270;&#20026;&#23436;&#20840;&#38543;&#26426;&#23884;&#20837;&#30340;&#23450;&#21046;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#24341;&#20837;&#20102;&#24494;&#23567;&#30340;&#39069;&#22806;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter cost
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.11695</link><description>&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11695
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#22240;&#32032;&#30340;&#27169;&#22411;&#12290;&#23558;&#21464;&#21270;&#30340;&#28508;&#22312;&#35201;&#32032;&#20998;&#31163;&#25104;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#21464;&#37327;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#27169;&#20223;&#20154;&#31867;&#35266;&#23519;&#23545;&#35937;&#25110;&#20851;&#31995;&#26102;&#30340;&#26377;&#24847;&#20041;&#29702;&#35299;&#36807;&#31243;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;DRL&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;DRL&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#12289;&#24212;&#29992;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20004;&#20010;&#20844;&#35748;&#23450;&#20041;&#65288;&#30452;&#35266;&#23450;&#20041;&#21644;&#32676;&#35770;&#23450;&#20041;&#65289;&#30340;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;DRL&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2211.09325</link><description>&lt;p&gt;
TAX-Pose&#65306;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#36171;&#20104;&#26426;&#22120;&#20154;&#26377;&#25928;&#22320;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22522;&#20110;&#31034;&#33539;&#36716;&#31227;&#30456;&#20851;&#25216;&#33021;&#65311;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#25110;&#26410;&#35265;&#36807;&#30340;&#37197;&#32622;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20132;&#20114;&#23545;&#35937;&#30456;&#20851;&#37096;&#20998;&#30340;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#20851;&#31995;&#26159;&#19968;&#31181;&#21487;&#20197;&#36716;&#31227;&#21040;&#21516;&#19968;&#31867;&#21035;&#26032;&#29289;&#20307;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#27010;&#24565;&#65307;&#20363;&#22914;&#65292;&#24179;&#24213;&#38149;&#30456;&#23545;&#20110;&#28900;&#31665;&#30340;&#23039;&#21183;&#20851;&#31995;&#25110;&#32773;&#26479;&#23376;&#30456;&#23545;&#20110;&#26479;&#26550;&#30340;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#20026;&#8220;&#36328;&#23039;&#21183;&#8221;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#27010;&#24565;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#23398;&#20064;&#30340;&#23545;&#35937;&#38388;&#23545;&#24212;&#20851;&#31995;&#26469;&#23398;&#20064;&#20272;&#35745;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#12290;&#28982;&#21518;&#65292;&#20272;&#35745;&#30340;&#36328;&#23039;&#21183;&#29992;&#20110;&#24341;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#23558;&#23545;&#35937;&#25805;&#32437;&#21040;&#25152;&#38656;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
&lt;/p&gt;</description></item></channel></rss>