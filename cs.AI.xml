<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01723</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#21487;&#20197;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#65292;&#20294;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20110;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#40065;&#26834;&#24494;&#35843;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#20197;&#21450;&#24494;&#35843;&#30340;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#32622;&#20449;&#24230;&#26657;&#20934;&#36825;&#19968;&#26631;&#20934;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#32454;&#35843;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26657;&#20934;&#30340;&#25285;&#24551;&#65292;&#24182;&#36890;&#36807;&#26174;&#31034;&#26222;&#36890;&#24494;&#35843;&#29978;&#33267;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#36896;&#25104;&#20102;&#25439;&#23475;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#65292;&#23427;&#22312;&#26657;&#20934;&#21644;&#40065;&#26834;&#24615;&#19978;&#25552;&#20379;&#20102;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16573</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;ARRT: &#26032;&#33539;&#24335;&#21450;&#20854;&#25361;&#25112;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16573
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#19987;&#26377;&#31995;&#32479;&#65292;&#21482;&#33021;&#36890;&#36807;&#65288;&#36890;&#24120;&#26159;&#38480;&#21046;&#24615;&#30340;&#65289;&#32593;&#32476;&#25110;&#36719;&#20214;&#32534;&#31243;&#25509;&#21475;&#35775;&#38382;&#12290;&#36825;&#23601;&#26159;&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#30340;&#33539;&#24335;&#12290;&#19982;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#30340;&#24773;&#20917;&#30456;&#21453;&#65292;&#22914;&#24320;&#25918;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#23553;&#38381;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35780;&#20272;&#12289;&#22522;&#20934;&#27979;&#35797;&#21644;&#27979;&#35797;&#36896;&#25104;&#20102;&#29305;&#23450;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30028;&#23450;&#21069;&#36848;&#25361;&#25112;&#22914;&#20309;&#20316;&#20026;&#23545;LMaaS&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65288;ARRT&#65289;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19982;&#27599;&#20010;&#36825;&#22235;&#20010;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#20449;&#24687;&#19981;&#36275;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#24314;&#35758;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26412;&#25991;&#26159;&#24403;&#21069;&#20027;&#35201;LMaaS&#29616;&#26377;&#30693;&#35782;&#30340;&#19968;&#31449;&#24335;&#38598;&#38182;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. Contrasting with scenarios where full model access is available, as in the case of open-source models, such closed-off language models create specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness (ARRT) of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We shed light on current solutions, provide some recommendations, and highlight the directions for future advancements. On the other hand, it serves as a one-stop-shop for the extant knowledge about current, major LMaaS, offering a synthesized o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#26469;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#36712;&#36857;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20132;&#36890;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35774;&#35745;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#36712;&#36857;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.12677</link><description>&lt;p&gt;
TrTr&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#27969;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#30340;&#36712;&#36857;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population. (arXiv:2309.12677v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#26469;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#36712;&#36857;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20132;&#36890;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35774;&#35745;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#36712;&#36857;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36712;&#36857;&#22810;&#26679;&#24615;&#26159;&#35299;&#20915;&#23454;&#38469;&#20132;&#36890;&#20219;&#21153;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#35268;&#27169;&#21442;&#25968;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#36712;&#36857;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#20852;&#30340;Transformer&#25216;&#26415;&#20197;&#20854;&#24182;&#34892;&#35745;&#31639;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#21487;&#20197;&#21033;&#29992;&#20855;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;Transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#65292;&#26088;&#22312;&#23398;&#20064;&#36710;&#36742;&#32676;&#20307;&#20869;&#30340;&#36712;&#36857;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;&#20854;&#36866;&#24212;&#20132;&#36890;&#20219;&#21153;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#38543;&#21518;&#35774;&#35745;&#20102;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#26102;&#31354;&#38656;&#27714;&#23545;&#24212;&#30340;&#22122;&#22768;&#65292;&#36825;&#20123;&#22122;&#22768;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#34987;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding trajectory diversity is a fundamental aspect of addressing practical traffic tasks. However, capturing the diversity of trajectories presents challenges, particularly with traditional machine learning and recurrent neural networks due to the requirement of large-scale parameters. The emerging Transformer technology, renowned for its parallel computation capabilities enabling the utilization of models with hundreds of millions of parameters, offers a promising solution. In this study, we apply the Transformer architecture to traffic tasks, aiming to learn the diversity of trajectories within vehicle populations. We analyze the Transformer's attention mechanism and its adaptability to the goals of traffic tasks, and subsequently, design specific pre-training tasks. To achieve this, we create a data structure tailored to the attention mechanism and introduce a set of noises that correspond to spatio-temporal demands, which are incorporated into the structured data during the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.12673</link><description>&lt;p&gt;
&#20851;&#20110;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20316;&#20026;&#29616;&#20195; Hopfield &#27169;&#22411;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#19982;&#20854;&#23494;&#38598;&#30340;&#23545;&#24212;&#29289;&#19968;&#26679;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20855;&#22791;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#20854;&#19968;&#27493;&#36817;&#20284;&#23545;&#24212;&#20110;&#31232;&#30095;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#31232;&#30095;&#29109;&#27491;&#21017;&#21270;&#22120;&#30340;&#20984;&#20849;&#36717;&#23548;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#30340;&#31232;&#30095; Hopfield &#33021;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20174;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#20013;&#25512;&#23548;&#20986;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#19968;&#27493;&#36817;&#20284;&#31561;&#20215;&#20110;&#31232;&#30095;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#31232;&#30095;&#24230;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22312;&#35777;&#26126;&#19978;&#35201;&#27604;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#26356;&#32039;&#20945;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#31232;&#30095;&#20248;&#21183;&#20986;&#29616;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#24555;&#36895;&#30340;&#22266;&#23450;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point conver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10997</link><description>&lt;p&gt;
SPEGTI: &#32467;&#26500;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26082;&#36924;&#30495;&#21448;&#19982;&#25991;&#26412;&#25552;&#31034;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36136;&#37327;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#22823;&#27169;&#22411;&#12290;&#36825;&#31181;&#36845;&#20195;&#36807;&#31243;&#26159;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#19981;&#20165;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#65292;&#36824;&#19982;&#20854;&#20182;&#21306;&#22495;&#30456;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Muse&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;MRF&#32534;&#30721;&#20102;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23481;&#24615;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;Muse&#39044;&#27979;&#27493;&#39588;&#12290;&#20351;&#29992;MRF&#30340;&#25512;&#26029;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#24555;&#36895;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#36890;&#36807;&#23545;MRF&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03913</link><description>&lt;p&gt;
&#22312;&#21457;&#23637;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20013;&#24212;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20197;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20849;&#21516;&#35748;&#30693;&#31995;&#32479;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21644;&#24212;&#29992;&#24050;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#23558;&#20316;&#20026;&#19968;&#21517;&#38431;&#21451;&#32780;&#19981;&#20165;&#20165;&#26159;&#24037;&#20855;&#19982;&#20154;&#31867;&#21327;&#20316;&#12290;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#27599;&#20010;&#25104;&#21592;&#30340;&#24050;&#30693;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#65292;&#24182;&#23558;&#32852;&#21512;&#24615;&#33021;&#25552;&#39640;&#21040;&#20219;&#20309;&#23454;&#20307;&#20043;&#19978;&#12290;2023&#24180;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25112;&#30053;&#35745;&#21010;&#26356;&#26032;&#35748;&#35782;&#21040;&#65292;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29420;&#31435;&#24615;&#33021;&#30340;&#30740;&#31350;&#35745;&#21010;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#24517;&#39035;&#25552;&#20379;&#30340;&#21151;&#33021;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#33021;&#20316;&#20026;&#20154;&#31867;&#30340;&#38431;&#21451;&#23384;&#22312;&#20105;&#35758;&#12290;&#20027;&#35201;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#37319;&#29992;"&#21327;&#20316;"&#33539;&#24335;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human
&lt;/p&gt;</description></item><item><title>Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03170</link><description>&lt;p&gt;
Focused Transformer: &#21453;&#24046;&#35757;&#32451;&#23545;&#19978;&#19979;&#25991;&#32553;&#25918;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03170
&lt;/p&gt;
&lt;p&gt;
Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#21270;&#30340;&#26041;&#24335;&#21560;&#32435;&#26032;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#27880;&#24847;&#21147;&#23618;&#25552;&#20379;&#35775;&#38382;&#22806;&#37096;&#23384;&#20648;&#22120;&#30340;&#33021;&#21147;&#65292;&#35813;&#23384;&#20648;&#22120;&#30001;&#65288;&#38190;&#65292;&#20540;&#65289;&#23545;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#38190;&#19982;&#26080;&#20851;&#38190;&#30340;&#27604;&#20363;&#20943;&#23569;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#26080;&#20851;&#38190;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#24515;&#38382;&#39064;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#19982;&#19981;&#21516;&#35821;&#20041;&#20540;&#30456;&#20851;&#32852;&#30340;&#38190;&#21487;&#33021;&#37325;&#21472;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Focused Transformer&#65288;FoT&#65289;&#65292;&#19968;&#31181;&#21463;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#65288;&#38190;&#65292;&#20540;&#65289;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20351;&#19978;&#19979;&#25991;&#38271;&#24230;&#24471;&#20197;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12230</link><description>&lt;p&gt;
&#22855;&#22937;&#30340;&#26435;&#37325;&#21450;&#20854;&#26597;&#25214;&#26041;&#27861;&#65306;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20248;&#21270;&#20854;&#31232;&#30095;&#21021;&#22987;&#21270;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;DST&#33021;&#22815;&#32988;&#36807;&#23494;&#38598;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21098;&#26525;&#21644;&#29983;&#38271;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#21453;&#22797;&#24212;&#29992;&#20197;&#35843;&#25972;&#32593;&#32476;&#30340;&#31232;&#30095;&#36830;&#25509;&#12290;&#34429;&#28982;&#29983;&#38271;&#26631;&#20934;&#23545;DST&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#36739;&#22909;&#22320;&#30740;&#31350;&#20102;&#65292;&#20294;&#21098;&#26525;&#26631;&#20934;&#30340;&#24433;&#21709;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23545;&#21508;&#31181;&#21098;&#26525;&#26631;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#23545; DST &#35299;&#20915;&#26041;&#26696;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#30740;&#31350;&#26041;&#27861;&#37117;&#20135;&#29983;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
&lt;/p&gt;</description></item><item><title>MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10012</link><description>&lt;p&gt;
MagicBrush: &#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10012
&lt;/p&gt;
&lt;p&gt;
MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#20174;&#20010;&#20154;&#20351;&#29992;&#21040;&#19987;&#19994;&#24212;&#29992;&#65288;&#22914;Photoshop&#65289;&#24191;&#27867;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#65292;&#35201;&#20040;&#26159;&#22312;&#33258;&#21160;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21547;&#26377;&#22823;&#37327;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicBrush&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#65292;&#21253;&#25324;&#21333;&#20010;&#25805;&#20316;&#12289;&#22810;&#20010;&#25805;&#20316;&#12289;&#25552;&#20379;&#25513;&#30721;&#21644;&#19981;&#25552;&#20379;&#25513;&#30721;&#31561;&#19981;&#21516;&#22330;&#26223;&#12290;MagicBrush&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65288;&#28304;&#22270;&#20687;&#65292;&#25351;&#20196;&#65292;&#30446;&#26631;&#22270;&#20687;&#65289;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;MagicBrush&#19978;&#24494;&#35843;InstructPix2Pix&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;Motion-DVAE&#65292;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#20381;&#36182;&#20851;&#31995;&#30340;&#36816;&#21160;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#20154;&#20307;&#36816;&#21160;&#21435;&#22122;&#12290;&#20854;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#32467;&#21512;&#20102;&#22238;&#24402;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05846</link><description>&lt;p&gt;
Motion-DVAE: &#38754;&#21521;&#24555;&#36895;&#20154;&#20307;&#36816;&#21160;&#21435;&#22122;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Motion-DVAE: Unsupervised learning for fast human motion denoising. (arXiv:2306.05846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;Motion-DVAE&#65292;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#20381;&#36182;&#20851;&#31995;&#30340;&#36816;&#21160;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#20154;&#20307;&#36816;&#21160;&#21435;&#22122;&#12290;&#20854;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#32467;&#21512;&#20102;&#22238;&#24402;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#22024;&#26434;&#30340;&#35266;&#27979;&#20013;&#24674;&#22797;&#30495;&#23454;&#20934;&#30830;&#30340;&#20154;&#20307;&#36816;&#21160;&#20013;&#65292;&#23039;&#24577;&#21644;&#36816;&#21160;&#20808;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20808;&#39564;&#26469;&#25552;&#28860;&#36880;&#24103;&#39044;&#27979;&#30340;&#32467;&#26524;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#28982;&#32780;&#65292;&#35768;&#22810;&#36816;&#21160;&#20808;&#39564;&#20165;&#27169;&#25311;&#30456;&#37051;&#23039;&#24577;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#22312;&#32791;&#26102;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#20351;&#29992;&#65292;&#36825;&#23545;&#35768;&#22810;&#38656;&#35201;&#23454;&#26102;&#36816;&#21160;&#25429;&#25417;&#30340;&#24212;&#29992;&#31243;&#24207;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;Motion-DVAE&#65292;&#36825;&#26159;&#19968;&#31181;&#36816;&#21160;&#20808;&#39564;&#65292;&#29992;&#20110;&#25429;&#33719;&#20154;&#31867;&#36816;&#21160;&#30340;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290; &#20316;&#20026;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DVAE&#65289;&#27169;&#22411;&#31995;&#21015;&#30340;&#19968;&#37096;&#20998;&#65292;Motion-DVAE&#32467;&#21512;&#20102;VAE&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24490;&#29615;&#32467;&#26500;&#30340;&#26102;&#38388;&#24314;&#27169;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#32467;&#21512;&#22238;&#24402;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#20110;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#23454;&#26102;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Pose and motion priors are crucial for recovering realistic and accurate human motion from noisy observations. Substantial progress has been made on pose and shape estimation from images, and recent works showed impressive results using priors to refine frame-wise predictions. However, a lot of motion priors only model transitions between consecutive poses and are used in time-consuming optimization procedures, which is problematic for many applications requiring real-time motion capture. We introduce Motion-DVAE, a motion prior to capture the short-term dependencies of human motion. As part of the dynamical variational autoencoder (DVAE) models family, Motion-DVAE combines the generative capability of VAE models and the temporal modeling of recurrent architectures. Together with Motion-DVAE, we introduce an unsupervised learned denoising method unifying regression- and optimization-based approaches in a single framework for real-time 3D human pose estimation. Experiments show that the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35745;&#25968;&#27169;&#29702;&#35770;&#65288;#SMT&#65289;&#30340;&#32534;&#35793;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#30340;DPLL&#65288;T&#65289;&#25628;&#32034;&#30165;&#36857;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#32534;&#35793;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.04541</link><description>&lt;p&gt;
&#33258;&#39030;&#21521;&#19979;&#30340;&#30693;&#35782;&#32534;&#35793;&#29992;&#20110;&#35745;&#25968;&#27169;&#29702;&#35770;&#65288;Counting Modulo Theories&#65289;
&lt;/p&gt;
&lt;p&gt;
Top-Down Knowledge Compilation for Counting Modulo Theories. (arXiv:2306.04541v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35745;&#25968;&#27169;&#29702;&#35770;&#65288;#SMT&#65289;&#30340;&#32534;&#35793;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#30340;DPLL&#65288;T&#65289;&#25628;&#32034;&#30165;&#36857;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#32534;&#35793;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#20844;&#24335;&#26159;&#30830;&#23450;&#24615;&#21487;&#20998;&#35299;&#21542;&#23450;&#33539;&#24335;&#65288;d-DNNF&#65289;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21629;&#39064;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#65288;&#65283;SAT&#65289;&#12290;&#23558;&#20219;&#24847;&#20844;&#24335;&#36716;&#25442;&#20026;&#20801;&#35768;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#31216;&#20026;&#30693;&#35782;&#32534;&#35793;&#12290;&#33258;&#39030;&#21521;&#19979;&#30340;&#30693;&#35782;&#32534;&#35793;&#26159;&#35299;&#20915;&#65283;SAT&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#20840;&#38754;&#30340;DPLL&#25628;&#32034;&#30340;&#30165;&#36857;&#26469;&#33719;&#24471;d-DNNF&#34920;&#31034;&#12290;&#34429;&#28982;&#30693;&#35782;&#32534;&#35793;&#22312;&#21629;&#39064;&#26041;&#27861;&#26041;&#38754;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#65288;&#26080;&#37327;&#21270;&#65289;&#35745;&#25968;&#27169;&#29702;&#35770;&#35774;&#32622;&#65288;&#65283;SMT&#65289;&#30340;&#30693;&#35782;&#32534;&#35793;&#30740;&#31350;&#35201;&#23569;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#65283;SMT&#30340;&#32534;&#35793;&#31574;&#30053;&#12290;&#25105;&#20204;&#29305;&#21035;&#20513;&#23548;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#30340;DPLL&#65288;T&#65289;&#25628;&#32034;&#30165;&#36857;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#32534;&#35793;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional model counting (#SAT) can be solved efficiently when the input formula is in deterministic decomposable negation normal form (d-DNNF). Translating an arbitrary formula into a representation that allows inference tasks, such as counting, to be performed efficiently, is called knowledge compilation. Top-down knowledge compilation is a state-of-the-art technique for solving #SAT problems that leverages the traces of exhaustive DPLL search to obtain d-DNNF representations. While knowledge compilation is well studied for propositional approaches, knowledge compilation for the (quantifier free) counting modulo theory setting (#SMT) has been studied to a much lesser degree. In this paper, we discuss compilation strategies for #SMT. We specifically advocate for a top-down compiler based on the traces of exhaustive DPLL(T) search.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01286</link><description>&lt;p&gt;
KL-Divergence&#24341;&#23548;&#19979;&#30340;&#28201;&#24230;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28201;&#24230;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35268;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#22810;&#26679;&#21270;&#12290;&#38543;&#30528;&#28201;&#24230;&#30340;&#21319;&#39640;&#65292;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#20294;&#20063;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#8212;&#8212;&#29983;&#25104;&#30475;&#20284;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20196;&#29260;&#12290;&#32531;&#35299;&#24187;&#35273;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#25552;&#20379;&#28304;/&#22522;&#30784;&#25991;&#26723;&#65292;&#24182;&#20351;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#26469;&#28304;&#30456;&#20851;&#19988;&#21487;&#24402;&#22240;&#30340;&#39044;&#27979;&#12290;&#30475;&#26469;&#23384;&#22312;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26494;&#24347;&#22266;&#23450;&#28201;&#24230;&#21644;&#36890;&#36807;KL-&#25955;&#24230;&#26681;&#25454;&#20854;&#19982;&#28304;&#30340;&#30456;&#20851;&#24615;&#24341;&#23548;&#21160;&#24577;&#28201;&#24230;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#20248;&#20110;&#24120;&#35268;&#30340;top-k&#21644;top-p&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temperature sampling is a conventional approach to diversify large language model predictions. As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual. One common approach to mitigate hallucinations is to provide source/grounding documents and the model is trained to produce predictions that bind to and are attributable to the provided source. It appears that there is a trade-off between diversity and attribution. To mitigate any such trade-off, we propose to relax the constraint of having a fixed temperature over decoding steps, and a mechanism to guide the dynamic temperature according to its relevance to the source through KL-divergence. Our experiments justifies the trade-off, and shows that our sampling algorithm outperforms the conventional top-k and top-p algorithms in conversational question-answering and summarization tasks.
&lt;/p&gt;</description></item><item><title>ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.18498</link><description>&lt;p&gt;
ANPL&#65306;&#20351;&#29992;&#20132;&#20114;&#24335;&#20998;&#35299;&#32534;&#35793;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18498
&lt;/p&gt;
&lt;p&gt;
ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#22686;&#24378;&#32534;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23558;&#24120;&#35265;&#30340;&#20351;&#29992;&#27169;&#24335;&#32534;&#35793;&#20026;&#32534;&#31243;&#35821;&#35328;&#65292;&#20363;&#22914;Python&#65292;&#20294;&#22914;&#20309;&#32534;&#36753;&#21644;&#35843;&#35797;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ANPL&#65292;&#19968;&#31181;&#32534;&#31243;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20998;&#35299;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#12290;&#22312;ANPL&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#35813;&#33609;&#22270;&#25351;&#23450;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#25968;&#25454;&#27969;&#12290;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#23558;&#29983;&#25104;&#21151;&#33021;&#30340;&#26114;&#36149;&#20219;&#21153;&#21368;&#36733;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;ANPL&#31243;&#24207;&#65292;ANPL&#32534;&#35793;&#22120;&#20250;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#23380;&#20013;&#30340;&#21151;&#33021;&#65292;&#24182;&#36981;&#23432;&#33609;&#22270;&#20013;&#25351;&#23450;&#30340;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#23558;ANPL&#37096;&#32626;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#65292;&#23427;&#26159;&#19968;&#32452;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06988</link><description>&lt;p&gt;
&#33258;&#25105;&#38142;&#24335;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#19982;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06988
&lt;/p&gt;
&lt;p&gt;
SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#38382;&#31572;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#21551;&#21160;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#35270;&#39057;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#36827;&#34892;&#20018;&#25509;&#65292;&#32780;&#26410;&#36827;&#34892;&#26174;&#24335;&#30340;&#35821;&#35328;&#24863;&#30693;&#21644;&#26102;&#38388;&#24314;&#27169;&#12290;&#24403;&#35270;&#39057;&#36755;&#20837;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#19982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#26102;&#65292;&#36825;&#31181;&#22343;&#21248;&#24103;&#37319;&#26679;&#36890;&#24120;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#20002;&#22833;&#12290;&#23613;&#31649;&#20154;&#31867;&#36890;&#24120;&#20250;&#25214;&#21040;&#35270;&#39057;&#20013;&#35201;&#20851;&#27880;&#30340;&#29255;&#27573;&#24182;&#20498;&#24102;&#29255;&#21051;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#35757;&#32451;&#19968;&#20010;&#26126;&#30830;&#30340;&#35270;&#39057;&#29255;&#27573;&#23616;&#37096;&#21270;&#22120;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeViLA&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65288;BLIP-2&#65289;&#26469;&#22788;&#29702;&#35270;&#39057;&#30340;&#26102;&#38388;&#20851;&#38190;&#24103;&#23450;&#20301;&#21644;&#38382;&#31572;&#12290;SeViLA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#65292;&#20004;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#20197;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#22312;TVQA&#12289;TVR&#21644;How2QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SeViLA&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27880;&#37322;&#23601;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.09991</link><description>&lt;p&gt;
&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#23457;&#35745;LLM&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37096;&#32626;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#21644;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#20998;&#31867;&#36824;&#26159;&#29983;&#25104;&#65292;&#37117;&#34920;&#29616;&#20986;&#26377;&#20559;&#24046;&#21644;&#19981;&#36127;&#36131;&#20219;&#30340;&#34892;&#20026;&#65292;&#23545;&#20154;&#31867;&#36896;&#25104;&#20102;&#35268;&#27169;&#24615;&#30340;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23457;&#35745;&#24037;&#20855;&#21033;&#29992;&#20154;&#21644;&#25110;AI&#26469;&#21457;&#29616;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#30340;&#25991;&#29486;&#65292;&#24182;&#37319;&#35775;&#20102;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#19987;&#23478;&#65292;&#20197;&#22686;&#24378;&#23457;&#35745;&#24037;&#20855;&#8220;AdaTest&#8221;&#65288;Ribeiro&#21644;Lundberg&#65292;2022&#65289;&#65292;&#35813;&#24037;&#20855;&#30001;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36890;&#36807;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24863;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#22312;&#21327;&#20316;&#23457;&#35745;&#20013;&#21033;&#29992;&#20154;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#22686;&#24378;&#24037;&#20855;AdaTest ++&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20351;&#21442;&#19982;&#32773;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;Regional Point-Language Contrastive Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#28857;&#29420;&#31435;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#23545;&#26032;&#39062;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00962</link><description>&lt;p&gt;
RegionPLC&#65306;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;Regional Point-Language Contrastive Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#28857;&#29420;&#31435;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#23545;&#26032;&#39062;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#22312;&#38381;&#38598;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26080;&#27861;&#22788;&#29702;&#26032;&#39062;&#31867;&#21035;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RegionPLC&#30340;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#32463;&#36807;&#23553;&#38381;&#38598;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#22791;&#24320;&#25918;&#35789;&#27719;&#35782;&#21035;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36890;&#36807;&#26631;&#39064;&#29983;&#25104;&#20174;2D&#22522;&#30784;&#27169;&#22411;&#20013;&#24341;&#21457;&#21306;&#22495;&#32423;&#35270;&#35273;-&#35821;&#35328;&#30693;&#35782;&#65292;&#36827;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#23494;&#38598;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28857;&#21028;&#21035;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#20351;&#24471;&#20174;&#26631;&#39064;&#20013;&#36827;&#34892;&#28857;&#29420;&#31435;&#23398;&#20064;&#20197;&#23454;&#29616;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;ScanNet&#12289;ScanNet200&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340;&#22522;&#20110;&#27880;&#37322;&#30340;3D&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#29702;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;RegionPLC&#22312;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#30340;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;11.6%&#21644;6.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing 3D scene understanding tasks have achieved high performance on close-set benchmarks but fail to handle novel categories in real-world applications. To this end, we propose a Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, which equips models trained on closed-set datasets with open-vocabulary recognition capabilities. We propose dense visual prompts to elicit region-level visual-language knowledge from 2D foundation models via captioning, which further allows us to build dense regional point-language associations. Then, we design a point-discriminative contrastive learning objective to enable point-independent learning from captions for dense scene understanding. We conduct extensive experiments on ScanNet, ScanNet200, and nuScenes datasets. Our RegionPLC significantly outperforms previous base-annotated 3D open-world scene understanding approaches by an average of 11.6\% and 6.6\% for semantic and instance segme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2211.06841</link><description>&lt;p&gt;
Point-MA2E:&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning. (arXiv:2211.06841v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#20013;&#65292;&#25513;&#33180;&#24314;&#27169;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20174;&#20854;&#25513;&#33180;&#23545;&#24212;&#37096;&#20998;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#12290;&#32771;&#34385;&#21040;&#25513;&#33180;&#21482;&#20250;&#25439;&#22351;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#28857;&#65292;&#26412;&#25991;&#25512;&#24191;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#35268;&#21017;&#30772;&#22351;&#25152;&#26377;&#36755;&#20837;&#28857;&#65292;&#20197;&#34917;&#20805;&#27969;&#34892;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;Point-MA2E&#65289;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#28857;&#20113;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#21644;&#25513;&#33180;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20174;&#20854;&#25439;&#22351;&#29256;&#26412;&#20013;&#37325;&#24314;&#21407;&#22987;&#28857;&#20113;&#12290;&#25506;&#32034;&#20102;&#21508;&#31181;&#28857;&#20113;&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#38750;Transformer&#32534;&#30721;&#22120;&#65292;&#25353;&#29031;&#24120;&#35265;&#20570;&#27861;&#30452;&#25509;&#37325;&#24314;&#26410;&#25439;&#22351;&#30340;&#28857;&#20113;&#12290;&#23545;&#20110;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23616;&#37096;&#34917;&#19969;&#21644;&#31895;&#30053;&#30340;&#20840;&#23616;&#24418;&#29366;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked modeling has demonstrated its effectiveness in self-supervised point cloud learning by reconstructing the complete point cloud from its masked counterpart. Considering that masking only corrupts partial points of the input, in this paper, we promote the affine transformation, which corrupts all input points with certain rules, to complement the popular masking strategy, leading to the Masked and Affine transformed AutoEncoder for point cloud learning (Point-MA2E). Generally, we corrupt the point cloud with affine transformation and masking as input and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Various point cloud encoders are explored in this study. For non-Transformer encoders, we follow the common practice to reconstruct the uncorrupted point cloud directly. For Transformer-based encoders, we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape
&lt;/p&gt;</description></item></channel></rss>