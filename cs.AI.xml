<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#26367;&#25442;&#31070;&#32463;&#20803;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#25277;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#24179;&#34913;&#20943;&#23569;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10891</link><description>&lt;p&gt;
&#21477;&#27861;&#19982;&#35821;&#20041;&#32447;&#24615;&#25277;&#35937;&#21644;&#32454;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#26367;&#25442;&#31070;&#32463;&#20803;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#25277;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#24179;&#34913;&#20943;&#23569;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#26159;&#19968;&#31181;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#20851;&#38190;&#39564;&#35777;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20351;&#29992;&#36804;&#20170;&#38750;&#24120;&#26377;&#38480;&#12290;&#20197;&#21069;&#30340;&#20998;&#31867;&#32593;&#32476;&#25277;&#35937;&#26041;&#27861;&#23558;&#22810;&#20010;&#31070;&#32463;&#20803;&#26367;&#25442;&#20026;&#36275;&#22815;&#30456;&#20284;&#30340;&#20854;&#20013;&#19968;&#20010;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#30456;&#20284;&#24615;&#23450;&#20041;&#20026;&#21477;&#27861;&#19978;&#65288;&#20351;&#29992;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36830;&#25509;&#25968;&#37327;&#65289;&#25110;&#35821;&#20041;&#19978;&#65288;&#23545;&#20110;&#21508;&#31181;&#36755;&#20837;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#20540;&#65289;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#33021;&#36798;&#21040;&#36866;&#24230;&#30340;&#20943;&#23569;&#65292;&#32780;&#19988;&#23454;&#29616;&#36215;&#26469;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#21487;&#20197;&#34987;&#20854;&#20182;&#31070;&#32463;&#20803;&#30340;&#32447;&#24615;&#32452;&#21512;&#26367;&#25442;&#65292;&#20174;&#32780;&#25913;&#21892;&#20943;&#23569;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#25277;&#35937;&#19978;&#24212;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#29616;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#25105;&#20204;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#26356;&#22909;&#30340;&#20943;&#23569;&#21644;&#31934;&#30830;&#24230;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#35299;&#32806;&#21487;&#36798;&#24615;&#35268;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#25193;&#23637;&#21644;&#25910;&#38598;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10846</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;&#35299;&#32806;&#30340;&#21487;&#36798;&#24615;&#35268;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning. (arXiv:2307.10846v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#35299;&#32806;&#21487;&#36798;&#24615;&#35268;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#25193;&#23637;&#21644;&#25910;&#38598;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#21487;&#20197;&#20351;Agent&#33258;&#21457;&#22320;&#35774;&#23450;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#25216;&#33021;&#12290;&#23613;&#31649;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#20986;&#33394;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#26159;&#22312;&#26102;&#38388;&#24310;&#23637;&#20219;&#21153;&#20013;&#36798;&#21040;&#36828;&#36317;&#31163;&#30446;&#26631;&#20173;&#28982;&#26159;GCRL&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#35268;&#21010;&#31639;&#27861;&#26469;&#35745;&#21010;&#20013;&#38388;&#23376;&#30446;&#26631;&#26469;&#22686;&#24378;GCRL&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#20004;&#20010;&#20851;&#38190;&#35201;&#27714;&#65306;&#65288;i&#65289;&#29366;&#24577;&#34920;&#31034;&#31354;&#38388;&#26469;&#25628;&#32034;&#26377;&#25928;&#30340;&#23376;&#30446;&#26631;&#65292;&#65288;ii&#65289;&#36317;&#31163;&#20989;&#25968;&#26469;&#27979;&#37327;&#23376;&#30446;&#26631;&#30340;&#21487;&#36798;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#23427;&#20204;&#24456;&#38590;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#19978;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;GC&#31574;&#30053;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#20934;&#30830;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#20250;&#24433;&#21709;&#35268;&#21010;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#35299;&#32806;&#21487;&#36798;&#24615;&#35268;&#21010;&#65288;REPlan&#65289;&#30340;&#30446;&#26631;&#26465;&#20214;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20462;&#25913;&#20102;Miller&#23545;&#23545;&#27604;&#24615;&#35299;&#37322;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;&#23450;&#20041;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10832</link><description>&lt;p&gt;
&#20462;&#25913;Miller&#23545;&#23545;&#27604;&#24615;&#35299;&#37322;&#30340;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Modifications of the Miller definition of contrastive (counterfactual) explanations. (arXiv:2307.10832v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#25913;&#20102;Miller&#23545;&#23545;&#27604;&#24615;&#35299;&#37322;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;&#23450;&#20041;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Miller&#22522;&#20110;&#33879;&#21517;&#30340;Halpern-Pearl&#65288;HP&#65289;&#22240;&#26524;&#21644;&#65288;&#38750;&#23545;&#27604;&#24615;&#65289;&#35299;&#37322;&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#23545;&#27604;&#24615;&#65288;&#21453;&#20107;&#23454;&#65289;&#35299;&#37322;&#30340;&#23450;&#20041;&#12290;&#20851;&#38190;&#26159;&#65292;Miller&#30340;&#23450;&#20041;&#22522;&#20110;&#21407;&#22987;&#30340;HP&#35299;&#37322;&#23450;&#20041;&#65292;&#20294;&#36825;&#24050;&#32463;&#34987;Halpern&#20462;&#25913;&#36807;&#20102;&#65307;&#21487;&#33021;&#26159;&#22240;&#20026;&#21407;&#22987;&#23450;&#20041;&#22312;&#35768;&#22810;&#26631;&#20934;&#31034;&#20363;&#20013;&#20135;&#29983;&#20102;&#36829;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;Borner&#25552;&#20986;&#20102;&#31532;&#19977;&#20010;&#23450;&#20041;&#65292;&#35266;&#23519;&#21040;&#36825;&#20010;&#20462;&#25913;&#21518;&#30340;HP&#23450;&#20041;&#20063;&#21487;&#33021;&#20135;&#29983;&#36829;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Miller&#23450;&#20041;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#32487;&#25215;&#33258;&#21407;&#22987;HP&#23450;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20004;&#20010;&#22522;&#20110;&#26356;&#20581;&#22766;&#30340;&#20462;&#25913;&#21518;&#30340;HP&#21644;Borner&#23450;&#20041;&#30340;&#25913;&#36827;&#29256;&#26412;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#20445;&#30041;&#20102;Miller&#23450;&#20041;&#30340;&#31934;&#31070;&#65292;&#20854;&#20013;&#25152;&#26377;&#19977;&#20010;&#21464;&#20307;&#37117;&#28385;&#36275;&#19968;&#20010;&#19982;&#24213;&#23618;&#38750;&#23545;&#27604;&#24615;&#23450;&#20041;&#30456;&#20851;&#30340;&#21478;&#19968;&#20010;&#32479;&#19968;&#23450;&#20041;&#30340;&#27169;&#22359;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miller recently proposed a definition of contrastive (counterfactual) explanations based on the well-known Halpern-Pearl (HP) definitions of causes and (non-contrastive) explanations. Crucially, the Miller definition was based on the original HP definition of explanations, but this has since been modified by Halpern; presumably because the original yields counterintuitive results in many standard examples. More recently Borner has proposed a third definition, observing that this modified HP definition may also yield counterintuitive results. In this paper we show that the Miller definition inherits issues found in the original HP definition. We address these issues by proposing two improved variants based on the more robust modified HP and Borner definitions. We analyse our new definitions and show that they retain the spirit of the Miller definition where all three variants satisfy an alternative unified definition that is modular with respect to an underlying definition of non-contra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10811</link><description>&lt;p&gt;
"&#24863;&#35273;&#20687;&#26377;&#31532;&#20108;&#20010;&#24605;&#32500;": &#25506;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#21019;&#24847;&#21487;&#20889;&#24615;&#39044;&#20889;&#30340;&#20154;&#26426;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20889;&#26159;&#22312;&#31532;&#19968;&#31295;&#20043;&#21069;&#21457;&#29616;&#21644;&#21457;&#23637;&#24605;&#24819;&#30340;&#36807;&#31243;&#65292;&#23427;&#38656;&#35201;&#21457;&#25955;&#24615;&#24605;&#32500;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#26080;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#22914;&#22270;&#34920;&#12289;&#27010;&#36848;&#21644;&#33258;&#30001;&#20889;&#20316;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#65292;&#20294;&#23545;&#29992;&#25143;&#22914;&#20309;&#19982;LLMs&#21512;&#20316;&#26469;&#25903;&#25345;&#39044;&#20889;&#30340;&#26041;&#24335;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#31181;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#65292;LLMs&#30340;&#39318;&#36873;&#21512;&#20316;&#35282;&#33394;&#21644;&#20027;&#21160;&#24615;&#20063;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#30740;&#31350;&#20154;&#31867;&#19982;LLMs&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;15&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#20010;&#21019;&#36896;&#24615;&#20219;&#21153;&#65306;&#20889;&#25925;&#20107;&#21644;&#20889;&#21475;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#20316;&#30340;&#39044;&#20889;&#36807;&#31243;&#20013;&#65292;&#20284;&#20046;&#23384;&#22312;&#30528;&#19968;&#20010;&#19977;&#38454;&#27573;&#36845;&#20195;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#38454;&#27573;&#12290;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20197;&#20154;&#31867;&#22312;&#20027;&#23548;&#35282;&#33394;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#32467;&#21512;&#22810;&#20010;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#26356;&#21512;&#29702;&#30340;&#31034;&#33539;&#20960;&#20309;&#24179;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10810</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#32467;&#21512;&#19987;&#23478;&#31034;&#33539;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Combining Expert Demonstrations in Imitation Learning via Optimal Transport. (arXiv:2307.10810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10810
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#32467;&#21512;&#22810;&#20010;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#26356;&#21512;&#29702;&#30340;&#31034;&#33539;&#20960;&#20309;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#19987;&#23478;&#31034;&#33539;&#26469;&#25945;&#25480;&#26234;&#33021;&#20307;&#29305;&#23450;&#20219;&#21153;&#12290;&#27169;&#20223;&#23398;&#20064;&#30340;&#20851;&#38190;&#26041;&#27861;&#20043;&#19968;&#26159;&#23450;&#20041;&#26234;&#33021;&#20307;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25214;&#21040;&#20351;&#35813;&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#34913;&#37327;&#26234;&#33021;&#20307;&#21644;&#19987;&#23478;&#36712;&#36857;&#20043;&#38388;&#26377;&#24847;&#20041;&#36317;&#31163;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#20339;&#22320;&#32467;&#21512;&#22810;&#20010;&#19987;&#23478;&#31034;&#33539;&#30340;&#38382;&#39064;&#24182;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#20018;&#32852;&#29366;&#24577;&#65288;-&#21160;&#20316;&#65289;&#36712;&#36857;&#65292;&#20294;&#22312;&#36712;&#36857;&#20026;&#22810;&#27169;&#24577;&#26102;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#36793;&#38469;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#65292;&#33021;&#22815;&#22312;&#26368;&#20248;&#20256;&#36755;&#30340;&#24847;&#20041;&#19979;&#32467;&#21512;&#22810;&#20010;&#21644;&#22810;&#26679;&#21270;&#30340;&#29366;&#24577;&#36712;&#36857;&#65292;&#25552;&#20379;&#26356;&#21512;&#29702;&#30340;&#31034;&#33539;&#20960;&#20309;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#22810;&#20010;&#19987;&#23478;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;OpenAI Gym&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#25928;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SplitFC&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#36825;&#20123;&#31574;&#30053;&#20998;&#21035;&#26159;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.10805</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Split Learning via Adaptive Feature-Wise Compression. (arXiv:2307.10805v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SplitFC&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#36825;&#20123;&#31574;&#30053;&#20998;&#21035;&#26159;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SplitFC&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26694;&#26550;&#65292;&#23427;&#20943;&#23569;&#20102;&#22312;SL&#22521;&#35757;&#36807;&#31243;&#20013;&#20256;&#36755;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#25152;&#38656;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;SplitFC&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#30697;&#38453;&#30340;&#21015;&#25152;&#23637;&#31034;&#30340;&#19981;&#21516;&#30340;&#31163;&#25955;&#31243;&#24230;&#12290;SplitFC&#25972;&#21512;&#20102;&#20004;&#31181;&#21387;&#32553;&#31574;&#30053;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#65288;ii&#65289;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;&#22312;&#31532;&#19968;&#31181;&#31574;&#30053;&#20013;&#65292;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#26681;&#25454;&#36825;&#20123;&#21521;&#37327;&#30340;&#26631;&#20934;&#20559;&#24046;&#30830;&#23450;&#33258;&#36866;&#24212;&#25481;&#33853;&#27010;&#29575;&#36827;&#34892;&#25481;&#33853;&#12290;&#28982;&#21518;&#65292;&#30001;&#20110;&#38142;&#24335;&#35268;&#21017;&#65292;&#19982;&#34987;&#20002;&#24323;&#30340;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#32852;&#30340;&#20013;&#38388;&#26799;&#24230;&#21521;&#37327;&#20063;&#20250;&#34987;&#20002;&#24323;&#12290;&#22312;&#31532;&#20108;&#31181;&#31574;&#30053;&#20013;&#65292;&#38750;&#20002;&#24323;&#30340;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#33539;&#22260;&#30830;&#23450;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#32423;&#21035;&#36827;&#34892;&#37327;&#21270;&#12290;&#20026;&#20102;&#23613;&#37327;&#20943;&#23567;&#37327;&#21270;&#35823;&#24046;&#65292;&#26368;&#20248;&#37327;&#21270;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantizatio
&lt;/p&gt;</description></item><item><title>Meta-Transformer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#65292;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.10802</link><description>&lt;p&gt;
Meta-Transformer: &#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10802
&lt;/p&gt;
&lt;p&gt;
Meta-Transformer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#65292;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#22788;&#29702;&#21644;&#20851;&#32852;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26377;&#22810;&#24180;&#30340;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#65292;&#35774;&#35745;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#32593;&#32476;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Meta-Transformer&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#12290;&#22312;Meta-Transformer&#20013;&#65292;&#26469;&#33258;&#21508;&#31181;&#27169;&#24577;&#30340;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#26631;&#35760;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#21518;&#32493;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#26631;&#35760;&#22120;&#65292;&#19968;&#20010;&#27169;&#24577;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#65292;Meta-Transformer&#26159;&#31532;&#19968;&#20010;&#22312;12&#31181;&#27169;&#24577;&#19978;&#36827;&#34892;&#32479;&#19968;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modaliti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#21644;&#22823;&#37327;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;PatchCore&#31639;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#20511;&#37492;&#23569;&#37327;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10792</link><description>&lt;p&gt;
&#20248;&#21270;PatchCore&#20197;&#29992;&#20110;&#23569;&#37327;/&#22823;&#37327;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#21644;&#22823;&#37327;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;PatchCore&#31639;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#20511;&#37492;&#23569;&#37327;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#37327;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#24322;&#24120;&#26816;&#27979;&#23376;&#39046;&#22495;&#65292;&#23427;&#35797;&#22270;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#36873;&#23450;&#30340;&#26679;&#26412;&#26469;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#12290;&#23613;&#31649;&#26032;&#25552;&#20986;&#30340;&#23569;&#37327;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#19982;&#29992;&#20110;&#23436;&#20840;&#26679;&#26412;&#39046;&#22495;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#20316;&#20026;&#22522;&#32447;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#23569;&#37327;&#26679;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#39044;&#20808;&#23384;&#22312;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#21542;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;PatchCore&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#31639;&#27861;&#26159;&#30446;&#21069;&#29366;&#24577;&#26368;&#20339;&#30340;&#23436;&#20840;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#31639;&#27861;&#65292;&#30740;&#31350;&#20854;&#22312;&#23569;&#37327;&#26679;&#26412;&#21644;&#22823;&#37327;&#26679;&#26412;&#35774;&#32622;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#24615;&#33021;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#65288;I&#65289;&#20248;&#21270;&#20854;&#21508;&#31181;&#36229;&#21442;&#25968;&#21644;&#65288;II&#65289;&#36716;&#31227;&#24050;&#30693;&#21487;&#25913;&#21892;&#23569;&#37327;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#30340;&#25216;&#26415;&#21040;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#65292;&#21487;&#20197;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23545;&#20844;&#20849;VisA&#21644;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#65288;I&#65289;sign
&lt;/p&gt;
&lt;p&gt;
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) sign
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.10768</link><description>&lt;p&gt;
&#35299;&#30721;&#35868;&#22242;&#65306;&#22312;&#24037;&#20316;&#35760;&#24518;&#30340;&#22810;&#20010;&#26041;&#38754;&#19978;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#65288;WM&#65289;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23427;&#20419;&#36827;&#20102;&#20449;&#24687;&#30340;&#20020;&#26102;&#23384;&#20648;&#12289;&#25972;&#21512;&#12289;&#25805;&#20316;&#21644;&#26816;&#32034;&#65292;&#22312;&#25512;&#29702;&#21644;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25429;&#25417;&#24037;&#20316;&#35760;&#24518;&#22810;&#26041;&#38754;&#29305;&#24449;&#30340;&#21487;&#38752;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#20110;&#26377;&#25928;&#22320;&#24320;&#21457;&#21644;&#35780;&#20272;AI&#24037;&#20316;&#35760;&#24518;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#65288;WorM&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;WorM&#21253;&#25324;10&#20010;&#20219;&#21153;&#21644;&#24635;&#20849;100&#19975;&#27425;&#35797;&#39564;&#65292;&#35780;&#20272;&#20102;WM&#30340;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#19978;&#20849;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20154;&#31867;&#34892;&#20026;&#22522;&#20934;&#20316;&#20026;&#23545;&#27604;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#27169;&#25311;&#20102;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#24615;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM
&lt;/p&gt;</description></item><item><title>MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.10763</link><description>&lt;p&gt;
MSQNet: &#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10763
&lt;/p&gt;
&lt;p&gt;
MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#65292;&#22240;&#20026;&#28436;&#21592;&#20043;&#38388;&#20855;&#26377;&#22266;&#26377;&#30340;&#25299;&#25169;&#21644;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#23601;&#38656;&#35201;&#29305;&#23450;&#28436;&#21592;&#30340;&#23039;&#24577;&#20272;&#35745;&#65288;&#20363;&#22914;&#20154;&#31867;&#19982;&#21160;&#29289;&#65289;&#65292;&#23548;&#33268;&#27169;&#22411;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#23398;&#20064;&#35270;&#35273;&#27169;&#24577;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#28304;&#65288;&#20363;&#22914;&#31867;&#21517;&#25991;&#26412;&#65289;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#8221;&#65292;&#20026;&#21253;&#25324;&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#20869;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#28436;&#21592;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#20363;&#22914;DETR&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#26597;&#35810;&#32593;&#32476;&#65288;MSQNet&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#12290;&#28040;&#38500;&#20102;&#28436;&#21592;&#29305;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#30693;&#35782;&#24037;&#20316;&#30340;&#21019;&#36896;&#21147;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25209;&#35780;&#32773;&#35748;&#20026;&#20854;&#36755;&#20986;&#21482;&#26159;&#38543;&#26426;&#30340;&#25220;&#34989;&#21644;&#28151;&#25645;&#12290;&#28982;&#32780;&#65292;&#21019;&#36896;&#21147;&#21644;&#21407;&#21019;&#24615;&#30340;&#23450;&#20041;&#26159;&#22797;&#26434;&#30340;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#36807;&#31243;&#12289;&#19968;&#20010;&#20316;&#32773;&#25110;&#19968;&#20010;&#35266;&#30475;&#32773;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10751</link><description>&lt;p&gt;
&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#23545;&#30693;&#35782;&#24037;&#20316;&#21019;&#36896;&#21147;&#30340;&#24433;&#21709;&#65306;&#36229;&#36234;&#26426;&#26800;&#21270;&#25220;&#34989;&#21644;&#38543;&#26426;&#40550;&#40521;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots. (arXiv:2307.10751v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10751
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#30693;&#35782;&#24037;&#20316;&#30340;&#21019;&#36896;&#21147;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25209;&#35780;&#32773;&#35748;&#20026;&#20854;&#36755;&#20986;&#21482;&#26159;&#38543;&#26426;&#30340;&#25220;&#34989;&#21644;&#28151;&#25645;&#12290;&#28982;&#32780;&#65292;&#21019;&#36896;&#21147;&#21644;&#21407;&#21019;&#24615;&#30340;&#23450;&#20041;&#26159;&#22797;&#26434;&#30340;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#36807;&#31243;&#12289;&#19968;&#20010;&#20316;&#32773;&#25110;&#19968;&#20010;&#35266;&#30475;&#32773;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#23588;&#20854;&#26159;&#29983;&#25104;&#27169;&#22411;&#65292;&#26159;&#30693;&#35782;&#24037;&#20316;&#30340;&#21464;&#38761;&#24615;&#24037;&#20855;&#12290;&#23427;&#20204;&#38382;&#39064;&#21270;&#20102;&#21019;&#36896;&#21147;&#12289;&#21407;&#21019;&#24615;&#12289;&#25220;&#34989;&#12289;&#24402;&#23646;&#26435;&#21644;&#29256;&#26435;&#25152;&#26377;&#26435;&#30340;&#27010;&#24565;&#12290;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25209;&#35780;&#32773;&#24378;&#35843;&#20854;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#35270;&#20026;&#38543;&#26426;&#30340;&#25220;&#34989;&#12289;&#28151;&#25645;&#25110;&#32773;&#25340;&#36148;&#28304;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#30001;&#65292;&#35768;&#22810;&#20154;&#20027;&#24352;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#37096;&#32626;&#12289;&#20351;&#29992;&#21644;&#36755;&#20986;&#30340;&#24402;&#23646;&#21152;&#24378;&#30417;&#31649;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#24182;&#19981;&#26159;&#20154;&#24037;&#26234;&#33021;&#29420;&#26377;&#30340;&#65292;&#20063;&#19981;&#26159;&#26032;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20351;&#29992;&#25991;&#23398;&#25209;&#35780;&#12289;&#33402;&#26415;&#21490;&#21644;&#29256;&#26435;&#27861;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#21019;&#36896;&#21147;&#21644;&#21407;&#21019;&#24615;&#22914;&#20309;&#25269;&#25239;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#23545;&#35937;&#30340;&#21487;&#27880;&#37322;&#25110;&#20449;&#24687;&#35770;&#23646;&#24615;&#65292;&#32780;&#21487;&#35265;&#20026;&#19968;&#20010;&#36807;&#31243;&#12289;&#19968;&#20010;&#20316;&#32773;&#25110;&#19968;&#20010;&#35266;&#30475;&#32773;&#30340;&#23646;&#24615;&#12290;&#19968;&#20123;&#26367;&#20195;&#35266;&#28857;&#35748;&#20026;&#21019;&#36896;&#24615;&#24037;&#20316;&#23454;&#36136;&#19978;&#26159;&#19968;&#31181;&#38656;&#27714;&#19968;&#23450;&#31243;&#24230;&#30340;&#27169;&#20223;&#12289;&#39072;&#20498;&#21644;&#37325;&#26032;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI), and in particular generative models, are transformative tools for knowledge work. They problematise notions of creativity, originality, plagiarism, the attribution of credit, and copyright ownership. Critics of generative models emphasise the reliance on large amounts of training data, and view the output of these models as no more than randomised plagiarism, remix, or collage of the source data. On these grounds, many have argued for stronger regulations on the deployment, use, and attribution of the output of these models. However, these issues are not new or unique to artificial intelligence. In this position paper, using examples from literary criticism, the history of art, and copyright law, I show how creativity and originality resist definition as a notatable or information-theoretic property of an object, and instead can be seen as the property of a process, an author, or a viewer. Further alternative views hold that all creative work is essentiall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;FairFedCS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#23458;&#25143;&#31471;&#30340;&#22768;&#35465;&#12289;&#21442;&#19982;&#27425;&#25968;&#21644;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10738</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Client Selection for Federated Learning. (arXiv:2307.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;FairFedCS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#23458;&#25143;&#31471;&#30340;&#22768;&#35465;&#12289;&#21442;&#19982;&#27425;&#25968;&#21644;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#30001;&#20110;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#27599;&#36718;&#35757;&#32451;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#23458;&#25143;&#31471;&#65292;&#23458;&#25143;&#31471;&#36873;&#25321;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#20110;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#25110;&#25552;&#39640;&#23458;&#25143;&#31471;&#30340;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#36873;&#25321;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#26102;&#24179;&#34913;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#32771;&#34385;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;FairFedCS&#65289;&#26041;&#27861;&#12290;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#23458;&#25143;&#31471;&#30340;&#22768;&#35465;&#12289;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#27425;&#25968;&#21644;&#23545;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#21160;&#24577;&#35843;&#25972;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#27010;&#29575;&#12290;&#36890;&#36807;&#19981;&#20351;&#29992;&#22522;&#20110;&#38408;&#20540;&#30340;&#22768;&#35465;&#36807;&#28388;&#65292;&#23427;&#20026;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25552;&#20379;&#20102;&#36174;&#22238;&#22768;&#35465;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.10719</link><description>&lt;p&gt;
LLM&#23457;&#26597;&#65306;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#36824;&#26159;&#35745;&#31639;&#26426;&#23433;&#20840;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?. (arXiv:2307.10719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25552;&#20379;&#30340;&#25351;&#20196;&#30340;&#30450;&#30446;&#36981;&#24490;&#24341;&#21457;&#20102;&#23545;&#24694;&#24847;&#20351;&#29992;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;LLM&#30340;&#27169;&#22411;&#24494;&#35843;&#25110;&#20351;&#29992;LLM&#36827;&#34892;&#36755;&#20986;&#23457;&#26597;&#65292;&#24050;&#35777;&#26126;&#26159;&#26377;&#32570;&#38519;&#30340;&#65292;&#22240;&#20026;LLM&#20173;&#28982;&#21487;&#20197;&#29983;&#25104;&#26377;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#24120;&#29992;&#30340;&#23457;&#26597;&#26041;&#27861;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20381;&#36182;&#20110;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;LLM&#36755;&#20986;&#20013;&#30340;&#19981;&#33391;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#36825;&#31181;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#30340;&#23457;&#26597;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#38480;&#20110;&#35821;&#20041;&#23457;&#26597;&#65292;&#22240;&#20026;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#39118;&#38505;&#36974;&#25377;&#26041;&#27861;&#26469;&#22788;&#29702;&#22478;&#24066;&#39550;&#39542;&#20013;&#30340;&#32676;&#20307;&#20114;&#21160;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20998;&#26512;&#32676;&#20307;&#20013;&#19977;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25214;&#21040;&#22312;&#34892;&#20026;&#35268;&#21010;&#20013;&#19981;&#38656;&#35201;&#32771;&#34385;&#30340;&#20195;&#29702;&#65292;&#24182;&#33021;&#22815;&#35268;&#21010;&#20986;&#26356;&#20026;&#20915;&#31574;&#21644;&#33298;&#36866;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.10714</link><description>&lt;p&gt;
&#24341;&#20837;&#39118;&#38505;&#36974;&#25377;&#20197;&#23454;&#29616;&#20915;&#31574;&#21644;&#33298;&#36866;&#30340;&#34892;&#20026;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing Risk Shadowing For Decisive and Comfortable Behavior Planning. (arXiv:2307.10714v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10714
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#39118;&#38505;&#36974;&#25377;&#26041;&#27861;&#26469;&#22788;&#29702;&#22478;&#24066;&#39550;&#39542;&#20013;&#30340;&#32676;&#20307;&#20114;&#21160;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20998;&#26512;&#32676;&#20307;&#20013;&#19977;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25214;&#21040;&#22312;&#34892;&#20026;&#35268;&#21010;&#20013;&#19981;&#38656;&#35201;&#32771;&#34385;&#30340;&#20195;&#29702;&#65292;&#24182;&#33021;&#22815;&#35268;&#21010;&#20986;&#26356;&#20026;&#20915;&#31574;&#21644;&#33298;&#36866;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22478;&#24066;&#39550;&#39542;&#20013;&#30340;&#32676;&#20307;&#20114;&#21160;&#38382;&#39064;&#12290;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#26368;&#26032;&#34892;&#20026;&#35268;&#21010;&#22120;&#20027;&#35201;&#21333;&#29420;&#32771;&#34385;&#27599;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#20989;&#25968;&#25214;&#21040;&#26368;&#20248;&#30340;&#33258;&#25105;&#20195;&#29702;&#34892;&#20026;&#65292;&#20363;&#22914;&#36991;&#20813;&#19982;&#20854;&#20182;&#20195;&#29702;&#21457;&#29983;&#30896;&#25758;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#36974;&#25377;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19977;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#32676;&#20307;&#20114;&#21160;&#65292;&#21435;&#36229;&#36234;&#21333;&#20010;&#20132;&#20114;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#38656;&#35201;&#22312;&#33258;&#25105;&#20195;&#29702;&#30340;&#34892;&#20026;&#35268;&#21010;&#22120;&#20013;&#32771;&#34385;&#21738;&#20010;&#31532;&#19968;&#20010;&#20854;&#20182;&#20195;&#29702;&#65292;&#22240;&#20026;&#36825;&#20010;&#31532;&#19968;&#20010;&#20854;&#20182;&#20195;&#29702;&#30001;&#20110;&#31532;&#20108;&#20010;&#20854;&#20182;&#20195;&#29702;&#30340;&#38459;&#25377;&#32780;&#26080;&#27861;&#25509;&#36817;&#33258;&#25105;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39118;&#38505;&#36974;&#25377;&#20316;&#20026;&#19978;&#28216;&#31579;&#36873;&#27169;&#22359;&#29992;&#20110;&#34892;&#20026;&#35268;&#21010;&#22120;&#21487;&#20197;&#35268;&#21010;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20915;&#31574;&#21644;&#33298;&#36866;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21069;&#25552;&#26159;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#25152;&#25552;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of group interactions in urban driving. State-of-the-art behavior planners for self-driving cars mostly consider each single agent-to-agent interaction separately in a cost function in order to find an optimal behavior for the ego agent, such as not colliding with any of the other agents. In this paper, we develop risk shadowing, a situation understanding method that allows us to go beyond single interactions by analyzing group interactions between three agents. Concretely, the presented method can find out which first other agent does not need to be considered in the behavior planner of an ego agent, because this first other agent cannot reach the ego agent due to a second other agent obstructing its way. In experiments, we show that using risk shadowing as an upstream filter module for a behavior planner allows to plan more decisive and comfortable driving strategies than state of the art, given that safety is ensured in these cases. The usability of the appro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;SlowTV&#23398;&#20064;&#37325;&#24314;&#19990;&#30028;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#22312;&#23460;&#20869;/&#23460;&#22806;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24050;&#30693;-&#26410;&#30693;&#25512;&#24191;&#33021;&#21147;&#12290;&#36825;&#20010;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;SlowTV&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31995;&#21015;&#26368;&#20339;&#23454;&#36341;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.10713</link><description>&lt;p&gt;
&#20241;&#38386;&#25918;&#26494;&#65306;&#36890;&#36807;&#35266;&#30475;SlowTV&#23398;&#20064;&#37325;&#24314;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Kick Back &amp; Relax: Learning to Reconstruct the World by Watching SlowTV. (arXiv:2307.10713v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10713
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;SlowTV&#23398;&#20064;&#37325;&#24314;&#19990;&#30028;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#22312;&#23460;&#20869;/&#23460;&#22806;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24050;&#30693;-&#26410;&#30693;&#25512;&#24191;&#33021;&#21147;&#12290;&#36825;&#20010;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;SlowTV&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31995;&#21015;&#26368;&#20339;&#23454;&#36341;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;SS-MDE&#65289;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38480;&#21046;&#22312;&#27773;&#36710;&#39046;&#22495;&#65292;&#23548;&#33268;&#27169;&#22411;&#26080;&#27861;&#25512;&#24191;&#21040;&#33258;&#28982;&#25110;&#23460;&#20869;&#29615;&#22659;&#31561;&#22797;&#26434;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;YouTube&#30340;&#22823;&#35268;&#27169;SlowTV&#25968;&#25454;&#38598;&#65292;&#27604;&#29616;&#26377;&#30340;&#27773;&#36710;&#25968;&#25454;&#38598;&#21253;&#21547;&#26356;&#22810;&#25968;&#25454;&#12290;SlowTV&#21253;&#21547;&#20102;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#22235;&#23395;&#24466;&#27493;&#26053;&#34892;&#65292;&#39118;&#26223;&#39550;&#39542;&#21644;&#28508;&#27700;&#31561;&#20016;&#23500;&#22810;&#26679;&#30340;&#29615;&#22659;&#30340;170&#19975;&#24352;&#22270;&#29255;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;SS-MDE&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23460;&#20869;/&#23460;&#22806;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#24050;&#30693;-&#26410;&#30693;&#25512;&#24191;&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#26356;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#20294;&#32467;&#26524;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#25509;&#36817;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;SoTA&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26368;&#20339;&#23454;&#36341;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#24050;&#30693;-&#26410;&#30693;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data. Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.  To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets. SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving. Using this dataset, we train an SS-MDE model that provides zero-shot generalization to a large collection of indoor/outdoor datasets. The resulting model outperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.  We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization. This includes 1) a
&lt;/p&gt;</description></item><item><title>AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10711</link><description>&lt;p&gt;
AdjointDPM: &#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10711
&lt;/p&gt;
&lt;p&gt;
AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21046;&#21270;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#26679;&#20363;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#27010;&#24565;&#23545;&#40784;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#24403;&#21807;&#19968;&#21487;&#29992;&#30340;&#30417;&#30563;&#26159;&#23450;&#20041;&#22312;&#29983;&#25104;&#20869;&#23481;&#19978;&#30340;&#21487;&#24494;&#24230;&#37327;&#26102;&#30340;DPM&#23450;&#21046;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;DPM&#30340;&#37319;&#26679;&#36807;&#31243;&#28041;&#21450;&#23545;&#21435;&#22122;UNet&#30340;&#36882;&#24402;&#35843;&#29992;&#65292;&#26420;&#32032;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#23384;&#20648;&#25152;&#26377;&#36845;&#20195;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#23548;&#33268;&#20869;&#23384;&#28040;&#32791;&#26497;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdjointDPM&#65292;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;ODE&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#28982;&#21518;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#21478;&#19968;&#20010;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;(&#21253;&#25324;&#35843;&#21046;&#20449;&#21495;&#12289;&#32593;&#32476;&#26435;&#37325;&#21644;&#21021;&#22987;&#22122;&#22768;)&#12290;&#20026;&#20102;&#20943;&#23569;&#27491;&#21521;&#29983;&#25104;&#21644;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#25968;&#20540;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#30340;&#26234;&#33021;&#34394;&#25311;&#20195;&#29702;&#26550;&#26500;&#26694;&#26550;KorraAI&#21487;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#32771;&#34385;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#19981;&#30830;&#23450;&#20449;&#24687;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#21644;&#31215;&#26497;&#20027;&#21160;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.10693</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#32534;&#31243;&#30340;&#26234;&#33021;&#34394;&#25311;&#20195;&#29702;&#26550;&#26500;&#26694;&#26550;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an architectural framework for intelligent virtual agents using probabilistic programming. (arXiv:2307.10693v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10693
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#30340;&#26234;&#33021;&#34394;&#25311;&#20195;&#29702;&#26550;&#26500;&#26694;&#26550;KorraAI&#21487;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#32771;&#34385;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#19981;&#30830;&#23450;&#20449;&#24687;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#21644;&#31215;&#26497;&#20027;&#21160;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;KorraAI&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#35774;&#35745;&#20855;&#26377;&#20307;&#39564;&#23545;&#35805;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20363;&#22914;&#29615;&#22659;&#21644;&#20114;&#21160;&#26102;&#38388;&#65292;&#20197;&#21450;&#20154;&#31867;&#20114;&#21160;&#20249;&#20276;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#20449;&#24687;&#65292;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;KorraAI&#26500;&#24314;&#30340;&#20195;&#29702;&#21487;&#20197;&#34920;&#29616;&#20986;&#31215;&#26497;&#20027;&#21160;&#30340;&#34892;&#20026;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20027;&#21160;&#19982;&#20154;&#31867;&#20249;&#20276;&#36827;&#34892;&#20114;&#21160;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#65292;KorraAI&#37319;&#29992;&#20102;&#27010;&#29575;&#32534;&#31243;&#12290;KorraAI&#20013;&#30340;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#19982;&#29992;&#25143;&#30340;&#20114;&#21160;&#12290;&#23427;&#20204;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#20195;&#29702;&#20013;&#24341;&#20837;&#19968;&#23450;&#31243;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#33258;&#28982;&#30340;&#34892;&#20026;&#12290;KorraAI&#21487;&#20197;&#20351;&#29992;&#20998;&#24067;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#24314;&#27169;&#20154;&#31867;&#19968;&#26679;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20363;&#22914;&#24773;&#32490;&#12289;&#21916;&#22909;&#21644;&#24773;&#24863;&#65288;&#22914;&#24778;&#21916;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#19982;&#29992;&#25143;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;ECA&#27169;&#22411;&#20316;&#20026;&#25554;&#20214;&#23454;&#29616;&#65292;&#24182;&#20849;&#20139;&#19968;&#20010;&#20849;&#21516;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new framework called KorraAI for conceiving and building embodied conversational agents (ECAs). Our framework models ECAs' behavior considering contextual information, for example, about environment and interaction time, and uncertain information provided by the human interaction partner. Moreover, agents built with KorraAI can show proactive behavior, as they can initiate interactions with human partners. For these purposes, KorraAI exploits probabilistic programming. Probabilistic models in KorraAI are used to model its behavior and interactions with the user. They enable adaptation to the user's preferences and a certain degree of indeterminism in the ECAs to achieve more natural behavior. Human-like internal states, such as moods, preferences, and emotions (e.g., surprise), can be modeled in KorraAI with distributions and Bayesian networks. These models can evolve over time, even without interaction with the user. ECA models are implemented as plugins and share a commo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Answer Set Programming&#30340;&#26377;&#30028;&#32452;&#21512;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#32452;&#21512;&#37325;&#26500;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22269;&#38469;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2307.10688</link><description>&lt;p&gt;
&#22522;&#20110;Answer Set Programming&#30340;&#26377;&#30028;&#32452;&#21512;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bounded Combinatorial Reconfiguration with Answer Set Programming. (arXiv:2307.10688v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Answer Set Programming&#30340;&#26377;&#30028;&#32452;&#21512;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#32452;&#21512;&#37325;&#26500;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22269;&#38469;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#26377;&#30028;&#32452;&#21512;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;Answer Set Programming&#65288;ASP&#65289;&#30340;&#32452;&#21512;&#37325;&#26500;&#38382;&#39064;&#12290;&#24635;&#20307;&#20219;&#21153;&#26159;&#30740;&#31350;&#28304;&#32452;&#21512;&#38382;&#39064;&#30340;&#35299;&#31354;&#38388;&#65292;&#24182;&#20915;&#23450;&#26159;&#21542;&#23384;&#22312;&#20855;&#26377;&#29305;&#27530;&#23646;&#24615;&#30340;&#21487;&#34892;&#35299;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;recongo&#27714;&#35299;&#22120;&#35206;&#30422;&#20102;&#26368;&#36817;&#22269;&#38469;&#32452;&#21512;&#37325;&#26500;&#31454;&#36187;&#65288;CoRe Challenge 2022&#65289;&#30340;&#27714;&#35299;&#22120;&#31867;&#21035;&#20013;&#30340;&#25152;&#26377;&#25351;&#26631;&#12290;recongo&#22312;&#21333;&#24341;&#25806;&#27714;&#35299;&#22120;&#26041;&#38754;&#30340;&#26368;&#30701;&#25351;&#26631;&#25490;&#21517;&#31532;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26377;&#30028;&#32452;&#21512;&#37325;&#26500;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#29420;&#31435;&#38598;&#37325;&#26500;&#38382;&#39064;&#30340;ASP&#32534;&#30721;&#65292;&#36825;&#26159;&#26368;&#24120;&#30740;&#31350;&#30340;&#32452;&#21512;&#37325;&#26500;&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;CoRe Challenge 2022&#30340;&#25152;&#26377;&#23454;&#20363;&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an approach called bounded combinatorial reconfiguration for solving combinatorial reconfiguration problems based on Answer Set Programming (ASP). The general task is to study the solution spaces of source combinatorial problems and to decide whether or not there are sequences of feasible solutions that have special properties. The resulting recongo solver covers all metrics of the solver track in the most recent international competition on combinatorial reconfiguration (CoRe Challenge 2022). recongo ranked first in the shortest metric of the single-engine solvers track. In this paper, we present the design and implementation of bounded combinatorial reconfiguration, and present an ASP encoding of the independent set reconfiguration problem that is one of the most studied combinatorial reconfiguration problems. Finally, we present empirical analysis considering all instances of CoRe Challenge 2022.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26500;&#24314;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#36710;&#36742;&#36141;&#20080;/&#38144;&#21806;&#39046;&#22495;&#20013;&#23637;&#29616;&#20102;&#20854;&#33391;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10680</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Personalized Recommender System Based-on Knowledge Graph Embeddings. (arXiv:2307.10680v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26500;&#24314;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#36710;&#36742;&#36141;&#20080;/&#38144;&#21806;&#39046;&#22495;&#20013;&#23637;&#29616;&#20102;&#20854;&#33391;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#36890;&#36807;&#26412;&#20307;&#35770;&#23545;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#20449;&#24687;&#24314;&#27169;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#23558;&#30693;&#35782;&#22270;&#35889;&#29992;&#20316;&#20449;&#24687;&#24314;&#27169;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#22240;&#27492;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20063;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#21644;&#29289;&#21697;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#32852;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#20110;&#36710;&#36742;&#36141;&#20080;/&#38144;&#21806;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#26500;&#24314;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#19982;&#20010;&#20307;&#29992;&#25143;&#19968;&#33268;&#30340;&#30456;&#20851;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs have proven to be effective for modeling entities and their relationships through the use of ontologies. The recent emergence in interest for using knowledge graphs as a form of information modeling has led to their increased adoption in recommender systems. By incorporating users and items into the knowledge graph, these systems can better capture the implicit connections between them and provide more accurate recommendations. In this paper, we investigate and propose the construction of a personalized recommender system via knowledge graphs embedding applied to the vehicle purchase/sale domain. The results of our experimentation demonstrate the efficacy of the proposed method in providing relevant recommendations that are consistent with individual users.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#23545;&#39046;&#22495;&#22806;&#32467;&#26500;&#21644;&#24211;&#30340;&#27719;&#32534;&#20811;&#38534;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#24050;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38480;&#21046;&#20110;&#23569;&#25968;&#35757;&#32451;&#36807;&#30340;&#24037;&#20855;&#38142;&#21464;&#20307;&#65292;&#32780;&#26412;&#30740;&#31350;&#39318;&#27425;&#35299;&#20915;&#20102;&#26410;&#35265;&#36807;&#26550;&#26500;&#21644;&#24211;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.10631</link><description>&lt;p&gt;
Pluvio:&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#23545;&#39046;&#22495;&#22806;&#32467;&#26500;&#21644;&#24211;&#30340;&#27719;&#32534;&#20811;&#38534;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Pluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck. (arXiv:2307.10631v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#23545;&#39046;&#22495;&#22806;&#32467;&#26500;&#21644;&#24211;&#30340;&#27719;&#32534;&#20811;&#38534;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#24050;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38480;&#21046;&#20110;&#23569;&#25968;&#35757;&#32451;&#36807;&#30340;&#24037;&#20855;&#38142;&#21464;&#20307;&#65292;&#32780;&#26412;&#30740;&#31350;&#39318;&#27425;&#35299;&#20915;&#20102;&#26410;&#35265;&#36807;&#26550;&#26500;&#21644;&#24211;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#37325;&#29992;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#21152;&#24555;&#21644;&#25552;&#39640;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#65292;&#20195;&#30721;&#37325;&#29992;&#23384;&#22312;&#30528;&#25511;&#21046;&#19981;&#24403;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#28431;&#27934;&#20256;&#25773;&#21644;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#31561;&#38382;&#39064;&#12290;&#27719;&#32534;&#20811;&#38534;&#25628;&#32034;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#21491;&#31227;&#38450;&#24481;&#26426;&#21046;&#65292;&#24050;&#32463;&#22312;&#35782;&#21035;&#30001;&#24050;&#21457;&#24067;&#30340;&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#23548;&#33268;&#30340;&#28431;&#27934;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#26368;&#36817;&#20851;&#20110;&#27719;&#32534;&#20811;&#38534;&#25628;&#32034;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36234;&#26469;&#36234;&#22810;&#37319;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21305;&#37197;&#30001;&#19981;&#21516;&#24037;&#20855;&#38142;&#29983;&#25104;&#30340;&#27719;&#32534;&#20195;&#30721;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#20174;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#23569;&#25968;&#24037;&#20855;&#38142;&#21464;&#20307;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#26550;&#26500;&#21450;&#20854;&#30456;&#24212;&#30340;&#32534;&#35793;&#24037;&#20855;&#38142;&#21464;&#20307;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#26550;&#26500;&#21644;&#24211;&#20013;&#30340;&#27719;&#32534;&#20811;&#38534;&#25628;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#20154;&#31867;&#20849;&#26377;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practice of code reuse is crucial in software development for a faster and more efficient development lifecycle. In reality, however, code reuse practices lack proper control, resulting in issues such as vulnerability propagation and intellectual property infringements. Assembly clone search, a critical shift-right defence mechanism, has been effective in identifying vulnerable code resulting from reuse in released executables. Recent studies on assembly clone search demonstrate a trend towards using machine learning-based methods to match assembly code variants produced by different toolchains. However, these methods are limited to what they learn from a small number of toolchain variants used in training, rendering them inapplicable to unseen architectures and their corresponding compilation toolchain variants.  This paper presents the first study on the problem of assembly clone search with unseen architectures and libraries. We propose incorporating human common knowledge throu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10617</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26816;&#27979;&#34394;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#35780;&#35770;&#22312;&#25512;&#24191;&#20219;&#20309;&#20135;&#21697;&#25110;&#26381;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20225;&#19994;&#21487;&#33021;&#20250;&#23884;&#20837;&#34394;&#20551;&#35780;&#35770;&#20197;&#21560;&#24341;&#23458;&#25143;&#36141;&#20080;&#20182;&#20204;&#30340;&#20135;&#21697;&#12290;&#20182;&#20204;&#29978;&#33267;&#21487;&#33021;&#31361;&#20986;&#24378;&#35843;&#33258;&#24049;&#20135;&#21697;&#30340;&#20248;&#28857;&#25110;&#25209;&#35780;&#31454;&#20105;&#23545;&#25163;&#30340;&#20135;&#21697;&#12290;&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#22312;&#32447;&#21830;&#19994;&#29992;&#25143;&#26377;&#21160;&#26426;&#20026;&#20182;&#20204;&#24819;&#35201;&#25512;&#24191;&#30340;&#20135;&#21697;&#32534;&#20889;&#34394;&#20551;&#30340;&#27491;&#38754;&#35780;&#35770;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#30495;&#27491;&#19981;&#21916;&#27426;&#30340;&#20135;&#21697;&#25552;&#20379;&#34394;&#20551;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#25345;&#32493;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#19968;&#20010;&#39184;&#39302;&#35780;&#35770;&#30340;&#34394;&#20551;&#24847;&#35265;&#22403;&#22334;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22810;&#27425;&#23454;&#39564;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;n-gram&#27169;&#22411;&#21644;&#26368;&#22823;&#29305;&#24449;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.10616</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65306;&#29616;&#29366;&#19982;&#30740;&#31350;&#25361;&#25112;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10616
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#27169;&#22411;&#21516;&#36136;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#12290;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064; (HFL) &#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#19988;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#20851;&#20110;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#30340;&#31995;&#32479;&#35843;&#26597;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102; HFL &#20013;&#26469;&#33258;&#20116;&#20010;&#26041;&#38754;&#30340;&#21508;&#31181;&#30740;&#31350;&#25361;&#25112;&#65306;&#32479;&#35745;&#24322;&#36136;&#24615;&#12289;&#27169;&#22411;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#24322;&#36136;&#24615;&#12289;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102; HFL &#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377; HFL &#26041;&#27861;&#30340;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#20854;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#30340;&#26222;&#36941;&#25361;&#25112;&#65306;&#22914;&#20559;&#35265;&#12289;&#27495;&#35270;&#21644;&#19981;&#21487;&#20449;&#31561;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;&#35774;&#35745;&#20013;&#32771;&#34385;&#22810;&#26679;&#24615;&#19982;&#21253;&#23481;&#24615;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20934;&#30830;&#20998;&#26512;48&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;55&#20010;&#29420;&#29305;&#25361;&#25112;&#21644;33&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#26377;24&#20010;&#29420;&#29305;&#25361;&#25112;&#21644;23&#20010;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#22686;&#24378;AI&#23454;&#36341;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#23545;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#26377;&#24456;&#22823;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10600</link><description>&lt;p&gt;
AI for All&#20013;&#30340;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Challenges and Solutions in AI for All. (arXiv:2307.10600v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10600
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#30340;&#26222;&#36941;&#25361;&#25112;&#65306;&#22914;&#20559;&#35265;&#12289;&#27495;&#35270;&#21644;&#19981;&#21487;&#20449;&#31561;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;&#35774;&#35745;&#20013;&#32771;&#34385;&#22810;&#26679;&#24615;&#19982;&#21253;&#23481;&#24615;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20934;&#30830;&#20998;&#26512;48&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;55&#20010;&#29420;&#29305;&#25361;&#25112;&#21644;33&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#26377;24&#20010;&#29420;&#29305;&#25361;&#25112;&#21644;23&#20010;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#22686;&#24378;AI&#23454;&#36341;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#23545;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#26377;&#24456;&#22823;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#22810;&#26679;&#24615;&#38656;&#35201;&#22312;&#20854;&#35774;&#35745;&#20013;&#32771;&#34385;&#22810;&#26679;&#24615;&#19982;&#21253;&#23481;&#24615;&#65288;D&#65286;I&#65289;&#21407;&#21017;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32771;&#34385;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20559;&#35265;&#12289;&#27495;&#35270;&#21644;&#19981;&#21487;&#20449;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#25581;&#31034;&#20102;&#19982;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;D&#65286;I&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20005;&#26684;&#25628;&#32034;&#20102;2017&#24180;&#33267;2022&#24180;&#38388;&#21457;&#34920;&#30340;48&#31687;&#30740;&#31350;&#25991;&#31456;&#12290;&#23545;&#36825;&#20123;&#35770;&#25991;&#36827;&#34892;&#32534;&#30721;&#20998;&#26512;&#65292;&#20849;&#21457;&#29616;55&#20010;&#20851;&#20110;AI&#20013;D&#65286;I&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;33&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;24&#20010;&#20851;&#20110;&#20351;&#29992;AI&#22686;&#24378;&#36825;&#20123;&#23454;&#36341;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;23&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#26412;&#30740;&#31350;&#23558;&#20026;&#23547;&#27714;&#23558;&#36825;&#20123;&#21407;&#21017;&#34701;&#20837;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI)'s pervasive presence and variety necessitate diversity and inclusivity (D&amp;I) principles in its design for fairness, trust, and transparency. Yet, these considerations are often overlooked, leading to issues of bias, discrimination, and perceived untrustworthiness. In response, we conducted a Systematic Review to unearth challenges and solutions relating to D&amp;I in AI. Our rigorous search yielded 48 research articles published between 2017 and 2022. Open coding of these papers revealed 55 unique challenges and 33 solutions for D&amp;I in AI, as well as 24 unique challenges and 23 solutions for enhancing such practices using AI. This study, by offering a deeper understanding of these issues, will enlighten researchers and practitioners seeking to integrate these principles into future AI systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#36125;&#21494;&#26031;&#20998;&#25955;&#25968;&#25454;&#34701;&#21512;&#20013;&#8220;&#35875;&#35328;&#20256;&#25773;&#8221;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27010;&#29575;&#29420;&#31435;&#32467;&#26500;&#21644;&#38750;&#25972;&#20307;&#21152;&#26435;&#22240;&#23376;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#32039;&#23454;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#36890;&#29992;&#20248;&#21270;&#26041;&#26696;&#20805;&#20998;&#21033;&#29992;&#20219;&#24847;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.10594</link><description>&lt;p&gt;
&#21033;&#29992;&#32467;&#26500;&#23454;&#29616;&#26368;&#20339;&#22810;&#26234;&#33021;&#20307;&#36125;&#21494;&#26031;&#20998;&#25955;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Exploiting Structure for Optimal Multi-Agent Bayesian Decentralized Estimation. (arXiv:2307.10594v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#36125;&#21494;&#26031;&#20998;&#25955;&#25968;&#25454;&#34701;&#21512;&#20013;&#8220;&#35875;&#35328;&#20256;&#25773;&#8221;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27010;&#29575;&#29420;&#31435;&#32467;&#26500;&#21644;&#38750;&#25972;&#20307;&#21152;&#26435;&#22240;&#23376;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#32039;&#23454;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#36890;&#29992;&#20248;&#21270;&#26041;&#26696;&#20805;&#20998;&#21033;&#29992;&#20219;&#24847;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#25955;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#8220;&#35875;&#35328;&#20256;&#25773;&#8221;&#25110;&#8220;&#37325;&#22797;&#35745;&#25968;&#8221;&#29616;&#35937;&#65292;&#21363;&#20808;&#21069;&#21457;&#36865;&#30340;&#25968;&#25454;&#20250;&#36820;&#22238;&#32473;&#21457;&#36865;&#32773;&#12290;&#36890;&#24120;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#22914;&#21327;&#26041;&#24046;&#20132;&#38598;&#65288;CI&#65289;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23545;&#20272;&#35745;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#35745;&#31639;&#36793;&#30028;&#12290;&#38382;&#39064;&#26159;&#36825;&#20010;&#36793;&#30028;&#24182;&#19981;&#32039;&#23454;&#65292;&#21363;&#20272;&#35745;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#20998;&#25955;&#34701;&#21512;&#38382;&#39064;&#20013;&#30340;&#27010;&#29575;&#29420;&#31435;&#32467;&#26500;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#65288;i&#65289;CI&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#65288;&#38750;&#25972;&#20307;&#30340;&#65289;&#21152;&#26435;&#22240;&#23376;&#32780;&#19981;&#26159;&#21407;&#22987;CI&#20013;&#30340;&#19968;&#20010;&#65288;&#25972;&#20307;&#30340;&#65289;&#22240;&#23376;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#36890;&#29992;&#20248;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#35745;&#31639;&#26368;&#20339;&#36793;&#30028;&#24182;&#20805;&#20998;&#21033;&#29992;&#20219;&#24847;&#20381;&#36182;&#32467;&#26500;&#65292;&#26469;&#25214;&#21040;&#26356;&#32039;&#23454;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#19968;&#20010;&#31616;&#21333;&#38382;&#39064;&#19978;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#35299;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#26032;&#30340;&#38750;&#25972;&#20307;CI&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in Bayesian decentralized data fusion is the `rumor propagation' or `double counting' phenomenon, where previously sent data circulates back to its sender. It is often addressed by approximate methods like covariance intersection (CI) which takes a weighted average of the estimates to compute the bound. The problem is that this bound is not tight, i.e. the estimate is often over-conservative. In this paper, we show that by exploiting the probabilistic independence structure in multi-agent decentralized fusion problems a tighter bound can be found using (i) an expansion to the CI algorithm that uses multiple (non-monolithic) weighting factors instead of one (monolithic) factor in the original CI and (ii) a general optimization scheme that is able to compute optimal bounds and fully exploit an arbitrary dependency structure. We compare our methods and show that on a simple problem, they converge to the same solution. We then test our new non-monolithic CI algorithm on a l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#27979;&#35797;&#29983;&#25104;&#22120;&#65288;GenBo&#65289;&#65292;&#23427;&#36890;&#36807;&#22312;&#26080;&#25925;&#38556;&#29615;&#22659;&#20013;&#21464;&#24322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#26465;&#20214;&#26469;&#29983;&#25104;&#36793;&#30028;&#29366;&#24577;&#23545;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10590</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#27979;&#35797;&#19982;&#25913;&#36827;&#30340;&#36793;&#30028;&#29366;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boundary State Generation for Testing and Improvement of Autonomous Driving Systems. (arXiv:2307.10590v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#27979;&#35797;&#29983;&#25104;&#22120;&#65288;GenBo&#65289;&#65292;&#23427;&#36890;&#36807;&#22312;&#26080;&#25925;&#38556;&#29615;&#22659;&#20013;&#21464;&#24322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#26465;&#20214;&#26469;&#29983;&#25104;&#36793;&#30028;&#29366;&#24577;&#23545;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#20855;&#26377;&#20102;&#36234;&#26469;&#36234;&#39640;&#30340;&#33258;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;ADS&#27979;&#35797;&#26041;&#27861;&#20462;&#25913;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#30340;&#21487;&#25511;&#23646;&#24615;&#65292;&#30452;&#21040;ADS&#20986;&#29616;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;1&#65289;&#23545;&#27169;&#25311;&#29615;&#22659;&#30340;&#20462;&#25913;&#21487;&#33021;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23454;&#38469;&#27979;&#35797;&#29615;&#22659;&#65288;&#20363;&#22914;&#25913;&#21464;&#36947;&#36335;&#24418;&#29366;&#65289;&#65307;&#65288;2&#65289;&#21363;&#20351;ADS&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#36825;&#20123;&#29615;&#22659;&#23454;&#20363;&#20063;&#20250;&#34987;&#20002;&#24323;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#21253;&#21547;ADS&#21487;&#33021;&#20986;&#29616;&#38382;&#39064;&#30340;&#28508;&#22312;&#39550;&#39542;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ADS&#27979;&#35797;&#29983;&#25104;&#22120;&#8212;&#8212;GenBo&#65288;GENerator of BOundary state pairs&#65289;&#12290;GenBo&#22312;&#19968;&#20010;&#26080;&#25925;&#38556;&#29615;&#22659;&#23454;&#20363;&#20013;&#21464;&#24322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#26465;&#20214;&#65288;&#20301;&#32622;&#65292;&#36895;&#24230;&#21644;&#26041;&#21521;&#65289;&#65292;&#24182;&#26377;&#25928;&#22320;&#29983;&#25104;&#21487;&#36793;&#30028;&#21270;&#30340;&#29366;&#24577;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Deep Neural Networks (DNNs) and sensor technologies are enabling autonomous driving systems (ADSs) with an ever-increasing level of autonomy. However, assessing their dependability remains a critical concern. State-of-the-art ADS testing approaches modify the controllable attributes of a simulated driving environment until the ADS misbehaves. Such approaches have two main drawbacks: (1) modifications to the simulated environment might not be easily transferable to the in-field test setting (e.g., changing the road shape); (2) environment instances in which the ADS is successful are discarded, despite the possibility that they could contain hidden driving conditions in which the ADS may misbehave.  In this paper, we present GenBo (GENerator of BOundary state pairs), a novel test generator for ADS testing. GenBo mutates the driving conditions of the ego vehicle (position, velocity and orientation), collected in a failure-free environment instance, and efficiently gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#32858;&#31867;&#21644;SMOTE&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#20107;&#20214;&#65292;&#20026;&#30005;&#21147;&#36127;&#33655;&#32858;&#21512;&#22120;&#21644;&#30005;&#21147;&#31649;&#29702;&#20154;&#21592;&#25552;&#20379;&#25552;&#20379;&#20805;&#30005;&#31449;&#21644;&#30005;&#21147;&#23481;&#37327;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10588</link><description>&lt;p&gt;
&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#34892;&#20026;&#65306;&#37319;&#29992;&#24494;&#32858;&#31867;&#21644;SMOTE&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques. (arXiv:2307.10588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#32858;&#31867;&#21644;SMOTE&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#20107;&#20214;&#65292;&#20026;&#30005;&#21147;&#36127;&#33655;&#32858;&#21512;&#22120;&#21644;&#30005;&#21147;&#31649;&#29702;&#20154;&#21592;&#25552;&#20379;&#25552;&#20379;&#20805;&#30005;&#31449;&#21644;&#30005;&#21147;&#23481;&#37327;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#21464;&#21270;&#21644;&#20844;&#20849;&#20581;&#24247;&#26159;&#25512;&#21160;&#20132;&#36890;&#30005;&#27668;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20840;&#29699;&#33539;&#22260;&#20869;&#27491;&#22312;&#25512;&#24191;&#20132;&#36890;&#30005;&#27668;&#21270;&#20197;&#20943;&#23569;&#25490;&#25918;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#27773;&#36710;&#21046;&#36896;&#21830;&#23558;&#24456;&#24555;&#24320;&#22987;&#21482;&#29983;&#20135;&#30005;&#27744;&#30005;&#21160;&#27773;&#36710;&#65288;BEV&#65289;&#12290;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#21644;&#31354;&#27668;&#27745;&#26579;&#30340;&#25285;&#24551;&#65292;&#21152;&#21033;&#31119;&#23612;&#20122;&#30340;BEV&#37319;&#29992;&#29575;&#27491;&#22312;&#19978;&#21319;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#27668;&#20505;&#21644;&#27745;&#26579;&#30446;&#26631;&#26469;&#35828;&#24456;&#22909;&#65292;&#20294;&#26410;&#22949;&#21892;&#31649;&#29702;&#30340;BEV&#20805;&#30005;&#21487;&#33021;&#23548;&#33268;&#20805;&#30005;&#22522;&#30784;&#35774;&#26045;&#19981;&#36275;&#21644;&#20572;&#30005;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#32858;&#31867;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;MCDNN&#65289;&#65292;&#35813;&#31639;&#27861;&#22312;&#23398;&#20064;BEV&#34892;&#31243;&#21644;&#20805;&#30005;&#25968;&#25454;&#20197;&#39044;&#27979;BEV&#20805;&#30005;&#20107;&#20214;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36825;&#23545;&#20110;&#30005;&#21147;&#36127;&#33655;&#32858;&#21512;&#22120;&#21644;&#30005;&#21147;&#31649;&#29702;&#20154;&#21592;&#26377;&#25928;&#25552;&#20379;&#20805;&#30005;&#31449;&#21644;&#30005;&#21147;&#23481;&#37327;&#30340;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;MCDNN&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#21457;&#29983;&#30340;&#34892;&#31243;&#21644;&#20805;&#30005;&#30340;&#31283;&#20581;&#25968;&#25454;&#38598;&#36827;&#34892;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in Ca
&lt;/p&gt;</description></item><item><title>Ethosight&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#32454;&#24494;&#34892;&#20026;&#21644;&#22330;&#26223;&#32454;&#33410;&#30340;&#20934;&#30830;&#24863;&#30693;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.10577</link><description>&lt;p&gt;
Ethosight: &#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#24230;&#37327;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#36827;&#34892;&#32454;&#33268;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning. (arXiv:2307.10577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10577
&lt;/p&gt;
&lt;p&gt;
Ethosight&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#32454;&#24494;&#34892;&#20026;&#21644;&#22330;&#26223;&#32454;&#33410;&#30340;&#20934;&#30830;&#24863;&#30693;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21162;&#21147;&#26469;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#21644;&#39564;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#27979;&#32454;&#24494;&#30340;&#34892;&#20026;&#32454;&#33410;&#25110;&#20107;&#20214;&#26102;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21306;&#20998;&#24120;&#35268;&#34892;&#20026;&#21644;&#28508;&#22312;&#39118;&#38505;&#30340;&#22256;&#38590;&#65292;&#22914;&#21306;&#20998;&#24120;&#35268;&#36141;&#29289;&#21644;&#28508;&#22312;&#25170;&#31363;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Ethosight&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#12290;Ethosight&#28040;&#38500;&#20102;&#23545;&#39044;&#20808;&#23384;&#22312;&#30340;&#31526;&#21495;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#20174;&#29992;&#25143;&#38656;&#27714;&#21644;&#24863;&#20852;&#36259;&#30340;&#35821;&#20041;&#30693;&#35782;&#20986;&#21457;&#36827;&#34892;&#33258;&#20027;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#26631;&#31614;&#20851;&#32852;&#24230;&#35745;&#31639;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#36845;&#20195;&#23398;&#20064;&#24490;&#29615;&#65292;Ethosight&#25512;&#26029;&#22330;&#26223;&#32454;&#33410;&#24182;&#36845;&#20195;&#22320;&#20248;&#21270;&#26631;&#31614;&#38598;&#12290;&#25512;&#29702;&#26426;&#21046;&#21487;&#20197;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT4&#12289;&#31526;&#21495;&#25512;&#29702;&#22120;&#22914;OpenNARS&#25110;&#28151;&#21512;&#31995;&#32479;&#12290;Ethosight&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;ImageBind&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events. The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.  We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest. Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set. Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.  Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#36136;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;FedAvg&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;3.3%&#21644;8.9%&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10575</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22411;&#27491;&#21017;&#21270;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#36136;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;FedAvg&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;3.3%&#21644;8.9%&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35201;&#27714;&#23458;&#25143;&#31471;&#22312;&#19981;&#27844;&#38706;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#19982;&#36793;&#32536;&#26381;&#21153;&#22120;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#24448;&#24448;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27491;&#21017;&#21270;&#36807;&#31243;&#28041;&#21450;&#26381;&#21153;&#22120;&#20174;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#32858;&#21512;&#26412;&#22320;&#21407;&#22411;&#20197;&#29983;&#25104;&#20840;&#23616;&#21407;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#22238;&#20010;&#20307;&#23458;&#25143;&#31471;&#20197;&#25351;&#23548;&#20854;&#26412;&#22320;&#35757;&#32451;&#12290;&#22312;MNIST&#21644;Fashion-MNIST&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#27969;&#34892;&#30340;&#22522;&#20934;FedAvg&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#20998;&#21035;&#22312;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#19978;&#23454;&#29616;&#20102;3.3%&#21644;8.9%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#20351;&#29992;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#20960;&#20046;&#21487;&#20197;&#25552;&#20379;&#19982;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;&#25552;&#31034;&#30456;&#20284;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#32780;&#19988;&#22312;&#26368;&#22256;&#38590;&#30340;&#20219;&#21153;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;</title><link>http://arxiv.org/abs/2307.10573</link><description>&lt;p&gt;
&#26080;&#25928;&#36923;&#36753;&#65292;&#31561;&#25928;&#25910;&#30410;&#65306;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20013;&#30340;&#22855;&#24618;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting. (arXiv:2307.10573v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10573
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#20351;&#29992;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#20960;&#20046;&#21487;&#20197;&#25552;&#20379;&#19982;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;&#25552;&#31034;&#30456;&#20284;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#32780;&#19988;&#22312;&#26368;&#22256;&#38590;&#30340;&#20219;&#21153;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#25552;&#31034;&#20197;&#19968;&#31181;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#36825;&#26679;&#30340;&#25552;&#31034;&#20250;&#25552;&#39640;&#24615;&#33021;&#36824;&#19981;&#28165;&#26970;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;CoT&#25552;&#31034;&#20960;&#20046;&#21487;&#20197;&#20687;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;CoT&#25552;&#31034;&#19968;&#26679;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#23558;CoT&#25552;&#31034;&#20013;&#30340;&#29305;&#23450;&#38382;&#39064;&#20449;&#24687;&#26367;&#25442;&#20026;&#25277;&#35937;&#20449;&#24687;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#20449;&#24687;&#36890;&#24120;&#19981;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#25209;&#35780;&#20154;&#22763;&#22238;&#24212;&#35828;&#65292;&#36825;&#20123;&#21457;&#29616;&#26159;&#22522;&#20110;&#22826;&#23569;&#12289;&#22826;&#31616;&#21333;&#30340;&#20219;&#21153;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20105;&#35758;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22312;BIG-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#22256;&#38590;&#30340;&#20219;&#21153;&#19978;&#65292;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;CoT&#25552;&#31034;&#26159;&#21542;&#25552;&#20379;&#19982;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;&#25552;&#31034;&#30456;&#21516;&#27700;&#24179;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36825;&#20123;&#20219;&#21153;&#34987;&#31216;&#20026;BIG-Bench Hard&#65288;BBH&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;BBH&#20219;&#21153;&#19978;&#65292;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;&#25512;&#29702;&#25552;&#31034;&#30830;&#23454;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can be prompted to reason through problems in a manner that significantly improves performance. However, \textit{why} such prompting improves performance is unclear. Recent work showed that using logically \textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost as much as logically \textit{valid} CoT prompting, and that editing CoT prompts to replace problem-specific information with abstract information or out-of-distribution information typically doesn't harm performance. Critics have responded that these findings are based on too few and too easy tasks to draw meaningful conclusions. To resolve this dispute, we test whether logically invalid CoT prompts offer the same level of performance gains as logically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed BIG-Bench Hard (BBH). We find that the logically \textit{invalid} reasoning prompts do indeed achieve similar performance gains on BBH tasks as logically valid reasoning pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.10569</link><description>&lt;p&gt;
&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#20197;&#21450;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#27835;&#26435;&#19981;&#26029;&#25193;&#22823;&#65292;&#19968;&#20010;&#26032;&#30340;&#23545;&#25163;&#20986;&#29616;&#20102;&#65306;&#27169;&#22411;&#26412;&#36523;&#12290;&#19968;&#20010;&#27169;&#22411;&#30475;&#20284;&#21512;&#29702;&#22320;&#34892;&#20026;&#65292;&#21364;&#26263;&#20013;&#12289;&#24494;&#22937;&#22320;&#20462;&#25913;&#20854;&#34892;&#20026;&#20197;&#36798;&#21040;&#21035;&#30340;&#30446;&#30340;&#30340;&#23041;&#32961;&#65292;&#36890;&#24120;&#22312;AI&#23433;&#20840;&#19982;&#23545;&#40784;&#31038;&#21306;&#20013;&#34987;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#26041;&#21521;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#26041;&#21521;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#23545;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#19988;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#39046;&#22495;&#30340;&#36827;&#27493;&#26082;&#25552;&#20986;&#20102;&#38271;&#26399;&#25361;&#25112;&#65292;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21628;&#21505;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#26356;&#22810;&#22320;&#21442;&#19982;&#36825;&#20123;&#26032;&#20852;&#26041;&#21521;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety &amp; Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
&lt;/p&gt;</description></item><item><title>FACADE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#27010;&#29575;&#21644;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26426;&#29702;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#21644;&#24378;&#22823;&#24037;&#20855;&#65292;&#20197;&#25581;&#31034;&#21644;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10563</link><description>&lt;p&gt;
FACADE&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#30005;&#36335;&#24322;&#24120;&#26816;&#27979;&#21644;&#35780;&#20272;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation. (arXiv:2307.10563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10563
&lt;/p&gt;
&lt;p&gt;
FACADE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#27010;&#29575;&#21644;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26426;&#29702;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#21644;&#24378;&#22823;&#24037;&#20855;&#65292;&#20197;&#25581;&#31034;&#21644;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FACADE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#21644;&#20960;&#20309;&#26694;&#26550;&#65292;&#26088;&#22312;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#26426;&#29702;&#24322;&#24120;&#26816;&#27979;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#23545;&#25239;&#25915;&#20987;&#30340;&#29702;&#35299;&#21644;&#20943;&#36731;&#12290;FACADE&#26088;&#22312;&#29983;&#25104;&#30005;&#36335;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20026;&#20266;&#31867;&#30340;&#27969;&#24418;&#29305;&#24615;&#21464;&#21270;&#20197;&#21450;&#28608;&#27963;&#31354;&#38388;&#20013;&#39640;&#32500;&#27169;&#24335;&#30340;&#36129;&#29486;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#65292;&#20174;&#32780;&#20026;&#25581;&#31034;&#21644;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22686;&#24378;&#21487;&#25193;&#23637;&#27169;&#22411;&#30417;&#25511;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10559</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#26159;&#19968;&#20010;&#23433;&#20840;&#20851;&#38190;&#30340;&#26381;&#21153;&#31995;&#32479;&#65292;&#35201;&#27714;&#22320;&#38754;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCo&#65289;&#26102;&#21051;&#20851;&#27880;&#20197;&#32500;&#25345;&#26085;&#24120;&#33322;&#31354;&#36816;&#33829;&#12290;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#21487;&#33021;&#23545;&#36816;&#33829;&#23433;&#20840;&#21644;&#31354;&#22495;&#20351;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#36733;&#24182;&#30830;&#20445;ATCo&#30340;&#21487;&#25509;&#21463;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#65292;&#20934;&#30830;&#39044;&#27979;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#23545;&#20110;&#37319;&#21462;&#32531;&#35299;&#25514;&#26045;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;ATCo&#24037;&#20316;&#36127;&#33655;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20027;&#35201;&#20174;&#31354;&#20013;&#20132;&#36890;&#30340;&#35282;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#19982;&#36864;&#20241;ATCo&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#33719;&#24471;&#20102;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#12290;&#27169;&#25311;&#22312;&#19977;&#31181;&#33778;&#23612;&#20811;&#26031;&#25509;&#36817;&#22330;&#26223;&#19979;&#36827;&#34892;&#65292;&#35201;&#27714;&#20154;&#31867;ATCo&#33258;&#25105;&#35780;&#20272;&#20854;&#24037;&#20316;&#36127;&#33655;&#35780;&#32423;&#65288;&#21363;&#65292;&#20302;-1&#21040;&#39640;-7&#65289;&#12290;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#21512;&#35268;&#21270;&#39044;&#27979;&#65292;&#26469;&#23545;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21270;&#31639;&#27861;&#30340;&#33258;&#21160;&#25628;&#32034;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#20013;&#30340;&#35757;&#32451;-free &#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;MQ-Bench-101&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#20195;&#29702;&#22312;&#37327;&#21270;&#31934;&#24230;&#26041;&#38754;&#23384;&#22312;&#30456;&#20851;&#24615;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#28436;&#21270;&#25628;&#32034;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#30456;&#20851;&#30340;MQ&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.10554</link><description>&lt;p&gt;
EMQ&#65306;&#28436;&#21270;&#26080;&#38656;&#35757;&#32451;&#30340;&#20195;&#29702;&#29992;&#20110;&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization. (arXiv:2307.10554v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21270;&#31639;&#27861;&#30340;&#33258;&#21160;&#25628;&#32034;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#20013;&#30340;&#35757;&#32451;-free &#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;MQ-Bench-101&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#20195;&#29702;&#22312;&#37327;&#21270;&#31934;&#24230;&#26041;&#38754;&#23384;&#22312;&#30456;&#20851;&#24615;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#28436;&#21270;&#25628;&#32034;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#30456;&#20851;&#30340;MQ&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65288;MQ&#65289;&#21487;&#20197;&#22312;&#27169;&#22411;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#31934;&#24230;-&#22797;&#26434;&#24230;&#26435;&#34913;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#25628;&#32034;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#20505;&#36873;&#35757;&#32451;&#65292;&#20197;&#25628;&#32034;MQ&#20013;&#20248;&#21270;&#30340;&#36880;&#23618;&#27604;&#29305;&#23485;&#24230;&#37197;&#32622;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#22810;&#31181;MQ&#20195;&#29702;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20195;&#29702;&#19982;&#37327;&#21270;&#31934;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20173;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;MQ-Bench-101&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#27604;&#29305;&#37197;&#32622;&#21644;&#37327;&#21270;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#20195;&#29702;&#22312;MQ-Bench-101&#19978;&#34920;&#29616;&#20986;&#24369;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#23547;&#25214;&#20248;&#31168;&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#36890;&#36807;&#28436;&#21270;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#25628;&#32034;&#20195;&#29702;&#30340;&#26694;&#26550;&#29992;&#20110;MQ&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#25324;&#29616;&#26377;&#20195;&#29702;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#36827;&#34892;&#28436;&#21270;&#25628;&#32034;&#20197;&#21457;&#29616;&#26368;&#20339;&#30456;&#20851;&#30340;MQ&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixed-Precision Quantization~(MQ) can achieve a competitive accuracy-complexity trade-off for models. Conventional training-based search methods require time-consuming candidate training to search optimized per-layer bit-width configurations in MQ. Recently, some training-free approaches have presented various MQ proxies and significantly improve search efficiency. However, the correlation between these proxies and quantization accuracy is poorly understood. To address the gap, we first build the MQ-Bench-101, which involves different bit configurations and quantization results. Then, we observe that the existing training-free proxies perform weak correlations on the MQ-Bench-101. To efficiently seek superior proxies, we develop an automatic search of proxies framework for MQ via evolving algorithms. In particular, we devise an elaborate search space involving the existing proxies and perform an evolution search to discover the best correlated MQ proxy. We proposed a diversity-promptin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PPN&#30340;&#24182;&#34892;&#25351;&#38024;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;CLEX&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#24067;&#23616;&#21644;&#35821;&#20041;&#23454;&#20307;&#31867;&#21035;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#24182;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10551</link><description>&lt;p&gt;
PPN: &#24182;&#34892;&#25351;&#38024;&#32593;&#32476;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#24067;&#23616;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
PPN: Parallel Pointer-based Network for Key Information Extraction with Complex Layouts. (arXiv:2307.10551v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PPN&#30340;&#24182;&#34892;&#25351;&#38024;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;CLEX&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#24067;&#23616;&#21644;&#35821;&#20041;&#23454;&#20307;&#31867;&#21035;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#24182;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#65288;KIE&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#23454;&#20307;&#20540;&#12290;&#23613;&#31649;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#24067;&#23616;&#30456;&#23545;&#22266;&#23450;&#65292;&#24182;&#22312;&#35821;&#20041;&#23454;&#20307;&#31867;&#21035;&#25968;&#37327;&#19978;&#26377;&#38480;&#65292;&#23548;&#33268;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#27969;&#27700;&#32447;&#31574;&#30053;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#26032;&#20986;&#29616;&#30340;&#35821;&#20041;&#23454;&#20307;&#31867;&#21035;&#20986;&#29616;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22797;&#26434;&#24067;&#23616;&#34920;&#21333;&#30340;&#26032;&#30340;&#22823;&#35268;&#27169;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;CLEX&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;5,860&#20010;&#22270;&#20687;&#21644;1,162&#20010;&#35821;&#20041;&#23454;&#20307;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24182;&#34892;&#25351;&#38024;&#32593;&#32476;&#65288;PPN&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key Information Extraction (KIE) is a challenging multimodal task that aims to extract structured value semantic entities from visually rich documents. Although significant progress has been made, there are still two major challenges that need to be addressed. Firstly, the layout of existing datasets is relatively fixed and limited in the number of semantic entity categories, creating a significant gap between these datasets and the complex real-world scenarios. Secondly, existing methods follow a two-stage pipeline strategy, which may lead to the error propagation problem. Additionally, they are difficult to apply in situations where unseen semantic entity categories emerge. To address the first challenge, we propose a new large-scale human-annotated dataset named Complex Layout form for key information EXtraction (CLEX), which consists of 5,860 images with 1,162 semantic entity categories. To solve the second challenge, we introduce Parallel Pointer-based Network (PPN), an end-to-end
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#21306;&#22359;&#38142;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#25552;&#20379;&#26080;&#27861;&#31713;&#25913;&#30340;&#20132;&#26131;&#20998;&#31867;&#24080;&#30340;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#36830;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20026;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.10549</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Large Language Models on Blockchains. (arXiv:2307.10549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#21306;&#22359;&#38142;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#25552;&#20379;&#26080;&#27861;&#31713;&#25913;&#30340;&#20132;&#26131;&#20998;&#31867;&#24080;&#30340;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#36830;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20026;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#25991;&#26412;&#25317;&#26377;&#25968;&#21315;&#20010;&#26631;&#35760;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38745;&#24577;&#30340;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#21518;&#23601;&#34987;&#22266;&#23450;&#19979;&#26469;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#22312;&#21306;&#22359;&#38142;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21306;&#22359;&#38142;&#20855;&#26377;&#39640;&#35745;&#31639;&#24615;&#33021;&#24182;&#20998;&#24067;&#22312;&#19968;&#20010;&#35745;&#31639;&#26426;&#32593;&#32476;&#19978;&#12290;&#21306;&#22359;&#38142;&#26159;&#19968;&#20010;&#23433;&#20840;&#12289;&#20998;&#25955;&#21644;&#36879;&#26126;&#30340;&#31995;&#32479;&#65292;&#20801;&#35768;&#21019;&#24314;&#19968;&#20010;&#26080;&#27861;&#31713;&#25913;&#30340;&#20132;&#26131;&#20998;&#31867;&#24080;&#65292;&#26080;&#38656;&#20013;&#20171;&#26426;&#26500;&#12290;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21518;&#19981;&#26029;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20026;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24102;&#26469;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>TREA&#26159;&#19968;&#31181;&#26641;&#29366;&#32467;&#26500;&#25512;&#29702;&#27169;&#24335;&#65292;&#21487;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#23427;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#21487;&#25193;&#23637;&#26641;&#26469;&#28548;&#28165;&#23545;&#35805;&#20013;&#25552;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#29983;&#25104;&#26356;&#21512;&#29702;&#21644;&#36866;&#24403;&#30340;&#25512;&#33616;&#32467;&#26524;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.10543</link><description>&lt;p&gt;
TREA: &#26641;&#29366;&#32467;&#26500;&#25512;&#29702;&#27169;&#24335;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TREA: Tree-Structure Reasoning Schema for Conversational Recommendation. (arXiv:2307.10543v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10543
&lt;/p&gt;
&lt;p&gt;
TREA&#26159;&#19968;&#31181;&#26641;&#29366;&#32467;&#26500;&#25512;&#29702;&#27169;&#24335;&#65292;&#21487;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#23427;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#21487;&#25193;&#23637;&#26641;&#26469;&#28548;&#28165;&#23545;&#35805;&#20013;&#25552;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#29983;&#25104;&#26356;&#21512;&#29702;&#21644;&#36866;&#24403;&#30340;&#25512;&#33616;&#32467;&#26524;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#23545;&#35805;&#36861;&#36394;&#29992;&#25143;&#30340;&#21160;&#24577;&#20852;&#36259;&#65292;&#24182;&#29983;&#25104;&#19982;&#29289;&#21697;&#25512;&#33616;&#30456;&#20851;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#22686;&#24378;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#21508;&#31181;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#29305;&#21035;&#26159;&#30693;&#35782;&#22270;&#35889;&#65289;&#34987;&#32435;&#20837;&#21040;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22522;&#20110;&#25512;&#29702;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#31616;&#21270;&#30340;&#32467;&#26500;&#65292;&#22914;&#32447;&#24615;&#32467;&#26500;&#25110;&#22266;&#23450;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#19982;&#22806;&#37096;&#30693;&#35782;&#30456;&#20851;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TREA&#30340;&#26032;&#39062;&#30340;&#26641;&#29366;&#32467;&#26500;&#25512;&#29702;&#27169;&#24335;&#12290;TREA&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#21487;&#25193;&#23637;&#30340;&#26641;&#20316;&#20026;&#25512;&#29702;&#32467;&#26500;&#65292;&#20197;&#28548;&#28165;&#25152;&#25552;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#29983;&#25104;&#26356;&#21512;&#29702;&#21644;&#36866;&#24403;&#30340;&#25512;&#33616;&#32467;&#26524;&#21709;&#24212;&#12290;&#23545;&#20004;&#20010;&#20844;&#20849;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;TREA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into CRS to enhance the understanding of conversation contexts. However, recent reasoning-based models heavily rely on simplified structures such as linear structures or fixed-hierarchical structures for causality reasoning, hence they cannot fully figure out sophisticated relationships among utterances with external knowledge. To address this, we propose a novel Tree structure Reasoning schEmA named TREA. TREA constructs a multi-hierarchical scalable tree as the reasoning structure to clarify the causal relationships between mentioned entities, and fully utilizes historical conversations to generate more reasonable and suitable responses for recommended results. Extensive experiments on two public CRS datasets have demonstrated the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.10529</link><description>&lt;p&gt;
&#24555;&#36895;&#26080;&#30417;&#30563;&#28145;&#24230;&#24322;&#24120;&#20540;&#27169;&#22411;&#36873;&#25321;&#19982;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;(OD)&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#65292;&#24182;&#26377;&#35768;&#22810;&#25216;&#26415;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;OD(DOD)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#36827;&#23637;&#32780;&#21463;&#21040;&#20102;&#26368;&#36817;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#30417;&#30563;DOD&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;(HP)&#35843;&#25972;/&#27169;&#22411;&#36873;&#25321;&#12290;&#34429;&#28982;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#25253;&#21578;&#20102;OD&#27169;&#22411;&#23545;HP&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#23545;&#20110;&#23637;&#31034;&#20102;&#38271;&#21015;&#34920;HP&#30340;&#29616;&#20195;DOD&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HYPER&#26469;&#35843;&#25972;DOD&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;(1)&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#39564;&#35777;(&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#24322;&#24120;&#20540;)&#65292;&#20197;&#21450;(2) HP/&#27169;&#22411;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034; (&#30001;&#20110;HP&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;)&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#26032;&#39062;&#30340;&#36229;&#32593;&#32476;(HN)&#65292;&#20854;&#23558;HP&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;&#21453;&#36807;&#26469;&#65292;HYPER&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;HN&#65292;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;DOD&#27169;&#22411;&#30340;&#26435;&#37325; (&#23545;&#24212;&#20110;...)&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31038;&#21306;&#21442;&#19982;&#30340;&#21162;&#21147;&#65292;&#22312;&#21360;&#24230;&#31038;&#20250;&#32972;&#26223;&#19979;&#25193;&#23637;&#20102;&#23545;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#36825;&#20010;&#24037;&#20316;&#24378;&#35843;&#20102;&#21253;&#23481;&#19981;&#21516;&#25991;&#21270;&#21644;&#31038;&#20250;&#32972;&#26223;&#30340;&#20154;&#20204;&#21644;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#23545;&#20260;&#23475;&#27979;&#37327;&#30340;&#20005;&#37325;&#20302;&#20272;&#25110;&#25197;&#26354;&#12290;</title><link>http://arxiv.org/abs/2307.10514</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#21442;&#19982;&#26500;&#24314;&#19982;&#31038;&#20250;&#25991;&#21270;&#21253;&#23481;&#24615;&#30340;&#21051;&#26495;&#21360;&#35937;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Building Socio-culturally Inclusive Stereotype Resources with Community Engagement. (arXiv:2307.10514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10514
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#21442;&#19982;&#30340;&#21162;&#21147;&#65292;&#22312;&#21360;&#24230;&#31038;&#20250;&#32972;&#26223;&#19979;&#25193;&#23637;&#20102;&#23545;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#36825;&#20010;&#24037;&#20316;&#24378;&#35843;&#20102;&#21253;&#23481;&#19981;&#21516;&#25991;&#21270;&#21644;&#31038;&#20250;&#32972;&#26223;&#30340;&#20154;&#20204;&#21644;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#23545;&#20260;&#23475;&#27979;&#37327;&#30340;&#20005;&#37325;&#20302;&#20272;&#25110;&#25197;&#26354;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#37096;&#32626;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#22312;&#27979;&#37327;&#20260;&#23475;&#26041;&#38754;&#36827;&#34892;&#25193;&#23637;&#65292;&#19981;&#20165;&#21253;&#25324;&#35206;&#30422;&#30340;&#20260;&#23475;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#36824;&#35201;&#32771;&#34385;&#21040;&#24403;&#22320;&#25991;&#21270;&#32972;&#26223;&#65292;&#21253;&#25324;&#36793;&#32536;&#21270;&#36523;&#20221;&#21644;&#20182;&#20204;&#25152;&#32463;&#21382;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#33539;&#24335;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20195;&#34920;&#22810;&#20803;&#21270;&#12289;&#26412;&#22320;&#21270;&#20294;&#20840;&#29699;&#21270;&#30340;&#31038;&#20250;&#25991;&#21270;&#35270;&#35282;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#20260;&#23475;&#27979;&#37327;&#20135;&#29983;&#20005;&#37325;&#20302;&#20272;&#25110;&#25197;&#26354;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#36890;&#36807;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#21644;&#31038;&#20250;&#30340;&#20154;&#20204;&#21644;&#32463;&#39564;&#26469;&#22686;&#24378;&#21644;&#26657;&#20934;&#25105;&#20204;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21360;&#24230;&#31038;&#20250;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#23545;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#23454;&#26045;&#31038;&#20250;&#25991;&#21270;&#24847;&#35782;&#30340;&#35780;&#20272;&#36164;&#28304;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31038;&#21306;&#21442;&#19982;&#30340;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#21051;&#26495;&#21360;&#35937;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotype
&lt;/p&gt;</description></item><item><title>IvyGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20013;&#25991;&#20114;&#21160;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#38382;&#31572;&#31034;&#20363;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#33021;&#22815;&#36755;&#20986;&#26356;&#20016;&#23500;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31572;&#26696;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;GPT&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10512</link><description>&lt;p&gt;
IvyGPT&#65306;&#22522;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20114;&#21160;&#24335;&#20013;&#25991;&#36335;&#24452;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IvyGPT: InteractiVe Chinese pathwaY language model in medical domain. (arXiv:2307.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10512
&lt;/p&gt;
&lt;p&gt;
IvyGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20013;&#25991;&#20114;&#21160;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#38382;&#31572;&#31034;&#20363;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#33021;&#22815;&#36755;&#20986;&#26356;&#20016;&#23500;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31572;&#26696;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;GPT&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31934;&#24230;&#19981;&#39640;&#21644;&#26080;&#27861;&#25552;&#20379;&#21307;&#30103;&#24314;&#35758;&#65292;&#36825;&#20123;LLM&#22312;&#21307;&#23398;&#39046;&#22495;&#24182;&#26410;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLaMA&#30340;LLM IvyGPT&#65292;&#23427;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#31034;&#20363;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#32463;&#36807;&#26377;&#30417;&#30563;&#24494;&#35843;&#21518;&#65292;IvyGPT&#20855;&#26377;&#33391;&#22909;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#22312;&#32508;&#21512;&#35786;&#26029;&#31561;&#20854;&#20182;&#26041;&#38754;&#19981;&#33021;&#20687;&#21307;&#29983;&#19968;&#26679;&#36816;&#34892;&#12290;&#36890;&#36807;RLHF&#65292;IvyGPT&#21487;&#20197;&#36755;&#20986;&#26356;&#20016;&#23500;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31572;&#26696;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;QLoRA&#22312;&#23569;&#37327;NVIDIA A100&#65288;80GB) GPU&#19978;&#35757;&#32451;&#20102;330&#20159;&#20010;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IvyGPT&#22312;&#21307;&#23398;GPT&#27169;&#22411;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26102;&#21464;&#25240;&#25187;&#22240;&#23376;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#37319;&#29992;&#21338;&#24328;&#35770;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#31639;&#27861;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.10491</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#21464;&#20960;&#20309;&#25240;&#25187;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Markov Decision Processes with Time-Varying Geometric Discounting. (arXiv:2307.10491v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26102;&#21464;&#25240;&#25187;&#22240;&#23376;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#37319;&#29992;&#21338;&#24328;&#35770;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#31639;&#27861;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#36890;&#24120;&#32771;&#34385;&#22522;&#20110;&#24120;&#25968;&#25240;&#25187;&#22240;&#23376;&#30340;&#20960;&#20309;&#25240;&#25187;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#25240;&#25187;&#22240;&#23376;&#36827;&#34892;&#24314;&#27169;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26102;&#21464;&#25240;&#25187;&#22240;&#23376;&#30340;&#26080;&#31351;&#26102;&#38388;&#31243;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#65292;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#35270;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#20915;&#31574;&#32773;&#65292;&#25317;&#26377;&#33258;&#24049;&#30340;&#65288;&#22266;&#23450;&#30340;&#65289;&#25240;&#25187;&#22240;&#23376;&#65292;&#24182;&#30740;&#31350;&#25152;&#24471;&#21040;&#30340;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#20197;&#21450;&#30456;&#20851;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#30340;&#26500;&#36896;&#24615;&#35777;&#26126;&#65292;&#24182;&#23637;&#31034;&#20102;&#35745;&#31639;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;EXPTIME&#22256;&#38590;&#24615;&#12290;&#25105;&#20204;&#36824;&#36716;&#21521;&#36817;&#20284;&#30340;$\epsilon$-&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#22312;&#36739;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;$\epsilon$-&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#35745;&#31639;$\epsilon$-&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Canonical models of Markov decision processes (MDPs) usually consider geometric discounting based on a constant discount factor. While this standard modeling approach has led to many elegant results, some recent studies indicate the necessity of modeling time-varying discounting in certain applications. This paper studies a model of infinite-horizon MDPs with time-varying discount factors. We take a game-theoretic perspective -- whereby each time step is treated as an independent decision maker with their own (fixed) discount factor -- and we study the subgame perfect equilibrium (SPE) of the resulting game as well as the related algorithmic problems. We present a constructive proof of the existence of an SPE and demonstrate the EXPTIME-hardness of computing an SPE. We also turn to the approximate notion of $\epsilon$-SPE and show that an $\epsilon$-SPE exists under milder assumptions. An algorithm is presented to compute an $\epsilon$-SPE, of which an upper bound of the time complexit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#26816;&#27979;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#22312;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#32473;&#20986;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#21028;&#26029;&#12290;&#36825;&#23545;&#20110;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#22914;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#20005;&#37325;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2307.10487</link><description>&lt;p&gt;
&#24694;&#24847;&#27880;&#37322;&#19979;&#30340;&#29289;&#20307;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack against Object Detection with Clean Annotation. (arXiv:2307.10487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#26816;&#27979;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#22312;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#32473;&#20986;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#21028;&#26029;&#12290;&#36825;&#23545;&#20110;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#22914;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#20005;&#37325;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20063;&#21457;&#29616;DNN&#23545;&#22810;&#31181;&#25915;&#20987;&#65292;&#21253;&#25324;&#21518;&#38376;&#25915;&#20987;&#65292;&#26159;&#33030;&#24369;&#30340;&#12290;&#36890;&#36807;&#36825;&#31181;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#25104;&#21151;&#22320;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#23884;&#20837;&#21040;DNN&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#33391;&#24615;&#25968;&#25454;&#26679;&#26412;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#22312;&#39044;&#23450;&#20041;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#32473;&#20986;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#21028;&#26029;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#23581;&#35797;&#20102;&#22823;&#37327;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#23578;&#26410;&#24471;&#21040;&#36866;&#24403;&#30340;&#35843;&#26597;&#21644;&#25506;&#32034;&#12290;&#30001;&#20110;&#29289;&#20307;&#26816;&#27979;&#24050;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#30340;&#37325;&#35201;&#27169;&#22359;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#65292;&#23545;&#29289;&#20307;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#36896;&#25104;&#26356;&#20005;&#37325;&#30340;&#23041;&#32961;&#12290;&#21463;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#22266;&#26377;&#23646;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29289;&#20307;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#32780;&#19981;&#20462;&#25913;&#22320;&#38754;&#30495;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10472</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#35782;&#21035;&#31038;&#20250;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#19981;&#26029;&#25193;&#23637;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#21644;&#20943;&#36731;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#25110;&#32487;&#25215;&#30340;&#31038;&#20250;&#20559;&#35265;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#22312;LLaMA&#21450;&#20854;&#20004;&#20010;&#25351;&#20196;&#24494;&#35843;&#29256;&#26412;&#20013;&#65292;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20934;&#30830;&#29575;&#36798;56.7%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25193;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#26159;&#25105;&#20204;&#20559;&#35265;&#32531;&#35299;&#26694;&#26550;&#30340;&#31532;&#19968;&#37096;&#20998;&#65292;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#26681;&#25454;&#33719;&#24471;&#30340;&#26356;&#22810;&#32467;&#26524;&#19981;&#26029;&#26356;&#26032;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#19987;&#21033;&#22270;&#20687;&#20013;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;CLEF-IP&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#20419;&#36827;&#19987;&#21033;&#25506;&#32034;&#21644;&#26816;&#32034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10471</link><description>&lt;p&gt;
&#19987;&#21033;&#20013;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Visualization Types and Perspectives in Patents. (arXiv:2307.10471v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#19987;&#21033;&#22270;&#20687;&#20013;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;CLEF-IP&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#20419;&#36827;&#19987;&#21033;&#25506;&#32034;&#21644;&#26816;&#32034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27599;&#24180;&#19987;&#21033;&#30003;&#35831;&#25968;&#37327;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#20419;&#36827;&#19987;&#21033;&#25506;&#32034;&#21644;&#26816;&#32034;&#30340;&#20449;&#24687;&#21644;&#22810;&#23186;&#20307;&#26816;&#32034;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#21487;&#35270;&#21270;&#65288;&#20363;&#22914;&#65292;&#22270;&#24418;&#12289;&#25216;&#26415;&#22270;&#32440;&#65289;&#21644;&#35270;&#35282;&#65288;&#20363;&#22914;&#65292;&#20391;&#35270;&#12289;&#36879;&#35270;&#65289;&#34987;&#29992;&#26469;&#21487;&#35270;&#21270;&#19987;&#21033;&#21019;&#26032;&#30340;&#32454;&#33410;&#12290;&#23545;&#36825;&#20123;&#22270;&#20687;&#30340;&#20998;&#31867;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25628;&#32034;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#29992;&#20110;&#22270;&#20687;&#31867;&#22411;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#32570;&#23569;&#19968;&#20123;&#37325;&#35201;&#30340;&#19987;&#21033;&#21487;&#35270;&#21270;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#30456;&#20851;&#30740;&#31350;&#27809;&#26377;&#20351;&#29992;&#21253;&#25324;transformers&#22312;&#20869;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#31867;&#19987;&#21033;&#22270;&#20687;&#20013;&#30340;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#12290;&#25105;&#20204;&#23545;&#19987;&#21033;&#20013;&#22270;&#20687;&#31867;&#22411;&#20998;&#31867;&#30340;CLEF-IP&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#22686;&#21152;&#21040;&#20102;&#21313;&#20010;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#19968;&#32452;&#23618;&#32423;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.10460</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65306;&#25968;&#25454;&#31185;&#23398;&#30340;&#26412;&#36136;&#12289;&#20215;&#20540;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#26080;&#27861;&#39044;&#27979;&#30340;&#33539;&#22260;&#12289;&#35268;&#27169;&#12289;&#22797;&#26434;&#24615;&#21644;&#30693;&#35782;&#21457;&#29616;&#33021;&#21147;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#24335;&#26080;&#27861;&#23454;&#29616;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23427;&#24050;&#32463;&#22312;AI&#20891;&#22791;&#31454;&#36187;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#20197;&#19975;&#35745;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24050;&#32463;&#23454;&#36136;&#24615;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#30340;&#19990;&#30028;&#65292;&#20294;&#30001;&#20110;&#20854;&#19981;&#21487;&#24605;&#35758;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#33021;&#24102;&#26469;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#20854;&#26174;&#33879;&#32780;&#20915;&#23450;&#24615;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#20102;&#35299;&#21644;&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65292;&#35748;&#35782;&#21040;&#20854;&#28508;&#22312;&#30340;&#30410;&#22788;&#12289;&#39118;&#38505;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#31185;&#23398;&#26412;&#36136;&#19978;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#27604;&#25105;&#20204;&#23545;&#31185;&#23398;&#30830;&#23450;&#24615;&#30340;&#20559;&#22909;&#26356;&#21152;&#29616;&#23454;&#12290;&#25968;&#25454;&#31185;&#23398;&#23558;&#20135;&#29983;&#36828;&#36828;&#36229;&#20986;&#30693;&#35782;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#30828;&#32422;&#26463;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26144;&#23556;&#38544;&#34255;&#21442;&#25968;&#21521;&#37327;&#21040;&#19968;&#20010;&#31526;&#21512;&#32422;&#26463;&#38598;&#30340;&#28857;&#23454;&#29616;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#38468;&#21152;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#36827;&#34892;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#19981;&#20165;&#23545;&#36755;&#20986;&#21521;&#37327;&#26045;&#21152;&#32422;&#26463;&#65292;&#36824;&#23545;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32852;&#21512;&#32422;&#26463;&#26045;&#21152;&#32422;&#26463;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32422;&#26463;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20108;&#27425;&#32422;&#26463;&#12289;&#31561;&#24335;&#32422;&#26463;&#21644;&#21160;&#24577;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.10459</link><description>&lt;p&gt;
&#19968;&#31181;&#23454;&#29616;&#20855;&#26377;&#30828;&#32422;&#26463;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10459
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#30828;&#32422;&#26463;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26144;&#23556;&#38544;&#34255;&#21442;&#25968;&#21521;&#37327;&#21040;&#19968;&#20010;&#31526;&#21512;&#32422;&#26463;&#38598;&#30340;&#28857;&#23454;&#29616;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#38468;&#21152;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#36827;&#34892;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#19981;&#20165;&#23545;&#36755;&#20986;&#21521;&#37327;&#26045;&#21152;&#32422;&#26463;&#65292;&#36824;&#23545;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32852;&#21512;&#32422;&#26463;&#26045;&#21152;&#32422;&#26463;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32422;&#26463;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20108;&#27425;&#32422;&#26463;&#12289;&#31561;&#24335;&#32422;&#26463;&#21644;&#21160;&#24577;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20540;&#19978;&#26045;&#21152;&#30828;&#20984;&#32422;&#26463;&#30340;&#35745;&#31639;&#31616;&#21333;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#38544;&#34255;&#21442;&#25968;&#21521;&#37327;&#26144;&#23556;&#21040;&#19968;&#20010;&#28857;&#65292;&#30830;&#20445;&#23427;&#22312;&#30001;&#19968;&#32452;&#32422;&#26463;&#23450;&#20041;&#30340;&#21487;&#34892;&#38598;&#20869;&#12290;&#26144;&#23556;&#26159;&#36890;&#36807;&#20855;&#26377;&#36755;&#20986;&#32422;&#26463;&#30340;&#38468;&#21152;&#31070;&#32463;&#32593;&#32476;&#23618;&#23454;&#29616;&#30340;&#12290;&#23558;&#35813;&#26041;&#27861;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#19981;&#20165;&#23545;&#36755;&#20986;&#21521;&#37327;&#26045;&#21152;&#32422;&#26463;&#65292;&#36824;&#23545;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32852;&#21512;&#32422;&#26463;&#26045;&#21152;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#31616;&#21333;&#22320;&#23454;&#29616;&#23545;&#36755;&#20986;&#30340;&#32422;&#26463;&#25237;&#24433;&#26041;&#27861;&#12290;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#32422;&#26463;&#24341;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20108;&#27425;&#32422;&#26463;&#12289;&#31561;&#24335;&#32422;&#26463;&#21644;&#21160;&#24577;&#32422;&#26463;&#65292;&#20197;&#21450;&#36793;&#30028;&#24418;&#24335;&#30340;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#23427;&#30340;&#35745;&#31639;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35782;&#21035;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#30340;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20221;&#38382;&#21367;&#26469;&#25552;&#20379;&#23450;&#37327;&#25968;&#25454;&#65292;&#20998;&#26512;&#21457;&#29616;&#19981;&#21516;&#21512;&#35268;&#31867;&#21035;&#19979;&#32452;&#32455;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#32771;&#23519;&#20102;&#32452;&#32455;&#29305;&#24449;&#23545;&#21512;&#35268;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#36981;&#23432;&#27431;&#30431;AI&#27861;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#20851;&#35299;&#20915;&#26041;&#26696;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2307.10458</link><description>&lt;p&gt;
&#36981;&#23432;&#27431;&#30431;AI&#27861;&#26696;
&lt;/p&gt;
&lt;p&gt;
Complying with the EU AI Act. (arXiv:2307.10458v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35782;&#21035;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#30340;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20221;&#38382;&#21367;&#26469;&#25552;&#20379;&#23450;&#37327;&#25968;&#25454;&#65292;&#20998;&#26512;&#21457;&#29616;&#19981;&#21516;&#21512;&#35268;&#31867;&#21035;&#19979;&#32452;&#32455;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#32771;&#23519;&#20102;&#32452;&#32455;&#29305;&#24449;&#23545;&#21512;&#35268;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#36981;&#23432;&#27431;&#30431;AI&#27861;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#20851;&#35299;&#20915;&#26041;&#26696;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;AI&#27861;&#26696;&#26159;&#20851;&#20110;AI&#31995;&#32479;&#30340;&#27431;&#30431;&#25311;&#35758;&#31435;&#27861;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#20960;&#20010;AI&#27861;&#26696;&#30340;&#31867;&#21035;&#65292;&#24182;&#22522;&#20110;&#27492;&#20998;&#31867;&#21046;&#23450;&#20102;&#19968;&#20221;&#38382;&#21367;&#20316;&#20026;&#25552;&#20379;&#35265;&#35299;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21019;&#24314;&#23450;&#37327;&#25968;&#25454;&#12290;&#25968;&#25454;&#20998;&#26512;&#26174;&#31034;&#20102;&#19981;&#21516;&#21512;&#35268;&#31867;&#21035;&#19979;&#32452;&#32455;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;&#36824;&#32771;&#23519;&#20102;&#32452;&#32455;&#29305;&#24449;&#65288;&#22914;&#35268;&#27169;&#21644;&#34892;&#19994;&#65289;&#23545;&#21512;&#35268;&#30340;&#24433;&#21709;&#65292;&#24182;&#20998;&#20139;&#20102;&#20851;&#20110;&#34987;&#35843;&#26597;&#32773;&#26222;&#36941;&#20851;&#27880;&#30340;&#38382;&#39064;&#30340;&#23450;&#24615;&#25968;&#25454;&#65292;&#21253;&#25324;AI&#27861;&#26696;&#30340;&#20869;&#23481;&#21644;&#24212;&#29992;&#12290;&#25991;&#31456;&#24471;&#20986;&#32467;&#35770;&#65292;&#35748;&#20026;&#22312;&#36981;&#23432;&#27431;&#30431;AI&#27861;&#26696;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#25552;&#21040;&#20102;&#19968;&#20010;&#30456;&#20851;&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#24110;&#21161;&#36825;&#20123;&#32452;&#32455;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The EU AI Act is the proposed EU legislation concerning AI systems. This paper identifies several categories of the AI Act. Based on this categorization, a questionnaire is developed that serves as a tool to offer insights by creating quantitative data. Analysis of the data shows various challenges for organizations in different compliance categories. The influence of organization characteristics, such as size and sector, is examined to determine the impact on compliance. The paper will also share qualitative data on which questions were prevalent among respondents, both on the content of the AI Act as the application. The paper concludes by stating that there is still room for improvement in terms of compliance with the AIA and refers to a related project that examines a solution to help these organizations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10455</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#35780;&#20272;&#36808;&#20986;&#30340;&#19968;&#27493;&#65306;BIOSCAN-1M&#26118;&#34411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21363;BIOSCAN-Insect&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#30001;&#19987;&#23478;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20851;&#30340;&#36951;&#20256;&#20449;&#24687;&#65292;&#21253;&#25324;&#21407;&#22987;&#26680;&#33527;&#37240;&#26465;&#24418;&#30721;&#24207;&#21015;&#21644;&#20998;&#37197;&#30340;&#26465;&#24418;&#30721;&#32034;&#24341;&#21495;&#65292;&#36825;&#20123;&#26159;&#22522;&#20110;&#36951;&#20256;&#30340;&#29289;&#31181;&#20998;&#31867;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#36873;&#30340;&#30334;&#19975;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#35780;&#20272;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22266;&#26377;&#30340;&#29983;&#29289;&#24615;&#36136;&#65292;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#26631;&#31614;&#26159;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#26041;&#26696;&#65292;&#22312;&#36739;&#20302;&#32423;&#21035;&#19978;&#21576;&#29616;&#20986;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#38500;&#20102;&#28608;&#21457;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30740;&#31350;&#30340;&#20852;&#36259;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#20419;&#36827;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35831;&#27714;&#25104;&#21592;&#26631;&#31614;&#21644;&#25104;&#23545;&#20559;&#22909;&#26469;&#25193;&#23637;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23398;&#20064;&#25104;&#21592;&#21644;&#20559;&#22909;&#30340;&#32452;&#21512;&#21487;&#20197;&#31283;&#23450;&#21644;&#26041;&#20415;&#22320;&#35782;&#21035;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.10434</link><description>&lt;p&gt;
&#20174;&#25104;&#21592;&#21644;&#20559;&#22909;&#26597;&#35810;&#20013;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning Formal Specifications from Membership and Preference Queries. (arXiv:2307.10434v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35831;&#27714;&#25104;&#21592;&#26631;&#31614;&#21644;&#25104;&#23545;&#20559;&#22909;&#26469;&#25193;&#23637;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23398;&#20064;&#25104;&#21592;&#21644;&#20559;&#22909;&#30340;&#32452;&#21512;&#21487;&#20197;&#31283;&#23450;&#21644;&#26041;&#20415;&#22320;&#35782;&#21035;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#30740;&#31350;&#24191;&#27867;&#30340;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#33258;&#21160;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#25193;&#23637;&#21040;&#35831;&#27714;&#32452;&#21512;&#25104;&#21592;&#26631;&#31614;&#21644;&#25104;&#23545;&#20559;&#22909;&#65288;&#23545;&#25104;&#21592;&#26631;&#31614;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#24335;&#65289;&#12290;&#25104;&#23545;&#20559;&#22909;&#21644;&#25104;&#21592;&#26631;&#31614;&#30340;&#32452;&#21512;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20808;&#21069;&#20165;&#20381;&#36182;&#25104;&#21592;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24191;&#27867;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#20004;&#31181;&#27169;&#24335;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#25104;&#21592;&#21644;&#20559;&#22909;&#26469;&#31283;&#20581;&#21644;&#26041;&#20415;&#22320;&#35782;&#21035;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.10422</link><description>&lt;p&gt;
PreDiff: &#20351;&#29992;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20027;&#35201;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26102;&#31354;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#31354;&#21069;&#22686;&#21152;&#20351;&#24471;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#35201;&#20040;&#38590;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#24573;&#35270;&#29305;&#23450;&#39046;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#27169;&#31946;&#25110;&#20135;&#29983;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26102;&#31354;&#39044;&#27979;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65306;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;PreDiff&#30340;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#65307;2&#65289;&#25105;&#20204;&#34701;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#20351;&#39044;&#27979;&#31526;&#21512;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20272;&#35745;&#19982;&#25152;&#26045;&#21152;&#32422;&#26463;&#30340;&#20559;&#24046;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoi
&lt;/p&gt;</description></item><item><title>GOOSE&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10420</link><description>&lt;p&gt;
GOOSE&#31639;&#27861;: &#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#24037;&#20855;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#21450;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond. (arXiv:2307.10420v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10420
&lt;/p&gt;
&lt;p&gt;
GOOSE&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOOSE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#22312;&#20241;&#24687;&#21644;&#35269;&#39135;&#26102;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#40517;&#38752;&#30528;&#19968;&#21482;&#33151;&#20445;&#25345;&#24179;&#34913;&#65292;&#20197;&#23432;&#25252;&#21644;&#20445;&#25252;&#32676;&#20307;&#20013;&#30340;&#20854;&#20182;&#20010;&#20307;&#12290;GOOSE&#31639;&#27861;&#22312;19&#20010;&#30693;&#21517;&#30340;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#19982;&#36951;&#20256;&#31639;&#27861;(GA)&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;(PSO)&#12289;&#34619;&#34579;&#31639;&#27861;(DA)&#21644;&#36866;&#24212;&#24615;&#20381;&#36182;&#20248;&#21270;&#22120;(FDO)&#30340;&#27604;&#36739;&#30740;&#31350;&#26469;&#39564;&#35777;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#22312;10&#20010;&#29616;&#20195;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#34619;&#34579;&#31639;&#27861;&#12289;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;(WOA)&#21644;&#40144;&#40060;&#32676;&#31639;&#27861;(SSA)&#31561;&#19977;&#20010;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;GOOSE&#31639;&#27861;&#36824;&#22312;5&#20010;&#32463;&#20856;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#25152;&#24471;&#32467;&#26524;&#19982;&#36866;&#24212;&#24615;&#20381;&#36182;&#20248;&#21270;&#22120;(FDO)&#12289;FOX&#20248;&#21270;&#22120;&#12289;&#34678;&#20248;&#21270;&#31639;&#27861;(BOA)&#12289;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#12289;&#20154;&#24037;&#34562;&#32676;&#31639;&#27861;&#21644;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#31561;&#20845;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes the GOOSE algorithm as a novel metaheuristic algorithm based on the goose's behavior during rest and foraging. The goose stands on one leg and keeps his balance to guard and protect other individuals in the flock. The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions, and the results are verified by a comparative study with genetic algorithm (GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10 modern benchmark functions, and the gained results are compared with three recent algorithms, such as the dragonfly algorithm, whale optimization algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm is tested on 5 classical benchmark functions, and the obtained results are evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX optimizer, butterfly optimization algorithm (BOA), whale optimization algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#35299;&#37322;&#33258;&#20027;&#39550;&#39542;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38382;&#31572;&#24335;&#22240;&#26524;&#25512;&#29702;&#26469;&#23454;&#29616;&#39550;&#39542;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25910;&#38598;&#39550;&#39542;&#35270;&#39057;&#24182;&#25163;&#21160;&#26631;&#27880;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10408</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#35299;&#37322;&#33258;&#20027;&#39550;&#39542;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining Autonomous Driving Actions with Visual Question Answering. (arXiv:2307.10408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#35299;&#37322;&#33258;&#20027;&#39550;&#39542;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38382;&#31572;&#24335;&#22240;&#26524;&#25512;&#29702;&#26469;&#23454;&#29616;&#39550;&#39542;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25910;&#38598;&#39550;&#39542;&#35270;&#39057;&#24182;&#25163;&#21160;&#26631;&#27880;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#30340;&#24555;&#36895;&#36827;&#27493;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#33021;&#21147;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#36947;&#36335;&#20107;&#25925;&#21644;&#26082;&#23450;&#30340;&#30417;&#31649;&#21407;&#21017;&#35201;&#27714;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26234;&#33021;&#34892;&#20026;&#36873;&#25321;&#36827;&#34892;&#35299;&#37322;&#12290;&#20026;&#20102;&#20419;&#36827;&#33258;&#20027;&#39550;&#39542;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38382;&#31572;&#24335;&#22240;&#26524;&#25512;&#29702;&#26469;&#35299;&#37322;&#39550;&#39542;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25910;&#38598;&#39550;&#39542;&#35270;&#39057;&#65292;&#24182;&#20174;&#36825;&#20010;&#26085;&#24535;&#25968;&#25454;&#20013;&#22343;&#21248;&#25552;&#21462;&#20116;&#20010;&#36873;&#23450;&#34892;&#20026;&#31867;&#21035;&#30340;&#36830;&#32493;&#24103;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38382;&#39064;-&#31572;&#26696;&#23545;&#23545;&#25552;&#21462;&#30340;&#24103;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#65292;&#20316;&#20026;&#27599;&#31181;&#24773;&#26223;&#20013;&#36873;&#25321;&#30340;&#34892;&#20026;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
The end-to-end learning ability of self-driving vehicles has achieved significant milestones over the last decade owing to rapid advances in deep learning and computer vision algorithms. However, as autonomous driving technology is a safety-critical application of artificial intelligence (AI), road accidents and established regulatory principles necessitate the need for the explainability of intelligent action choices for self-driving vehicles. To facilitate interpretability of decision-making in autonomous driving, we present a Visual Question Answering (VQA) framework, which explains driving actions with question-answering-based causal reasoning. To do so, we first collect driving videos in a simulation environment using reinforcement learning (RL) and extract consecutive frames from this log data uniformly for five selected action categories. Further, we manually annotate the extracted frames using question-answer pairs as justifications for the actions chosen in each scenario. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#29983;&#25104;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#25968;&#25454;&#38598;GenVQA&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#29983;&#25104;&#26032;&#30340;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#20102;&#19971;&#31181;&#19981;&#21516;&#30340;VQA&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10405</link><description>&lt;p&gt;
&#29983;&#25104;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Generative Visual Question Answering. (arXiv:2307.10405v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#29983;&#25104;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#25968;&#25454;&#38598;GenVQA&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#29983;&#25104;&#26032;&#30340;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#20102;&#19971;&#31181;&#19981;&#21516;&#30340;VQA&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#28041;&#21450;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#20197;&#36229;&#36234;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#26032;&#27169;&#22411;&#12290;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#27861;&#36866;&#24212;&#26410;&#26469;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20808;&#36827;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#65292;&#33021;&#22312;&#26102;&#38388;&#19978;&#36827;&#34892;&#27867;&#21270;&#65292;&#24182;&#21462;&#24471;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;GenVQA&#65292;&#21033;&#29992;VQAv2&#21644;MS-COCO&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#36890;&#36807;&#31283;&#23450;&#30340;&#25193;&#25955;&#29983;&#25104;&#26032;&#30340;&#22270;&#20687;&#12290;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#22686;&#24191;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#19971;&#31181;&#22522;&#32447;&#21644;&#23574;&#31471;&#30340;VQA&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#38382;&#39064;&#19982;&#21407;&#22987;VQAv2&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#38382;&#39064;&#65292;&#31572;&#26696;&#24050;&#32463;&#26681;&#25454;&#26032;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#35843;&#26597;&#20960;&#20010;&#25104;&#21151;&#30340;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal tasks involving vision and language in deep learning continue to rise in popularity and are leading to the development of newer models that can generalize beyond the extent of their training data. The current models lack temporal generalization which enables models to adapt to changes in future data. This paper discusses a viable approach to creating an advanced Visual Question Answering (VQA) model which can produce successful results on temporal generalization. We propose a new data set, GenVQA, utilizing images and captions from the VQAv2 and MS-COCO dataset to generate new images through stable diffusion. This augmented dataset is then used to test a combination of seven baseline and cutting edge VQA models. Performance evaluation focuses on questions mirroring the original VQAv2 dataset, with the answers having been adjusted to the new images. This paper's purpose is to investigate the robustness of several successful VQA models to assess their performance on future da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.10404</link><description>&lt;p&gt;
&#20351;&#29992;PIP-Net&#35299;&#37322;&#21644;&#32416;&#27491;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#26159;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#33258;&#21160;&#35786;&#26029;&#25903;&#25345;&#12290;PIP-Net&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20856;&#22411;&#22270;&#20687;&#37096;&#20998;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;PIP-Net&#30340;&#20915;&#31574;&#36807;&#31243;&#31526;&#21512;&#21307;&#23398;&#20998;&#31867;&#26631;&#20934;&#65292;&#20165;&#25552;&#20379;&#22270;&#20687;&#32423;&#21035;&#30340;&#31867;&#26631;&#31614;&#12290;&#30001;&#20110;PIP-Net&#23545;&#21407;&#22411;&#36827;&#34892;&#20102;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;X&#20809;&#20013;&#30340;&#19981;&#33391;&#25991;&#26412;&#25110;&#26631;&#31614;&#38169;&#35823;&#31561;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#26174;&#31034;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#25163;&#21160;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#23545;&#21307;&#23398;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#30456;&#20114;&#21442;&#32771;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10318</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#28040;&#38500;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20351;&#24471;&#20855;&#26377;&#20849;&#21516;&#29992;&#25143;&#38598;&#21512;&#30340;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#22312;&#19981;&#20998;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#27169;&#22411;&#22312;VFL&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#26641;&#22411;VFL&#30340;&#33030;&#24369;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#65288;&#21363;&#23454;&#20363;&#31354;&#38388;&#65289;&#20998;&#37197;&#30340;&#35760;&#24405;&#26631;&#35782;&#38598;&#21512;&#26469;&#25512;&#23548;&#31169;&#26377;&#35757;&#32451;&#26631;&#31614;&#12290;ID2Graph&#25915;&#20987;&#29983;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#20174;&#22270;&#20013;&#25552;&#21462;&#31038;&#21306;&#65292;&#24182;&#20351;&#29992;&#31038;&#21306;&#20449;&#24687;&#23545;&#23616;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#25269;&#24481;&#23454;&#20363;&#31354;&#38388;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;ID-LMID&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;ID2Graph&#25915;&#20987;&#21576;&#29616;&#20986;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents signif
&lt;/p&gt;</description></item><item><title>FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10317</link><description>&lt;p&gt;
FedBug: &#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10317
&lt;/p&gt;
&lt;p&gt;
FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26694;&#26550;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20026;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#24615;&#65292;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#24182;&#19982;&#24444;&#27492;&#21457;&#25955;&#65292;&#36825;&#34987;&#31216;&#20026;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedBug&#65288;&#20855;&#26377;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20943;&#36731;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;FedBug&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#26381;&#21153;&#22120;&#20998;&#21457;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#36328;&#23458;&#25143;&#31471;&#23545;&#40784;&#30340;&#21442;&#32771;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#19978;&#65292;FedBug&#20174;&#20923;&#32467;&#25972;&#20010;&#27169;&#22411;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#35299;&#20923;&#23618;&#65292;&#20174;&#36755;&#20837;&#23618;&#21040;&#36755;&#20986;&#23618;&#12290;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#35757;&#32451;&#35299;&#20923;&#30340;&#26032;&#23618;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20998;&#31163;&#36229;&#24179;&#38754;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;FedBug
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#20027;&#20041;AI&#30340;&#27010;&#24565;&#65292;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#20351;&#29992;&#32477;&#23545;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;AI&#23433;&#20840;&#38382;&#39064;&#65292;&#36825;&#26679;&#20570;&#21487;&#20197;&#36991;&#20813;&#26368;&#31967;&#31957;&#30340;&#32467;&#26524;&#12289;&#38450;&#27490;&#28798;&#38590;&#12289;&#22686;&#21152;&#31995;&#32479;&#30340;&#21487;&#30699;&#27491;&#24615;&#24182;&#24110;&#21161;&#31995;&#32479;&#26356;&#23433;&#20840;&#22320;&#25506;&#32034;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.10315</link><description>&lt;p&gt;
&#32477;&#23545;&#20027;&#20041;AI
&lt;/p&gt;
&lt;p&gt;
Absolutist AI. (arXiv:2307.10315v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#20027;&#20041;AI&#30340;&#27010;&#24565;&#65292;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#20351;&#29992;&#32477;&#23545;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;AI&#23433;&#20840;&#38382;&#39064;&#65292;&#36825;&#26679;&#20570;&#21487;&#20197;&#36991;&#20813;&#26368;&#31967;&#31957;&#30340;&#32467;&#26524;&#12289;&#38450;&#27490;&#28798;&#38590;&#12289;&#22686;&#21152;&#31995;&#32479;&#30340;&#21487;&#30699;&#27491;&#24615;&#24182;&#24110;&#21161;&#31995;&#32479;&#26356;&#23433;&#20840;&#22320;&#25506;&#32034;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#65292;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#37319;&#29992;&#32477;&#23545;&#32422;&#26463;&#26465;&#20214; - &#21363;&#31105;&#27490;&#26576;&#20123;&#34892;&#20026;&#65292;&#26080;&#35770;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#22810;&#23569;&#20215;&#20540; - &#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#22312;&#35768;&#22810;AI&#23433;&#20840;&#38382;&#39064;&#19978;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#23427;&#20026;&#36991;&#20813;&#26368;&#31967;&#31957;&#30340;&#35823;&#23545;&#40784;&#32467;&#26524;&#25552;&#20379;&#20102;&#25252;&#26639;&#12290;&#20854;&#27425;&#65292;&#23427;&#21487;&#20197;&#38450;&#27490;AI&#20026;&#20102;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#32780;&#24341;&#21457;&#28798;&#38590;&#65292;&#20363;&#22914;&#29992;&#26356;&#22810;&#25968;&#37327;&#21644;&#26356;&#39640;&#31119;&#21033;&#27700;&#24179;&#30340;&#29983;&#29289;&#26367;&#20195;&#20154;&#31867;&#12290;&#31532;&#19977;&#65292;&#23427;&#20351;&#31995;&#32479;&#26356;&#21152;&#21487;&#30699;&#27491;&#65292;&#20801;&#35768;&#21019;&#24314;&#32773;&#36827;&#34892;&#32416;&#27491;&#24615;&#24178;&#39044;&#65292;&#27604;&#22914;&#25913;&#21464;&#23427;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#25110;&#20851;&#38381;&#23427;&#20204;&#12290;&#31532;&#22235;&#65292;&#23427;&#36890;&#36807;&#31105;&#27490;&#29305;&#21035;&#21361;&#38505;&#30340;&#34892;&#20026;&#65292;&#24110;&#21161;&#31995;&#32479;&#26356;&#23433;&#20840;&#22320;&#25506;&#32034;&#29615;&#22659;&#12290;&#25105;&#25552;&#20379;&#20102;&#23545;&#32477;&#23545;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#20915;&#31574;&#35770;&#24418;&#24335;&#21270;&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#35777;&#26126;&#20102;&#26377;&#20851;&#22521;&#35757;&#21644;&#34892;&#20026;&#30340;&#19968;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper argues that training AI systems with absolute constraints -- which forbid certain acts irrespective of the amount of value they might produce -may make considerable progress on many AI safety problems in principle. First, it provides a guardrail for avoiding the very worst outcomes of misalignment. Second, it could prevent AIs from causing catastrophes for the sake of very valuable consequences, such as replacing humans with a much larger number of beings living at a higher welfare level. Third, it makes systems more corrigible, allowing creators to make corrective interventions in them, such as altering their objective functions or shutting them down. And fourth, it helps systems explore their environment more safely by prohibiting them from exploring especially dangerous acts. I offer a decision-theoretic formalization of an absolute constraints, improving on existing models in the literature, and use this model to prove some results about the training and behavior of ab
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20351;&#29992;&#30340;&#26415;&#35821;&#36827;&#34892;&#20102;&#24314;&#35774;&#24615;&#25209;&#35780;&#65292;&#25351;&#20986;AI&#30340;&#35752;&#35770;&#32570;&#20047;&#23545;&#38544;&#21947;&#30340;&#25209;&#21028;&#24615;&#36317;&#31163;&#65292;&#23548;&#33268;&#23545;&#36131;&#20219;&#21644;&#28508;&#22312;&#29992;&#36884;&#30340;&#21453;&#24605;&#34987;&#25197;&#26354;&#12290;&#25991;&#31456;&#36890;&#36807;&#25552;&#20986;&#26356;&#21512;&#36866;&#30340;&#26415;&#35821;&#26469;&#20419;&#36827;&#26356;&#23500;&#26377;&#25104;&#26524;&#30340;&#36777;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.10292</link><description>&lt;p&gt;
&#35821;&#35328;&#36855;&#23467;&#65306;&#23545;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#26415;&#35821;&#20351;&#29992;&#30340;&#24314;&#35774;&#24615;&#25209;&#35780;
&lt;/p&gt;
&lt;p&gt;
The Language Labyrinth: Constructive Critique on the Terminology Used in the AI Discourse. (arXiv:2307.10292v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10292
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20351;&#29992;&#30340;&#26415;&#35821;&#36827;&#34892;&#20102;&#24314;&#35774;&#24615;&#25209;&#35780;&#65292;&#25351;&#20986;AI&#30340;&#35752;&#35770;&#32570;&#20047;&#23545;&#38544;&#21947;&#30340;&#25209;&#21028;&#24615;&#36317;&#31163;&#65292;&#23548;&#33268;&#23545;&#36131;&#20219;&#21644;&#28508;&#22312;&#29992;&#36884;&#30340;&#21453;&#24605;&#34987;&#25197;&#26354;&#12290;&#25991;&#31456;&#36890;&#36807;&#25552;&#20986;&#26356;&#21512;&#36866;&#30340;&#26415;&#35821;&#26469;&#20419;&#36827;&#26356;&#23500;&#26377;&#25104;&#26524;&#30340;&#36777;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#23398;&#31185;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#65292;&#26415;&#35821;&#26126;&#30830;&#24615;&#30340;&#38382;&#39064;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;AI&#30340;&#35752;&#35770;&#20173;&#28982;&#32570;&#20047;&#23545;&#35832;&#22914;&#8220;&#35757;&#32451;&#8221;&#12289;&#8220;&#23398;&#20064;&#8221;&#25110;&#8220;&#20915;&#31574;&#8221;&#31561;&#38544;&#21947;&#30340;&#25209;&#21028;&#24615;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#36131;&#20219;&#25110;&#28508;&#22312;&#29992;&#36884;&#30340;&#21453;&#24605;&#34987;&#20005;&#37325;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#30456;&#20851;&#20915;&#31574;&#32773;&#30456;&#20449;AI&#21487;&#20197;&#21457;&#23637;&#8220;&#29702;&#35299;&#8221;&#25110;&#27491;&#30830;&#8220;&#35299;&#37322;&#8221;&#38382;&#39064;&#65292;&#37027;&#20040;&#23427;&#22312;&#20915;&#23450;&#31038;&#20250;&#31119;&#21033;&#25110;&#23457;&#21028;&#26696;&#20214;&#31561;&#25935;&#24863;&#20219;&#21153;&#26102;&#30340;&#24120;&#35268;&#20351;&#29992;&#23558;&#20250;&#20986;&#29616;&#12290;&#26412;&#31456;&#36890;&#36807;&#20998;&#26512;AI&#36777;&#35770;&#30340;&#26680;&#24515;&#27010;&#24565;&#26469;&#25903;&#25345;&#20854;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26356;&#21512;&#36866;&#30340;&#26415;&#35821;&#26469;&#20419;&#36827;&#26356;&#23500;&#26377;&#25104;&#26524;&#30340;&#36777;&#35770;&#12290;&#23427;&#26159;&#19968;&#39033;&#22312;&#25209;&#21028;&#24615;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#35821;&#35328;&#21746;&#23398;&#20043;&#38388;&#20132;&#21449;&#30340;&#27010;&#24565;&#24615;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the interdisciplinary field of artificial intelligence (AI) the problem of clear terminology is especially momentous. This paper claims, that AI debates are still characterised by a lack of critical distance to metaphors like 'training', 'learning' or 'deciding'. As consequence, reflections regarding responsibility or potential use-cases are greatly distorted. Yet, if relevant decision-makers are convinced that AI can develop an 'understanding' or properly 'interpret' issues, its regular use for sensitive tasks like deciding about social benefits or judging court cases looms. The chapter argues its claim by analysing central notions of the AI debate and tries to contribute by proposing more fitting terminology and hereby enabling more fruitful debates. It is a conceptual work at the intersection of critical computer science and philosophy of language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37326;&#22806;&#34460;&#34411;&#32676;&#20307;&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#37327;&#34460;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26469;&#23450;&#20301;&#21644;&#21943;&#27922;&#20892;&#30000;&#20013;&#30340;&#34411;&#23475;&#65292;&#20174;&#32780;&#38477;&#20302;&#20892;&#33647;&#20351;&#29992;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10267</link><description>&lt;p&gt;
&#37326;&#22806;&#34460;&#34411;&#32676;&#20307;&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild. (arXiv:2307.10267v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37326;&#22806;&#34460;&#34411;&#32676;&#20307;&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#37327;&#34460;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26469;&#23450;&#20301;&#21644;&#21943;&#27922;&#20892;&#30000;&#20013;&#30340;&#34411;&#23475;&#65292;&#20174;&#32780;&#38477;&#20302;&#20892;&#33647;&#20351;&#29992;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34460;&#34411;&#21361;&#23475;&#33021;&#20005;&#37325;&#25439;&#23475;&#23567;&#40614;&#21644;&#39640;&#31921;&#30000;&#65292;&#24182;&#20256;&#25773;&#26893;&#29289;&#30149;&#27602;&#65292;&#23548;&#33268;&#20892;&#19994;&#20135;&#37327;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20892;&#27665;&#24120;&#24120;&#20381;&#36182;&#21270;&#23398;&#20892;&#33647;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#24191;&#38420;&#30340;&#30000;&#22320;&#19978;&#20351;&#29992;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#22823;&#37327;&#20892;&#33647;&#34987;&#28010;&#36153;&#22312;&#27809;&#26377;&#23475;&#34411;&#30340;&#22320;&#26041;&#65292;&#21516;&#26102;&#22312;&#20005;&#37325;&#34411;&#23475;&#21306;&#22495;&#20351;&#29992;&#19981;&#36275;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#65292;&#33021;&#22815;&#23450;&#20301;&#24182;&#21943;&#27922;&#20892;&#30000;&#20013;&#30340;&#34411;&#23475;&#65292;&#20174;&#32780;&#20943;&#23569;&#20892;&#33647;&#20351;&#29992;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#37326;&#22806;&#25910;&#38598;&#21644;&#26631;&#27880;&#20102;&#22823;&#37327;&#30340;&#34460;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26469;&#20998;&#21106;&#34460;&#34411;&#32676;&#20307;&#12290;&#21033;&#29992;&#22810;&#23610;&#24230;&#25968;&#25454;&#38598;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#23610;&#24230;&#19979;&#30340;&#34460;&#34411;&#32676;&#20307;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#20998;&#21106;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphid infestations can cause extensive damage to wheat and sorghum fields and spread plant viruses, resulting in significant yield losses in agriculture. To address this issue, farmers often rely on chemical pesticides, which are inefficiently applied over large areas of fields. As a result, a considerable amount of pesticide is wasted on areas without pests, while inadequate amounts are applied to areas with severe infestations. The paper focuses on the urgent need for an intelligent autonomous system that can locate and spray infestations within complex crop canopies, reducing pesticide use and environmental impact. We have collected and labeled a large aphid image dataset in the field, and propose the use of real-time semantic segmentation models to segment clusters of aphids. A multiscale dataset is generated to allow for learning the clusters at different scales. We compare the segmentation speeds and accuracy of four state-of-the-art real-time semantic segmentation models on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#31185;&#23398;&#30740;&#31350;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21313;&#31181;&#26041;&#24335;&#65306;&#24378;&#22823;&#30340;&#24341;&#29992;&#24037;&#20855;&#12289;&#23545;&#30740;&#31350;&#38382;&#39064;&#30340;&#26356;&#22909;&#29702;&#35299;&#12289;&#25913;&#36827;&#30340;&#30740;&#31350;&#38382;&#39064;&#29983;&#25104;&#12289;&#20248;&#21270;&#30340;&#30740;&#31350;&#35774;&#35745;&#12289;&#34394;&#25311;&#25968;&#25454;&#29983;&#25104;&#12289;&#25968;&#25454;&#36716;&#21270;&#12289;&#39640;&#32423;&#25968;&#25454;&#20998;&#26512;&#21644;AI&#36741;&#21161;&#25253;&#21578;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#20102;&#24456;&#22810;&#22909;&#22788;&#65292;&#20294;&#20063;&#38754;&#20020;&#20559;&#35265;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22686;&#24378;&#31185;&#23398;&#20013;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#20294;&#19981;&#33021;&#21462;&#20195;&#23427;&#12290;</title><link>http://arxiv.org/abs/2307.10265</link><description>&lt;p&gt;
AI&#36171;&#33021;&#30740;&#31350;&#65306;&#31185;&#23398;&#22914;&#20309;&#21463;&#30410;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;10&#31181;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI empowering research: 10 ways how science can benefit from AI. (arXiv:2307.10265v1 [cs.GL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#31185;&#23398;&#30740;&#31350;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21313;&#31181;&#26041;&#24335;&#65306;&#24378;&#22823;&#30340;&#24341;&#29992;&#24037;&#20855;&#12289;&#23545;&#30740;&#31350;&#38382;&#39064;&#30340;&#26356;&#22909;&#29702;&#35299;&#12289;&#25913;&#36827;&#30340;&#30740;&#31350;&#38382;&#39064;&#29983;&#25104;&#12289;&#20248;&#21270;&#30340;&#30740;&#31350;&#35774;&#35745;&#12289;&#34394;&#25311;&#25968;&#25454;&#29983;&#25104;&#12289;&#25968;&#25454;&#36716;&#21270;&#12289;&#39640;&#32423;&#25968;&#25454;&#20998;&#26512;&#21644;AI&#36741;&#21161;&#25253;&#21578;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#20102;&#24456;&#22810;&#22909;&#22788;&#65292;&#20294;&#20063;&#38754;&#20020;&#20559;&#35265;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22686;&#24378;&#31185;&#23398;&#20013;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#20294;&#19981;&#33021;&#21462;&#20195;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#31185;&#23398;&#30740;&#31350;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#12290;&#23427;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#21270;&#20102;&#31185;&#23398;&#23478;&#30340;&#24037;&#20316;&#30340;&#21313;&#31181;&#26041;&#24335;&#65292;&#21253;&#25324;&#24378;&#22823;&#30340;&#24341;&#29992;&#24037;&#20855;&#12289;&#23545;&#30740;&#31350;&#38382;&#39064;&#30340;&#26356;&#22909;&#29702;&#35299;&#12289;&#25913;&#36827;&#30340;&#30740;&#31350;&#38382;&#39064;&#29983;&#25104;&#12289;&#20248;&#21270;&#30340;&#30740;&#31350;&#35774;&#35745;&#12289;&#34394;&#25311;&#25968;&#25454;&#29983;&#25104;&#12289;&#25968;&#25454;&#36716;&#21270;&#12289;&#39640;&#32423;&#25968;&#25454;&#20998;&#26512;&#21644;AI&#36741;&#21161;&#25253;&#21578;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#20102;&#24456;&#22810;&#22909;&#22788;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#21040;&#25361;&#25112;&#65292;&#22914;&#20559;&#35265;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#38656;&#27714;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22686;&#24378;&#31185;&#23398;&#20013;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#20294;&#19981;&#33021;&#21462;&#20195;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article explores the transformative impact of artificial intelligence (AI) on scientific research. It highlights ten ways in which AI is revolutionizing the work of scientists, including powerful referencing tools, improved understanding of research problems, enhanced research question generation, optimized research design, stub data generation, data transformation, advanced data analysis, and AI-assisted reporting. While AI offers numerous benefits, challenges such as bias, privacy concerns, and the need for human-AI collaboration must be considered. The article emphasizes that AI can augment human creativity in science but not replace it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;spotPython&#36827;&#34892;scikit-learn&#12289;PyTorch&#21644;river&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#20840;&#38754;&#25351;&#21335;&#12290;&#37325;&#28857;&#20171;&#32461;&#20102;spotPython&#30340;&#20248;&#21270;&#36807;&#31243;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#25351;&#21335;&#20026;&#23545;Python&#36229;&#21442;&#25968;&#35843;&#25972;&#24863;&#20852;&#36259;&#30340;&#20154;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10262</link><description>&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#25351;&#21335;&#65306;&#36866;&#29992;&#20110;scikit-learn&#12289;PyTorch&#12289;river&#21644;spotPython&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython. (arXiv:2307.10262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;spotPython&#36827;&#34892;scikit-learn&#12289;PyTorch&#21644;river&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#20840;&#38754;&#25351;&#21335;&#12290;&#37325;&#28857;&#20171;&#32461;&#20102;spotPython&#30340;&#20248;&#21270;&#36807;&#31243;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#25351;&#21335;&#20026;&#23545;Python&#36229;&#21442;&#25968;&#35843;&#25972;&#24863;&#20852;&#36259;&#30340;&#20154;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;spotPython&#23545;scikit-learn&#12289;PyTorch&#21644;river&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#20840;&#38754;&#25351;&#21335;&#12290;&#31532;&#19968;&#37096;&#20998;&#20171;&#32461;&#20102;spotPython&#30340;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#31532;&#20108;&#37096;&#20998;&#30528;&#37325;&#20171;&#32461;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25991;&#20013;&#25552;&#20379;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;scikit-learn&#27169;&#22411;&#65288;&#22914;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#21644;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#65289;&#20197;&#21450;river&#20013;&#30340;Hoeffding&#33258;&#36866;&#24212;&#26641;&#22238;&#24402;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36824;&#35752;&#35770;&#20102;&#23558;spotPython&#38598;&#25104;&#21040;PyTorch&#21644;PyTorch Lightning&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#36341;&#21644;&#36880;&#27493;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#26412;&#25163;&#20876;&#20026;&#23545;&#20351;&#29992;Python&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#24863;&#20852;&#36259;&#30340;&#20219;&#20309;&#20154;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#36215;&#28857;&#12290;&#37325;&#28857;&#21253;&#25324;Tensorboard&#12289;PyTorch Lightning&#12289;spotPython&#21644;river&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#20986;&#29256;&#29289;&#27491;&#22312;&#24320;&#21457;&#20013;&#65292;&#26356;&#26032;&#20869;&#23481;&#21487;&#22312;&#23545;&#24212;&#30340;&#32593;&#39029;&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document provides a comprehensive guide to hyperparameter tuning using spotPython for scikit-learn, PyTorch, and river. The first part introduces spotPython's surrogate model-based optimization process, while the second part focuses on hyperparameter tuning. Several case studies are presented, including hyperparameter tuning for sklearn models such as Support Vector Classification, Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from river. The integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed. With a hands-on approach and step-by-step explanations, this cookbook serves as a practical starting point for anyone interested in hyperparameter tuning with Python. Highlights include the interplay between Tensorboard, PyTorch Lightning, spotPython, and river. This publication is under development, with updates available on the corresponding webpage.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#29359;&#32618;&#23398;&#21644;&#23431;&#23449;&#23398;&#31561;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;LLM&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#26368;&#22823;&#21270;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10250</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#25512;&#29702;&#65306;&#29359;&#32618;&#35843;&#26597;&#12289;&#21307;&#23398;&#23454;&#36341;&#21644;&#31185;&#23398;&#30740;&#31350;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research. (arXiv:2307.10250v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#29359;&#32618;&#23398;&#21644;&#23431;&#23449;&#23398;&#31561;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;LLM&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#26368;&#22823;&#21270;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#29359;&#32618;&#23398;&#21644;&#23431;&#23449;&#23398;&#31561;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20132;&#20114;&#24335;&#35775;&#35848;&#24418;&#24335;&#65292;AI&#21161;&#25163;&#23637;&#31034;&#20102;&#29983;&#25104;&#21644;&#36873;&#25321;&#20551;&#35774;&#30340;&#21487;&#38752;&#24615;&#12290;&#23427;&#22522;&#20110;&#24739;&#32773;&#25968;&#25454;&#25512;&#26029;&#20986;&#21512;&#29702;&#30340;&#21307;&#23398;&#35786;&#26029;&#65292;&#24182;&#22312;&#29359;&#32618;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#21644;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;LLMs&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology. Using an interactive interview format, the AI assistant demonstrated reliability in generating and selecting hypotheses. It inferred plausible medical diagnoses based on patient data and provided potential causes and explanations in criminology and cosmology. The results highlight the potential of LLMs in complex problem-solving and the need for further research to maximize their practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>CoNAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#32422;&#26463;&#20154;&#33080;&#29305;&#24449;&#34701;&#21512;&#30340;&#26465;&#20214;&#31070;&#32463;&#32858;&#21512;&#32593;&#32476;&#65292;&#38024;&#23545;&#22312;&#38271;&#36317;&#31163;&#21644;&#39640;&#39640;&#24230;&#29615;&#22659;&#19979;&#25429;&#33719;&#30340;&#26497;&#20302;&#20998;&#36776;&#29575;&#20154;&#33080;&#65292;&#21033;&#29992;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#26495;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.10237</link><description>&lt;p&gt;
CoNAN&#65306;&#26080;&#32422;&#26463;&#20154;&#33080;&#29305;&#24449;&#34701;&#21512;&#30340;&#26465;&#20214;&#31070;&#32463;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion. (arXiv:2307.10237v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10237
&lt;/p&gt;
&lt;p&gt;
CoNAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#32422;&#26463;&#20154;&#33080;&#29305;&#24449;&#34701;&#21512;&#30340;&#26465;&#20214;&#31070;&#32463;&#32858;&#21512;&#32593;&#32476;&#65292;&#38024;&#23545;&#22312;&#38271;&#36317;&#31163;&#21644;&#39640;&#39640;&#24230;&#29615;&#22659;&#19979;&#25429;&#33719;&#30340;&#26497;&#20302;&#20998;&#36776;&#29575;&#20154;&#33080;&#65292;&#21033;&#29992;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#26495;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21463;&#38480;&#21046;&#21644;&#26080;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#65292;&#22914;&#38271;&#36317;&#31163;&#12289;&#20302;&#20998;&#36776;&#29575;&#12289;&#19981;&#21516;&#35270;&#35282;&#12289;&#20809;&#29031;&#12289;&#23039;&#24577;&#21644;&#22823;&#27668;&#26465;&#20214;&#19979;&#65292;&#20174;&#22270;&#20687;&#38598;&#20013;&#36827;&#34892;&#20154;&#33080;&#35782;&#21035;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20154;&#33080;&#29305;&#24449;&#32858;&#21512;&#22312;&#36825;&#31867;&#35782;&#21035;&#31995;&#32479;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#28041;&#21450;&#23558;&#27169;&#26495;&#20013;&#30340;N&#20010;&#29305;&#24449;&#34920;&#31034;&#32858;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#20256;&#32479;&#20154;&#33080;&#29305;&#24449;&#32858;&#21512;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#20803;&#25968;&#25454;&#65292;&#35201;&#20040;&#21033;&#29992;&#39640;&#32500;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#26469;&#20272;&#35745;&#29305;&#24449;&#36136;&#37327;&#36827;&#34892;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#36828;&#36317;&#31163;&#21644;&#39640;&#39640;&#24230;&#29615;&#22659;&#19979;&#25429;&#33719;&#30340;&#26497;&#20302;&#20998;&#36776;&#29575;&#20154;&#33080;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20803;&#25968;&#25454;&#25110;&#39118;&#26684;&#20449;&#24687;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoNAN&#30340;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#26495;&#32858;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#22312;&#36755;&#20837;&#29305;&#24449;&#20998;&#24067;&#20449;&#24687;&#26465;&#20214;&#19979;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition from image sets acquired under unregulated and uncontrolled settings, such as at large distances, low resolutions, varying viewpoints, illumination, pose, and atmospheric conditions, is challenging. Face feature aggregation, which involves aggregating a set of N feature representations present in a template into a single global representation, plays a pivotal role in such recognition systems. Existing works in traditional face feature aggregation either utilize metadata or high-dimensional intermediate feature representations to estimate feature quality for aggregation. However, generating high-quality metadata or style information is not feasible for extremely low-resolution faces captured in long-range and high altitude settings. To overcome these limitations, we propose a feature distribution conditioning approach called CoNAN for template aggregation. Specifically, our method aims to learn a context vector conditioned over the distribution information of the incomi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;</title><link>http://arxiv.org/abs/2307.10234</link><description>&lt;p&gt;
SentimentGPT&#65306;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#21450;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#20013;&#21508;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#29305;&#21035;&#26159;&#22312;SemEval 2017&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;4&#20013;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;GPT-3.5 Turbo&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;2&#65289;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;3&#65289;&#37319;&#29992;&#21019;&#26032;&#30340;&#23884;&#20837;&#20998;&#31867;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#21644;&#20010;&#21035;GPT&#27169;&#22411;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#21516;&#26102;&#20195;&#12289;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;22%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;GPT&#26041;&#27861;&#30340;&#37325;&#35201;&#20215;&#20540;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20174;&#22269;&#23478;&#32508;&#21512;&#30284;&#30151;&#32593;&#32476;&#65288;NCCN&#65289;&#32959;&#30244;&#23398;&#20020;&#24202;&#25351;&#21335;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#35813;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#30284;&#30151;&#20998;&#26399;&#20449;&#24687;&#12289;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#21644;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#30340;&#35789;&#24211;&#65288;NCIt&#65289;&#27010;&#24565;&#20197;&#21450;&#33410;&#28857;&#20998;&#31867;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31243;&#24207;&#21270;&#36941;&#21382;&#21644;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2307.10231</link><description>&lt;p&gt;
&#30284;&#30151;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#30340;&#33258;&#21160;&#21270;&#30693;&#35782;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Automated Knowledge Modeling for Cancer Clinical Practice Guidelines. (arXiv:2307.10231v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20174;&#22269;&#23478;&#32508;&#21512;&#30284;&#30151;&#32593;&#32476;&#65288;NCCN&#65289;&#32959;&#30244;&#23398;&#20020;&#24202;&#25351;&#21335;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#35813;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#30284;&#30151;&#20998;&#26399;&#20449;&#24687;&#12289;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#21644;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#30340;&#35789;&#24211;&#65288;NCIt&#65289;&#27010;&#24565;&#20197;&#21450;&#33410;&#28857;&#20998;&#31867;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31243;&#24207;&#21270;&#36941;&#21382;&#21644;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31215;&#26497;&#30740;&#31350;&#20135;&#29983;&#30340;&#26032;&#35777;&#25454;&#65292;&#30284;&#30151;&#30142;&#30149;&#30340;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#65288;CPGs&#65289;&#21457;&#23637;&#36805;&#36895;&#12290;&#30446;&#21069;&#65292;CPGs&#20027;&#35201;&#20197;&#19981;&#36866;&#21512;&#31649;&#29702;&#36825;&#31181;&#21457;&#23637;&#30693;&#35782;&#30340;&#25991;&#26723;&#26684;&#24335;&#21457;&#24067;&#12290;&#38656;&#35201;&#19968;&#31181;&#36866;&#29992;&#20110;&#31243;&#24207;&#20132;&#20114;&#30340;&#25351;&#21335;&#25991;&#26723;&#30340;&#30693;&#35782;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#32508;&#21512;&#30284;&#30151;&#32593;&#32476;&#65288;NCCN&#65289;&#32959;&#30244;&#23398;CPGs&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#29983;&#25104;&#21253;&#21547;&#25552;&#21462;&#30340;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#20351;&#29992;&#20004;&#20010;&#29256;&#26412;&#30340;NCCN&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;CPG&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#23637;&#31034;&#20854;&#24544;&#23454;&#25552;&#21462;&#21644;&#24314;&#27169;&#30693;&#35782;&#30340;&#25928;&#26524;&#12290;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#22686;&#24378;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;&#30284;&#30151;&#20998;&#26399;&#20449;&#24687;&#12289;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#30340;&#20803;&#35789;&#24211;&#21644;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#30340;&#35789;&#24211;&#65288;NCIt&#65289;&#27010;&#24565;&#20197;&#21450;&#33410;&#28857;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#31243;&#24207;&#21270;&#36941;&#21382;&#21644;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to new evidence generated by active research. Currently, CPGs are primarily published in a document format that is ill-suited for managing this developing knowledge. A knowledge model of the guidelines document suitable for programmatic interaction is required. This work proposes an automated method for extraction of knowledge from National Comprehensive Cancer Network (NCCN) CPGs in Oncology and generating a structured model containing the retrieved knowledge. The proposed method was tested using two versions of NCCN Non-Small Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful extraction and modeling of knowledge. Three enrichment strategies using Cancer staging information, Unified Medical Language System (UMLS) Metathesaurus &amp; National Cancer Institute thesaurus (NCIt) concepts, and Node classification are also presented to enhance the model towards enabling programmatic traversal and q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#22240;&#26524;&#36923;&#36753;&#21644;&#35821;&#35328;C&#30340;&#35821;&#35328;C+&#65292;&#20351;&#20854;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#38750;&#31354;&#38598;&#21512;&#20013;&#30340;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#25551;&#36848;&#21160;&#20316;&#30340;&#23646;&#24615;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#23558;C+&#23884;&#20837;&#21040;&#20855;&#26377;&#22810;&#20540;&#24120;&#37327;&#30340;&#22240;&#26524;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22810;&#20540;&#24120;&#37327;&#26367;&#25442;&#20026;&#24067;&#23572;&#24120;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.10227</link><description>&lt;p&gt;
&#22240;&#26524;&#23450;&#24459;&#19982;&#22810;&#20540;&#27969;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Causal Laws and Multi-Valued Fluents. (arXiv:2307.10227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#22240;&#26524;&#36923;&#36753;&#21644;&#35821;&#35328;C&#30340;&#35821;&#35328;C+&#65292;&#20351;&#20854;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#38750;&#31354;&#38598;&#21512;&#20013;&#30340;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#25551;&#36848;&#21160;&#20316;&#30340;&#23646;&#24615;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#23558;C+&#23884;&#20837;&#21040;&#20855;&#26377;&#22810;&#20540;&#24120;&#37327;&#30340;&#22240;&#26524;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22810;&#20540;&#24120;&#37327;&#26367;&#25442;&#20026;&#24067;&#23572;&#24120;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24310;&#32493;&#20102;&#22312;&#38750;&#21333;&#35843;&#24418;&#24335;&#21270;&#20013;&#34920;&#31034;&#34892;&#20026;&#29305;&#24615;&#30340;&#24037;&#20316;&#32447;&#65292;&#24378;&#35843;&#20102;&#8220;&#30495;&#23454;&#8221;&#21644;&#8220;&#22240;&#26524;&#8221;&#30340;&#21306;&#21035;&#65292;&#31867;&#20284;&#20110;McCain&#21644;Turner&#24341;&#20837;&#30340;&#22240;&#26524;&#36923;&#36753;&#31995;&#32479;&#21644;Giunchiglia&#21644;Lifschitz&#25552;&#20986;&#30340;&#34892;&#21160;&#35821;&#35328;C&#12290;&#22312;&#35821;&#35328;C+&#20013;&#65292;&#21807;&#19968;&#21487;&#20197;&#30452;&#25509;&#34920;&#31034;&#30340;&#27969;&#21464;&#37327;&#26159;&#30495;&#20540;&#27969;&#21464;&#37327;&#65292;&#36825;&#36890;&#24120;&#26159;&#19981;&#26041;&#20415;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22240;&#26524;&#36923;&#36753;&#21644;&#35821;&#35328;C&#37117;&#21487;&#20197;&#25193;&#23637;&#20197;&#20801;&#35768;&#26469;&#33258;&#20219;&#24847;&#38750;&#31354;&#38598;&#21512;&#30340;&#20540;&#12290;&#25105;&#20204;&#31216;&#20026;C+&#30340;&#35821;&#35328;&#25193;&#23637;&#36824;&#21487;&#20197;&#26681;&#25454;&#23646;&#24615;&#26469;&#25551;&#36848;&#21160;&#20316;&#65292;&#36825;&#22312;&#24378;&#35843;&#25193;&#20805;&#23481;&#24525;&#24615;&#30340;&#35282;&#24230;&#19978;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;C+&#23884;&#20837;&#21040;&#20855;&#26377;&#22810;&#20540;&#24120;&#37327;&#30340;&#22240;&#26524;&#29702;&#35770;&#20013;&#65292;&#23558;C+&#19982;Pednault&#30340;&#34892;&#21160;&#35821;&#35328;ADL&#30456;&#20851;&#32852;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22810;&#20540;&#24120;&#37327;&#26367;&#25442;&#20026;&#24067;&#23572;&#24120;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper continues the line of work on representing properties of actions in nonmonotonic formalisms that stresses the distinction between being "true" and being "caused", as in the system of causal logic introduced by McCain and Turner and in the action language C proposed by Giunchiglia and Lifschitz. The only fluents directly representable in language C+ are truth-valued fluents, which is often inconvenient. We show that both causal logic and language C can be extended to allow values from arbitrary nonempty sets. Our extension of language C, called C+, also makes it possible to describe actions in terms of their attributes, which is important from the perspective of elaboration tolerance. We describe an embedding of C+ in causal theories with multi-valued constants, relate C+ to Pednault's action language ADL, and show how multi-valued constants can be eliminated in favor of Boolean constants.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#31283;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;grounding&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#24847;&#19968;&#38454;&#21477;&#23376;&#30340;&#35821;&#27861;&#12290;&#36890;&#36807;&#25193;&#23637;&#24490;&#29615;&#20844;&#24335;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#21547;&#26377;&#25110;&#30340;&#31243;&#24207;&#21644;&#20219;&#24847;&#19968;&#38454;&#21477;&#23376;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#26102;&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#32467;&#26524;&#65292;&#24182;&#31616;&#21270;&#26597;&#35810;&#22238;&#31572;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.10226</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#21464;&#37327;&#30340;&#24490;&#29615;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
On Loop Formulas with Variables. (arXiv:2307.10226v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10226
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#31283;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;grounding&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#24847;&#19968;&#38454;&#21477;&#23376;&#30340;&#35821;&#27861;&#12290;&#36890;&#36807;&#25193;&#23637;&#24490;&#29615;&#20844;&#24335;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#21547;&#26377;&#25110;&#30340;&#31243;&#24207;&#21644;&#20219;&#24847;&#19968;&#38454;&#21477;&#23376;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#26102;&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#32467;&#26524;&#65292;&#24182;&#31616;&#21270;&#26597;&#35810;&#22238;&#31572;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ferraris&#65292;Lee&#21644;Lifschitz&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31283;&#23450;&#27169;&#22411;&#23450;&#20041;&#65292;&#19981;&#28041;&#21450;grounding&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#19968;&#38454;&#21477;&#23376;&#30340;&#35821;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#19982;&#38472;&#65292;&#26519;&#65292;&#29579;&#21644;&#24352;&#25552;&#20986;&#30340;&#24102;&#26377;&#21464;&#37327;&#30340;&#24490;&#29615;&#20844;&#24335;&#30340;&#20851;&#31995;&#65292;&#24182;&#23558;&#20182;&#20204;&#30340;&#24490;&#29615;&#20844;&#24335;&#25512;&#24191;&#21040;&#20102;&#21547;&#26377;&#25110;&#30340;&#31243;&#24207;&#21644;&#20219;&#24847;&#19968;&#38454;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#36923;&#36753;&#31243;&#24207;&#30340;&#35821;&#27861;&#65292;&#20801;&#35768;&#26174;&#24335;&#37327;&#35789;&#65292;&#24182;&#23558;&#20854;&#35821;&#20041;&#23450;&#20041;&#20026;Ferraris&#31561;&#20154;&#30340;&#31283;&#23450;&#27169;&#22411;&#26032;&#35821;&#35328;&#30340;&#23376;&#31867;&#12290;&#36825;&#26679;&#30340;&#31243;&#24207;&#32487;&#25215;&#20102;&#19968;&#33324;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21363;&#21363;&#20351;&#22312;&#27809;&#26377;&#21807;&#19968;&#21517;&#31216;&#21644;&#22495;&#38381;&#21253;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;&#19979;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#65292;&#21516;&#26102;&#30001;&#20110;&#21463;&#38480;&#30340;&#35821;&#27861;&#32780;&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#24490;&#29615;&#20844;&#24335;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#23450;&#30340;&#21477;&#27861;&#26465;&#20214;&#65292;&#20351;&#24471;&#25193;&#23637;&#31243;&#24207;&#30340;&#26597;&#35810;&#22238;&#31572;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#30340;&#34164;&#28085;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Ferraris, Lee and Lifschitz proposed a new definition of stable models that does not refer to grounding, which applies to the syntax of arbitrary first-order sentences. We show its relation to the idea of loop formulas with variables by Chen, Lin, Wang and Zhang, and generalize their loop formulas to disjunctive programs and to arbitrary first-order sentences. We also extend the syntax of logic programs to allow explicit quantifiers, and define its semantics as a subclass of the new language of stable models by Ferraris et al. Such programs inherit from the general language the ability to handle nonmonotonic reasoning under the stable model semantics even in the absence of the unique name and the domain closure assumptions, while yielding more succinct loop formulas than the general language due to the restricted syntax. We also show certain syntactic conditions under which query answering for an extended program can be reduced to entailment checking in first-order logic, prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#19968;&#38454;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;&#65292;&#20801;&#35768;&#20351;&#29992;&#20869;&#28085;&#20989;&#25968;&#65292;&#27604;&#36739;&#20102;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#65292;&#36824;&#21033;&#29992;&#36825;&#20010;&#25193;&#23637;&#23450;&#20041;&#20102;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#22411;&#35770;&#65288;ASPMT&#65289;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;SMT&#30340;&#26377;&#25928;&#19968;&#38454;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.10225</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#28085;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
First-Order Stable Model Semantics with Intensional Functions. (arXiv:2307.10225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#19968;&#38454;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;&#65292;&#20801;&#35768;&#20351;&#29992;&#20869;&#28085;&#20989;&#25968;&#65292;&#27604;&#36739;&#20102;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#65292;&#36824;&#21033;&#29992;&#36825;&#20010;&#25193;&#23637;&#23450;&#20041;&#20102;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#22411;&#35770;&#65288;ASPMT&#65289;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;SMT&#30340;&#26377;&#25928;&#19968;&#38454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#20856;&#36923;&#36753;&#20013;&#65292;&#38750;&#24067;&#23572;&#20989;&#25968;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#20301;&#32622;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20989;&#25968;&#26469;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22312;&#31572;&#26696;&#38598;&#31243;&#24207;&#20013;&#65292;&#20989;&#25968;&#30340;&#20540;&#26159;&#39044;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30340;&#38750;&#21333;&#35843;&#24615;&#19982;&#26368;&#23567;&#21270;&#35859;&#35789;&#30340;&#33539;&#22260;&#26377;&#20851;&#65292;&#32780;&#19982;&#20989;&#25968;&#26080;&#20851;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Ferraris&#12289;Lee&#21644;Lifschitz&#30340;&#19968;&#38454;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;&#65292;&#20801;&#35768;&#20869;&#28085;&#20989;&#25968;&#8212;&#8212;&#21363;&#30001;&#36923;&#36753;&#31243;&#24207;&#25351;&#23450;&#30340;&#20989;&#25968;&#65292;&#23601;&#20687;&#35859;&#35789;&#19968;&#26679;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;&#23646;&#24615;&#22312;&#36825;&#20010;&#24418;&#24335;&#31995;&#32479;&#20013;&#22914;&#20309;&#33258;&#28982;&#25193;&#23637;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#34701;&#20837;&#20869;&#28085;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25193;&#23637;&#20026;&#23450;&#20041;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#22411;&#35770;&#65288;ASPMT&#65289;&#25552;&#20379;&#22522;&#30784;&#65292;&#31867;&#20284;&#20110;&#23450;&#20041;SMT&#30340;&#26041;&#24335;&#65292;&#20801;&#35768;&#22312;ASP&#29615;&#22659;&#20013;&#36827;&#34892;&#31867;&#20284;SMT&#30340;&#26377;&#25928;&#19968;&#38454;&#25512;&#29702;&#12290;&#20351;&#29992;SM...
&lt;/p&gt;
&lt;p&gt;
In classical logic, nonBoolean fluents, such as the location of an object, can be naturally described by functions. However, this is not the case in answer set programs, where the values of functions are pre-defined, and nonmonotonicity of the semantics is related to minimizing the extents of predicates but has nothing to do with functions. We extend the first-order stable model semantics by Ferraris, Lee, and Lifschitz to allow intensional functions -- functions that are specified by a logic program just like predicates are specified. We show that many known properties of the stable model semantics are naturally extended to this formalism and compare it with other related approaches to incorporating intensional functions. Furthermore, we use this extension as a basis for defining Answer Set Programming Modulo Theories (ASPMT), analogous to the way that Satisfiability Modulo Theories (SMT) is defined, allowing for SMT-like effective first-order reasoning in the context of ASP. Using SM
&lt;/p&gt;</description></item><item><title>RL-ViGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10224</link><description>&lt;p&gt;
RL-ViGen: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization. (arXiv:2307.10224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10224
&lt;/p&gt;
&lt;p&gt;
RL-ViGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;Visual RL&#65289;&#19982;&#39640;&#32500;&#35266;&#23519;&#30456;&#32467;&#21512;&#65292;&#19968;&#30452;&#38754;&#20020;&#30528;&#38271;&#26399;&#23384;&#22312;&#30340;&#27867;&#21270;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#37325;&#28857;&#30740;&#31350;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#27867;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#22522;&#20934;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#23616;&#38480;&#20110;&#23396;&#31435;&#30340;&#20219;&#21153;&#21644;&#27867;&#21270;&#31867;&#21035;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RL-ViGen&#65306;&#19968;&#31181;&#26032;&#22411;&#30340;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#24471;&#20986;&#26356;&#21487;&#38752;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;RL-ViGen&#23558;&#26368;&#26032;&#30340;&#27867;&#21270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#29616;&#26377;&#31639;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#26222;&#36941;&#21344;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#24895;&#26223;&#26159;RL-ViGen&#23558;&#22312;&#36825;&#20010;&#39046;&#22495;&#36215;&#21040;&#20652;&#21270;&#21058;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that RL-ViGen will serve as a catalyst in this a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#22312;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20559;&#35265;&#21644;&#20260;&#23475;&#26102;&#25972;&#21512;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#37239;&#20799;&#31038;&#21306;&#20026;&#35270;&#35282;&#37325;&#26032;&#35774;&#35745;&#20559;&#35265;&#22870;&#37329;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10223</link><description>&lt;p&gt;
&#21463;&#22870;&#36175;&#32422;&#26463;&#65306;&#20849;&#21516;&#26500;&#24314;&#35780;&#20272;&#37239;&#20799;&#20154;&#24037;&#26234;&#33021;&#20260;&#23475;&#30340;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. (arXiv:2307.10223v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#22312;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20559;&#35265;&#21644;&#20260;&#23475;&#26102;&#25972;&#21512;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#37239;&#20799;&#31038;&#21306;&#20026;&#35270;&#35282;&#37325;&#26032;&#35774;&#35745;&#20559;&#35265;&#22870;&#37329;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#35780;&#20272;&#22522;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#25991;&#26723;&#24050;&#25104;&#20026;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20559;&#35265;&#21644;&#20260;&#23475;&#30340;&#26680;&#24515;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23457;&#35745;&#36807;&#31243;&#22240;&#26410;&#25972;&#21512;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#30693;&#35782;&#24182;&#32771;&#34385;&#23457;&#35745;&#21592;&#19982;&#31038;&#21306;&#20043;&#38388;&#30340;&#26435;&#21147;&#21160;&#24577;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#19982;&#21463;&#24433;&#21709;&#31038;&#21306;&#35782;&#21035;&#21644;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20260;&#23475;&#30340;&#20559;&#35265;&#35780;&#20272;&#26041;&#24335;&#65288;&#20363;&#22914;&#20559;&#35265;&#22870;&#37329;&#65289;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20851;&#20110;&#36793;&#32536;&#21270;&#31038;&#21306;&#23545;&#27492;&#31867;&#23457;&#35745;&#36807;&#31243;&#30340;&#26399;&#26395;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21521;&#37239;&#20799;&#31038;&#21306;&#24449;&#27714;&#20182;&#20204;&#23545;&#23457;&#35745;&#36807;&#31243;&#30340;&#31435;&#22330;&#21644;&#26399;&#26395;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#20010;&#21442;&#19982;&#24335;&#30740;&#35752;&#20250;&#65292;&#20174;&#37239;&#20799;&#30340;&#35282;&#24230;&#23545;&#20559;&#35265;&#22870;&#37329;&#36827;&#34892;&#25209;&#21028;&#24615;&#30340;&#37325;&#26032;&#35774;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#26377;&#31354;&#38388;&#26102;&#65292;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#33539;&#22260;&#36828;&#36828;&#36229;&#20986;&#20102;&#20559;&#35265;&#22870;&#37329;&#25152;&#33021;&#25552;&#20379;&#30340;&#33539;&#22260;&#65292;&#21442;&#19982;&#32773; que
&lt;/p&gt;
&lt;p&gt;
Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants que
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;&#20849;&#21516;&#21019;&#36896;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20215;&#20540;&#35266;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#32467;&#26500;&#21644;&#25191;&#34892;&#20914;&#31361;&#20215;&#20540;&#35266;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#31574;&#30053;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.10221</link><description>&lt;p&gt;
&#12300;&#30446;&#21069;&#26159;&#22823;&#26434;&#28905;&#12301;&#65306;&#25506;&#31350;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;&#20849;&#21516;&#21019;&#36896;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20215;&#20540;&#35266;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. (arXiv:2307.10221v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;&#20849;&#21516;&#21019;&#36896;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20215;&#20540;&#35266;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#32467;&#26500;&#21644;&#25191;&#34892;&#20914;&#31361;&#20215;&#20540;&#35266;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#31574;&#30053;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30028;&#34920;&#31034;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#19968;&#37096;&#20998;&#24314;&#31435;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;RAI&#65289;&#30340;&#20215;&#20540;&#35266;&#21644;&#23454;&#36341;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#19968;&#20123;&#32452;&#32455;&#21644;&#31038;&#21306;&#27491;&#22312;&#21709;&#24212;&#36825;&#19968;&#21628;&#21505;&#65292;&#20998;&#20139;RAI&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#22810;&#23398;&#31185;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;&#24847;&#35782;&#12289;&#35752;&#35770;&#21644;&#25191;&#34892;&#27492;&#31867;&#23454;&#36341;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#26512;&#20174;&#19994;&#32773;&#22312;&#23545;&#40784;RAI&#20215;&#20540;&#35266;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#20849;&#21516;&#21019;&#36896;&#25361;&#25112;&#65292;&#20026;&#27492;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#23545;10&#20010;&#32452;&#32455;&#20013;&#30340;23&#21517;&#20010;&#20307;&#36827;&#34892;&#20102;&#35775;&#35848;&#65292;&#36825;&#20123;&#20010;&#20307;&#36127;&#36131;&#21457;&#24067;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#30340;&#20135;&#21697;&#65292;&#21516;&#26102;&#36981;&#23432;RAI&#35268;&#33539;&#65292;&#21457;&#29616;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#32452;&#32455;&#32467;&#26500;&#20026;&#19981;&#21516;&#32844;&#36131;&#36896;&#25104;&#20102;&#36127;&#25285;&#65292;&#20351;&#20854;&#26080;&#27861;&#22362;&#23432;RAI&#20215;&#20540;&#35266;&#65292;&#36825;&#19968;&#25361;&#25112;&#22312;&#25191;&#34892;&#20914;&#31361;&#30340;&#20215;&#20540;&#35266;&#26102;&#36827;&#19968;&#27493;&#21152;&#21095;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#20174;&#19994;&#32773;&#29992;&#20316;&#35299;&#20915;&#25361;&#25112;&#31574;&#30053;&#30340;&#22810;&#20010;&#20215;&#20540;&#26464;&#26438;&#12290;&#25105;&#20204;&#22312;&#35770;&#25991;&#30340;&#26368;&#21518;&#25552;&#20986;&#20102;&#21253;&#23481;&#21644;&#21487;&#34892;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the AI/ML research community has indicated an urgent need to establish Responsible AI (RAI) values and practices as part of the AI/ML lifecycle. Several organizations and communities are responding to this call by sharing RAI guidelines. However, there are gaps in awareness, deliberation, and execution of such practices for multi-disciplinary ML practitioners. This work contributes to the discussion by unpacking co-production challenges faced by practitioners as they align their RAI values. We interviewed 23 individuals, across 10 organizations, tasked to ship AI/ML based products while upholding RAI norms and found that both top-down and bottom-up institutional structures create burden for different roles preventing them from upholding RAI values, a challenge that is further exacerbated when executing conflicted values. We share multiple value levers used as strategies by the practitioners to resolve their challenges. We end our paper with recommendations for inclusive and e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.10219</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#30340;&#19981;&#21464;&#20851;&#31995;&#30693;&#35782;&#19978;&#25506;&#32034;&#36229;&#20851;&#31995;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;(HKGs)&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;(KGs)&#30340;&#24310;&#20280;&#65292;&#20026;&#27599;&#20010;KG&#20107;&#23454;&#25552;&#20379;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;(&#21363;&#38480;&#23450;&#35789;)&#65292;&#20197;&#26356;&#22909;&#22320;&#38480;&#21046;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#22312;HKGs&#19978;&#36827;&#34892;&#22270;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#22823;&#37327;&#24179;&#34892;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;&#26102;&#38388;KGs(TKGs)&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;TKG&#20107;&#23454;&#21487;&#20197;&#34987;&#35270;&#20026;&#24102;&#26377;&#26102;&#38388;&#25139;(&#25110;&#26102;&#38388;&#27573;)&#30340;KG&#20107;&#23454;&#65292;&#25351;&#23450;&#20854;&#26102;&#38388;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;HKG&#25512;&#29702;&#26041;&#27861;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#20026;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#26174;&#24335;&#22320;&#25351;&#23450;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#21482;&#37325;&#35270;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#27809;&#26377;&#21150;&#27861;&#20174;&#38480;&#23450;&#35789;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;TKG&#25512;&#29702;&#21644;HKG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG(HTKG)&#25968;&#25454;&#38598;&#65292;&#21363;Wiki-hy&#21644;...
&lt;/p&gt;
&lt;p&gt;
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20808;&#20351;&#29992;&#20998;&#31867;&#22120;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#28982;&#21518;&#21033;&#29992;&#25552;&#31034;&#29983;&#25104;&#26356;&#23569;&#20559;&#35265;&#25110;&#26080;&#20559;&#35265;&#30340;&#26367;&#20195;&#35821;&#35328;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20026;&#20943;&#23569;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27807;&#36890;&#29615;&#22659;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10213</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#20943;&#23569;&#20559;&#35265;&#65306;&#19968;&#20010;&#24102;&#25552;&#31034;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#21644;&#21435;&#20559;&#35265;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts. (arXiv:2307.10213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20808;&#20351;&#29992;&#20998;&#31867;&#22120;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#28982;&#21518;&#21033;&#29992;&#25552;&#31034;&#29983;&#25104;&#26356;&#23569;&#20559;&#35265;&#25110;&#26080;&#20559;&#35265;&#30340;&#26367;&#20195;&#35821;&#35328;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20026;&#20943;&#23569;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27807;&#36890;&#29615;&#22659;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#35270;&#24615;&#35821;&#35328;&#21644;&#20559;&#35265;&#36890;&#24120;&#22312;&#23545;&#35805;&#20013;&#23384;&#22312;&#65292;&#36825;&#36890;&#24120;&#23545;&#22522;&#20110;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#23447;&#25945;&#30340;&#30446;&#26631;&#32676;&#20307;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#28982;&#21518;&#21033;&#29992;&#19968;&#20010;&#21435;&#20559;&#35265;&#32452;&#20214;&#36890;&#36807;&#25552;&#31034;&#29983;&#25104;&#26356;&#23569;&#20559;&#35265;&#25110;&#26080;&#20559;&#35265;&#30340;&#26367;&#20195;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#20167;&#24680;&#35328;&#35770;&#23548;&#33268;&#30340;&#36127;&#38754;&#24615;&#20943;&#23569;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27807;&#36890;&#29615;&#22659;&#30340;&#21162;&#21147;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discriminatory language and biases are often present in hate speech during conversations, which usually lead to negative impacts on targeted groups such as those based on race, gender, and religion. To tackle this issue, we propose an approach that involves a two-step process: first, detecting hate speech using a classifier, and then utilizing a debiasing component that generates less biased or unbiased alternatives through prompts. We evaluated our approach on a benchmark dataset and observed reduction in negativity due to hate speech comments. The proposed method contributes to the ongoing efforts to reduce biases in online discourse and promote a more inclusive and fair environment for communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#65292;&#25506;&#32034;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#38656;&#35201;&#23545;&#29616;&#26377;&#36164;&#28304;&#36827;&#34892;&#20462;&#27491;&#26469;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;</title><link>http://arxiv.org/abs/2307.10200</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#20559;&#35265;&#20013;&#20998;&#31163;&#31038;&#20250;&#19981;&#24179;&#31561;&#65306;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#20013;&#30340;&#24615;&#21035;&#19981;&#24179;&#31561;
&lt;/p&gt;
&lt;p&gt;
Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings. (arXiv:2307.10200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#65292;&#25506;&#32034;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#38656;&#35201;&#23545;&#29616;&#26377;&#36164;&#28304;&#36827;&#34892;&#20462;&#27491;&#26469;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23130;&#26159;&#27861;&#38498;&#27861;&#24459;&#35299;&#38500;&#23130;&#23035;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#30001;&#20110;&#36825;&#36890;&#24120;&#26159;&#23130;&#23035;&#32852;&#21512;&#30340;&#19981;&#24841;&#24555;&#32467;&#26524;&#65292;&#27599;&#19968;&#26041;&#37117;&#21487;&#33021;&#26377;&#29702;&#30001;&#35201;&#27714;&#36864;&#20986;&#20915;&#23450;&#65292;&#36825;&#36890;&#24120;&#22312;&#27861;&#24237;&#35785;&#35772;&#20013;&#26377;&#35814;&#32454;&#35760;&#24405;&#12290;&#36890;&#36807;&#19968;&#20221;&#21253;&#21547;17,306&#20221;&#27861;&#24237;&#35785;&#35772;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#65292;&#26412;&#25991;&#36890;&#36807;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#34429;&#28982;&#26032;&#20852;&#30340;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#20844;&#20849;&#27861;&#24237;&#35760;&#24405;&#65289;&#22312;&#36741;&#21161;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#24178;&#25200;&#25110;&#24433;&#21709;&#27492;&#31867;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23545;&#29616;&#26377;NLP&#36164;&#28304;&#20013;&#30340;&#28508;&#22312;&#24046;&#36317;&#21644;&#38480;&#21046;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;NLP&#36164;&#28304;&#38656;&#35201;&#36827;&#34892;&#20960;&#20010;&#38750;&#24179;&#20961;&#30340;&#20462;&#25913;&#65292;&#20197;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#22312;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#22823;&#37327;&#30340;&#27861;&#24237;&#26696;&#20214;&#21487;&#33021;&#26263;&#31034;&#30528;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divorce is the legal dissolution of a marriage by a court. Since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. Via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. While emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (NLP) methods may interfere with or affect such studies. We thus require a thorough analysis of potential gaps and limitations present in extant NLP resources. In this paper, on the methodological side, we demonstrate that existing NLP resources required several non-trivial modifications to quantify societal inequalities. On the substantive side, we find that while a large number of court cases perhaps suggest chan
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26041;&#38754;&#36229;&#36807;&#32654;&#22269;&#30340;&#25968;&#37327;&#65292;&#20294;&#22312;&#36136;&#37327;&#19978;&#20173;&#31245;&#36874;&#65292;&#20854;&#20013;&#30340;&#21407;&#22240;&#21253;&#25324;&#20840;&#29699;&#36235;&#21183;&#12289;&#20392;&#27665;&#21644;&#22238;&#22269;&#20154;&#21592;&#30340;&#36129;&#29486;&#20197;&#21450;&#30456;&#23545;&#23485;&#26494;&#30340;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.10198</link><description>&lt;p&gt;
&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26041;&#38754;&#36214;&#19978;&#32654;&#22269;&#20102;&#21527;&#65311;&#19968;&#39033;&#20851;&#20110;&#21518;&#21457;&#24037;&#19994;&#21270;&#22269;&#23478;&#30340;&#27169;&#20223;&#21516;&#26500;&#27169;&#22411;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Has China caught up to the US in AI research? An exploration of mimetic isomorphism as a model for late industrializers. (arXiv:2307.10198v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10198
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26041;&#38754;&#36229;&#36807;&#32654;&#22269;&#30340;&#25968;&#37327;&#65292;&#20294;&#22312;&#36136;&#37327;&#19978;&#20173;&#31245;&#36874;&#65292;&#20854;&#20013;&#30340;&#21407;&#22240;&#21253;&#25324;&#20840;&#29699;&#36235;&#21183;&#12289;&#20392;&#27665;&#21644;&#22238;&#22269;&#20154;&#21592;&#30340;&#36129;&#29486;&#20197;&#21450;&#30456;&#23545;&#23485;&#26494;&#30340;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;21&#19990;&#32426;&#25216;&#26415;&#30340;&#22522;&#30707;&#65292;&#20013;&#22269;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#22269;&#30340;AI&#21457;&#23637;&#36807;&#31243;&#65292;&#34920;&#26126;&#20854;&#29305;&#28857;&#26159;&#24555;&#36895;&#23398;&#20064;&#21644;&#24046;&#24322;&#21270;&#21457;&#23637;&#65292;&#36229;&#36807;&#20102;&#26089;&#26399;&#20122;&#27954;&#24037;&#19994;&#21270;&#22269;&#23478;&#20381;&#38752;&#22806;&#22269;&#30452;&#25509;&#25237;&#36164;&#25512;&#21160;&#30340;&#20986;&#21475;&#23548;&#21521;&#22411;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26174;&#31034;&#65292;&#20013;&#22269;&#30446;&#21069;&#22312;AI&#30456;&#20851;&#30740;&#31350;&#35770;&#25991;&#30340;&#25968;&#37327;&#26041;&#38754;&#39046;&#20808;&#20110;&#32654;&#22269;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#26681;&#25454;&#20855;&#20307;&#25351;&#26631;&#28145;&#20837;&#30740;&#31350;&#36825;&#20123;&#35770;&#25991;&#30340;&#36136;&#37327;&#26102;&#65292;&#32654;&#22269;&#20173;&#30053;&#21344;&#20248;&#21183;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20013;&#22269;&#30340;AI&#21457;&#23637;&#30340;&#36895;&#24230;&#21644;&#35268;&#27169;&#20173;&#20540;&#24471;&#20851;&#27880;&#12290;&#25105;&#20204;&#23558;&#20013;&#22269;&#21152;&#36895;&#30340;AI&#36827;&#23637;&#24402;&#22240;&#20110;&#20960;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20840;&#29699;&#36235;&#21183;&#25903;&#25345;&#31639;&#27861;&#21644;&#30740;&#31350;&#35770;&#25991;&#30340;&#24320;&#25918;&#33719;&#21462;&#12289;&#20013;&#22269;&#24191;&#27867;&#30340;&#20392;&#27665;&#21644;&#22238;&#22269;&#20154;&#21592;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#30456;&#23545;&#23485;&#26494;&#30340;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#12290; &#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34913;&#37327;&#20013;&#22269;&#27169;&#20223;&#32654;&#22269;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI), a cornerstone of 21st-century technology, has seen remarkable growth in China. In this paper, we examine China's AI development process, demonstrating that it is characterized by rapid learning and differentiation, surpassing the export-oriented growth propelled by Foreign Direct Investment seen in earlier Asian industrializers.  Our data indicates that China currently leads the USA in the volume of AI-related research papers. However, when we delve into the quality of these papers based on specific metrics, the USA retains a slight edge. Nevertheless, the pace and scale of China's AI development remain noteworthy.  We attribute China's accelerated AI progress to several factors, including global trends favoring open access to algorithms and research papers, contributions from China's broad diaspora and returnees, and relatively lax data protection policies.  In the vein of our research, we have developed a novel measure for gauging China's imitation of US
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#23545;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#26368;&#26032;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;ChatGPT&#22312;&#25968;&#23383;&#21462;&#35777;&#29992;&#20363;&#20013;&#30340;&#20248;&#21183;&#21644;&#39118;&#38505;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#24635;&#20307;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.10195</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;: &#22909;&#30340;&#65292;&#22351;&#30340;&#21644;&#26410;&#30693;&#30340; (arXiv:2307.10195v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Digital Forensic Investigation: The Good, The Bad, and The Unknown. (arXiv:2307.10195v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#23545;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#26368;&#26032;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;ChatGPT&#22312;&#25968;&#23383;&#21462;&#35777;&#29992;&#20363;&#20013;&#30340;&#20248;&#21183;&#21644;&#39118;&#38505;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#24635;&#20307;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30028;&#21644;&#31038;&#20250;&#20013;&#65292;ChatGPT (GPT-3.5&#65292;GPT-4) &#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#30772;&#22351;&#24615;&#24212;&#29992;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#35752;&#35770;&#30340;&#35805;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;BERT&#12289;Bard&#12289;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#12289;LLaMA&#31561;&#65292;&#20855;&#26377;&#26681;&#25454;&#22823;&#37327;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#25509;&#25910;&#29992;&#25143;&#25351;&#20196;&#25110;&#25552;&#31034;&#65292;&#24182;&#29983;&#25104;&#31572;&#26696;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#23545;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#26368;&#26032;&#39044;&#35757;&#32451;LLM&#8212;&#8212;GPT-4&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#23383;&#21462;&#35777;&#29992;&#20363;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35777;&#25454;&#29702;&#35299;&#12289;&#35777;&#25454;&#25628;&#32034;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#20107;&#20214;&#21709;&#24212;&#21644;&#25945;&#32946;&#12290;&#25991;&#31456;&#38416;&#36848;&#20102;&#23427;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#20248;&#21183;&#21644;&#39118;&#38505;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#24635;&#20307;&#32467;&#35770;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#26377;&#19968;&#20123;&#28508;&#22312;&#30340;&#20302;&#39118;&#38505;&#24212;&#29992;&#21487;&#33021;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. Large Language Models (LLMs), e.g., BERT, Bard, Generative Pre-trained Transformers (GPTs), LLaMA, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. This paper assesses the impact and potential impact of ChatGPT on the field of digital forensics, specifically looking at its latest pre-trained LLM, GPT-4. A series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. Across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. Overall this paper concludes that while there are some potential low-risk applicati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUBA&#30340;&#21452;&#37325;&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#35302;&#21457;&#22120;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#38544;&#21311;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10184</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#65306;&#20174;&#31354;&#38388;&#21644;&#39057;&#29575;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. (arXiv:2307.10184v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUBA&#30340;&#21452;&#37325;&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#35302;&#21457;&#22120;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#38544;&#21311;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26500;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#21518;&#38376;&#27169;&#22411;&#22312;&#24102;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#19978;&#20250;&#20219;&#24847;&#65288;&#26377;&#38024;&#23545;&#24615;&#22320;&#65289;&#20986;&#29616;&#38169;&#35823;&#39044;&#27979;&#65292;&#32780;&#22312;&#24178;&#20928;&#30340;&#36755;&#20837;&#19978;&#34920;&#29616;&#27491;&#24120;&#12290;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#38544;&#21311;&#24615;&#20197;&#25552;&#39640;&#25915;&#20987;&#30340;&#38544;&#31192;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#21482;&#32771;&#34385;&#20102;&#31354;&#38388;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#22312;&#39057;&#29575;&#22495;&#20013;&#29983;&#25104;&#38544;&#21311;&#35302;&#21457;&#22120;&#65292;&#20351;&#29983;&#25104;&#30340;&#27602;&#23475;&#22270;&#20687;&#23481;&#26131;&#34987;&#26368;&#36817;&#30340;&#38450;&#24481;&#26041;&#27861;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUBA&#30340;DUal&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#35302;&#21457;&#22120;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#20445;&#24378;&#22823;&#30340;&#38544;&#21311;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#23558;&#35302;&#21457;&#22120;&#22270;&#20687;&#30340;&#39640;&#39057;&#20449;&#24687;&#23884;&#20837;&#24178;&#20928;&#22270;&#20687;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs embedded with well-designed triggers while behaving normally on clean inputs. Many works have explored the invisibility of backdoor triggers to improve attack stealthiness. However, most of them only consider the invisibility in the spatial domain without explicitly accounting for the generation of invisible triggers in the frequency domain, making the generated poisoned images be easily detected by recent defense methods. To address this issue, in this paper, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Discrete Wavelet Transform to embed the high-frequency information of the trigger image into the clean image 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#19982;&#23454;&#38469;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#25311;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10182</link><description>&lt;p&gt;
&#36890;&#36807;&#30495;&#23454;&#21402;&#20999;&#29255;CT&#27169;&#25311;&#25913;&#36827;&#36229;&#20998;&#36776;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Enhancing Super-Resolution Networks through Realistic Thick-Slice CT Simulation. (arXiv:2307.10182v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#19982;&#23454;&#38469;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#25311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;AAPM-Mayo's 2016&#20302;&#21058;&#37327;CT&#22823;&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#22270;&#20687;&#23494;&#20999;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#20551;&#35774;&#25105;&#20204;&#30340;&#27169;&#25311;&#23558;&#20135;&#29983;&#19982;&#30495;&#23454;&#22270;&#20687;&#26356;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;PSNR&#21644;RMSE&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#26368;&#39640;PSNR&#20540;&#20026;D45&#21644;B30&#37325;&#24314;&#26680;&#20998;&#21035;&#20026;49.7369&#177;2.5223&#21644;48.5801&#177;7.3271&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#20197;0.0068&#177;0.0020&#21644;0.0108&#177;0.0099&#30340;RMSE&#20540;&#27880;&#20876;&#26368;&#20302;&#30340;&#35823;&#24046;&#65292;&#34920;&#26126;&#20854;&#20998;&#24067;&#26356;&#25509;&#36817;&#20110;&#30495;&#23454;&#30340;&#21402;&#20999;&#29255;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to develop and evaluate an innovative simulation algorithm for generating thick-slice CT images that closely resemble actual images in the AAPM-Mayo's 2016 Low Dose CT Grand Challenge dataset. The proposed method was evaluated using Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error (RMSE) metrics, with the hypothesis that our simulation would produce images more congruent with their real counterparts. Our proposed method demonstrated substantial enhancements in terms of both PSNR and RMSE over other simulation methods. The highest PSNR values were obtained with the proposed method, yielding 49.7369 $\pm$ 2.5223 and 48.5801 $\pm$ 7.3271 for D45 and B30 reconstruction kernels, respectively. The proposed method also registered the lowest RMSE with values of 0.0068 $\pm$ 0.0020 and 0.0108 $\pm$ 0.0099 for D45 and B30, respectively, indicating a distribution more closely aligned with the authentic thick-slice image. Further validation of the proposed simulation al
&lt;/p&gt;</description></item><item><title>DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.10172</link><description>&lt;p&gt;
DialogStudio&#65306;&#38754;&#21521;&#20250;&#35805; AI &#30340;&#26368;&#20016;&#23500;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#32479;&#19968;&#25968;&#25454;&#38598;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10172
&lt;/p&gt;
&lt;p&gt;
DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20250;&#35805; AI &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; DialogStudio&#65306;&#26368;&#22823;&#12289;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20197;&#19968;&#33268;&#30340;&#26684;&#24335;&#32479;&#19968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#38750;&#24120;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378; DialogStudio &#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30830;&#23450;&#20102;&#35768;&#21487;&#35777;&#65292;&#24182;&#20026;&#36873;&#23450;&#23545;&#35805;&#35774;&#35745;&#20102;&#39046;&#22495;&#24863;&#30693;&#25552;&#31034;&#65292;&#20197;&#20415;&#20419;&#36827;&#25351;&#23548;&#24863;&#30693;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#38598;&#21512;&#24320;&#21457;&#20102;&#20250;&#35805; AI &#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#25688;&#35201;&#29983;&#25104;&#21644;&#20998;&#24067;&#24335;&#25991;&#23383;&#22522;&#20934;&#23545;&#35805;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.09906</link><description>&lt;p&gt;
&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#20174;&#30446;&#26631;&#39537;&#21160;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#21160;&#24577;&#23039;&#21183;&#21644;&#34920;&#24773;&#26469;&#32473;&#38745;&#24577;&#22270;&#20687;&#20013;&#30340;&#20154;&#33080;&#28155;&#21152;&#21160;&#30011;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#20013;&#30340;&#20010;&#20154;&#36523;&#20221;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#35270;&#39057;&#20013;&#25103;&#21095;&#24615;&#21644;&#22797;&#26434;&#30340;&#36816;&#21160;&#20250;&#23548;&#33268;&#29983;&#25104;&#27169;&#31946;&#19981;&#28165;&#65292;&#22240;&#20026;&#38745;&#24577;&#28304;&#22270;&#20687;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#22806;&#35266;&#20449;&#24687;&#26469;&#22788;&#29702;&#34987;&#36974;&#25377;&#21306;&#22495;&#25110;&#24494;&#22937;&#30340;&#34920;&#24773;&#21464;&#21270;&#65292;&#36825;&#20250;&#20135;&#29983;&#20005;&#37325;&#30340;&#20266;&#24433;&#24182;&#20005;&#37325;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20840;&#23616;&#20154;&#33080;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#31216;&#20026;MCNet&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image. However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation.~Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.08930</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31232;&#30095;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24212;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21305;&#37197;&#12290;&#19982;&#26631;&#20934;&#30340;&#8220;&#30417;&#30563;&#8221;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#23545;&#20043;&#38388;&#30340;&#30495;&#23454;&#23545;&#24212;&#12290;&#30456;&#21453;&#65292;&#23427;&#36890;&#36807;&#24378;&#21046;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#12290;&#30001;&#20110;&#21305;&#37197;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#26159;&#31163;&#25955;&#30340;&#65292;&#23427;&#20204;&#30340;&#23548;&#25968;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#40657;&#30418;&#24494;&#20998;&#30340;&#26368;&#26032;&#32467;&#26524;&#22522;&#30784;&#19978;&#26500;&#24314;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#21644;&#32452;&#21512;&#27714;&#35299;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.07666</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#26088;&#22312;&#22312;&#19981;&#30830;&#23450;&#24615;&#38754;&#21069;&#25214;&#21040;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#65292;&#20854;&#20013;&#20195;&#29702;&#26426;&#22120;&#19981;&#24635;&#26159;&#25353;&#29031;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#36827;&#34892;&#65292;&#32780;&#26159;&#20197;&#27010;&#29575;$1-\rho$&#25191;&#34892;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#65292;&#20197;&#27010;&#29575;$\rho$&#25191;&#34892;&#26367;&#20195;&#30340;&#23545;&#25239;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23384;&#22312;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#20854;&#30340;&#34892;&#21160;&#40065;&#26834;&#36125;&#23572;&#26364;&#26368;&#20248;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#35777;&#20070;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;(ARRLC)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;ARRLC&#20248;&#20110;&#38750;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#40065;&#26834;TD&#31639;&#27861;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#20851;&#27880;&#12289;&#25968;&#25454;&#20559;&#24046;&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05921</link><description>&lt;p&gt;
&#38405;&#35835;&#25918;&#23556;&#23398;&#25104;&#20687;&#30340;&#26041;&#24335;&#65292;&#23601;&#20687;&#25918;&#23556;&#31185;&#21307;&#29983;&#19968;&#26679;
&lt;/p&gt;
&lt;p&gt;
Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#20851;&#27880;&#12289;&#25968;&#25454;&#20559;&#24046;&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#21253;&#21547;&#25918;&#23556;&#23398;&#25104;&#20687;&#30340;&#20016;&#23500;&#12289;&#31934;&#32454;&#25551;&#36848;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#19982;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#30340;&#22270;&#20687;&#25551;&#36848;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#65292;&#20165;&#22312;&#30142;&#30149;&#21457;&#29983;&#30340;&#32454;&#24494;&#24046;&#24322;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#37492;&#20110;&#36825;&#20123;&#32454;&#24494;&#24046;&#24322;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#40723;&#21169;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#30142;&#30149;&#21457;&#29983;&#30340;&#24494;&#22937;&#21306;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#20559;&#24046;&#30340;&#38382;&#39064;&#24456;&#20005;&#37325;&#12290;&#19981;&#20165;&#27491;&#24120;&#30149;&#20363;&#21344;&#25968;&#25454;&#38598;&#30340;&#22823;&#37096;&#20998;&#65292;&#36824;&#25551;&#32472;&#26377;&#30149;&#21464;&#21306;&#22495;&#30340;&#21477;&#23376;&#21482;&#21344;&#27573;&#33853;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#28041;&#21450;&#21040;&#38271;&#25991;&#26412;&#30340;&#29983;&#25104;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#21307;&#23398;&#30693;&#35782;&#30340;&#19987;&#19994;&#24615;&#21644;&#32463;&#39564;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#27492;&#31867;&#25253;&#21578;&#30340;&#38590;&#24230;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging. Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases. Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence. Secondly, the problem of visual and textual data biases is serious. Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph. Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge. As a result, the difficulty of generating such reports is increased. To address these challenges, we propose a disease-ori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22238;&#38899;&#23460;&#25928;&#24212;&#12290;&#36890;&#36807;&#35745;&#31639;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#32467;&#21512;Echo Chamber Score(ECS)&#25351;&#26631;&#26469;&#35780;&#20272;&#29992;&#25143;&#31038;&#21306;&#30340;&#20957;&#32858;&#21147;&#21644;&#20998;&#31163;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#29992;&#25143;&#24847;&#35782;&#24418;&#24577;&#30340;&#26631;&#31614;&#21644;&#20132;&#20114;&#22270;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.04668</link><description>&lt;p&gt;
&#37327;&#21270;&#22238;&#38899;&#23460;&#25928;&#24212;&#65306;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach. (arXiv:2307.04668v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22238;&#38899;&#23460;&#25928;&#24212;&#12290;&#36890;&#36807;&#35745;&#31639;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#32467;&#21512;Echo Chamber Score(ECS)&#25351;&#26631;&#26469;&#35780;&#20272;&#29992;&#25143;&#31038;&#21306;&#30340;&#20957;&#32858;&#21147;&#21644;&#20998;&#31163;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#29992;&#25143;&#24847;&#35782;&#24418;&#24577;&#30340;&#26631;&#31614;&#21644;&#20132;&#20114;&#22270;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#20852;&#36215;&#20419;&#36827;&#20102;&#22238;&#38899;&#23460;&#30340;&#24418;&#25104;&#65292;&#22238;&#38899;&#23460;&#26159;&#22312;&#32447;&#31354;&#38388;&#65292;&#29992;&#25143;&#20027;&#35201;&#36935;&#21040;&#24378;&#21270;&#20182;&#20204;&#29616;&#26377;&#20449;&#24565;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#25490;&#38500;&#19981;&#21516;&#24847;&#35265;&#12290;&#36825;&#31181;&#29616;&#35937;&#26174;&#33879;&#38459;&#30861;&#20102;&#20449;&#24687;&#22312;&#31038;&#21306;&#20043;&#38388;&#30340;&#20256;&#25773;&#65292;&#21152;&#21095;&#20102;&#31038;&#20250;&#26497;&#21270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#37327;&#21270;&#22238;&#38899;&#23460;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22238;&#38899;&#23460;&#24471;&#20998;&#65288;ECS&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#29992;&#25143;&#31038;&#21306;&#20957;&#32858;&#21147;&#21644;&#20998;&#31163;&#24230;&#30340;&#25351;&#26631;&#65292;&#36890;&#36807;&#27979;&#37327;&#23884;&#20837;&#31354;&#38388;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;ECS&#33021;&#22815;&#22312;&#19981;&#20855;&#22791;&#29992;&#25143;&#24847;&#35782;&#24418;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20316;&#29992;&#65292;&#24182;&#19988;&#19981;&#23545;&#20132;&#20114;&#22270;&#30340;&#32467;&#26500;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#12290;&#20026;&#20102;&#20415;&#20110;&#27979;&#37327;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EchoGAE&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#24086;&#23376;&#21644;&#20132;&#20114;&#22270;&#23558;&#29992;&#25143;&#23884;&#20837;&#21040;&#19968;&#31181;&#26041;&#24335;&#20013;
&lt;/p&gt;
&lt;p&gt;
The rise of social media platforms has facilitated the formation of echo chambers, which are online spaces where users predominantly encounter viewpoints that reinforce their existing beliefs while excluding dissenting perspectives. This phenomenon significantly hinders information dissemination across communities and fuels societal polarization. Therefore, it is crucial to develop methods for quantifying echo chambers. In this paper, we present the Echo Chamber Score (ECS), a novel metric that assesses the cohesion and separation of user communities by measuring distances between users in the embedding space. In contrast to existing approaches, ECS is able to function without labels for user ideologies and makes no assumptions about the structure of the interaction graph. To facilitate measuring distances between users, we propose EchoGAE, a self-supervised graph autoencoder-based user embedding model that leverages users' posts and the interaction graph to embed them in a manner that
&lt;/p&gt;</description></item><item><title>&#31532;&#21313;&#20061;&#23626;&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#28041;&#21450;&#29702;&#24615;&#21644;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04005</link><description>&lt;p&gt;
&#31532;&#21313;&#20061;&#23626;&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings Ninetheenth conference on Theoretical Aspects of Rationality and Knowledge. (arXiv:2307.04005v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04005
&lt;/p&gt;
&lt;p&gt;
&#31532;&#21313;&#20061;&#23626;&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#28041;&#21450;&#29702;&#24615;&#21644;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TARK&#20250;&#35758;(&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;)&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#21338;&#24328;&#35770;&#12289;&#20915;&#31574;&#35770;&#12289;&#21746;&#23398;&#12289;&#36923;&#36753;&#12289;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#29702;&#35299;&#28041;&#21450;&#29702;&#24615;&#21644;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#38382;&#39064;&#12290;&#33258;1986&#24180;&#20197;&#26469;&#65292;&#35813;&#20250;&#35758;&#20197;&#20004;&#24180;&#20026;&#26399;&#38388;&#65292;&#22312;&#19990;&#30028;&#21508;&#22320;&#20030;&#34892;&#65292;&#20854;&#21019;&#22987;&#20154;&#26159;&#32422;&#29791;&#22827;&#183;&#21704;&#23572;&#26222;&#24681; (&#24247;&#22856;&#23572;&#22823;&#23398;)&#12290;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#30693;&#35782;&#12289;&#20449;&#24565;&#12289;&#24847;&#35782;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#27169;&#22411;&#65292;&#26377;&#38480;&#29702;&#24615;&#21644;&#36164;&#28304;&#21463;&#38480;&#25512;&#29702;&#65292;&#24120;&#35782;&#30693;&#35782;&#25512;&#29702;&#65292;&#35748;&#30693;&#36923;&#36753;&#65292;&#35748;&#30693;&#21338;&#24328;&#35770;&#65292;&#30693;&#35782;&#19982;&#34892;&#21160;&#65292;&#22312;&#30693;&#35782;&#21644;&#20854;&#20182;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#29702;&#24212;&#29992;&#65292;&#20449;&#24565;&#20462;&#27491;&#65292;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#65292;&#31639;&#27861;&#21338;&#24328;&#35770;&#20197;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;Informa
&lt;/p&gt;
&lt;p&gt;
The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, awareness and uncertainty, bounded rationality and resource-bounded reasoning, commonsense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems. Informa
&lt;/p&gt;</description></item><item><title>PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00470</link><description>&lt;p&gt;
PatternGPT: &#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00470
&lt;/p&gt;
&lt;p&gt;
PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PatternGPT&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#20016;&#23500;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#20511;&#37492;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#23454;&#29616;&#20849;&#20139;&#20197;&#33719;&#21462;&#26356;&#22810;&#26679;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#21028;&#26029;&#26631;&#20934;&#21644;&#20248;&#21270;&#31639;&#27861;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#25628;&#32034;&#21040;&#30340;&#27169;&#24335;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17582</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#35774;&#35745;&#21407;&#21017;&#21644;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#25552;&#31034;&#24037;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#19982;&#39640;&#32423;&#20989;&#25968;&#24211;&#30340;&#21019;&#24314;&#30456;&#32467;&#21512;&#65292;&#20351;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12289;&#27169;&#25311;&#22120;&#21644;&#24418;&#24577;&#12290;&#25105;&#20204;&#37325;&#28857;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#21644;&#23545;&#35805;&#31574;&#30053;&#23545;&#25191;&#34892;&#21508;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#23545;&#35805;&#12289;&#35299;&#26512;XML&#26631;&#35760;&#21644;&#21512;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#20989;&#25968;&#21644;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#38381;&#29615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#22522;&#26412;&#30340;&#36923;&#36753;&#12289;&#20960;&#20309;&#21644;&#25968;&#23398;&#25512;&#29702;&#21040;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#31354;&#20013;&#23548;&#33322;&#12289;&#25805;&#32437;&#21644;&#20855;&#36523;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.13960</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#27491;&#21017;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(G-CNN)&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#25552;&#39640;&#23545;&#19981;&#21516;&#20960;&#20309;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;SE(3)&#38382;&#39064;&#65292;&#21363;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#22312;&#20307;&#31215;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;&#20307;&#31215;&#22270;&#20687;&#25968;&#25454;&#22312;&#35768;&#22810;&#21307;&#30103;&#35774;&#32622;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#21463;&#21487;&#20998;&#31163;&#32452;&#21367;&#31215;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;SE(3)&#32676;&#21367;&#31215;&#26680;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#36830;&#32493;&#30340;SO(3)&#65288;&#26059;&#36716;&#65289;&#26680;&#21644;&#31354;&#38388;&#26680;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22343;&#21248;&#30340;SO(3)&#32593;&#26684;&#26469;&#36817;&#20284;&#36830;&#32493;&#35774;&#23450;&#19979;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#36830;&#32493;SO(3)&#26680;&#26159;&#36890;&#36807;&#31867;&#20284;&#22343;&#21248;&#32593;&#26684;&#30340;RBF&#25554;&#20540;&#21442;&#25968;&#21270;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;SE(3)&#31561;&#21464;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;CNN&#21644;&#24120;&#35268;&#31163;&#25955;G-CNN&#65292;&#24182;&#26174;&#31034;&#20986;&#26174;&#30528;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22122;&#22768;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#36798;&#21040;16.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#26368;&#36817;&#30340;LLM&#20855;&#26377;&#26032;&#20852;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#21151;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#25512;&#29702;&#21644;&#20462;&#35746;&#26469;&#35299;&#20915;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2306.13805</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Potential Benefits of Employing Large Language Models in Research in Moral Education and Development. (arXiv:2306.13805v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#26368;&#36817;&#30340;LLM&#20855;&#26377;&#26032;&#20852;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#21151;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#25512;&#29702;&#21644;&#20462;&#35746;&#26469;&#35299;&#20915;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#21644;&#20154;&#24037;&#24378;&#21270;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290; LLM&#24050;&#25104;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#31934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;LLM&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#20154;&#31867;&#35748;&#30693;&#30340;&#26032;&#20852;&#21151;&#33021;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;&#20197;&#21069;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#23558;&#25506;&#35752;LLM&#22914;&#20309;&#21487;&#33021;&#20026;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#23558;&#22238;&#39038;&#26368;&#36817;&#21457;&#34920;&#30340;&#20250;&#35758;&#35770;&#25991;&#21644;ArXiv&#39044;&#21360;&#26412;&#65292;&#27010;&#36848;LLM&#20013;&#23454;&#29616;&#30340;&#26032;&#39062;&#21151;&#33021;&#29305;&#24615;&#12290;&#25105;&#36824;&#25171;&#31639;&#20351;&#29992;ChatGPT&#36827;&#34892;&#31616;&#30701;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;LLM&#22788;&#29702;&#36947;&#24503;&#22256;&#22659;&#21644;&#22806;&#37096;&#21453;&#39304;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#33021;&#22815;&#22522;&#20110;&#25512;&#29702;&#21644;&#20462;&#35746;&#26469;&#35299;&#20915;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.12619</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#23558;CIL&#23450;&#24335;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#26631;&#31614;&#29983;&#25104;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;CF&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CIL&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#35789;&#27719;&#34920;&#30340;&#31232;&#30095;&#24615;&#20197;&#20415;&#20110;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#26631;&#31614;&#35821;&#20041;&#21019;&#24314;&#20266;&#37325;&#25773;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAG&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#22823;&#24133;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15299</link><description>&lt;p&gt;
&#22312;ChatGPT&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#31185;&#23398;&#65306;&#30740;&#31350;&#20262;&#29702;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20294;&#26377;&#20105;&#35758;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#26102;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#26088;&#22312;&#20026;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#22880;&#23450;&#26032;&#30340;&#21450;&#26102;&#22522;&#30784;&#12290;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#21644;&#30740;&#31350;&#23545;&#35937;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#35814;&#32454;&#23457;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31185;&#23398;&#23478;&#12289;&#21442;&#19982;&#32773;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#30340;&#26032;&#20852;&#23454;&#36341;&#65292;&#24182;&#32473;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20026;&#22312;AI&#26102;&#20195;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13426</link><description>&lt;p&gt;
&#23545;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24517;&#39035;&#38754;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20197;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#26681;&#25454;&#22312;&#25972;&#20010;&#30740;&#31350;&#26102;&#38388;&#27573;&#38543;&#26426;&#25277;&#21462;&#30340;&#24739;&#32773;&#26469;&#25286;&#20998;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#21463;&#21040;&#21453;&#21521;&#27979;&#35797;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;EMDOT&#27169;&#25311;&#23454;&#36341;&#32773;&#21487;&#33021;&#33021;&#22815;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#25191;&#34892;&#30340;&#28508;&#22312;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#22312;&#25152;&#26377;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#21307;&#30103;&#25968;&#25454;&#28304;&#65288;&#34920;&#26684;&#21644;&#25104;&#20687;&#65289;&#19978;&#35780;&#20272;&#32447;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25152;&#26377;&#21382;&#21490;&#25968;&#25454;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#29702;&#24819;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20351;&#29992;&#26368;&#36817;&#25968;&#25454;&#30340;&#31383;&#21475;&#21487;&#33021;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#27169;&#22411;&#31361;&#28982;&#21463;&#21040;&#24433;&#21709;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#22312;&#30456;&#23545;&#36739;&#36817;&#30340;&#25968;&#25454;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models deployed in healthcare systems must face data drawn from continually evolving environments. However, researchers proposing such models typically evaluate them in a time-agnostic manner, splitting datasets according to patients sampled randomly throughout the entire study time period. This work proposes the Evaluation on Medical Datasets Over Time (EMDOT) framework, which evaluates the performance of a model class across time. Inspired by the concept of backtesting, EMDOT simulates possible training procedures that practitioners might have been able to execute at each point in time and evaluates the resulting models on all future time points. Evaluating both linear and more complex models on six distinct medical data sources (tabular and imaging), we show how depending on the dataset, using all historical data may be ideal in many cases, whereas using a window of the most recent data could be advantageous in others. In datasets where models suffer from sudde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#21453;&#28436;&#22312;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#20013;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;&#20854;&#29992;&#20316;&#32479;&#35745;&#25512;&#26029;&#30340;&#31867;&#22411;&#39537;&#21160;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06112</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Compositional Structure of Bayesian Inference. (arXiv:2305.06112v1 [math.CT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#21453;&#28436;&#22312;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#20013;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;&#20854;&#29992;&#20316;&#32479;&#35745;&#25512;&#26029;&#30340;&#31867;&#22411;&#39537;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#35268;&#21017;&#21578;&#35785;&#25105;&#20204;&#22914;&#20309;&#21453;&#36716;&#22240;&#26524;&#36807;&#31243;&#20197;&#26681;&#25454;&#26032;&#35777;&#25454;&#26356;&#26032;&#25105;&#20204;&#30340;&#20449;&#24565;&#12290;&#22914;&#26524;&#35748;&#20026;&#35813;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#35266;&#23519;&#21040;&#25972;&#20010;&#36807;&#31243;&#30340;&#21453;&#36716;&#21487;&#20197;&#25353;&#37096;&#20214;&#36807;&#31243;&#35745;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#32452;&#25104;&#35268;&#21017;&#30340;&#32467;&#26500;&#65292;&#27880;&#24847;&#21040;&#23427;&#19982;&#20989;&#25968;&#24335;&#32534;&#31243;&#20013;&#30340;&#20984;&#36879;&#38236;&#27169;&#24335;&#30456;&#20851;&#12290;&#22312;&#36866;&#24403;&#30340;Markov&#26680;&#33539;&#30068;&#30340;&#20844;&#29702;&#21270;&#34920;&#36848;&#20013;&#24037;&#20316;&#65292;&#25105;&#20204;&#30475;&#21040;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#21453;&#28436;&#30475;&#20316;&#26159;&#32420;&#32500;&#33539;&#30068;&#20013;&#29366;&#24577;&#20381;&#36182;&#24577;&#23556;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#32452;&#21512;&#24615;&#36136;&#65292;&#20197;&#24213;&#23618;&#31867;&#21035;&#19978;&#30340;&#20989;&#23376;&#34920;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#26356;&#21152;&#31867;&#22411;&#39537;&#21160;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayes' rule tells us how to invert a causal process in order to update our beliefs in light of new evidence. If the process is believed to have a complex compositional structure, we may observe that the inversion of the whole can be computed piecewise in terms of the component processes. We study the structure of this compositional rule, noting that it relates to the lens pattern in functional programming. Working in a suitably general axiomatic presentation of a category of Markov kernels, we see how we can think of Bayesian inversion as a particular instance of a state-dependent morphism in a fibred category. We discuss the compositional nature of this, formulated as a functor on the underlying category and explore how this can used for a more type-driven approach to statistical inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#35760;&#25968;&#22120;&#31034;&#20363;&#24341;&#23548;&#30340;&#31934;&#28860;&#25277;&#35937;(CEGAR)&#65292;&#29992;&#20110;&#35299;&#20915;&#24067;&#23572;&#32593;&#32476;&#30340;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;(MTSs)&#30340;&#36890;&#29992;&#23646;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#25152;&#26377;MTSs&#19978;&#25191;&#34892;&#32473;&#23450;&#23646;&#24615;&#30340;&#24067;&#23572;&#21464;&#37327;&#30340;&#27704;&#20037;&#20923;&#32467;&#30340;&#37325;&#32534;&#31243;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02442</link><description>&lt;p&gt;
&#22788;&#29702;&#24067;&#23572;&#32593;&#32476;&#30340;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;&#30340;&#36890;&#29992;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tackling Universal Properties of Minimal Trap Spaces of Boolean Networks. (arXiv:2305.02442v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#35760;&#25968;&#22120;&#31034;&#20363;&#24341;&#23548;&#30340;&#31934;&#28860;&#25277;&#35937;(CEGAR)&#65292;&#29992;&#20110;&#35299;&#20915;&#24067;&#23572;&#32593;&#32476;&#30340;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;(MTSs)&#30340;&#36890;&#29992;&#23646;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#25152;&#26377;MTSs&#19978;&#25191;&#34892;&#32473;&#23450;&#23646;&#24615;&#30340;&#24067;&#23572;&#21464;&#37327;&#30340;&#27704;&#20037;&#20923;&#32467;&#30340;&#37325;&#32534;&#31243;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;(MTSs)&#25429;&#25417;&#24067;&#23572;&#21160;&#24577;&#34987;&#22256;&#30340;&#23376;&#31354;&#38388;&#65292;&#26080;&#35770;&#26356;&#26032;&#27169;&#24335;&#22914;&#20309;&#65292;&#23427;&#20204;&#37117;&#23545;&#24212;&#20110;&#26368;&#35753;&#20154;&#28385;&#24847;&#30340;&#27169;&#24335;&#30340;&#21560;&#24341;&#23376;&#12290;&#30001;&#20110;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#36817;&#24180;&#26469;&#35745;&#31639;MTSs&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#36890;&#36807;&#37325;&#28857;&#20851;&#27880;&#20854;&#26522;&#20030;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;MTS&#30340;&#36890;&#29992;&#23646;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#29992;&#20110;&#35782;&#21035;&#22312;&#25152;&#26377;MTSs&#19978;&#25191;&#34892;&#32473;&#23450;&#23646;&#24615;&#30340;&#24067;&#23572;&#21464;&#37327;&#30340;&#27704;&#20037;&#20923;&#32467;&#30340;&#37325;&#32534;&#31243;&#38382;&#39064;&#65292;&#24182;&#20174;&#20854;MTSs&#30340;&#36890;&#29992;&#23646;&#24615;&#21512;&#25104;&#24067;&#23572;&#32593;&#32476;&#12290;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#24402;&#32467;&#20026;&#35299;&#20915;&#20855;&#26377;3&#20010;&#37327;&#21270;&#22120;($\exists\forall\exists$)&#30340;&#21629;&#39064;&#36923;&#36753;&#20844;&#24335;&#30340;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35760;&#25968;&#22120;&#31034;&#20363;&#24341;&#23548;&#30340;&#31934;&#28860;&#25277;&#35937;(CEGAR)&#26469;&#36890;&#36807;&#32806;&#21512;&#35299;&#20915;&#20004;&#20010;&#26356;&#31616;&#21333;&#30340;&#20844;&#24335;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#20381;&#36182;&#20110;&#31572;&#26696;&#38598;&#32534;&#31243;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimal trap spaces (MTSs) capture subspaces in which the Boolean dynamics is trapped, whatever the update mode. They correspond to the attractors of the most permissive mode. Due to their versatility, the computation of MTSs has recently gained traction, essentially by focusing on their enumeration. In this paper, we address the logical reasoning on universal properties of MTSs in the scope of two problems: the reprogramming of Boolean networks for identifying the permanent freeze of Boolean variables that enforce a given property on all the MTSs, and the synthesis of Boolean networks from universal properties on their MTSs. Both problems reduce to solving the satisfiability of quantified propositional logic formula with 3 levels of quantifiers ($\exists\forall\exists$). In this paper, we introduce a Counter-Example Guided Refinement Abstraction (CEGAR) to efficiently solve these problems by coupling the resolution of two simpler formulas. We provide a prototype relying on Answer-Set 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10159</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#23545;&#20110;&#20419;&#36827;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#26356;&#39640;&#25968;&#25454;&#32500;&#24230;&#25110;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24635;&#20307;&#35757;&#32451;&#21442;&#25968;&#30340;&#38480;&#21046;&#20855;&#26377;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230; Q-Learning &#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#24182;&#22521;&#35757;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#26032;&#30340; Qiskit &#21644; PyTorch &#26694;&#26550;&#30340;&#26032;&#22411; PQC&#65292;&#20197;&#19982;&#23436;&#20840;&#32463;&#20856;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24102;&#25110;&#19981;&#24102;&#38598;&#25104; PQC&#12290;&#30740;&#31350;&#26368;&#21518;&#24635;&#32467;&#20102;&#20854;&#20851;&#20110;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#35299;&#20915;&#36855;&#23467;&#38382;&#39064;&#25110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09826</link><description>&lt;p&gt;
AI&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#37096;&#32626;&#24050;&#32463;&#20026;&#20010;&#20154;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#35768;&#22810;&#31215;&#26497;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#27979;&#30340;&#20559;&#35265;&#65292;AI&#31995;&#32479;&#20063;&#34987;&#35777;&#26126;&#23545;&#37096;&#20998;&#20154;&#21475;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;AI&#30340;&#20844;&#24179;&#24615;&#65292;&#20998;&#26512;&#20102;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#26102;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#38543;&#30528;&#26102;&#38388;&#30340;&#21152;&#28145;&#32780;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#12290;&#22914;&#26524;&#38382;&#39064;&#25345;&#32493;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#39118;&#38505;&#30340;&#20132;&#20114;&#26469;&#21152;&#24378;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25552;&#39640;AI&#20844;&#24179;&#24615;&#30340;&#24403;&#21069;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#30830;&#20445;&#25105;&#20204;&#22312;&#19981;&#25439;&#23475;&#31038;&#20250;&#37325;&#35201;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;AI&#30340;&#22909;&#22788;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07880</link><description>&lt;p&gt;
Sabi&#225;: &#33889;&#33796;&#29273;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07880
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#8221;&#19968;&#20992;&#20999;&#8220;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20840;&#29699;&#20351;&#29992;&#30340;&#35821;&#35328;&#25968;&#37327;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#23545;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#29992;3%&#25110;&#26356;&#23569;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#39044;&#31639;&#22312;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;GPT-J&#21644;LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Poeta&#65288;&#19968;&#22871;&#30001;14&#20010;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22871;&#20214;&#65289;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#36828;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Sabi&#225;-65B&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#24050;&#32463;&#35774;&#24819;&#20102;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#32463;&#36807;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as transl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2303.12112</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#20687;&#35270;&#39057;&#26631;&#39064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;CLIP&#27169;&#22411;&#22312;&#24456;&#22810;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#32467;&#26500;&#20013;&#29983;&#25104;&#30340;&#26631;&#39064;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#37197;&#26041;&#65292;&#21363;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#24230;&#23398;&#20064;&#20998;&#25968;&#65288;PAC-S&#65289;&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#23545;&#27604;&#24230;&#35270;&#35273;-&#35821;&#20041;&#31354;&#38388;&#30340;&#23398;&#20064;&#21644;&#31574;&#23637;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#28155;&#21152;&#12290;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25351;&#26631;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20248;&#20110;&#29616;&#26377;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CIDEr&#21644;SPICE&#65289;&#21644;&#26080;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CLIP-Score&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#22270;&#20687;&#26631;&#39064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#19981;&#21516;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#20803;&#20851;&#31995;&#26694;&#26550;&#26469;&#25351;&#23548;&#26426;&#26500;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#37319;&#32435;&#65292;&#35299;&#20915;&#31038;&#20250;&#25216;&#26415;&#35805;&#35821;&#20013;&#30340;&#20851;&#31995;&#38382;&#39064;&#65292;&#21253;&#25324;&#35821;&#20041;&#27169;&#31946;&#12289;&#27010;&#24565;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20851;&#31995;&#21644;&#19981;&#21516;&#30340;&#26631;&#20934;&#26415;&#35821;&#65292;&#24110;&#21161;&#35780;&#20272;&#26426;&#26500;AI&#31995;&#32479;&#65292;&#36991;&#20813;&#27010;&#24565;&#23396;&#31435;&#12290;</title><link>http://arxiv.org/abs/2303.10106</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#20803;&#20851;&#31995;&#26694;&#26550;&#25351;&#23548;&#26426;&#26500;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#37319;&#32435;
&lt;/p&gt;
&lt;p&gt;
A multidomain relational framework to guide institutional AI research and adoption. (arXiv:2303.10106v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#20803;&#20851;&#31995;&#26694;&#26550;&#26469;&#25351;&#23548;&#26426;&#26500;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#37319;&#32435;&#65292;&#35299;&#20915;&#31038;&#20250;&#25216;&#26415;&#35805;&#35821;&#20013;&#30340;&#20851;&#31995;&#38382;&#39064;&#65292;&#21253;&#25324;&#35821;&#20041;&#27169;&#31946;&#12289;&#27010;&#24565;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20851;&#31995;&#21644;&#19981;&#21516;&#30340;&#26631;&#20934;&#26415;&#35821;&#65292;&#24110;&#21161;&#35780;&#20272;&#26426;&#26500;AI&#31995;&#32479;&#65292;&#36991;&#20813;&#27010;&#24565;&#23396;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25351;&#23548;&#26426;&#26500;&#21644;&#20844;&#20849;&#31649;&#29702;&#20013;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#37319;&#32435;&#30340;&#26032;&#25351;&#26631;&#12289;&#25216;&#26415;&#26631;&#20934;&#21644;&#31649;&#29702;&#26426;&#21046;&#30340;&#21628;&#21505;&#29616;&#24050;&#21496;&#31354;&#35265;&#24815;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26088;&#22312;&#20102;&#35299;&#37319;&#32435;AI&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#21644;&#25919;&#31574;&#21162;&#21147;&#24448;&#24448;&#21482;&#20248;&#20808;&#32771;&#34385;&#23569;&#25968;&#24819;&#27861;&#65292;&#32780;&#26410;&#23436;&#20840;&#32771;&#34385;&#25152;&#26377;&#28508;&#22312;&#30456;&#20851;&#30340;&#19981;&#21516;&#35270;&#35282;&#21644;&#20027;&#39064;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#20214;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36951;&#28431;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#20110;&#25105;&#20204;&#25152;&#31216;&#30340;&#31038;&#20250;&#25216;&#26415;&#35805;&#35821;&#20013;&#30340;&#20851;&#31995;&#38382;&#39064;:&#22522;&#26412;&#30340;&#26412;&#20307;&#35770;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#21253;&#25324;&#35821;&#20041;&#27169;&#31946;&#12289;&#27010;&#24565;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20851;&#31995;&#21644;&#19981;&#21516;&#30340;&#26631;&#20934;&#26415;&#35821;&#12290;&#36825;&#23548;&#33268;&#22312;&#35780;&#20272;&#26426;&#26500;AI&#31995;&#32479;&#30340;&#19981;&#21516;&#25512;&#29702;&#27169;&#24335;&#20197;&#21450;&#30740;&#31350;&#23427;&#20204;&#30340;&#39046;&#22495;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#20154;&#31867;&#22240;&#32032;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#25919;&#31574;&#26041;&#38754;&#23384;&#22312;&#27010;&#24565;&#23396;&#31435;&#12290;&#22312;&#21457;&#23637;&#20102;&#36825;&#19968;&#25209;&#21028;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Calls for new metrics, technical standards and governance mechanisms to guide the adoption of Artificial Intelligence (AI) in institutions and public administration are now commonplace. Yet, most research and policy efforts aimed at understanding the implications of adopting AI tend to prioritize only a handful of ideas; they do not fully account for all the different perspectives and topics that are potentially relevant. In this position paper, we contend that this omission stems, in part, from what we call the relational problem in socio-technical discourse: fundamental ontological issues have not yet been settled-including semantic ambiguity, a lack of clear relations between concepts and differing standard terminologies. This contributes to the persistence of disparate modes of reasoning to assess institutional AI systems, and the prevalence of conceptual isolation in the fields that study them including ML, human factors, social science and policy. After developing this critique, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#23376;&#27169;&#20215;&#20540;&#30340;&#20195;&#29702;&#20154;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#29289;&#21697;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;leximin&#12289;max Nash welfare&#65288;MNW&#65289;&#21644;$p$-mean welfare&#26368;&#22823;&#21270;&#20998;&#37197;&#12290;&#22312;$ab$&#21487;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#27714;&#35299;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#34917;&#20805;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35813;&#25991;&#30740;&#31350;&#20102;leximin&#21644;MNW&#20998;&#37197;&#22312;&#26080;&#23241;&#22930;&#21644;&#26368;&#22823;&#26368;&#23567;&#20221;&#39069;&#20445;&#35777;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.03087</link><description>&lt;p&gt;
&#21033;&#29992;&#23376;&#27169;&#20215;&#20540;&#23558;&#29289;&#21697;&#20998;&#37197;&#32473;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Dividing Good and Better Items Among Agents with Submodular Valuations. (arXiv:2302.03087v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#23376;&#27169;&#20215;&#20540;&#30340;&#20195;&#29702;&#20154;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#29289;&#21697;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;leximin&#12289;max Nash welfare&#65288;MNW&#65289;&#21644;$p$-mean welfare&#26368;&#22823;&#21270;&#20998;&#37197;&#12290;&#22312;$ab$&#21487;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#27714;&#35299;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#34917;&#20805;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35813;&#25991;&#30740;&#31350;&#20102;leximin&#21644;MNW&#20998;&#37197;&#22312;&#26080;&#23241;&#22930;&#21644;&#26368;&#22823;&#26368;&#23567;&#20221;&#39069;&#20445;&#35777;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#23376;&#27169;&#20215;&#20540;&#30340;&#20195;&#29702;&#20154;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#19968;&#32452;&#19981;&#21487;&#20998;&#21106;&#30340;&#29289;&#21697;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#29289;&#21697;&#37117;&#25552;&#20379;$a$&#25110;$b$&#30340;&#36793;&#38469;&#25910;&#30410;&#65288;$a&lt;b$&#65289;&#65292;&#19988;&#29289;&#21697;&#30340;&#36793;&#38469;&#25910;&#30410;&#36882;&#20943;&#12290;&#36825;&#26159;&#20004;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#35780;&#20272;&#31867;&#21035;&#8212;&#8212;&#20108;&#20803;&#21152;&#24615;&#35780;&#20272;&#21644;&#20108;&#20803;&#23376;&#27169;&#35780;&#20272;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#31639;&#27861;&#26694;&#26550;&#65292;&#22522;&#20110;&#26368;&#36817;&#20171;&#32461;&#30340;&#8220;&#25196;&#22522;&#20132;&#25442;&#8221;&#26426;&#21046;&#65292;&#21487;&#20197;&#36866;&#24212;&#35745;&#31639;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#21253;&#25324;leximin&#12289;max Nash welfare&#65288;MNW&#65289;&#21644;&#24403;$a$&#38500;&#20197;$b$&#26102;$p$-mean welfare&#26368;&#22823;&#21270;&#20998;&#37197;&#12290;&#36825;&#20010;&#32467;&#26524;&#34917;&#20805;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#32467;&#26524;&#65292;&#21363;&#24403;$a$&#19981;&#33021;&#34987;$b$&#25972;&#38500;&#26102;&#65292;leximin&#21644;MNW&#20998;&#37197;&#30340;&#35745;&#31639;&#26080;&#27861;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#20004;&#20010;&#30693;&#21517;&#23646;&#24615;&#8212;&#8212;&#26080;&#23241;&#22930;&#21644;&#26368;&#22823;&#26368;&#23567;&#20221;&#39069;&#20445;&#35777;&#26041;&#38754;&#30340;leximin&#21644;MNW&#20998;&#37197;&#12290;&#22312;&#26080;&#23241;&#22930;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;leximin&#21644;MNW&#37117;&#19981;&#28385;&#36275;
&lt;/p&gt;
&lt;p&gt;
We study the problem of fairly allocating a set of indivisible goods among agents with bivalued submodular valuations -- each good provides a marginal gain of either $a$ or $b$ ($a &lt; b$) and goods have decreasing marginal gains. This is a natural generalization of two well-studied valuation classes -bivalued additive valuations and binary submodular valuations. We present a simple sequential algorithmic framework, based on the recently introduced Yankee Swap mechanism, that can be adapted to compute a variety of solution concepts, including leximin, max Nash welfare (MNW) and $p$-mean welfare maximizing allocations when $a$ divides $b$. This result is complemented by an existing result on the computational intractability of leximin and MNW allocations when $a$ does not divide $b$.  We further examine leximin and MNW allocations with respect to two well-known properties -- envy freeness and the maximin share guarantee. On envy freeness, we show that neither the leximin nor the MNW all
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#26041;&#27861;&#20197;&#21450;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#32500;&#24230;&#30340;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#24182;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20063;&#27979;&#35797;&#20102;&#23427;&#20204;&#20316;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#21161;&#25163;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13867</link><description>&lt;p&gt;
ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#26041;&#27861;&#20197;&#21450;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#32500;&#24230;&#30340;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#24182;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20063;&#27979;&#35797;&#20102;&#23427;&#20204;&#20316;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#21161;&#25163;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23545;ChatGPT&#65288;&#21457;&#24067;&#20110;2023&#24180;1&#26376;9&#26085;&#21644;1&#26376;30&#26085;&#65289;&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#27491;&#24335;&#25968;&#23398;&#19981;&#21516;&#65292;&#27491;&#24335;&#35777;&#26126;&#30340;&#22823;&#22411;&#25968;&#25454;&#24211;&#21487;&#20379;&#20351;&#29992;&#65288;&#20363;&#22914;&#65292;Lean&#25968;&#23398;&#24211;&#65289;&#65292;&#24403;&#21069;&#29992;&#20110;&#22522;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#25968;&#25454;&#38598;&#35201;&#20040;&#21482;&#28085;&#30422;&#22522;&#30784;&#25968;&#23398;&#65292;&#35201;&#20040;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#20844;&#24320;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;GHOSTS&#21644;miniGHOSTS&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#26159;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31934;&#24515;&#31574;&#21010;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#12289;&#25552;&#20379;&#23545;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#33021;&#21147;&#30340;&#25972;&#20307;&#27010;&#36848;&#65292;&#24182;&#21306;&#20998;&#25968;&#23398;&#25512;&#29702;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36824;&#27979;&#35797;&#20102;ChatGPT&#21644;GPT-4&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#30340;&#26377;&#29992;&#21161;&#25163;&#65292;&#27169;&#25311;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.13816</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13816
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#27169;&#22411;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#22312;&#20195;&#30721;&#23436;&#25104;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#31243;&#24207;&#21512;&#25104;&#31561;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#25991;&#26412;&#29983;&#25104;&#20013;&#20511;&#29992;&#30340;&#30417;&#30563;&#24494;&#35843;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#30340;&#29420;&#29305;&#24207;&#21015;&#32423;&#29305;&#24449;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#32534;&#35793;&#24615;&#20197;&#21450;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPOCoder&#65292;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;PL&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#30456;&#32467;&#21512;&#65292;PPO&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;PPOCoder&#23558;&#22806;&#37096;&#20195;&#30721;&#29305;&#23450;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
&lt;/p&gt;</description></item><item><title>ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.11596</link><description>&lt;p&gt;
ThoughtSource:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25968;&#25454;&#30340;&#20013;&#22830;&#26530;&#32445;&#12290;
&lt;/p&gt;
&lt;p&gt;
ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11596
&lt;/p&gt;
&lt;p&gt;
ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#19978;&#20173;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#19981;&#36879;&#26126;&#65292;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#20107;&#23454;&#65292;&#24182;&#19988;&#23384;&#22312;&#20854;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35753;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#34920;&#36798;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ThoughtSource&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#12290;ThoughtSource&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26469;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;ThoughtSource&#30340;&#39318;&#27425;&#21457;&#24067;&#38598;&#25104;&#20102;&#20845;&#20010;&#31185;&#23398;/&#21307;&#23398;&#12289;&#19977;&#20010;&#36890;&#29992;&#39046;&#22495;&#21644;&#20116;&#20010;&#25968;&#23398;&#39064;&#31572;&#26696;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
&lt;/p&gt;</description></item><item><title>pyRDDLGym&#26159;&#19968;&#20010;Python&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;RDDL&#22768;&#26126;&#24615;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;OpenAI Gym&#29615;&#22659;&#12290;&#23427;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968;&#25551;&#36848;RDDL&#20013;&#21464;&#37327;&#30340;&#31163;&#25955;&#26102;&#38388;&#27493;&#36827;&#28436;&#21270;&#65292;&#24182;&#25903;&#25345;&#31616;&#21333;&#30340;&#29615;&#22659;&#20462;&#25913;&#21644;&#25193;&#23637;&#12290;&#23427;&#20855;&#26377;&#29420;&#29305;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#24555;&#36895;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#20419;&#36827;&#21033;&#29992;&#27169;&#22411;&#30693;&#35782;&#36827;&#34892;&#20132;&#20114;&#24615;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.05939</link><description>&lt;p&gt;
pyRDDLGym&#65306;&#20174;RDDL&#21040;Gym&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
pyRDDLGym: From RDDL to Gym Environments. (arXiv:2211.05939v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05939
&lt;/p&gt;
&lt;p&gt;
pyRDDLGym&#26159;&#19968;&#20010;Python&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;RDDL&#22768;&#26126;&#24615;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;OpenAI Gym&#29615;&#22659;&#12290;&#23427;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968;&#25551;&#36848;RDDL&#20013;&#21464;&#37327;&#30340;&#31163;&#25955;&#26102;&#38388;&#27493;&#36827;&#28436;&#21270;&#65292;&#24182;&#25903;&#25345;&#31616;&#21333;&#30340;&#29615;&#22659;&#20462;&#25913;&#21644;&#25193;&#23637;&#12290;&#23427;&#20855;&#26377;&#29420;&#29305;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#24555;&#36895;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#20419;&#36827;&#21033;&#29992;&#27169;&#22411;&#30693;&#35782;&#36827;&#34892;&#20132;&#20114;&#24615;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;pyRDDLGym&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;RDDL&#22768;&#26126;&#24615;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;OpenAI Gym&#29615;&#22659;&#30340;Python&#26694;&#26550;&#12290;RDDL&#20013;&#30340;&#21464;&#37327;&#30340;&#31163;&#25955;&#26102;&#38388;&#27493;&#36827;&#28436;&#21270;&#30001;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968;&#25551;&#36848;&#65292;&#36825;&#19982;Gym&#27493;&#39588;&#26041;&#26696;&#33258;&#28982;&#22865;&#21512;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;RDDL&#26159;&#19968;&#20010;&#25277;&#35937;&#25551;&#36848;&#65292;&#23558;&#29615;&#22659;&#36827;&#34892;&#20462;&#25913;&#21644;&#25193;&#23637;&#20197;&#25903;&#25345;&#22810;&#20010;&#23454;&#20307;&#21644;&#19981;&#21516;&#37197;&#32622;&#21464;&#24471;&#31616;&#21333;&#32780;&#19981;&#26159;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#24076;&#26395;pyRDDLGym&#33021;&#22815;&#30001;&#20110;RDDL&#30340;&#29420;&#29305;&#34920;&#36798;&#33021;&#21147;&#65292;&#25104;&#20026;&#24378;&#21270;&#23398;&#20064;&#31038;&#21306;&#20013;&#19968;&#32929;&#26032;&#30340;&#21147;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#26131;&#20110;&#24555;&#36895;&#24320;&#21457;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#23545;RDDL&#25551;&#36848;&#20013;&#30340;&#27169;&#22411;&#30340;&#26126;&#30830;&#35775;&#38382;&#65292;pyRDDLGym&#36824;&#21487;&#20197;&#20419;&#36827;&#21033;&#29992;&#27169;&#22411;&#30693;&#35782;&#36827;&#34892;&#20132;&#20114;&#24615;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;pyRDDLGym&#30340;&#35774;&#35745;&#21644;&#20869;&#32622;&#31034;&#20363;&#65292;&#20197;&#21450;&#34701;&#20837;RDDL&#35821;&#35328;&#30340;&#38468;&#21152;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present pyRDDLGym, a Python framework for auto-generation of OpenAI Gym environments from RDDL declerative description. The discrete time step evolution of variables in RDDL is described by conditional probability functions, which fits naturally into the Gym step scheme. Furthermore, since RDDL is a lifted description, the modification and scaling up of environments to support multiple entities and different configurations becomes trivial rather than a tedious process prone to errors. We hope that pyRDDLGym will serve as a new wind in the reinforcement learning community by enabling easy and rapid development of benchmarks due to the unique expressive power of RDDL. By providing explicit access to the model in the RDDL description, pyRDDLGym can also facilitate research on hybrid approaches for learning from interaction while leveraging model knowledge. We present the design and built-in examples of pyRDDLGym, and the additions made to the RDDL language that were incorporated into t
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#35774;&#32622;&#65292;&#20026;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;MDPs&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24615;&#36136;&#21644;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.04974</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04974
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#35774;&#32622;&#65292;&#20026;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;MDPs&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24615;&#36136;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20986;&#29616;&#20102;&#20004;&#20010;&#26680;&#24515;&#33539;&#24335;&#65306;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#24517;&#39035;&#19982;&#29615;&#22659;&#20132;&#20114;&#20197;&#25214;&#21040;&#19968;&#20010; &#949;-&#26368;&#20248;&#31574;&#30053;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#22120;&#21487;&#20197;&#20174;&#19968;&#20010;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#20294;&#26080;&#27861;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24517;&#39035;&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#33719;&#21462;&#26368;&#20339;&#31574;&#30053;&#12290;&#23454;&#38469;&#24773;&#20917;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#20013;&#38388;&#30340;&#35774;&#32622;&#65306;&#22914;&#26524;&#25105;&#20204;&#26377;&#19968;&#20123;&#31163;&#32447;&#25968;&#25454;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#25105;&#20204;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#20943;&#23569;&#23398;&#20064;&#19968;&#20010; &#949;-&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#22312;&#32447;&#20132;&#20114;&#27425;&#25968;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FineTuneRL&#35774;&#32622;&#65292;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;MDPs&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#32473;&#23450;&#19968;&#20123;&#31163;&#32447;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#38656;&#35201;&#30340;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#24615;&#36136;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\epsilon$-optimal policy?  In this work, we consider this setting, which we call the \textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and de
&lt;/p&gt;</description></item><item><title>&#20808;&#34892;&#39044;&#27979;&#27604;&#21518;&#32493;&#39044;&#27979;&#26356;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AP&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24110;&#21161;&#27169;&#22411;&#37325;&#35270;&#20808;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.09998</link><description>&lt;p&gt;
&#20808;&#34892;&#39044;&#27979;&#27604;&#20320;&#24819;&#35937;&#30340;&#26356;&#37325;&#35201;&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation. (arXiv:2208.09998v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09998
&lt;/p&gt;
&lt;p&gt;
&#20808;&#34892;&#39044;&#27979;&#27604;&#21518;&#32493;&#39044;&#27979;&#26356;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AP&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24110;&#21161;&#27169;&#22411;&#37325;&#35270;&#20808;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36716;&#21270;&#20026;&#20195;&#30721;&#29255;&#27573;&#12290;&#25552;&#20986;&#20102;&#24207;&#21015;&#21040;&#26641;&#65288;Seq2Tree&#65289;&#26041;&#27861;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#65292;&#20445;&#35777;&#29983;&#25104;&#30340;&#20195;&#30721;&#35821;&#27861;&#27491;&#30830;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;AST&#33410;&#28857;&#30340;&#20808;&#34892;&#39044;&#27979;&#26469;&#29983;&#25104;&#21518;&#32493;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#33410;&#28857;&#12290;&#29616;&#26377;&#30340;Seq2Tree&#26041;&#27861;&#20542;&#21521;&#20110;&#24179;&#31561;&#23545;&#24453;&#20808;&#34892;&#39044;&#27979;&#21644;&#21518;&#32493;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#32422;&#26463;&#19979;&#65292;Seq2Tree&#27169;&#22411;&#24456;&#38590;&#22522;&#20110;&#38169;&#35823;&#30340;&#20808;&#34892;&#39044;&#27979;&#20135;&#29983;&#27491;&#30830;&#30340;&#21518;&#32493;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#20808;&#34892;&#39044;&#27979;&#24212;&#35813;&#27604;&#21518;&#32493;&#39044;&#27979;&#33719;&#24471;&#26356;&#22810;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20808;&#34892;&#20248;&#20808;&#65288;AP&#65289;&#25439;&#22833;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24110;&#21161;&#27169;&#22411;&#37325;&#35270;&#20808;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25277;&#35937;&#35821;&#27861;&#26641;&#21040;&#21521;&#37327;&#65288;AST2Vec&#65289;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Code generation focuses on the automatic conversion of natural language (NL) utterances into code snippets. The sequence-to-tree (Seq2Tree) approaches are proposed for code generation, with the guarantee of the grammatical correctness of the generated code, which generate the subsequent Abstract Syntax Tree (AST) node relying on antecedent predictions of AST nodes. Existing Seq2Tree methods tend to treat both antecedent predictions and subsequent predictions equally. However, under the AST constraints, it is difficult for Seq2Tree models to produce the correct subsequent prediction based on incorrect antecedent predictions. Thus, antecedent predictions ought to receive more attention than subsequent predictions. To this end, in this paper, we propose an effective method, named Antecedent Prioritized (AP) Loss, that helps the model attach importance to antecedent predictions by exploiting the position information of the generated AST nodes. We design an AST-to-Vector (AST2Vec) method, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2208.09418</link><description>&lt;p&gt;
SAFARI&#65306;&#40065;&#26834;&#24615;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#22810;&#21151;&#33021;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#36947;&#38556;&#30861;&#12290;&#23613;&#31649;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31038;&#21306;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#37322;&#32570;&#20047;&#40065;&#26834;&#24615;&#8212;&#8212;&#26080;&#27861;&#21306;&#20998;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#32473;&#23450;&#30340;XAI&#26041;&#27861;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#20849;&#21516;&#24212;&#23545;&#30340;&#20960;&#20010;&#25361;&#25112;&#65306;i)&#29616;&#26377;&#25351;&#26631;&#19981;&#20840;&#38754;&#65307;ii)XAI&#25216;&#26415;&#39640;&#24230;&#24322;&#36136;&#65307;iii)&#35823;&#35299;&#36890;&#24120;&#26159;&#32597;&#35265;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#20998;&#21035;&#28041;&#21450;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#12290;&#20351;&#29992;&#20855;&#26377;&#23450;&#21046;&#36866;&#24212;&#24230;&#20989;&#25968;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#32597;&#35265;&#20107;&#20214;&#27010;&#29575;&#30340;&#23376;&#38598;&#27169;&#25311;&#65288;SS&#65289;&#26469;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#20219;&#21153;&#23545;&#20110;&#20154;&#20204;&#23547;&#27714;&#26410;&#26469;&#35745;&#21010;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2208.06501</link><description>&lt;p&gt;
ForecastTKGQuestions: &#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24615;&#38382;&#39064;&#22238;&#31572;&#21644;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs. (arXiv:2208.06501v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#20219;&#21153;&#23545;&#20110;&#20154;&#20204;&#23547;&#27714;&#26410;&#26469;&#35745;&#21010;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#26102;&#38388;&#24615;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;TKGQA&#65289;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#21152;&#12290;TKGQA&#38656;&#35201;&#26102;&#38388;&#25512;&#29702;&#25216;&#26415;&#26469;&#20174;&#26102;&#38388;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;TKGQA&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#22522;&#20110;&#22266;&#23450;&#26102;&#38388;&#27573;&#30340;&#26102;&#38388;&#24615;&#38382;&#39064;&#65292;&#35813;&#26102;&#38388;&#27573;&#20869;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#21487;&#20197;&#23436;&#20840;&#29992;&#20110;&#31572;&#26696;&#25512;&#29702;&#65292;&#20801;&#35768;TKGQA&#27169;&#22411;&#21033;&#29992;&#26410;&#26469;&#30693;&#35782;&#26469;&#22238;&#31572;&#22522;&#20110;&#36807;&#21435;&#20107;&#23454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#37492;&#20110;&#21040;&#30446;&#21069;&#20026;&#27490;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#20063;&#24076;&#26395;TKGQA&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#26410;&#26469;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#20154;&#20204;&#19981;&#26029;&#23547;&#27714;&#26410;&#26469;&#30340;&#35745;&#21010;&#65292;&#26500;&#24314;&#33021;&#22815;&#22238;&#31572;&#36825;&#31181;&#39044;&#27979;&#24615;&#38382;&#39064;&#30340;TKGQA&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over temporal knowledge graphs (TKGQA) has recently found increasing interest. TKGQA requires temporal reasoning techniques to extract the relevant information from temporal knowledge bases. The only existing TKGQA dataset, i.e., CronQuestions, consists of temporal questions based on the facts from a fixed time period, where a temporal knowledge graph (TKG) spanning the same period can be fully used for answer inference, allowing the TKGQA models to use even the future knowledge to answer the questions based on the past facts. In real-world scenarios, however, it is also common that given the knowledge until now, we wish the TKGQA systems to answer the questions asking about the future. As humans constantly seek plans for the future, building TKGQA systems for answering such forecasting questions is important. Nevertheless, this has still been unexplored in previous research. In this paper, we propose a novel task: forecasting question answering over temporal knowled
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#21704;&#24076;&#25216;&#26415;&#25552;&#39640;&#38646;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#23384;&#20648;&#23494;&#38598;&#32034;&#24341;&#30340;&#39640;&#20869;&#23384;&#20351;&#29992;&#38382;&#39064;&#65292;&#24182;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2205.11498</link><description>&lt;p&gt;
&#20026;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#38646;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#27880;&#20837;&#39046;&#22495;&#36866;&#24212;&#30340;&#23398;&#20064;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval. (arXiv:2205.11498v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11498
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21704;&#24076;&#25216;&#26415;&#25552;&#39640;&#38646;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#23384;&#20648;&#23494;&#38598;&#32034;&#24341;&#30340;&#39640;&#20869;&#23384;&#20351;&#29992;&#38382;&#39064;&#65292;&#24182;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#22312;&#26080;&#26597;&#35810;&#35789;&#26816;&#32034;&#20013;&#20811;&#26381;&#20102;&#35789;&#27719;&#38548;&#38402;&#65292;&#24182;&#22312;&#33258;&#21160;&#20449;&#24687;&#26816;&#32034;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#23494;&#38598;&#26816;&#32034;&#22120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26381;&#21153;&#25104;&#26412;&#36739;&#39640;&#12290;&#23545;&#20110;&#38656;&#35201;&#20174;&#25968;&#30334;&#19975;&#20221;&#25991;&#26723;&#20013;&#25628;&#32034;&#30340;&#29992;&#20363;&#65292;&#23494;&#38598;&#32034;&#24341;&#21464;&#24471;&#24222;&#22823;&#65292;&#24182;&#19988;&#22312;&#23384;&#20648;&#32034;&#24341;&#26102;&#38656;&#35201;&#39640;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#26368;&#36817;&#30340;&#23398;&#20064;&#21704;&#24076;&#65288;LTH&#65289;&#25216;&#26415;&#65292;&#22914;BPR&#21644;JPQ&#65292;&#29983;&#25104;&#20108;&#36827;&#21046;&#25991;&#26723;&#21521;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23384;&#20648;&#23494;&#38598;&#32034;&#24341;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;LTH&#25216;&#26415;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#24182;&#20351;&#29992;&#25490;&#21517;&#25439;&#22833;&#23545;&#26816;&#32034;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#20204;&#20248;&#20110;&#20256;&#32479;&#30340;&#21521;&#37327;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;PCA&#25110;PQ&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#32570;&#23569;&#30340;&#19968;&#20010;&#29615;&#33410;&#26159;&#29616;&#26377;&#25216;&#26415;&#20165;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;&#20165;&#22312;&#21333;&#19968;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LTH&#21644;&#21521;&#37327;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;TAS-B d&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval overcome the lexical gap and has shown great success in ad-hoc information retrieval (IR). Despite their success, dense retrievers are expensive to serve across practical use cases. For use cases requiring to search from millions of documents, the dense index becomes bulky and requires high memory usage for storing the index. More recently, learning-to-hash (LTH) techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby reducing the memory requirement to efficiently store the dense index. LTH techniques are supervised and finetune the retriever using a ranking loss. They outperform their counterparts, i.e., traditional out-of-the-box vector compression techniques such as PCA or PQ. A missing piece from prior work is that existing techniques have been evaluated only in-domain, i.e., on a single dataset such as MS MARCO. In our work, we evaluate LTH and vector compression techniques for improving the downstream zero-shot retrieval accuracy of the TAS-B d
&lt;/p&gt;</description></item><item><title>HDGT&#26159;&#19968;&#31181;&#23558;&#39550;&#39542;&#22330;&#26223;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22330;&#26223;&#32534;&#30721;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#30340;&#26041;&#24335;&#36827;&#34892;&#31354;&#38388;&#20851;&#31995;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2205.09753</link><description>&lt;p&gt;
HDGT: &#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#30340;&#24322;&#26500;&#39550;&#39542;&#22270;&#21464;&#25442;&#22120;&#36890;&#36807;&#22330;&#26223;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09753
&lt;/p&gt;
&lt;p&gt;
HDGT&#26159;&#19968;&#31181;&#23558;&#39550;&#39542;&#22330;&#26223;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22330;&#26223;&#32534;&#30721;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#30340;&#26041;&#24335;&#36827;&#34892;&#31354;&#38388;&#20851;&#31995;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39550;&#39542;&#22330;&#26223;&#32534;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#20351;&#24471;&#36712;&#36857;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#39550;&#39542;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35937;&#65288;&#26234;&#33021;&#20307;&#12289;&#36710;&#36947;&#12289;&#20132;&#36890;&#26631;&#24535;&#65289;&#20197;&#21450;&#23545;&#35937;&#20043;&#38388;&#20016;&#23500;&#22810;&#26679;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#20803;&#32032;&#20043;&#38388;&#20063;&#23384;&#22312;&#30456;&#23545;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#31354;&#38388;&#20851;&#31995;&#26159;&#19968;&#20010;&#30456;&#23545;&#27010;&#24565;&#65292;&#38656;&#35201;&#20197;&#33258;&#25105;&#20013;&#24515;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#22352;&#26631;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;&#39550;&#39542;&#22270;&#21464;&#25442;&#22120;&#65288;HDGT&#65289;&#65292;&#23558;&#39550;&#39542;&#22330;&#26223;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#24322;&#26500;&#22270;&#12290;&#22312;&#24322;&#26500;&#22270;&#30340;&#26500;&#24314;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#22810;&#26679;&#30340;&#35821;&#20041;&#20851;&#31995;&#36830;&#25509;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#12290;&#22312;&#31354;&#38388;&#20851;&#31995;&#32534;&#30721;&#20013;&#65292;&#33410;&#28857;&#30340;&#22352;&#26631;&#20197;&#21450;&#20854;&#20837;&#36793;&#26159;&#22312;&#23616;&#37096;&#33410;&#28857;&#20013;&#24515;&#22352;&#26631;&#31995;&#20013;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding a driving scene into vector representations has been an essential task for autonomous driving that can benefit downstream tasks e.g. trajectory prediction. The driving scene often involves heterogeneous elements such as the different types of objects (agents, lanes, traffic signs) and the semantic relations between objects are rich and diverse. Meanwhile, there also exist relativity across elements, which means that the spatial relation is a relative concept and need be encoded in a ego-centric manner instead of in a global coordinate system. Based on these observations, we propose Heterogeneous Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a heterogeneous graph with different types of nodes and edges. For heterogeneous graph construction, we connect different types of nodes according to diverse semantic relations. For spatial relation encoding, the coordinates of the node as well as its in-edges are in the local node-centric coordinate system. Fo
&lt;/p&gt;</description></item><item><title>&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#26368;&#23567;-&#26368;&#22823;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#21644;&#25237;&#24433;&#33258;&#30001;&#30340;&#22312;&#32447;&#31639;&#27861;DORA&#65292;&#36890;&#36807;&#35753;&#38750;&#38459;&#22622;&#23398;&#20064;&#32773;&#23398;&#20250;&#25918;&#24323;&#36164;&#28304;&#24182;&#19982;&#38459;&#22622;&#32773;&#20849;&#20139;&#36164;&#28304;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#21644;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2112.03896</link><description>&lt;p&gt;
&#26799;&#24230;&#21644;&#25237;&#24433;&#33258;&#30001;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#26368;&#23567;-&#26368;&#22823;&#36164;&#28304;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gradient and Projection Free Distributed Online Min-Max Resource Optimization. (arXiv:2112.03896v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03896
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#26368;&#23567;-&#26368;&#22823;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#21644;&#25237;&#24433;&#33258;&#30001;&#30340;&#22312;&#32447;&#31639;&#27861;DORA&#65292;&#36890;&#36807;&#35753;&#38750;&#38459;&#22622;&#23398;&#20064;&#32773;&#23398;&#20250;&#25918;&#24323;&#36164;&#28304;&#24182;&#19982;&#38459;&#22622;&#32773;&#20849;&#20139;&#36164;&#28304;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#21644;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;&#19968;&#32452;&#24182;&#34892;&#20195;&#29702;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#26368;&#23567;-&#26368;&#22823;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#19968;&#32452;&#38543;&#26102;&#38388;&#21464;&#21270;&#19988;&#36882;&#20943;&#30340;&#25104;&#26412;&#20989;&#25968;&#30340;&#36880;&#28857;&#26368;&#22823;&#20540;&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#20110;&#36825;&#20123;&#20989;&#25968;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#22312;&#32447;&#36164;&#28304;&#37325;&#20998;&#37197;&#65288;DORA&#65289;&#65292;&#20854;&#20013;&#38750;&#38459;&#22622;&#23398;&#20064;&#32773;&#23398;&#20250;&#25918;&#24323;&#36164;&#28304;&#24182;&#19982;&#38459;&#22622;&#32773;&#20849;&#20139;&#36164;&#28304;&#12290;DORA&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#23427;&#19981;&#38656;&#35201;&#26799;&#24230;&#35745;&#31639;&#25110;&#25237;&#24433;&#25805;&#20316;&#65292;&#36825;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22312;&#32447;&#20248;&#21270;&#31574;&#30053;&#19981;&#21516;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#21644;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;DORA&#30340;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#65292;&#24182;&#25512;&#23548;&#20986;&#38750;&#20984;&#20989;&#25968;&#30340;&#21160;&#24577;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24102;&#23485;&#20998;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider distributed online min-max resource allocation with a set of parallel agents and a parameter server. Our goal is to minimize the pointwise maximum over a set of time-varying and decreasing cost functions, without a priori information about these functions. We propose a novel online algorithm, termed Distributed Online resource Re-Allocation (DORA), where non-stragglers learn to relinquish resource and share resource with stragglers. A notable feature of DORA is that it does not require gradient calculation or projection operation, unlike most existing online optimization strategies. This allows it to substantially reduce the computation overhead in large-scale and distributed networks. We analyze the worst-case performance of DORA and derive an upper bound on its dynamic regret for non-convex functions. We further consider an application to the bandwidth allocation problem in distributed online machine learning. Our numerical study demonstrates the efficacy of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21560;&#25910;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#20247;&#21253;&#20013;&#33258;&#36866;&#24212;&#24378;&#22810;&#25968;&#25237;&#31080;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#35813;&#25237;&#31080;&#36807;&#31243;&#30340;&#19968;&#20123;&#37325;&#35201;&#29305;&#24449;&#65292;&#21253;&#25324;&#20849;&#35782;&#25237;&#31080;&#30340;&#36136;&#37327;&#12289;&#25237;&#31080;&#25968;&#21644;&#38656;&#27714;&#30340;&#26041;&#24046;&#31561;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#38408;&#20540;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#24037;&#20154;&#21442;&#19982;&#30340;&#25237;&#31080;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31561;&#25928;&#12290;</title><link>http://arxiv.org/abs/2111.06390</link><description>&lt;p&gt;
&#20247;&#21253;&#20013;&#33258;&#36866;&#24212;&#24378;&#22810;&#25968;&#25237;&#31080;&#30340;&#20840;&#38754;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Full Characterization of Adaptively Strong Majority Voting in Crowdsourcing. (arXiv:2111.06390v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21560;&#25910;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#20247;&#21253;&#20013;&#33258;&#36866;&#24212;&#24378;&#22810;&#25968;&#25237;&#31080;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#35813;&#25237;&#31080;&#36807;&#31243;&#30340;&#19968;&#20123;&#37325;&#35201;&#29305;&#24449;&#65292;&#21253;&#25324;&#20849;&#35782;&#25237;&#31080;&#30340;&#36136;&#37327;&#12289;&#25237;&#31080;&#25968;&#21644;&#38656;&#27714;&#30340;&#26041;&#24046;&#31561;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#38408;&#20540;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#24037;&#20154;&#21442;&#19982;&#30340;&#25237;&#31080;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31561;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#20013;&#65292;&#36890;&#36807;&#35753;&#24037;&#20154;&#26816;&#26597;&#39033;&#30446;&#24182;&#23545;&#20854;&#27491;&#30830;&#24615;&#36827;&#34892;&#25237;&#31080;&#65292;&#21487;&#20197;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#19981;&#21487;&#38752;&#24037;&#20154;&#21709;&#24212;&#30340;&#24433;&#21709;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;$\delta$-&#36793;&#30028;&#25237;&#31080;&#36807;&#31243;&#65292;&#30452;&#21040;&#36229;&#36807;&#20102;&#24037;&#20154;&#20043;&#38388;&#36798;&#25104;&#19968;&#33268;&#30340;&#39044;&#23450;&#38408;&#20540;$\delta$&#20026;&#27490;&#65292;&#39069;&#22806;&#30340;&#25237;&#31080;&#23558;&#34987;&#24449;&#27714;&#24847;&#35265;&#12290;&#35813;&#36807;&#31243;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#20165;&#20316;&#20026;&#19968;&#31181;&#32463;&#39564;&#27861;&#21017;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21560;&#25910;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#20998;&#26512;&#22312;&#20247;&#21253;&#36807;&#31243;&#20013;&#19982;&#36825;&#31181;&#25237;&#31080;&#36807;&#31243;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#38381;&#24335;&#26041;&#31243;&#65292;&#29992;&#20110;&#25551;&#36848;&#25152;&#24471;&#20986;&#30340;&#20849;&#35782;&#25237;&#31080;&#30340;&#36136;&#37327;&#12289;&#36798;&#25104;&#20849;&#35782;&#25152;&#38656;&#30340;&#24179;&#22343;&#25237;&#31080;&#25968;&#12289;&#25237;&#31080;&#38656;&#27714;&#30340;&#26041;&#24046;&#21644;&#20854;&#20182;&#20998;&#24067;&#30697;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#35843;&#25972;&#38408;&#20540;$\delta$&#20197;&#23454;&#29616;&#22312;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#24037;&#20154;&#30340;&#25237;&#31080;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31561;&#25928;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19982;&#25237;&#31080;&#36807;&#31243;&#25928;&#29575;&#30456;&#31561;&#30340;&#25903;&#20184;&#36153;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In crowdsourcing, quality control is commonly achieved by having workers examine items and vote on their correctness. To minimize the impact of unreliable worker responses, a $\delta$-margin voting process is utilized, where additional votes are solicited until a predetermined threshold $\delta$ for agreement between workers is exceeded. The process is widely adopted but only as a heuristic. Our research presents a modeling approach using absorbing Markov chains to analyze the characteristics of this voting process that matter in crowdsourced processes. We provide closed-form equations for the quality of resulting consensus vote, the expected number of votes required for consensus, the variance of vote requirements, and other distribution moments. Our findings demonstrate how the threshold $\delta$ can be adjusted to achieve quality equivalence across voting processes that employ workers with varying accuracy levels. We also provide efficiency-equalizing payment rates for voting proces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2109.12509</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#28145;&#24230;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#24212;&#20174;&#24310;&#36831;&#21453;&#39304;&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#20391;&#37325;&#20110;&#20174;&#29992;&#25143;&#23545;&#21333;&#20010;&#25512;&#33616;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#24037;&#20316;&#21033;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#25918;&#24323;&#20102;&#23398;&#20064;&#29992;&#25143;&#20043;&#21518;&#30340;&#34892;&#20026;&#12290;&#22312;&#36807;&#21435;&#30340;&#24037;&#20316;&#20013;&#65292;&#34429;&#28982;&#33268;&#21147;&#20110;&#20174;&#38543;&#21518;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#24182;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#12290;&#24403;&#22870;&#21169;&#36739;&#23569;&#26102;&#65292;&#36890;&#36807;&#24341;&#23548;&#25506;&#32034;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#31995;&#32479;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#22312;&#21333;&#27493;&#25506;&#32034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22312;&#26032;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#24182;&#33021;&#26377;&#25928;&#25903;&#25345;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;&#20854;&#29305;&#33394;&#29305;&#28857;&#26159;&#39046;&#22495;&#27169;&#22411;&#30340;&#20195;&#25968;&#32467;&#26500;&#21644;&#21472;&#21152;&#35745;&#31639;&#65292;&#20351;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.05268</link><description>&lt;p&gt;
&#20316;&#20026;&#26032;&#20852;&#30828;&#20214;&#35745;&#31639;&#26694;&#26550;&#30340;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Vector Symbolic Architectures as a Computing Framework for Emerging Hardware. (arXiv:2106.05268v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05268
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22312;&#26032;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#24182;&#33021;&#26377;&#25928;&#25903;&#25345;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;&#20854;&#29305;&#33394;&#29305;&#28857;&#26159;&#39046;&#22495;&#27169;&#22411;&#30340;&#20195;&#25968;&#32467;&#26500;&#21644;&#21472;&#21152;&#35745;&#31639;&#65292;&#20351;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#26399;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#65288;&#20063;&#31216;&#20026;&#39640;&#32500;&#35745;&#31639;&#65289;&#30340;&#21457;&#23637;&#36827;&#23637;&#12290;&#36825;&#31181;&#26694;&#26550;&#38750;&#24120;&#36866;&#21512;&#22312;&#38543;&#26426;&#30340;&#12289;&#26032;&#20852;&#30340;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#24182;&#33258;&#28982;&#22320;&#34920;&#36798;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25152;&#38656;&#30340;&#35748;&#30693;&#25805;&#20316;&#31867;&#22411;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#20102;VSA&#20855;&#26377;&#39046;&#22495;&#27169;&#22411;&#30340;&#20195;&#25968;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#33021;&#22815;&#25903;&#25345;&#29616;&#20195;&#35745;&#31639;&#30340;&#25152;&#26377;&#25968;&#25454;&#32467;&#26500;&#21644;&#25805;&#20316;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#39640;&#32500;&#21521;&#37327;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;VSA&#30340;&#29305;&#33394;&#29305;&#28857;&#8220;&#21472;&#21152;&#35745;&#31639;&#8221;&#65292;&#23427;&#20351;&#20854;&#21306;&#21035;&#20110;&#20256;&#32479;&#35745;&#31639;&#65292;&#24182;&#20026;AI&#24212;&#29992;&#20013;&#30340;&#22256;&#38590;&#32452;&#21512;&#25628;&#32034;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23637;&#31034;VSA&#20855;&#26377;&#35745;&#31639;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#21487;&#20197;&#20316;&#20026;&#20998;&#24067;&#24335;&#34920;&#31034;&#35745;&#31639;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article reviews recent progress in the development of the computing framework vector symbolic architectures (VSA) (also known as hyperdimensional computing). This framework is well suited for implementation in stochastic, emerging hardware, and it naturally expresses the types of cognitive operations required for artificial intelligence (AI). We demonstrate in this article that the field-like algebraic structure of VSA offers simple but powerful operations on high-dimensional vectors that can support all data structures and manipulations relevant to modern computing. In addition, we illustrate the distinguishing feature of VSA, "computing in superposition," which sets it apart from conventional computing. It also opens the door to efficient solutions to the difficult combinatorial search problems inherent in AI applications. We sketch ways of demonstrating that VSA are computationally universal. We see them acting as a framework for computing with distributed representations that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26174;&#24335;&#30693;&#35782;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#24773;&#26223;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#26641;&#29366;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#35821;&#20041;&#35268;&#21017;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#39550;&#39542;&#22330;&#26223;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2106.04066</link><description>&lt;p&gt;
&#20855;&#26377;&#26174;&#24335;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#20041;&#23545;&#25239;&#24773;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantically Adversarial Scenario Generation with Explicit Knowledge Guidance. (arXiv:2106.04066v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26174;&#24335;&#30693;&#35782;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#24773;&#26223;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#26641;&#29366;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#35821;&#20041;&#35268;&#21017;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#39550;&#39542;&#22330;&#26223;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20855;&#26377;&#28508;&#22312;&#20351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22833;&#25928;&#30340;&#23545;&#25239;&#24773;&#26223;&#26159;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#19987;&#29992;&#27169;&#22411;&#25193;&#23637;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#28385;&#36275;&#20102;&#39069;&#22806;&#30340;&#21487;&#25511;&#35201;&#27714;&#65292;&#20363;&#22914;&#36890;&#36807;&#22312;&#31070;&#32463;&#20803;&#32423;&#21035;&#38544;&#24335;&#25805;&#20316;&#26469;&#23884;&#20837;&#20132;&#36890;&#26631;&#24535;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#65292;&#23454;&#29616;&#35821;&#20041;&#23545;&#25239;&#29983;&#25104;&#65288;SAG&#65289;&#12290;&#20026;&#20102;&#19982;&#39550;&#39542;&#22330;&#26223;&#30340;&#32452;&#25104;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#30693;&#35782;&#20998;&#20026;&#20004;&#31867;&#65292;&#21363;&#23545;&#35937;&#30340;&#23646;&#24615;&#21644;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#29366;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;T-VAE&#65289;&#26469;&#23398;&#20064;&#23618;&#27425;&#21270;&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#26641;&#32467;&#26500;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#23646;&#24615;&#26045;&#21152;&#35821;&#20041;&#35268;&#21017;&#65292;&#26174;&#24335;&#30693;&#35782;&#34701;&#21512;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#31034;&#20363;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating adversarial scenarios, which have the potential to fail autonomous driving systems, provides an effective way to improve robustness. Extending purely data-driven generative models, recent specialized models satisfy additional controllable requirements such as embedding a traffic sign in a driving scene by manipulating patterns implicitly in the neuron level. In this paper, we introduce a method to incorporate domain knowledge explicitly in the generation process to achieve the Semantically Adversarial Generation (SAG). To be consistent with the composition of driving scenes, we first categorize the knowledge into two types, the property of objects and the relationship among objects. We then propose a tree-structured variational auto-encoder (T-VAE) to learn hierarchical scene representation. By imposing semantic rules on the properties of nodes and edges in the tree structure, explicit knowledge integration enables controllable generation. We construct a synthetic example to
&lt;/p&gt;</description></item><item><title>&#24863;&#30693;&#22120;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2012.07881</link><description>&lt;p&gt;
&#24863;&#30693;&#22120;&#29702;&#35770;&#21487;&#20197;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Perceptron Theory Can Predict the Accuracy of Neural Networks. (arXiv:2012.07881v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.07881
&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#22120;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#25216;&#26415;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20174;&#20998;&#26512;&#21644;&#39044;&#27979;&#24615;&#33021;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#32593;&#32476;&#20173;&#28982;&#26159;&#40657;&#31665;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#21333;&#23618;&#24863;&#30693;&#22120;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#39044;&#27979;&#22810;&#31181;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25512;&#24191;&#29992;&#20110;&#20998;&#26512;&#20648;&#22791;&#35745;&#31639;&#27169;&#22411;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#36830;&#25509;&#20027;&#20041;&#27169;&#22411;&#30340;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#30340;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#24863;&#30693;&#22120;&#20998;&#31867;&#30340;&#19968;&#33324;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#29702;&#35770;&#25552;&#20379;&#20102;&#19977;&#20010;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#22686;&#21152;&#35814;&#32454;&#20449;&#24687;&#26469;&#21033;&#29992;&#20449;&#21495;&#32479;&#35745;&#12290;&#36825;&#20123;&#20844;&#24335;&#22312;&#35299;&#26512;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#12290;&#25429;&#25417;&#26368;&#35814;&#32454;&#20449;&#24687;&#30340;&#25551;&#36848;&#32423;&#21035;&#38656;&#35201;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#12290;&#26681;&#25454;&#32593;&#32476;&#27169;&#22411;&#30340;&#19981;&#21516;&#65292;&#36739;&#31616;&#21333;&#30340;&#20844;&#24335;&#24050;&#32463;&#33021;&#22815;&#25552;&#20379;&#39640;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilayer neural networks set the current state of the art for many technical classification problems. But, these networks are still, essentially, black boxes in terms of analyzing them and predicting their performance. Here, we develop a statistical theory for the one-layer perceptron and show that it can predict performances of a surprisingly large variety of neural networks with different architectures. A general theory of classification with perceptrons is developed by generalizing an existing theory for analyzing reservoir computing models and connectionist models for symbolic reasoning known as vector symbolic architectures. Our statistical theory offers three formulas leveraging the signal statistics with increasing detail. The formulas are analytically intractable, but can be evaluated numerically. The description level that captures maximum details requires stochastic sampling methods. Depending on the network model, the simpler formulas already yield high prediction accuracy
&lt;/p&gt;</description></item></channel></rss>