<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23548;&#24341;&#22270;&#20248;&#21270;&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#21160;&#29983;&#25104;&#23548;&#24341;&#30340;&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#33391;&#22909;&#23548;&#24341;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01446</link><description>&lt;p&gt;
&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#23548;&#24341;&#22270;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guidance Graph Optimization for Lifelong Multi-Agent Path Finding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23548;&#24341;&#22270;&#20248;&#21270;&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#21160;&#29983;&#25104;&#23548;&#24341;&#30340;&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#33391;&#22909;&#23548;&#24341;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#23548;&#24341;&#26469;&#25552;&#39640;&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#30340;&#21534;&#21520;&#37327;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23558;&#23548;&#24341;&#65288;&#22914;&#39640;&#36895;&#20844;&#36335;&#65289;&#32435;&#20837;MAPF&#31639;&#27861;&#21487;&#20197;&#21152;&#36895;&#35745;&#31639;&#65292;&#20294;&#36825;&#24448;&#24448;&#20250;&#19982;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#20135;&#29983;&#25240;&#20013;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#33391;&#22909;&#30340;&#23548;&#24341;&#20173;&#28982;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36824;&#26080;&#27861;&#36229;&#36234;&#25163;&#21160;&#35774;&#35745;&#30340;&#23548;&#24341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#21521;&#23548;&#24341;&#22270;&#20316;&#20026;&#38271;&#26399;MAPF&#20013;&#24341;&#23548;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#24182;&#23558;&#23548;&#24341;&#22270;&#20248;&#21270;&#65288;GGO&#65289;&#30340;&#20219;&#21153;&#23450;&#20041;&#20026;&#20248;&#21270;&#20854;&#36793;&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;GGO&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#26399;MAPF&#31639;&#27861;&#21644;&#22320;&#22270;&#30340;&#23548;&#24341;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;CMA-ES&#65288;&#19968;&#31181;&#40657;&#31665;&#20248;&#21270;&#31639;&#27861;&#65289;&#26469;&#35299;&#20915;GGO&#38382;&#39064;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;PIU&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#23548;&#24341;&#30340;&#26356;&#26032;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#20248;&#21270;&#36807;&#30340;&#23548;&#24341;&#22270;&#20256;&#25773;&#21040;&#36739;&#22823;&#22320;&#22270;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to use guidance to improve the throughput of lifelong Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while incorporating guidance, such as highways, can accelerate MAPF algorithms, this often results in a trade-off with solution quality. In addition, how to generate good guidance automatically remains largely unexplored, with current methods falling short of surpassing manually designed ones. In this work, we introduce the directed guidance graph as a versatile representation of guidance for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of optimizing its edge weights. We present two GGO algorithms to automatically generate guidance for arbitrary lifelong MAPF algorithms and maps. The first method directly solves GGO by employing CMA-ES, a black-box optimization algorithm. The second method, PIU, optimizes an update model capable of generating guidance, demonstrating the ability to transfer optimized guidance graphs to larger
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01002</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#38754;&#23380;&#25670;&#33073;&#20102;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
AI-generated faces free from racial and gender stereotypes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;Stable Diffusion&#20043;&#31867;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;AI&#27169;&#22411;&#27599;&#22825;&#37117;&#34987;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#23545;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#25918;&#22823;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#25552;&#20986;&#20102;&#20851;&#20999;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20219;&#24847;&#32473;&#23450;&#38754;&#37096;&#22270;&#20687;&#30340;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#32452;&#65292;&#24182;&#23637;&#31034;&#20854;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#23545;Stable Diffusion&#22312;&#20845;&#31181;&#31181;&#26063;&#12289;&#20004;&#31181;&#24615;&#21035;&#12289;&#20116;&#20010;&#24180;&#40836;&#32452;&#12289;32&#20010;&#32844;&#19994;&#21644;&#20843;&#20010;&#23646;&#24615;&#19978;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26367;&#20195;&#26041;&#26696;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;Stable Diffusion&#22312;&#25551;&#32472;&#21516;&#19968;&#31181;&#26063;&#30340;&#20010;&#20307;&#26102;&#30456;&#20284;&#31243;&#24230;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#20986;&#39640;&#24230;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#20363;&#22914;&#65292;&#23558;&#22823;&#22810;&#25968;&#20013;&#19996;&#30007;&#24615;&#25551;&#32472;&#20026;&#30382;&#32932;&#40669;&#40657;&#12289;&#30041;&#30528;&#32993;&#23376;&#12289;&#25140;&#30528;&#20256;&#32479;&#22836;&#39280;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#22686;&#21152;&#38754;&#37096;&#22810;&#26679;&#24615;&#30340;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, many have raised concerns regarding how these models amplify racial and gender stereotypes. To study this phenomenon, we develop a classifier to predict the race, gender, and age group of any given face image, and show that it achieves state-of-the-art performance. Using this classifier, we quantify biases in Stable Diffusion across six races, two genders, five age groups, 32 professions, and eight attributes. We then propose novel debiasing solutions that outperform state-of-the-art alternatives. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals a high degree of stereotyping, e.g., depicting most middle eastern males as being dark-skinned, bearded, and wearing a traditional headdress. We address these limitations by proposing yet another novel solution that increases facial div
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.19669</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#21644;&#35270;&#35273;&#22312;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Roles of Language and Vision in Learning from Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19669
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#35821;&#35328;&#26159;&#21542;&#26377;&#21161;&#20110;&#29702;&#35299;&#35270;&#35273;&#19990;&#30028;&#65311;&#23454;&#38469;&#35266;&#23519;&#19990;&#30028;&#38656;&#35201;&#30475;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#29992;&#25991;&#23383;&#25551;&#36848;&#21527;&#65311;&#20851;&#20110;&#26234;&#33021;&#26412;&#36136;&#30340;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#24456;&#38590;&#22238;&#31572;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#30340;&#20363;&#23376;&#8212;&#8212;&#20154;&#31867;&#8212;&#8212;&#20197;&#21450;&#26377;&#38480;&#30340;&#29420;&#31435;&#35821;&#35328;&#25110;&#35270;&#35273;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#25506;&#32034;&#35821;&#35328;&#21644;&#35270;&#35273;&#23545;&#20110;&#23398;&#20064;&#19990;&#30028;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#20013;&#20999;&#38500;&#32452;&#20214;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#24674;&#22797;&#20102;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23427;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#20284;&#20046;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#22122;&#22768;MRI&#22270;&#20687;&#36827;&#34892;&#33041;&#37096;&#32959;&#30244;&#20998;&#31867;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10698</link><description>&lt;p&gt;
&#38024;&#23545;&#22122;&#22768;&#33041;&#37096;MRI&#30340;&#31283;&#20581;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Influence-based Training Methods for Noisy Brain MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#22122;&#22768;MRI&#22270;&#20687;&#36827;&#34892;&#33041;&#37096;&#32959;&#30244;&#20998;&#31867;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#20998;&#31867;&#33041;&#37096;&#32959;&#30244;&#23545;&#21450;&#26102;&#21644;&#20934;&#30830;&#27835;&#30103;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20960;&#31181;&#22522;&#20110;&#32463;&#20856;&#22270;&#20687;&#22788;&#29702;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20998;&#31867;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#24555;&#36895;&#20998;&#31867;MR&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#65292;&#20294;&#22823;&#22810;&#25968;&#20551;&#23450;&#20102;&#35757;&#32451;&#25968;&#25454;&#26159;&#26080;&#22122;&#22768;&#30340;&#19981;&#20999;&#23454;&#38469;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22122;&#22768;&#30340;MR&#22270;&#20687;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#20998;&#31867;&#33041;&#37096;&#32959;&#30244;&#30340;&#22256;&#38590;&#20294;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31283;&#20581;&#20110;&#22122;&#22768;MRI&#35757;&#32451;&#25968;&#25454;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#65288;ISR&#65289;&#21644;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#26679;&#26412;&#25200;&#21160;&#65288;ISP&#65289;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22522;&#20110;&#40065;&#26834;&#32479;&#35745;&#20013;&#30340;&#24433;&#21709;&#20989;&#25968;&#12290;&#22312;ISR&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#35757;&#32451;&#36807;&#31243;&#20013;&#26679;&#26412;&#23545;&#35757;&#32451;&#30340;&#24110;&#21161;/&#21361;&#23475;&#31243;&#24230;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#21152;&#26435;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#22312;ISP&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#24433;&#21709;&#24471;&#20998;&#37327;&#36523;&#23450;&#21046;&#24182;&#27880;&#20837;&#26377;&#24110;&#21161;&#30340;&#25200;&#21160;&#12290;ISR&#21644;ISP&#22343;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10698v1 Announce Type: cross  Abstract: Correctly classifying brain tumors is imperative to the prompt and accurate treatment of a patient. While several classification algorithms based on classical image processing or deep learning methods have been proposed to rapidly classify tumors in MR images, most assume the unrealistic setting of noise-free training data. In this work, we study a difficult but realistic setting of training a deep learning model on noisy MR images to classify brain tumors. We propose two training methods that are robust to noisy MRI training data, Influence-based Sample Reweighing (ISR) and Influence-based Sample Perturbation (ISP), which are based on influence functions from robust statistics. Using the influence functions, in ISR, we adaptively reweigh training examples according to how helpful/harmful they are to the training process, while in ISP, we craft and inject helpful perturbation proportional to the influence score. Both ISR and ISP harden
&lt;/p&gt;</description></item><item><title>Nissist&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20197;&#25552;&#39640;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#20107;&#25925;&#31649;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17531</link><description>&lt;p&gt;
Nissist&#65306;&#22522;&#20110;&#25925;&#38556;&#25490;&#38500;&#25351;&#21335;&#30340;&#20107;&#25925;&#32531;&#35299;&#21103;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17531
&lt;/p&gt;
&lt;p&gt;
Nissist&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20197;&#25552;&#39640;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#20107;&#25925;&#31649;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#20107;&#25925;&#31649;&#29702;&#23545;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#39034;&#30021;&#36816;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290; &#20026;&#20102;&#21152;&#36895;&#20107;&#25925;&#32531;&#35299;&#65292;&#26381;&#21153;&#22242;&#38431;&#23558;&#25925;&#38556;&#25490;&#38500;&#30693;&#35782;&#32534;&#35793;&#25104;&#20379;&#20540;&#29677;&#24037;&#31243;&#24072;&#65288;OCEs&#65289;&#35775;&#38382;&#30340;&#25925;&#38556;&#25490;&#38500;&#25351;&#21335;&#65288;TSGs&#65289;&#12290; &#23613;&#31649;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#24050;&#33021;&#22815;&#35299;&#20915;&#26368;&#24120;&#35265;&#21644;&#31616;&#21333;&#30340;&#20107;&#25925;&#65292;&#20294;&#20173;&#23384;&#22312;&#38656;&#35201;OCE&#24178;&#39044;&#30340;&#22797;&#26434;&#20107;&#25925;&#12290; &#28982;&#32780;&#65292;TSGs&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#21644;&#19981;&#23436;&#25972;&#30340;&#65292;&#36825;&#38656;&#35201;OCE&#25163;&#21160;&#35299;&#37322;&#65292;&#23548;&#33268;&#20540;&#29677;&#30130;&#21171;&#21644;&#29983;&#20135;&#21147;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#26032;&#20837;&#32844;&#30340;OCE&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Nissist&#65292;&#23427;&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#12290; &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;Nissist&#20174;&#38750;&#32467;&#26500;&#21270;TSGs&#21644;&#21382;&#21490;&#20107;&#25925;&#32531;&#35299;&#35752;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24418;&#25104;&#20840;&#38754;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17531v1 Announce Type: cross  Abstract: Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-a
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.17045</link><description>&lt;p&gt;
&#23545;&#21508;&#31867;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24615;&#33021;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17045
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35745;&#31639;&#26426;&#21644;&#20449;&#24687;&#31995;&#32479;&#20813;&#21463;&#25915;&#20987;&#32773;&#21033;&#29992;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#36827;&#34892;&#32593;&#32476;&#29359;&#32618;&#30340;&#20405;&#23475;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#28431;&#27934;&#20197;&#25552;&#39640;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#26159;&#23454;&#29616;&#31995;&#32479;&#23433;&#20840;&#30340;&#25928;&#26524;&#26368;&#22909;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#21147;&#33539;&#22260;&#20174;&#26089;&#26399;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#21040;&#23454;&#26102;&#26816;&#27979;&#31995;&#32479;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#22949;&#21327;&#12290;&#30001;&#20110;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#27599;&#31181;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36825;&#20063;&#24433;&#21709;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#27599;&#19968;&#20010;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#25918;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#19978;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.15347</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Safe Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#35780;&#20272;&#36829;&#21453;&#20808;&#39564;&#26410;&#30693;&#65288;&#23433;&#20840;&#65289;&#32422;&#26463;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26410;&#30693;&#20989;&#25968;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#22312;&#26410;&#30693;&#20989;&#25968;&#19978;&#25918;&#32622;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#19988;&#20165;&#20801;&#35768;&#22312;&#39640;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#22495;&#30340;&#31163;&#25955;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#25193;&#23637;&#21040;&#36830;&#32493;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21033;&#29992;&#32422;&#26463;&#30340;&#35268;&#21017;&#20551;&#35774;&#30340;&#26041;&#24335;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#30452;&#25509;&#21033;&#29992;GP&#21518;&#39564;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#23433;&#20840;&#21442;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#23558;&#36825;&#19968;&#25506;&#32034;&#20934;&#21017;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#30142;&#30149;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#39640;&#25928;&#12289;&#26102;&#38388;&#33410;&#32422;&#21644;&#21171;&#21160;&#33410;&#32422;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.07399</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20020;&#24202;&#25512;&#29702;&#32773;&#65306;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#25512;&#29702;&#24863;&#30693;&#35786;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#30142;&#30149;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#39640;&#25928;&#12289;&#26102;&#38388;&#33410;&#32422;&#21644;&#21171;&#21160;&#33410;&#32422;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#26426;&#22120;&#25512;&#29702;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20026;&#39537;&#21160;&#30340;&#39033;&#30446;&#20027;&#35201;&#38598;&#20013;&#22312;&#20020;&#24202;&#20998;&#31867;&#25110;&#38405;&#35835;&#29702;&#35299;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#29702;&#24565;&#27880;&#35299;&#25104;&#26412;&#36739;&#39640;&#65292;&#23545;&#20110;&#30142;&#30149;&#35786;&#26029;&#30340;&#20020;&#24202;&#25512;&#29702;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#21644;&#21171;&#21160;&#26041;&#24335;&#21435;&#29702;&#24615;&#21270;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#23398;&#20064;&#23545;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#36827;&#34892;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#30142;&#30149;&#35786;&#26029;&#30340;&#20020;&#24202;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;LLM&#29983;&#25104;&#20102;&#35786;&#26029;&#24615;&#30340;&#29702;&#30001;&#65292;&#25552;&#20379;&#20854;&#23545;&#21576;&#29616;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#35265;&#35299;&#20197;&#21450;&#36798;&#21040;&#35786;&#26029;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#21363;&#20020;&#24202;&#24605;&#32500;&#38142;&#65288;Clinical CoT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#22312;&#29702;&#30001;&#29983;&#25104;&#21644;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#23454;&#35777;&#20102;LLMs/LMs&#30340;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2311.14688</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#26469;&#23454;&#29616;&#31243;&#24207;&#20844;&#24179;
&lt;/p&gt;
&lt;p&gt;
Procedural Fairness Through Decoupling Objectionable Data Generating Components
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#24182;&#35299;&#20915;&#20102;&#32463;&#24120;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21363;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#21363;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20013;&#31435;&#65288;&#21363;&#19981;&#25104;&#38382;&#39064;&#30340;&#65289;&#26041;&#38754;&#30340;&#21487;&#33021;&#26080;&#24847;&#30340;&#25913;&#21464;&#65292;&#21644;/&#25110;&#23545;&#26368;&#19981;&#21033;&#21033;&#30410;&#20010;&#20307;&#30340;&#23454;&#29616;&#27809;&#26377;&#31243;&#24207;&#20445;&#35777;&#12290;&#21463;&#32422;&#32752;&#183;&#32599;&#23572;&#26031;&#23545;&#32431;&#31243;&#24207;&#20844;&#27491;&#30340;&#20513;&#23548;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#20915;&#31574;&#35270;&#20026;&#31038;&#20250;&#21046;&#24230;&#30340;&#32553;&#24433;&#65292;&#24182;&#32771;&#34385;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26412;&#36523;&#22914;&#20309;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#32771;&#28857;&#21644;&#30456;&#20851;&#30340;&#20215;&#20540;&#23454;&#20363;&#21270;&#35268;&#21017;&#65292;&#23558;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#19982;&#20013;&#31435;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#35299;&#32806;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#30340;&#24517;&#35201;&#24615;&#65292;&#19981;&#20165;&#24341;&#36215;&#20102;&#25105;&#20204;&#21147;&#22270;&#32531;&#35299;&#30340;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#30340;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;S4MI&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#31616;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#26426;&#22120;&#30417;&#30563;&#36807;&#31243;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.10319</link><description>&lt;p&gt;
&#36716;&#21521;&#26426;&#22120;&#30417;&#30563;&#65306;&#29992;&#20110;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26631;&#27880;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10319
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;S4MI&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#31616;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#26426;&#22120;&#30417;&#30563;&#36807;&#31243;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#27835;&#30103;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#21463;&#21040;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#12290;&#26631;&#27880;&#36807;&#31243;&#19981;&#20165;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#23478;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;S4MI&#65288;&#21307;&#23398;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#65289;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#21442;&#19982;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26426;&#22120;&#30417;&#30563;&#30340;&#25193;&#23637;&#65292;&#30456;&#27604;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10319v3 Announce Type: replace-cross  Abstract: Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages the advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self-supervised learning significantly surpassed the performance of supervised methods in the classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;</title><link>http://arxiv.org/abs/2401.15108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31454;&#20105;&#20013;&#20026;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#25104;&#20026;&#20840;&#29699;&#26032;&#24314;&#20132;&#36890;&#30005;&#27668;&#21270;&#22522;&#30784;&#35774;&#26045;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#25215;&#36733;&#35768;&#22810;&#30452;&#27969;&#24555;&#36895;&#20805;&#30005;&#35774;&#22791;&#65292;&#20165;&#21487;&#20379;&#30005;&#21160;&#36710;&#36742;&#20805;&#30005;&#20351;&#29992;&#12290;&#31867;&#20284;&#20110;&#27773;&#27833;&#21152;&#27833;&#31449;&#65292;&#21516;&#19968;&#22320;&#21306;&#30340;&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#26681;&#25454;&#31454;&#20105;&#35843;&#25972;&#20215;&#26684;&#20197;&#21560;&#24341;&#21516;&#19968;&#32676;&#30005;&#21160;&#36710;&#20027;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#19982;&#30005;&#21147;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#22312;&#21069;&#19968;&#22825;&#30005;&#21147;&#24066;&#22330;&#19978;&#30340;&#30005;&#21147;&#38656;&#27714;&#65292;&#24182;&#22312;&#23454;&#26102;&#24066;&#22330;&#19978;&#28385;&#36275;&#24046;&#39069;&#38656;&#27714;&#12290;&#20805;&#30005;&#31449;&#21487;&#33021;&#37197;&#22791;&#34917;&#20805;&#30005;&#27744;&#20648;&#33021;&#31995;&#32479;&#29992;&#20110;&#22871;&#21033;&#12290;&#26412;&#25991;&#38024;&#23545;&#20805;&#30005;&#31449;&#31454;&#20105;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#23450;&#20215;&#26041;&#27861;&#12290;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#38543;&#26426;&#30340;&#21069;&#19968;&#22825;&#30005;&#21147;&#38656;&#27714;&#27169;&#22411;&#24471;&#21040;&#32435;&#20837;&#25215;&#35834;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#28216;&#25103;&#24314;&#27169;&#20026;&#31454;&#20105;&#26469;&#24471;&#21040;&#20805;&#30005;&#31449;&#30340;&#20215;&#26684;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09556</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;: &#23398;&#20064;&#20943;&#23569;&#27169;&#22411;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(ANN)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#36817;&#20284;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#26469;&#32771;&#34385;&#22810;&#20010;&#27963;&#21160;&#32500;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20197;&#26368;&#22823;&#21270;&#26679;&#26412;&#32423;&#20934;&#30830;&#24615;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#21307;&#23398;&#20379;&#24212;&#38142;&#20013;&#30340;&#38271;&#26399;&#25237;&#36164;&#35268;&#21010;&#21644;&#20013;&#26399;&#25112;&#26415;&#35268;&#21010;&#30340;&#22522;&#20110;&#27969;&#30340;&#35774;&#26045;&#20301;&#32622;&#20998;&#37197;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20195;&#30721;&#20070;&#26500;&#24314;&#26041;&#27861;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26356;&#31995;&#32479;&#22320;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;</title><link>http://arxiv.org/abs/2401.04122</link><description>&lt;p&gt;
&#20174;&#25552;&#31034;&#24037;&#31243;&#21040;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#25552;&#31034;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
From Prompt Engineering to Prompt Science With Human in the Loop. (arXiv:2401.04122v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04122
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#20070;&#26500;&#24314;&#26041;&#27861;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26356;&#31995;&#32479;&#22320;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#22312;&#25105;&#20204;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#26159;&#19968;&#20010;&#38656;&#35201;&#22686;&#21152;&#23457;&#26597;&#30340;&#22320;&#26041;&#12290;LLMs&#34987;&#29992;&#20110;&#29983;&#25104;&#25110;&#20998;&#26512;&#30740;&#31350;&#25968;&#25454;&#30340;&#24212;&#29992;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#24403;&#36825;&#31181;&#24212;&#29992;&#34987;&#20020;&#26102;&#20915;&#31574;&#21644;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#25152;&#22256;&#25200;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#23427;&#22914;&#20309;&#24433;&#21709;&#30740;&#31350;&#12289;&#30740;&#31350;&#32467;&#26524;&#25110;&#32773;&#22522;&#20110;&#35813;&#30740;&#31350;&#30340;&#20219;&#20309;&#26410;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#38656;&#35201;&#26356;&#31185;&#23398;&#30340;&#26041;&#27861;&#26469;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#19968;&#20123;&#31215;&#26497;&#30340;&#21162;&#21147;&#25903;&#25345;&#26356;&#31995;&#32479;&#30340;&#25552;&#31034;&#26500;&#24314;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20135;&#29983;&#21487;&#22797;&#21046;&#21644;&#20855;&#26377;&#36275;&#22815;&#36879;&#26126;&#24230;&#12289;&#23458;&#35266;&#24615;&#25110;&#20005;&#35880;&#24615;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#26500;&#24314;&#20195;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20026;&#26356;&#31995;&#32479;&#30340;&#30740;&#31350;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
As LLMs make their way into many aspects of our lives, one place that warrants increased scrutiny with LLM usage is scientific research. Using LLMs for generating or analyzing data for research purposes is gaining popularity. But when such application is marred with ad-hoc decisions and engineering solutions, we need to be concerned about how it may affect that research, its findings, or any future works based on that research. We need a more scientific approach to using LLMs in our research. While there are several active efforts to support more systematic construction of prompts, they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor. This article presents a new methodology inspired by codebook construction through qualitative methods to address that. Using humans in the loop and a multi-phase verification processes, this methodology lays a foundation for more systema
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12690</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#19978;&#30340;&#32452;&#21512;&#24335;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Cosmos&#65292;&#19968;&#20010;&#38024;&#23545;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#35774;&#35745;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22312;&#36890;&#36807;&#24050;&#30693;&#30340;&#35270;&#35273;&#8220;&#21407;&#23376;&#8221;&#32452;&#21512;&#33719;&#24471;&#30340;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;Cosmos&#30340;&#26680;&#24515;&#27934;&#23519;&#21147;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#24037;&#20855;&#65306;&#65288;i&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#22330;&#26223;&#32534;&#30721;&#65292;&#20351;&#29992;&#31070;&#32463;&#32534;&#30721;&#22120;&#35745;&#31639;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23454;&#20307;&#30340;&#23454;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25551;&#36848;&#23454;&#20307;&#23646;&#24615;&#30340;&#21487;&#32452;&#21512;&#31526;&#21495;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#36825;&#20123;&#23454;&#20307;&#19982;&#23398;&#20064;&#21040;&#30340;&#20132;&#20114;&#35268;&#21017;&#32465;&#23450;&#36215;&#26469;&#12290;Cosmos&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65307;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#23558;&#34920;&#31034;&#26144;&#23556;&#20026;&#31526;&#21495;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#30340;&#31526;&#21495;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;blocks&#22330;&#26223;&#36827;&#34892;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;CG&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Cosmos&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13063</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#24212;&#29992;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#21487;&#20197;&#25581;&#31034;&#29992;&#25143;&#19982;&#32593;&#32476;&#25628;&#32034;&#26381;&#21153;&#30340;&#20132;&#20114;&#26041;&#24335;&#12289;&#29992;&#25143;&#30340;&#38656;&#27714;&#20197;&#21450;&#28385;&#24847;&#31243;&#24230;&#31561;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#30340;&#32593;&#32476;&#25628;&#32034;&#24418;&#24335;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#12290;&#20026;&#20102;&#29702;&#35299;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#29992;&#26377;&#24847;&#20041;&#30340;&#20998;&#31867;&#26041;&#24335;&#26631;&#35760;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20854;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#35201;&#20040;&#20195;&#20215;&#39640;&#26114;&#35201;&#20040;&#19981;&#22815;&#28789;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20016;&#23500;&#19988;&#30456;&#20851;&#30340;&#27010;&#24565;&#12289;&#25551;&#36848;&#21644;&#31034;&#20363;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLM&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24535;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#20998;&#31867;&#24471;&#19981;&#21040;&#22806;&#37096;&#39564;&#35777;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#19981;&#33391;&#30340;&#21453;&#39304;&#22238;&#36335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#35780;&#20272;&#32773;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16045</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31354;&#38388;&#21435;&#21367;&#31215;&#21644;&#20449;&#24687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models. (arXiv:2303.16045v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#65292;&#24182;&#35745;&#31639;&#20986;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#21487;&#20197;&#25506;&#31350;&#38750;&#38543;&#26426;&#25968;&#25454;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#21495;&#25110;&#20449;&#24687;&#30340;&#32500;&#24230;&#21644;&#38271;&#24230;&#23610;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#19982;&#21487;&#35745;&#31639;&#25110;&#21322;&#21487;&#35745;&#31639;&#30340;&#36817;&#20284;&#26041;&#27861;&#25110;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#30456;&#20851;&#20294;&#19981;&#29420;&#31435;&#30340;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#23545;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26377;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence. This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions. This was used to investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated. This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable or semi-computable approximation method or encoding-decoding scheme. The results presented in this paper are useful for applications in coding theory, particularly in ze
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#28385;&#36275;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20998;&#37197;&#29289;&#21697;&#32473;&#24179;&#21488;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#36817;&#20284;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09951</link><description>&lt;p&gt;
&#20108;&#20998;&#22270;&#21305;&#37197;&#20013;&#30340;&#19981;&#21516;&#32676;&#20307;&#20844;&#24179;&#24615;&#19979;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#8212;&#8212;&#19968;&#31181;&#36817;&#20284;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Individual fairness under Varied Notions of Group Fairness in Bipartite Matching -- One Framework to Approximate Them Al. (arXiv:2208.09951v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28385;&#36275;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20998;&#37197;&#29289;&#21697;&#32473;&#24179;&#21488;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#36817;&#20284;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28385;&#36275;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20998;&#37197;&#29289;&#21697;&#32473;&#24179;&#21488;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#29289;&#21697;&#37117;&#19982;&#26576;&#20123;&#32676;&#20307;&#30456;&#20851;&#32852;&#65292;&#24182;&#19988;&#23545;&#24179;&#21488;&#26377;&#20248;&#20808;&#39034;&#24207;&#12290;&#27599;&#20010;&#24179;&#21488;&#36890;&#36807;&#25351;&#23450;&#27599;&#20010;&#32676;&#20307;&#21487;&#20197;&#19982;&#20043;&#21305;&#37197;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#26469;&#25191;&#34892;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#23613;&#31649;&#21487;&#33021;&#23384;&#22312;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#22810;&#20010;&#26368;&#20248;&#35299;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#20998;&#24067;&#26469;&#23454;&#29616;&#8220;&#38543;&#26426;&#20010;&#20307;&#20844;&#24179;&#24615;&#8221;&#65292;&#20351;&#24471;&#27599;&#20010;&#29289;&#21697;&#34987;&#21305;&#37197;&#21040;&#20854;&#21069;&#20960;&#20010;&#36873;&#25321;&#20043;&#19968;&#30340;&#21512;&#29702;&#27010;&#29575;&#12290;&#24403;&#27599;&#20010;&#29289;&#21697;&#21487;&#20197;&#23646;&#20110;&#22810;&#20010;&#32676;&#20307;&#26102;&#65292;&#21363;&#20351;&#25152;&#26377;&#32676;&#20307;&#19979;&#38480;&#22343;&#20026;0&#19988;&#27809;&#26377;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23547;&#25214;&#26368;&#22823;&#22823;&#23567;&#32676;&#20307;&#20844;&#24179;&#21305;&#37197;&#30340;&#38382;&#39064;&#20063;&#26159;NP-&#38590;&#30340;&#12290;&#23545;&#20110;&#19968;&#20849;$n$&#20010;&#29289;&#21697;&#65292;&#24403;&#19968;&#20010;&#29289;&#21697;&#26368;&#22810;&#23646;&#20110;$\Delta$&#20010;&#32676;&#20307;&#65292;&#24182;&#19988;&#25152;&#26377;&#32676;&#20307;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#37117;&#26159;&#24120;&#25968;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;$O(\Delta \log n)$&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#37117;&#26159;&#21306;&#38388;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#20010;&#20307;&#20844;&#24179;&#21305;&#37197;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of assigning items to platforms while satisfying group and individual fairness constraints. Each item is associated with certain groups and has a preference ordering over platforms. Each platform enforces group fairness by specifying an upper and a lower bound on the number of items that can be matched to it from each group. Although there may be multiple optimal solutions that satisfy the group fairness constraints, we aim to achieve `probabilistic individual fairness' by computing a distribution over `group fair' matchings such that each item has a reasonable probability of being matched to one of its top choices. When each item can belong to multiple groups, the problem of finding a maximum size group-fair matching is NP-hard even when all the group lower bounds are 0, and there are no individual fairness constraints. Given a total of $n$ items, we achieve a $O(\Delta \log n)$ approximation algorithm when an item can belong to at most $\Delta$ groups, and all
&lt;/p&gt;</description></item></channel></rss>