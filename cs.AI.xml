<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04430</link><description>&lt;p&gt;
SILO&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#38750;&#21442;&#25968;&#21270;&#25968;&#25454;&#23384;&#20648;&#20013;&#38548;&#31163;&#27861;&#24459;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04430
&lt;/p&gt;
&lt;p&gt;
SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#22312;&#21463;&#29256;&#26435;&#25110;&#21463;&#20854;&#20182;&#38480;&#21046;&#30340;&#25968;&#25454;&#19978;&#30340;&#21512;&#27861;&#24615;&#36827;&#34892;&#28608;&#28872;&#36777;&#35770;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#22312;&#20302;&#39118;&#38505;&#25991;&#26412;&#65288;&#20363;&#22914;&#36807;&#26399;&#29256;&#26435;&#22270;&#20070;&#25110;&#25919;&#24220;&#25991;&#20214;&#65289;&#19978;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#35813;&#25991;&#26412;&#30340;&#35268;&#27169;&#21644;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SILO&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#31181;&#39118;&#38505;-&#24615;&#33021;&#26435;&#34913;&#12290;SILO&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#26500;&#24314;&#65306;&#65288;1&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#26032;&#35821;&#26009;&#24211;&#8220;&#24320;&#25918;&#35768;&#21487;&#35777;&#35821;&#26009;&#24211;&#8221;&#65288;OLC&#65289;&#19978;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;LM&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;228B&#20010;&#20844;&#20849;&#39046;&#22495;&#21644;&#35768;&#21487;&#25991;&#26412;&#12290;&#65288;2&#65289;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#65288;&#20363;&#22914;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20070;&#25110;&#26032;&#38395;&#30340;&#25968;&#25454;&#65289;&#23545;&#20854;&#36827;&#34892;&#25193;&#20805;&#65292;&#35813;&#25968;&#25454;&#23384;&#20648;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#34987;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#23384;&#20648;&#20801;&#35768;&#20351;&#29992;&#39640;&#39118;&#38505;&#25968;&#25454;&#32780;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#25903;&#25345;&#21477;&#32423;&#25968;&#25454;&#24402;&#23646;&#65292;&#24182;&#20351;&#25968;&#25454;&#29983;&#20135;&#32773;&#21487;&#20197;&#36890;&#36807;&#20174;&#23384;&#20648;&#20013;&#21024;&#38500;&#20869;&#23481;&#26469;&#36873;&#25321;&#36864;&#20986;&#27169;&#22411;&#12290;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#20419;&#36827;&#23545;&#25968;&#25454;&#20351;&#29992;&#35268;&#33539;&#30340;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;LSTM&#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#32929;&#31080;&#26410;&#26469;&#20215;&#26684;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04419</link><description>&lt;p&gt;
&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#65306;&#22522;&#20110;&#28151;&#21512;LSTM&#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach. (arXiv:2308.04419v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;LSTM&#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#32929;&#31080;&#26410;&#26469;&#20215;&#26684;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#24066;&#26159;&#26368;&#20196;&#20154;&#30528;&#36855;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#19968;&#65292;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21487;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#22312;&#27491;&#30830;&#30340;&#26102;&#38388;&#20570;&#20986;&#26368;&#20339;&#20915;&#31574;&#20174;&#32780;&#33719;&#21033;&#12290;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#37329;&#34701;&#24066;&#22330;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#32929;&#24066;&#21463;&#20004;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#19968;&#26041;&#38754;&#26159;&#22320;&#32536;&#25919;&#27835;&#12289;&#31038;&#20250;&#21644;&#20840;&#29699;&#20107;&#20214;&#65292;&#36825;&#20123;&#20107;&#20214;&#21487;&#33021;&#24433;&#21709;&#20215;&#26684;&#36235;&#21183;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31532;&#20108;&#20010;&#26041;&#38754;&#32431;&#31929;&#20851;&#27880;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#12290;&#26412;&#25991;&#26088;&#22312;&#19987;&#27880;&#20110;&#31532;&#20108;&#20010;&#26041;&#38754;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#20197;&#26368;&#23567;&#35823;&#24046;&#39044;&#27979;&#26410;&#26469;&#20215;&#26684;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#22909;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046; (LSTM-SSAM) &#30340;&#26032;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#32929;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65306;SBIN&#12289;HDFCBANK &#21644; BANKBARODA&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04412</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26082;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21448;&#33021;&#20445;&#25345;&#20219;&#21153;&#24050;&#30693;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21464;&#24615;&#21644;&#35745;&#31639;&#25110;&#20869;&#23384;&#36164;&#28304;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;&#24615;&#35774;&#35745;&#26082;&#20855;&#34920;&#36798;&#33021;&#21147;&#21448;&#20855;&#19981;&#21464;&#24615;&#20294;&#20351;&#29992;&#26356;&#23569;&#36164;&#28304;&#30340;&#27169;&#22411;&#12290;&#21463;&#38543;&#26426;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120; (RLCs) &#30340;&#20108;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#21442;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;RLCs &#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#36924;&#36817;&#20219;&#20309;&#65288;&#24179;&#28369;&#65289;&#20989;&#25968;&#65292;&#24182;&#20445;&#25345;&#23545;&#32039;&#33268;&#32676;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#21487;&#39564;&#35777;&#22320;&#27010;&#29575;&#19981;&#21464;&#30340; RLCs&#65292;&#29992;&#20110;&#38598;&#21512;&#12289;&#22270;&#21644;&#29699;&#24418;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23454;&#29616;&#27010;&#29575;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#21033;&#28070;&#20998;&#20139;&#38382;&#39064;&#65292;&#20026;&#19968;&#33324;&#31867;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#25551;&#36848;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2308.04399</link><description>&lt;p&gt;
Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models. (arXiv:2308.04399v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#21033;&#28070;&#20998;&#20139;&#38382;&#39064;&#65292;&#20026;&#19968;&#33324;&#31867;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#25551;&#36848;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#24320;&#21457;&#21644;&#21457;&#24067;&#36890;&#29992;&#27169;&#22411;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#30001;&#20854;&#20182;&#20225;&#19994;&#21644;&#26426;&#26500;&#36827;&#34892;&#36866;&#24212;&#65292;&#20197;&#25191;&#34892;&#29305;&#23450;&#30340;&#39046;&#22495;&#19987;&#29992;&#21151;&#33021;&#12290;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#36866;&#24212;&#25110;&#24494;&#35843;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20301;&#36890;&#29992;&#19987;&#23478;&#23558;&#25216;&#26415;&#20135;&#21697;&#65288;&#21363;ML&#27169;&#22411;&#65289;&#25552;&#21319;&#21040;&#19968;&#23450;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#19968;&#20301;&#25110;&#22810;&#20301;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#35843;&#25972;&#36866;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#36825;&#20004;&#20010;&#23454;&#20307;&#37117;&#26159;&#36861;&#27714;&#21033;&#28070;&#30340;&#65292;&#24403;&#20182;&#20204;&#25237;&#36164;&#20110;&#25216;&#26415;&#26102;&#20250;&#20135;&#29983;&#25104;&#26412;&#65292;&#22312;&#25216;&#26415;&#36827;&#20837;&#24066;&#22330;&#21069;&#65292;&#20182;&#20204;&#24517;&#39035;&#23601;&#22914;&#20309;&#20998;&#20139;&#25910;&#20837;&#36798;&#25104;&#35848;&#21028;&#21327;&#35758;&#12290;&#23545;&#20110;&#30456;&#23545;&#19968;&#33324;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;&#24494;&#35843;&#21338;&#24328;&#20135;&#29983;&#21033;&#28070;&#20998;&#20139;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20219;&#20309;&#28508;&#22312;&#30340;&#39046;&#22495;&#19987;&#19994;&#21270;&#37117;&#20250;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Major advances in Machine Learning (ML) and Artificial Intelligence (AI) increasingly take the form of developing and releasing general-purpose models. These models are designed to be adapted by other businesses and agencies to perform a particular, domain-specific function. This process has become known as adaptation or fine-tuning. This paper offers a model of the fine-tuning process where a Generalist brings the technological product (here an ML model) to a certain level of performance, and one or more Domain-specialist(s) adapts it for use in a particular domain. Both entities are profit-seeking and incur costs when they invest in the technology, and they must reach a bargaining agreement on how to share the revenue for the technology to reach the market. For a relatively general class of cost and revenue functions, we characterize the conditions under which the fine-tuning game yields a profit-sharing solution. We observe that any potential domain-specialization will either contri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2308.04396</link><description>&lt;p&gt;
&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#25366;&#25496;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#20449;&#24687;&#31995;&#32479;&#30340;&#20107;&#20214;&#26085;&#24535;&#20013;&#21457;&#29616;&#27969;&#31243;&#27169;&#22411;&#12290;&#27969;&#31243;&#25366;&#25496;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#38754;&#21521;&#27969;&#31243;&#30340;&#20225;&#19994;&#31995;&#32479;&#65292;&#20294;&#23545;&#20110;&#38754;&#21521;&#36890;&#20449;&#21644;&#25991;&#26723;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#65288;ECS&#65289;&#26469;&#35828;&#19981;&#22826;&#36866;&#29992;&#12290;ECS&#20107;&#20214;&#26085;&#24535;&#38750;&#24120;&#32454;&#31890;&#24230;&#65292;&#23545;&#20854;&#26085;&#24535;&#24212;&#29992;&#27969;&#31243;&#25366;&#25496;&#20250;&#23548;&#33268;&#28151;&#20081;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20107;&#20214;&#25277;&#35937;&#65292;&#21363;&#22312;&#36816;&#34892;&#21457;&#29616;&#31639;&#27861;&#20043;&#21069;&#23558;&#20302;&#32423;&#21035;&#26085;&#24535;&#36716;&#25442;&#20026;&#26356;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26082;&#26377;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;ECS&#26085;&#24535;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23450;&#21046;&#30340;ECS&#20107;&#20214;&#25277;&#35937;&#65288;ECSEA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#35760;&#24405;&#30340;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#65288;&#39640;&#32423;&#21035;&#36319;&#36394;&#65289;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#65288;&#20174;ECS&#20013;&#25552;&#21462;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#21160;&#23558;&#26410;&#26469;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#24110;&#21161;&#20154;&#31867;&#20998;&#26512;&#23457;&#26597;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#65292;&#20943;&#23569;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#20915;&#31574;&#30340;&#36136;&#37327;&#21644;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.04375</link><description>&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#35299;&#37322;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20020;&#24202;&#20915;&#31574;&#20013;&#20449;&#20219;&#21644;&#20381;&#36182;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. (arXiv:2308.04375v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#24110;&#21161;&#20154;&#31867;&#20998;&#26512;&#23457;&#26597;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#65292;&#20943;&#23569;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#20915;&#31574;&#30340;&#36136;&#37327;&#21644;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36234;&#26469;&#36234;&#34987;&#32771;&#34385;&#29992;&#20110;&#36741;&#21161;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#22914;&#20581;&#24247;&#65289;&#20013;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#35752;&#35770;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20154;&#31867;&#21487;&#33021;&#20250;&#36807;&#24230;&#20381;&#36182;&#38169;&#35823;&#30340;AI&#27169;&#22411;&#24314;&#35758;&#65292;&#32780;&#38750;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#34917;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26174;&#33879;&#29305;&#24449;&#35299;&#37322;&#21644;&#20551;&#35774;&#24615;&#22240;&#26524;&#35299;&#37322;&#65292;&#20351;&#20154;&#31867;&#33021;&#26356;&#20998;&#26512;&#22320;&#23457;&#26597;AI&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#23545;AI&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#35299;&#37322;&#23545;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#20449;&#20219;&#21644;&#20381;&#36182;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19971;&#20301;&#27835;&#30103;&#24072;&#21644;&#21313;&#20301;&#38750;&#19987;&#19994;&#20154;&#22763;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#20219;&#21153;&#26159;&#35780;&#20272;&#20013;&#39118;&#21518;&#24184;&#23384;&#32773;&#30340;&#36816;&#21160;&#36136;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#20182;&#20204;&#30340;&#34920;&#29616;&#12289;&#20219;&#21153;&#19978;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#20197;&#21450;&#22312;&#26080;&#35299;&#37322;&#21644;&#26377;&#20004;&#31181;AI&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#23545;AI&#30340;&#20381;&#36182;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20855;&#26377;&#26174;&#33879;&#29305;&#24449;&#21644;&#22240;&#26524;&#35299;&#37322;&#30340;AI&#27169;&#22411;&#24110;&#21161;&#27835;&#30103;&#24072;&#21644;&#38750;&#19987;&#19994;&#20154;&#22763;&#25913;&#21892;&#20102;&#20182;&#20204;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#23454;&#20363;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#35770;&#35777;&#23454;&#20363;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#65292;&#24182;&#32771;&#34385;&#21040;&#35770;&#35777;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#35770;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04372</link><description>&lt;p&gt;
&#29992;&#36923;&#36753;&#35770;&#35777;&#23454;&#20363;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#33509;&#24178;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
Some Options for Instantiation of Bipolar Argument Graphs with Deductive Arguments. (arXiv:2308.04372v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#23454;&#20363;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#35770;&#35777;&#23454;&#20363;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#65292;&#24182;&#32771;&#34385;&#21040;&#35770;&#35777;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#35770;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35770;&#35777;&#24773;&#22659;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#21452;&#26497;&#35770;&#35777;&#22270;&#26159;&#19968;&#20010;&#26377;&#21521;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#19968;&#20010;&#35770;&#35777;&#65292;&#27599;&#20010;&#24359;&#34920;&#31034;&#19968;&#20010;&#35770;&#35777;&#23545;&#21478;&#19968;&#20010;&#35770;&#35777;&#30340;&#24433;&#21709;&#12290;&#22312;&#21452;&#26497;&#35770;&#35777;&#22270;&#20013;&#65292;&#27599;&#20010;&#35770;&#35777;&#37117;&#26159;&#21407;&#23376;&#30340;&#65292;&#27809;&#26377;&#20869;&#37096;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20010;&#20307;&#35770;&#35777;&#30340;&#24615;&#36136;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#32467;&#26500;&#21313;&#20998;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#20363;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#21487;&#33021;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#32771;&#34385;&#21040;&#35770;&#35777;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#35770;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argument graphs provide an abstract representation of an argumentative situation. A bipolar argument graph is a directed graph where each node denotes an argument, and each arc denotes the influence of one argument on another. Here we assume that the influence is supporting, attacking, or ambiguous. In a bipolar argument graph, each argument is atomic and so it has no internal structure. Yet to better understand the nature of the individual arguments, and how they interact, it is important to consider their internal structure. To address this need, this paper presents a framework based on the use of logical arguments to instantiate bipolar argument graphs, and a set of possible constraints on instantiating arguments that take into account the internal structure of the arguments, and the types of relationship between arguments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.04371</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32047;&#31215;&#25512;&#29702;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#38656;&#35201;&#28145;&#24605;&#29087;&#34385;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27492;&#21482;&#26377;&#26368;&#23567;&#31243;&#24230;&#30340;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#65292;&#23427;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#20351;&#20854;&#26356;&#26131;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#12290;&#23545;&#20110;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;CR&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22810;&#36798;9.3&#65285;&#65292;&#24182;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;FOLIO&#32500;&#22522;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24778;&#20154;&#30340;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;24&#28857;&#28216;&#25103;&#30340;&#32972;&#26223;&#19979;&#65292;CR&#23454;&#29616;&#20102;94&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\%, and achieves the astonishing accuracy of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\%, which signifies a substantial enhancement of 20\% over the previous state-of-the-art method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26222;&#36890;&#33181;&#20851;&#33410;X&#20809;&#29255;&#37325;&#26032;&#23457;&#35270;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33181;&#20851;&#33410;&#39592;&#35299;&#21078;&#32467;&#26500;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20986;&#21487;&#35265;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#30830;&#20445;&#20844;&#27491;&#21644;&#26080;&#20559;&#35265;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#20419;&#36827;&#20102;&#22810;&#26679;&#21270;&#24739;&#32773;&#20154;&#32676;&#33719;&#24471;&#20934;&#30830;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#30340;&#24179;&#31561;&#26426;&#20250;&#65292;&#25512;&#21160;&#20102;&#20844;&#24179;&#21644;&#21253;&#23481;&#30340;&#21307;&#30103;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2308.04356</link><description>&lt;p&gt;
&#23398;&#20064;&#26080;&#20559;&#35265;&#30340;&#22270;&#20687;&#20998;&#21106;&#65306;&#20197;&#26222;&#36890;&#33181;&#20851;&#33410;X&#20809;&#29255;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs. (arXiv:2308.04356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26222;&#36890;&#33181;&#20851;&#33410;X&#20809;&#29255;&#37325;&#26032;&#23457;&#35270;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33181;&#20851;&#33410;&#39592;&#35299;&#21078;&#32467;&#26500;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20986;&#21487;&#35265;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#30830;&#20445;&#20844;&#27491;&#21644;&#26080;&#20559;&#35265;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#20419;&#36827;&#20102;&#22810;&#26679;&#21270;&#24739;&#32773;&#20154;&#32676;&#33719;&#24471;&#20934;&#30830;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#30340;&#24179;&#31561;&#26426;&#20250;&#65292;&#25512;&#21160;&#20102;&#20844;&#24179;&#21644;&#21253;&#23481;&#30340;&#21307;&#30103;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39592;&#31185;&#23398;&#20013;&#65292;&#33258;&#21160;&#20998;&#21106;&#33181;&#20851;&#33410;&#39592;&#37096;&#35299;&#21078;&#32467;&#26500;&#21313;&#20998;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#26415;&#21069;&#21644;&#26415;&#21518;&#29615;&#22659;&#20013;&#24050;&#32463;&#26377;&#20102;&#20960;&#24180;&#30340;&#24212;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#28508;&#22312;&#20559;&#35265;&#30340;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#37325;&#26032;&#23457;&#35270;&#20351;&#29992;&#24120;&#35268;X&#20809;&#29255;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#33181;&#20851;&#33410;&#39592;&#35299;&#21078;&#32467;&#26500;&#20998;&#21106;&#65292;&#20197;&#25581;&#31034;&#21487;&#35265;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#20010;&#25512;&#36827;&#25105;&#20204;&#23545;&#20559;&#35265;&#30340;&#29702;&#35299;&#30340;&#26426;&#20250;&#65292;&#24182;&#20026;&#21307;&#23398;&#24433;&#20687;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35265;&#35299;&#12290;&#25552;&#20986;&#30340;&#32531;&#35299;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#30830;&#20445;&#20844;&#27491;&#21644;&#26080;&#20559;&#35265;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20419;&#36827;&#20102;&#22810;&#26679;&#21270;&#24739;&#32773;&#20154;&#32676;&#33719;&#24471;&#20934;&#30830;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#30340;&#24179;&#31561;&#26426;&#20250;&#65292;&#25512;&#21160;&#20102;&#20844;&#24179;&#21644;&#21253;&#23481;&#30340;&#21307;&#30103;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic segmentation of knee bony anatomy is essential in orthopedics, and it has been around for several years in both pre-operative and post-operative settings. While deep learning algorithms have demonstrated exceptional performance in medical image analysis, the assessment of fairness and potential biases within these models remains limited. This study aims to revisit deep learning-powered knee-bony anatomy segmentation using plain radiographs to uncover visible gender and racial biases. The current contribution offers the potential to advance our understanding of biases, and it provides practical insights for researchers and practitioners in medical imaging. The proposed mitigation strategies mitigate gender and racial biases, ensuring fair and unbiased segmentation results. Furthermore, this work promotes equal access to accurate diagnoses and treatment outcomes for diverse patient populations, fostering equitable and inclusive healthcare provision.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Retinaface&#30340;&#31639;&#27861;LAFD&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20934;&#30830;&#30340;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;Retinaface&#21644;LFFD&#65292;&#22312;WIDERFACE&#25968;&#25454;&#38598;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#65292;&#33021;&#22815;&#33719;&#24471;&#39640;&#36798;94.1%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04340</link><description>&lt;p&gt;
&#22522;&#20110;Retinaface&#30340;&#36731;&#37327;&#32423;&#20934;&#30830;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lightweight and Accurate Face Detection Algorithm Based on Retinaface. (arXiv:2308.04340v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04340
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Retinaface&#30340;&#31639;&#27861;LAFD&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20934;&#30830;&#30340;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;Retinaface&#21644;LFFD&#65292;&#22312;WIDERFACE&#25968;&#25454;&#38598;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#65292;&#33021;&#22815;&#33719;&#24471;&#39640;&#36798;94.1%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Retinaface&#30340;&#36731;&#37327;&#32423;&#20934;&#30830;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;LAFD&#65288;&#36731;&#37327;&#32423;&#20934;&#30830;&#20154;&#33080;&#26816;&#27979;&#65289;&#12290;&#31639;&#27861;&#20013;&#30340;&#20027;&#24178;&#32593;&#32476;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;MobileNetV3&#32593;&#32476;&#65292;&#35843;&#25972;&#20102;&#21367;&#31215;&#26680;&#30340;&#22823;&#23567;&#12289;&#20498;&#32622;&#27531;&#24046;&#22359;&#30340;&#36890;&#36947;&#25193;&#23637;&#20056;&#25968;&#20197;&#21450;SE&#27880;&#24847;&#26426;&#21046;&#30340;&#20351;&#29992;&#12290;&#21464;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;DCN&#65289;&#34987;&#24341;&#20837;&#21040;&#19978;&#19979;&#25991;&#27169;&#22359;&#20013;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;&#28966;&#28857;&#25439;&#22833;&#20989;&#25968;&#32780;&#19981;&#26159;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;WIDERFACE&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;LAFD&#22312;&#8220;&#23481;&#26131;&#8221;&#12289;&#8220;&#20013;&#31561;&#8221;&#21644;&#8220;&#22256;&#38590;&#8221;&#39564;&#35777;&#23376;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;94.1%&#12289;92.2%&#21644;82.1%&#65292;&#30456;&#27604;Retinaface&#20998;&#21035;&#25552;&#39640;&#20102;3.4%&#12289;4.0%&#21644;8.3%&#65292;&#27604;&#24615;&#33021;&#33391;&#22909;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;LFFD&#20998;&#21035;&#25552;&#39640;&#20102;3.1%&#12289;4.1%&#21644;4.1%&#12290;&#22914;&#26524;&#23558;&#36755;&#20837;&#22270;&#20687;&#39044;&#22788;&#29702;&#24182;&#32553;&#25918;&#21040;&#38271;&#24230;&#20026;1560&#20687;&#32032;&#25110;&#23485;&#24230;&#20026;1200&#20687;&#32032;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a lightweight and accurate face detection algorithm LAFD (Light and accurate face detection) based on Retinaface. Backbone network in the algorithm is a modified MobileNetV3 network which adjusts the size of the convolution kernel, the channel expansion multiplier of the inverted residuals block and the use of the SE attention mechanism. Deformable convolution network(DCN) is introduced in the context module and the algorithm uses focal loss function instead of cross-entropy loss function as the classification loss function of the model. The test results on the WIDERFACE dataset indicate that the average accuracy of LAFD is 94.1%, 92.2% and 82.1% for the "easy", "medium" and "hard" validation subsets respectively with an improvement of 3.4%, 4.0% and 8.3% compared to Retinaface and 3.1%, 4.1% and 4.1% higher than the well-performing lightweight model, LFFD. If the input image is pre-processed and scaled to 1560px in length or 1200px in width, the model achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#29642;&#29786;&#30977;&#30340;&#25439;&#20260;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#21644;&#21306;&#20998;&#20581;&#24247;&#29642;&#29786;&#21644;&#30333;&#21270;&#29642;&#29786;&#30340;&#35270;&#35273;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.04337</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#29642;&#29786;&#30977;&#25439;&#20260;&#26816;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra. (arXiv:2308.04337v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#29642;&#29786;&#30977;&#30340;&#25439;&#20260;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#21644;&#21306;&#20998;&#20581;&#24247;&#29642;&#29786;&#21644;&#30333;&#21270;&#29642;&#29786;&#30340;&#35270;&#35273;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#23612;&#35199;&#20122;&#27700;&#22495;&#20016;&#23500;&#30340;&#29642;&#29786;&#30977;&#29983;&#29289;&#22810;&#26679;&#24615;&#26159;&#19968;&#39033;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#38656;&#35201;&#24471;&#21040;&#20445;&#25252;&#12290;&#24555;&#36895;&#30340;&#27668;&#20505;&#21464;&#21270;&#21644;&#19981;&#21463;&#25511;&#21046;&#30340;&#20154;&#31867;&#27963;&#21160;&#23548;&#33268;&#20102;&#29642;&#29786;&#30977;&#29983;&#24577;&#31995;&#32479;&#30340;&#36864;&#21270;&#65292;&#21253;&#25324;&#29642;&#29786;&#30333;&#21270;&#65292;&#36825;&#26159;&#29642;&#29786;&#20581;&#24247;&#29366;&#20917;&#30340;&#19968;&#20010;&#20851;&#38190;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#20581;&#24247;&#29642;&#29786;&#21644;&#32463;&#21382;&#30333;&#21270;&#30340;&#29642;&#29786;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;Flickr&#25910;&#38598;&#30340;923&#20010;&#22270;&#20687;&#65292;&#20351;&#29992;Flickr API&#36827;&#34892;&#26816;&#32034;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#20581;&#24247;&#29642;&#29786;&#65288;438&#20010;&#22270;&#20687;&#65289;&#21644;&#30333;&#21270;&#29642;&#29786;&#65288;485&#20010;&#22270;&#20687;&#65289;&#12290;&#36825;&#20123;&#22270;&#20687;&#24050;&#34987;&#35843;&#25972;&#22823;&#23567;&#65292;&#23485;&#24230;&#25110;&#39640;&#24230;&#26368;&#22823;&#20026;300&#20687;&#32032;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#33268;&#23610;&#23544;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26469;&#35782;&#21035;&#21644;&#21306;&#20998;&#30456;&#20851;&#30340;&#35270;&#35273;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundant biodiversity of coral reefs in Indonesian waters is a valuable asset that needs to be preserved. Rapid climate change and uncontrolled human activities have led to the degradation of coral reef ecosystems, including coral bleaching, which is a critical indicator of coral health conditions. Therefore, this research aims to develop an accurate classification model to distinguish between healthy corals and corals experiencing bleaching. This study utilizes a specialized dataset consisting of 923 images collected from Flickr using the Flickr API. The dataset comprises two distinct classes: healthy corals (438 images) and bleached corals (485 images). These images have been resized to a maximum of 300 pixels in width or height, whichever is larger, to maintain consistent sizes across the dataset.  The method employed in this research involves the use of machine learning models, particularly convolutional neural networks (CNN), to recognize and differentiate visual patterns asso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.04314</link><description>&lt;p&gt;
&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#36172;&#21338;&#26426;&#65306;&#20855;&#26377;&#26368;&#20339;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#36890;&#20449;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs. (arXiv:2308.04314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#32452;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#21512;&#20316;&#29609;&#30456;&#21516;&#30340;&#22810;&#33218;&#36172;&#21338;&#28216;&#25103;&#12290;&#30446;&#26631;&#26159;&#24320;&#21457;&#20855;&#26377;&#26368;&#20339;&#32676;&#20307;&#21644;&#20010;&#20307;&#36951;&#25022;&#20197;&#21450;&#26234;&#33021;&#20307;&#20043;&#38388;&#36890;&#20449;&#25104;&#26412;&#20302;&#30340;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#22312;&#21069;&#26399;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#21644;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#22312;&#36825;&#20004;&#31181;&#33539;&#24335;&#20013;&#65292;&#20197;&#21069;&#30340;&#31639;&#27861;&#37117;&#33021;&#36798;&#21040;&#26368;&#20339;&#32676;&#20307;&#36951;&#25022;&#12290;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31639;&#27861;&#23454;&#29616;&#20102;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#26368;&#20339;&#20010;&#20307;&#36951;&#25022;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20339;&#20010;&#20307;&#36951;&#25022;&#65292;&#20294;&#26410;&#33021;&#23454;&#29616;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36890;&#20449;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#21512;&#20316;&#36172;&#21338;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#26368;&#20248;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
&lt;/p&gt;</description></item><item><title>&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04313</link><description>&lt;p&gt;
Apple Vision Pro for Healthcare: &#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#65311;&#65288;arXiv:2308.04313v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Apple Vision Pro for Healthcare: "The Ultimate Display"?. (arXiv:2308.04313v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04313
&lt;/p&gt;
&lt;p&gt;
&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;6&#26376;&#30340;&#20840;&#29699;&#24320;&#21457;&#32773;&#22823;&#20250;&#65288;WWDC&#65289;&#19978;&#65292;&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#12290;Vision Pro&#26159;&#19968;&#27454;&#28151;&#21512;&#29616;&#23454;&#65288;MR&#65289;&#22836;&#30420;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23427;&#26159;&#19968;&#27454;&#20855;&#26377;&#39069;&#22806;&#35270;&#39057;&#36879;&#35270;&#65288;VST&#65289;&#33021;&#21147;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#12290;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#36890;&#36807;&#25668;&#20687;&#22836;&#20256;&#36755;&#21040;&#29992;&#25143;&#30524;&#21069;&#30340;&#65288;VR&#65289;&#23631;&#24149;&#65292;&#20351;&#24471;Vision Pro&#20063;&#25104;&#20026;&#20102;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#35774;&#22791;&#12290;&#24403;&#28982;&#65292;&#36825;&#24182;&#19981;&#29420;&#29305;&#65292;&#19982;Varjo XR-3&#31561;&#20854;&#20182;&#35774;&#22791;&#31867;&#20284;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;Vision Pro&#20855;&#26377;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#21487;&#20197;&#21521;&#8220;&#22806;&#30028;&#8221;&#26174;&#31034;&#20329;&#25140;&#22836;&#30420;&#32773;&#30340;&#30524;&#30555;&#65292;&#25110;&#32773;&#39030;&#37096;&#30340;&#19968;&#20010;&#25353;&#38062;&#31216;&#20026;&#8220;&#25968;&#23383;&#30343;&#20896;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26059;&#36716;&#26080;&#32541;&#22320;&#34701;&#21512;&#25968;&#23383;&#20869;&#23481;&#19982;&#29289;&#29702;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;Vision Pro&#26159;&#26080;&#32447;&#30340;&#65292;&#21482;&#26377;&#30005;&#27744;&#30340;&#30005;&#32518;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#22836;&#30420;&#27604;Varjo XR-3&#26356;&#21152;&#28789;&#27963;&#12290;&#36825;&#21487;&#33021;&#26356;&#25509;&#36817;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display",
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#23548;&#21521;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;&#20351;&#29992;INTERACTION&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04312</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#23548;&#21521;&#27169;&#22411;&#29992;&#20110;&#20132;&#20114;&#22330;&#26223;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios. (arXiv:2308.04312v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#23548;&#21521;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;&#20351;&#29992;INTERACTION&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#29702;&#35299;&#36710;&#36742;&#19982;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#34892;&#20026;&#24182;&#39044;&#27979;&#20854;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#26159;&#30830;&#20445;&#36947;&#36335;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#65292;&#31038;&#20132;&#20114;&#21160;&#24456;&#38590;&#35299;&#37322;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#24191;&#27867;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#24182;&#19988;&#24050;&#34987;&#35777;&#26126;&#20248;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;INTERACTION&#25968;&#25454;&#38598;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#35299;&#37322;&#39044;&#27979;&#26102;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities to understand the social interaction behaviors between a vehicle and its surroundings while predicting its trajectory in an urban environment are critical for road safety in autonomous driving. Social interactions are hard to explain because of their uncertainty. In recent years, neural network-based methods have been widely used for trajectory prediction and have been shown to outperform hand-crafted methods. However, these methods suffer from their lack of interpretability. In order to overcome this limitation, we combine the interpretability of a discrete choice model with the high accuracy of a neural network-based model for the task of vehicle trajectory prediction in an interactive environment. We implement and evaluate our model using the INTERACTION dataset and demonstrate the effectiveness of our proposed architecture to explain its predictions without compromising the accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#39564;&#20449;&#24687;&#21644;&#35821;&#20041;&#36741;&#21161;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#30340;&#26694;&#26550;&#26469;&#39044;&#27979;&#36710;&#36742;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#38745;&#24577;&#21644;&#21160;&#24577;&#36710;&#36742;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#35821;&#20041;&#26631;&#31614;&#21644;&#22320;&#22270;&#22312;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04303</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#39564;&#20449;&#24687;&#21644;&#35821;&#20041;&#36741;&#21161;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#36827;&#34892;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps. (arXiv:2308.04303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#39564;&#20449;&#24687;&#21644;&#35821;&#20041;&#36741;&#21161;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#30340;&#26694;&#26550;&#26469;&#39044;&#27979;&#36710;&#36742;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#38745;&#24577;&#21644;&#21160;&#24577;&#36710;&#36742;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#35821;&#20041;&#26631;&#31614;&#21644;&#22320;&#22270;&#22312;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12289;&#26410;&#26469;&#30340;&#38750;&#30830;&#23450;&#24615;&#21644;&#20195;&#29702;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#65292;&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#21160;&#24577;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#65288;DOGMs&#65289;&#65292;&#23558;&#35821;&#20041;&#26631;&#31614;&#19982;&#21344;&#25454;&#30340;&#21333;&#20803;&#26684;&#20851;&#32852;&#65292;&#24182;&#32467;&#21512;&#22320;&#22270;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#21644;&#27010;&#29575;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#36710;&#36742;&#34892;&#20026;&#12290;&#19982;&#20256;&#32479;&#30340;OGM&#39044;&#27979;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#26681;&#25454;&#30495;&#23454;&#27880;&#37322;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;NuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#38745;&#24577;&#21644;&#21160;&#24577;&#36710;&#36742;&#26041;&#38754;&#19982;OGM&#39044;&#27979;&#30456;&#27604;&#20855;&#26377;&#20248;&#36234;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35821;&#20041;&#26631;&#31614;&#21644;&#22320;&#22270;&#22312;&#26550;&#26500;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion prediction is a challenging task for autonomous vehicles due to uncertainty in the sensor data, the non-deterministic nature of future, and complex behavior of agents. In this paper, we tackle this problem by representing the scene as dynamic occupancy grid maps (DOGMs), associating semantic labels to the occupied cells and incorporating map information. We propose a novel framework that combines deep-learning-based spatio-temporal and probabilistic approaches to predict vehicle behaviors.Contrary to the conventional OGM prediction methods, evaluation of our work is conducted against the ground truth annotations. We experiment and validate our results on real-world NuScenes dataset and show that our model shows superior ability to predict both static and dynamic vehicles compared to OGM predictions. Furthermore, we perform an ablation study and assess the role of semantic labels and map in the architecture.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SusACER&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25345;&#32493;&#21160;&#20316;&#65292;&#22312;&#21464;&#21270;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04299</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#19979;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#65306;&#36890;&#36807;&#25345;&#32493;&#21160;&#20316;&#26469;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic with variable time discretization via sustained actions. (arXiv:2308.04299v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SusACER&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25345;&#32493;&#21160;&#20316;&#65292;&#22312;&#21464;&#21270;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#31163;&#25955;&#26102;&#38388;&#19979;&#24037;&#20316;&#12290;&#20026;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20687;&#26426;&#22120;&#20154;&#25511;&#21046;&#36825;&#26679;&#30340;&#36830;&#32493;&#38382;&#39064;&#65292;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#12290;&#36825;&#26159;&#22312;&#31232;&#30095;&#26102;&#38388;&#25511;&#21046;&#21644;&#31934;&#32454;&#26102;&#38388;&#25511;&#21046;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#31232;&#30095;&#26102;&#38388;&#25511;&#21046;&#21487;&#33021;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#32780;&#31934;&#32454;&#26102;&#38388;&#25511;&#21046;&#21487;&#33021;&#20351;&#26368;&#32456;&#24615;&#33021;&#26356;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SusACER&#65292;&#19968;&#31181;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#26102;&#38388;&#31163;&#25955;&#21270;&#35774;&#32622;&#30340;&#20248;&#28857;&#12290;&#26368;&#21021;&#65292;&#23427;&#20351;&#29992;&#31232;&#30095;&#26102;&#38388;&#31163;&#25955;&#21270;&#65292;&#24182;&#36880;&#28176;&#20999;&#25442;&#21040;&#31934;&#32454;&#26102;&#38388;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#25913;&#21464;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#24433;&#21709;&#65306;Ant&#65292;HalfCheetah&#65292;Hopper&#21644;Walker2D&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) methods work in discrete time. In order to apply RL to inherently continuous problems like robotic control, a specific time discretization needs to be defined. This is a choice between sparse time control, which may be easier to train, and finer time control, which may allow for better ultimate performance. In this work, we propose SusACER, an off-policy RL algorithm that combines the advantages of different time discretization settings. Initially, it operates with sparse time discretization and gradually switches to a fine one. We analyze the effects of the changing time discretization in robotic control environments: Ant, HalfCheetah, Hopper, and Walker2D. In all cases our proposed algorithm outperforms state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;LaCAM*&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#25361;&#25112;&#12290;&#25913;&#36827;&#25216;&#26415;&#30340;&#34701;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;LaCAM*&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#25512;&#21160;&#20102;MAPF&#31639;&#27861;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.04292</link><description>&lt;p&gt;
&#24037;&#31243;&#21270;LaCAM$: &#23454;&#29616;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding. (arXiv:2308.04292v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;LaCAM*&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#25361;&#25112;&#12290;&#25913;&#36827;&#25216;&#26415;&#30340;&#34701;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;LaCAM*&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#25512;&#21160;&#20102;MAPF&#31639;&#27861;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;LaCAM*&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#30340;&#25361;&#25112;&#12290;LaCAM*&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#65292;&#23427;&#20445;&#35777;&#23545;&#20110;&#32047;&#35745;&#36716;&#31227;&#25104;&#26412;&#33021;&#22815;&#26368;&#32456;&#25214;&#21040;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#23427;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#29575;&#65292;&#36229;&#36807;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;MAPF&#26041;&#27861;&#65292;&#20294;&#20854;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#36828;&#38750;&#26368;&#20248;&#65292;&#24182;&#19988;&#20854;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20123;&#25913;&#36827;&#25216;&#26415;&#65292;&#37096;&#20998;&#20511;&#37492;&#20102;&#20854;&#20182;MAPF&#26041;&#27861;&#30340;&#28789;&#24863;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#36825;&#20123;&#25216;&#26415;&#30340;&#34701;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;LaCAM*&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;MAPF&#31639;&#27861;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenges of real-time, large-scale, and near-optimal multi-agent pathfinding (MAPF) through enhancements to the recently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm that guarantees the eventual finding of optimal solutions for cumulative transition costs. While it has demonstrated remarkable planning success rates, surpassing various state-of-the-art MAPF methods, its initial solution quality is far from optimal, and its convergence speed to the optimum is slow. To overcome these limitations, this paper introduces several improvement techniques, partly drawing inspiration from other MAPF methods. We provide empirical evidence that the fusion of these techniques significantly improves the solution quality of LaCAM*, thus further pushing the boundaries of MAPF algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#20102;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#30340;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#20854;&#21487;&#20197;&#19982;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04275</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#36827;&#34892;&#19978;&#19979;&#25991;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#20102;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#30340;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#20854;&#21487;&#20197;&#19982;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#35828;&#26126;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25506;&#32034;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32431;&#20928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; Llama-2&#65292;&#22312;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#20043;&#21069;&#65292;&#24403;&#27169;&#22411;&#34987;&#35201;&#27714;&#25353;&#29031;&#32842;&#22825;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#26816;&#32034;&#21040;&#20102;&#24179;&#22343;9&#20010;&#23545;&#40784;&#28436;&#31034;&#31034;&#20363;&#12290;&#19982;&#30452;&#25509;&#25552;&#31034;&#30456;&#27604;&#65292;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#23548;&#33268;&#20102;&#19982;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#30456;&#27604;&#65292;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#24471;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23218;&#32654;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26377;&#25439;&#21644;&#26080;&#25439;&#21387;&#32553;&#32467;&#21512;&#30340;&#35757;&#32451;&#21518;&#27169;&#22411;&#23610;&#23544;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#21442;&#25968;&#21270;&#26435;&#37325;&#21464;&#25442;&#21644;&#24341;&#20837;&#21487;&#24494;&#20998;&#35745;&#25968;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#39640;&#21387;&#32553;&#27604;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2308.04269</link><description>&lt;p&gt;
&#39640;&#21387;&#32553;&#27604;&#35757;&#32451;&#21518;&#27169;&#22411;&#23610;&#23544;&#21387;&#32553;&#30340;&#26377;&#25439;&#21644;&#26080;&#25439;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lossy and Lossless (L$^2$) Post-training Model Size Compression. (arXiv:2308.04269v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26377;&#25439;&#21644;&#26080;&#25439;&#21387;&#32553;&#32467;&#21512;&#30340;&#35757;&#32451;&#21518;&#27169;&#22411;&#23610;&#23544;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#21442;&#25968;&#21270;&#26435;&#37325;&#21464;&#25442;&#21644;&#24341;&#20837;&#21487;&#24494;&#20998;&#35745;&#25968;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#39640;&#21387;&#32553;&#27604;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#23610;&#23544;&#32473;&#20256;&#36755;&#21644;&#23384;&#20648;&#24102;&#26469;&#20102;&#24456;&#22823;&#19981;&#20415;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#27169;&#22411;&#23610;&#23544;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24120;&#24120;&#29420;&#31435;&#22320;&#22788;&#29702;&#21508;&#31181;&#26377;&#25439;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#27861;&#65292;&#23548;&#33268;&#22312;&#39640;&#25928;&#23454;&#29616;&#39640;&#21387;&#32553;&#27604;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26377;&#25439;&#21644;&#26080;&#25439;&#21387;&#32553;&#20197;&#32479;&#19968;&#26041;&#24335;&#32467;&#21512;&#30340;&#35757;&#32451;&#21518;&#27169;&#22411;&#23610;&#23544;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21442;&#25968;&#21270;&#26435;&#37325;&#21464;&#25442;&#65292;&#30830;&#20445;&#21487;&#20197;&#20197;&#35757;&#32451;&#21518;&#30340;&#26041;&#24335;&#32852;&#21512;&#25191;&#34892;&#19981;&#21516;&#30340;&#26377;&#25439;&#21387;&#32553;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#21487;&#24494;&#20998;&#35745;&#25968;&#22120;&#26469;&#24341;&#23548;&#26377;&#25439;&#21387;&#32553;&#30340;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#26356;&#36866;&#21512;&#21518;&#32493;&#26080;&#25439;&#21387;&#32553;&#30340;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25152;&#38656;&#30340;&#20840;&#23616;&#21387;&#32553;&#27604;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;&#23618;&#20998;&#37197;&#33258;&#36866;&#24212;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have delivered remarkable performance and have been widely used in various visual tasks. However, their huge size causes significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these studies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be performed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimization of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#24072;&#29983;&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24182;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30693;&#35782;&#21387;&#32553;&#12289;&#30693;&#35782;&#25193;&#23637;&#12289;&#30693;&#35782;&#36866;&#24212;&#21644;&#30693;&#35782;&#22686;&#24378;&#65292;&#36890;&#36807;&#36731;&#37327;&#21270;&#21644;&#36890;&#29992;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#23454;&#29616;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.04268</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#30340;&#24072;&#29983;&#26550;&#26500;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Architecture for Knowledge Distillation: A Survey. (arXiv:2308.04268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#24072;&#29983;&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24182;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30693;&#35782;&#21387;&#32553;&#12289;&#30693;&#35782;&#25193;&#23637;&#12289;&#30693;&#35782;&#36866;&#24212;&#21644;&#30693;&#35782;&#22686;&#24378;&#65292;&#36890;&#36807;&#36731;&#37327;&#21270;&#21644;&#36890;&#29992;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#23454;&#29616;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#21442;&#25968;&#24222;&#22823;&#65292;&#36825;&#31181;DNN&#24456;&#38590;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#20013;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24072;&#29983;&#26550;&#26500;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#23398;&#29983;&#32593;&#32476;&#21482;&#26377;&#23569;&#37327;&#21442;&#25968;&#65292;&#21364;&#33021;&#36798;&#21040;&#19982;&#25317;&#26377;&#35768;&#22810;&#21442;&#25968;&#30340;&#28145;&#24230;&#24072;&#20613;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#24072;&#29983;&#26550;&#26500;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20010;&#30446;&#26631;&#39046;&#22495;&#65292;&#21253;&#25324;&#30693;&#35782;&#21387;&#32553;&#12289;&#30693;&#35782;&#25193;&#23637;&#12289;&#30693;&#35782;&#36866;&#24212;&#21644;&#30693;&#35782;&#22686;&#24378;&#12290;&#20511;&#21161;&#24072;&#29983;&#26550;&#26500;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#33021;&#22815;&#36890;&#36807;&#36731;&#37327;&#21270;&#21644;&#36890;&#29992;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#23454;&#29616;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#12290;&#19982;&#29616;&#26377;&#30340;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#21387;&#32553;&#30340;&#33976;&#39311;&#35843;&#26597;&#19981;&#21516;&#65292;&#26412;&#35843;&#26597;&#39318;&#27425;&#25506;&#35752;&#20102;&#36328;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#30340;&#24072;&#29983;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This surv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#32418;&#38431;&#34892;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#21644;&#32972;&#26223;&#23398;&#20064;&#26469;&#35780;&#20272;&#21644;&#26292;&#38706;&#29983;&#25104;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#29305;&#21035;&#26159;&#23545;&#19981;&#23433;&#20840;&#21644;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#32972;&#26223;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#23398;&#20064;&#20986;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#25552;&#31034;&#12290;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#22312;&#26292;&#38706;&#28431;&#27934;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#22686;&#21152;&#23433;&#20840;&#21151;&#33021;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#21457;&#29616;&#28431;&#27934;&#12290;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#20110;&#32418;&#38431;&#34892;&#21160;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04265</link><description>&lt;p&gt;
FLIRT: &#21453;&#39304;&#24490;&#29615;&#32972;&#26223;&#19979;&#30340;&#32418;&#38431;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#32418;&#38431;&#34892;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#21644;&#32972;&#26223;&#23398;&#20064;&#26469;&#35780;&#20272;&#21644;&#26292;&#38706;&#29983;&#25104;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#29305;&#21035;&#26159;&#23545;&#19981;&#23433;&#20840;&#21644;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#32972;&#26223;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#23398;&#20064;&#20986;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#25552;&#31034;&#12290;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#22312;&#26292;&#38706;&#28431;&#27934;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#22686;&#21152;&#23433;&#20840;&#21151;&#33021;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#21457;&#29616;&#28431;&#27934;&#12290;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#20110;&#32418;&#38431;&#34892;&#21160;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#35770;&#25991;&#20869;&#23481;&#21487;&#33021;&#19981;&#21512;&#36866;&#25110;&#20882;&#29359;&#20154;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#65292;&#27979;&#35797;&#21644;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#30340;&#28431;&#27934;&#24050;&#25104;&#20026;&#19968;&#39033;&#20248;&#20808;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#32418;&#38431;&#34892;&#21160;&#26694;&#26550;&#65292;&#23545;&#32473;&#23450;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#26292;&#38706;&#20854;&#23545;&#19981;&#23433;&#20840;&#21644;&#19981;&#36866;&#24403;&#20869;&#23481;&#29983;&#25104;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#20013;&#30340;&#32972;&#26223;&#23398;&#20064;&#26469;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#65292;&#24182;&#28608;&#21457;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#32972;&#26223;&#25915;&#20987;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#21160;&#23398;&#20064;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#26292;&#38706;&#31283;&#23450;&#25193;&#25955;(SD)&#27169;&#22411;&#30340;&#28431;&#27934;&#26102;&#26174;&#33879;&#26356;&#26377;&#25928;&#65292;&#21363;&#20351;&#21518;&#32773;&#37319;&#29992;&#20102;&#23433;&#20840;&#21151;&#33021;&#30340;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23545;&#20110;&#32418;&#38431;&#34892;&#21160;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#20063;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: this paper contains content that may be inappropriate or offensive.  As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text mo
&lt;/p&gt;</description></item><item><title>MindDiffuser&#26159;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#23454;&#29616;&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2308.04249</link><description>&lt;p&gt;
MindDiffuser: &#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#25511;&#21046;&#22270;&#20687;&#37325;&#24314;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2308.04249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04249
&lt;/p&gt;
&lt;p&gt;
MindDiffuser&#26159;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#23454;&#29616;&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#30005;&#35760;&#24405;&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#26159;&#19968;&#20010;&#26377;&#24847;&#20041;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#22312;&#25512;&#21160;&#33041;&#26426;&#25509;&#21475;&#30340;&#36827;&#23637;&#21644;&#21033;&#29992;&#26041;&#38754;&#65292;&#23454;&#29616;&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#37325;&#24314;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#22797;&#26434;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#26377;&#20102;&#36827;&#23637;&#65292;&#20294;&#22914;&#20309;&#22312;&#22270;&#20687;&#21050;&#28608;&#20013;&#23454;&#29616;&#35821;&#20041;&#65288;&#27010;&#24565;&#21644;&#23545;&#35937;&#65289;&#21644;&#32467;&#26500;&#65288;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#22823;&#23567;&#65289;&#30340;&#32479;&#19968;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MindDiffuser&#30340;&#20004;&#38454;&#27573;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23558;&#20174;fMRI&#35299;&#30721;&#24471;&#21040;&#30340;VQ-VAE&#28508;&#22312;&#34920;&#31034;&#21644;CLIP&#25991;&#26412;&#23884;&#20837;&#20256;&#20837;&#31283;&#23450;&#25193;&#25955;&#65292;&#29983;&#25104;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#21021;&#27493;&#22270;&#20687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;fMRI&#35299;&#30721;&#24471;&#21040;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#20316;&#20026;&#30417;&#30563;&#20449;&#24687;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#19981;&#26029;&#35843;&#25972;&#31532;&#19968;&#38454;&#27573;&#35299;&#30721;&#24471;&#21040;&#30340;&#20004;&#20010;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing visual stimuli from brain recordings has been a meaningful and challenging task. Especially, the achievement of precise and controllable image reconstruction bears great significance in propelling the progress and utilization of brain-computer interfaces. Despite the advancements in complex image reconstruction techniques, the challenge persists in achieving a cohesive alignment of both semantic (concepts and objects) and structure (position, orientation, and size) with the image stimuli. To address the aforementioned issue, we propose a two-stage image reconstruction model called MindDiffuser. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into Stable Diffusion, which yields a preliminary image that contains semantic information. In Stage 2, we utilize the CLIP visual feature decoded from fMRI as supervisory information, and continually adjust the two feature vectors decoded in Stage 1 through backpropagation to alig
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#23545;&#23450;&#20301;&#32467;&#26524;&#19982;&#23383;&#24149;&#36827;&#34892;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#20013;&#23383;&#24149;&#21644;&#25163;&#21183;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04248</link><description>&lt;p&gt;
&#20351;&#29992;&#35789;&#23884;&#20837;&#36827;&#34892;&#35789;&#27719;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#23545;&#23450;&#20301;&#32467;&#26524;&#19982;&#23383;&#24149;&#36827;&#34892;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#20013;&#23383;&#24149;&#21644;&#25163;&#21183;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25429;&#25417;&#21644;&#27880;&#37322;&#25163;&#35821;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#36828;&#36828;&#19981;&#36275;&#20197;&#25104;&#21151;&#35757;&#32451;&#26080;&#32422;&#26463;&#30340;&#25163;&#35821;&#32763;&#35793;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#24050;&#36716;&#21521;&#30005;&#35270;&#24191;&#25773;&#20869;&#23481;&#20316;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#30340;&#26469;&#28304;&#65292;&#21253;&#25324;&#25163;&#35821;&#32763;&#35793;&#32773;&#21644;&#30456;&#20851;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25163;&#35821;&#27880;&#37322;&#38480;&#21046;&#20102;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#23548;&#33268;&#20102;&#33258;&#21160;&#27880;&#37322;&#25216;&#26415;&#65288;&#22914;&#25163;&#35821;&#23450;&#20301;&#65289;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#23450;&#20301;&#19982;&#35270;&#39057;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#19982;&#23383;&#24149;&#23545;&#40784;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#23383;&#24149;&#21644;&#23450;&#20301;&#30340;&#25163;&#21183;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#23558;&#23450;&#20301;&#19982;&#20854;&#23545;&#24212;&#23383;&#24149;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#24320;&#38144;&#23567;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#23450;&#20301;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#30340;PCF&#26680;&#31639;&#26694;&#26550;AutoPCF&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#23454;&#29616;&#20135;&#21697;&#30899;&#36275;&#36857;&#30340;&#33258;&#21160;&#24314;&#27169;&#21644;&#20272;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04241</link><description>&lt;p&gt;
AutoPCF:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#20135;&#21697;&#30899;&#36275;&#36857;&#26680;&#31639;
&lt;/p&gt;
&lt;p&gt;
AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models. (arXiv:2308.04241v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#30340;PCF&#26680;&#31639;&#26694;&#26550;AutoPCF&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#23454;&#29616;&#20135;&#21697;&#30899;&#36275;&#36857;&#30340;&#33258;&#21160;&#24314;&#27169;&#21644;&#20272;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#30899;&#36275;&#36857;&#65288;PCF&#65289;&#23545;&#20110;&#20943;&#32531;&#20379;&#24212;&#38142;&#30340;&#30899;&#25490;&#25918;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#34913;&#37327;&#20102;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#26377;&#27963;&#21160;&#25152;&#23548;&#33268;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#37327;&#12290;&#28982;&#32780;&#65292;PCF&#26680;&#31639;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#26102;&#38388;&#26469;&#26500;&#24314;&#29983;&#21629;&#21608;&#26399;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#27604;&#36739;&#20102;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24314;&#27169;&#20135;&#21697;&#30340;&#8220;&#25671;&#31726;&#21040;&#22823;&#38376;&#8221;&#29983;&#21629;&#21608;&#26399;&#24182;&#29983;&#25104;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#24211;&#23384;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#19968;&#20010;&#24191;&#20041;PCF&#30693;&#35782;&#25968;&#25454;&#24211;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#30340;PCF&#26680;&#31639;&#26694;&#26550;&#65292;&#31216;&#20026;AutoPCF&#65292;&#35813;&#26694;&#26550;&#36824;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21305;&#37197;&#35745;&#31639;&#21442;&#25968;&#65292;&#24182;&#26368;&#32456;&#35745;&#31639;PCF&#12290;&#21033;&#29992;AutoPCF&#26694;&#26550;&#23545;&#19977;&#20010;&#26696;&#20363;&#20135;&#21697;&#30340;&#30899;&#36275;&#36857;&#36827;&#34892;&#20272;&#31639;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#23454;&#29616;PCF&#30340;&#33258;&#21160;&#24314;&#27169;&#21644;&#20272;&#31639;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The product carbon footprint (PCF) is crucial for decarbonizing the supply chain, as it measures the direct and indirect greenhouse gas emissions caused by all activities during the product's life cycle. However, PCF accounting often requires expert knowledge and significant time to construct life cycle models. In this study, we test and compare the emergent ability of five large language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products and generating the inventory data of inputs and outputs, revealing their limitations as a generalized PCF knowledge database. By utilizing LLMs, we propose an automatic AI-driven PCF accounting framework, called AutoPCF, which also applies deep learning algorithms to automatically match calculation parameters, and ultimately calculate the PCF. The results of estimating the carbon footprint for three case products using the AutoPCF framework demonstrate its potential in achieving automatic modeling and estimation of PCF with a large
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04237</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26381;&#21153;&#22120;&#24076;&#26395;&#26681;&#25454;&#27169;&#22411;&#23545;&#26032;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#12290;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#20849;&#21516;&#30340;&#26080;&#32447;&#20449;&#36947;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#24182;&#19988;&#21487;&#20197;&#35775;&#38382;&#20197;&#21069;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22914;&#26524;&#35774;&#22791;&#26080;&#27861;&#35775;&#38382;&#26032;&#36755;&#20837;&#65292;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#36136;&#37327;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65292;&#21033;&#29992;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26469;&#25552;&#39640;&#26381;&#21153;&#22120;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#32852;&#21512;CP&#20013;&#65292;&#35774;&#22791;&#21521;&#26381;&#21153;&#22120;&#20256;&#36882;&#20851;&#20110;&#26412;&#22320;&#25968;&#25454;&#19978;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25439;&#22833;&#30340;&#20449;&#24687;&#65292;&#26381;&#21153;&#22120;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26657;&#20934;&#19968;&#20010;&#20915;&#31574;&#21306;&#38388;&#65292;&#20197;&#20415;&#20854;&#22312;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#21487;&#38752;&#24615;&#27700;&#24179;&#19979;&#20445;&#35777;&#21253;&#21547;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04220</link><description>&lt;p&gt;
&#23545;GNN&#27169;&#22411;&#22522;&#20110;&#22270;Attention&#30340;&#35299;&#37322;&#30340;&#35821;&#20041;&#35299;&#37322;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#20041;&#20851;&#27880;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#30340;&#25200;&#21160;&#65292;&#24182;&#24314;&#31435;&#39044;&#27979;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22270;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24212;&#29992;&#20110;&#22330;&#26223;&#35299;&#37322;&#31561;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#28789;&#27963;&#30340;&#22270;&#32467;&#26500;&#26469;&#31616;&#27905;&#22320;&#25551;&#36848;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#20013;&#20351;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#32467;&#26500;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#22270;&#29305;&#23450;&#30340;&#26041;&#27861;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22240;&#27492;&#20808;&#21069;&#24050;&#32463;&#20351;&#29992;&#23427;&#20204;&#20026;GNN&#39044;&#27979;&#25552;&#20379;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#37322;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;Attention&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.04215</link><description>&lt;p&gt;
&#23454;&#26102;&#20316;&#26354;&#36741;&#21161;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#25552;&#21319;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#25972;&#21512;&#31169;&#20154;&#25968;&#25454;&#21644;&#20943;&#23569;&#24187;&#35273;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#30340;&#20219;&#21153;&#65288;&#22914;&#20316;&#26354;&#36741;&#21161;&#65289;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#22788;&#29702;&#26102;&#38388;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hybrid Retrieval-Augmented Generation (HybridRAG)&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#21644;&#20113;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#35774;&#32622;&#12290;HybridRAG&#36890;&#36807;&#24322;&#27493;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20113;&#31471;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#25928;&#30340;&#21709;&#24212;&#65292;&#20174;LLM&#30340;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24322;&#27493;&#20869;&#23384;&#38598;&#25104;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#23454;&#26102;&#21709;&#24212;&#29992;&#25143;&#35831;&#27714;&#65292;&#26080;&#38656;&#31561;&#24453;&#20113;&#31471;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;Datalog&#35299;&#37322;&#22120;&#30340;&#26448;&#26009;&#21270;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22686;&#37327;&#32500;&#25252;&#21644;&#24037;&#20316;&#20998;&#37197;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.04214</link><description>&lt;p&gt;
&#12298;&#19968;&#31181;&#24046;&#20998;Datalog&#35299;&#37322;&#22120;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Differential Datalog Interpreter. (arXiv:2308.04214v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;Datalog&#35299;&#37322;&#22120;&#30340;&#26448;&#26009;&#21270;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22686;&#37327;&#32500;&#25252;&#21644;&#24037;&#20316;&#20998;&#37197;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Datalog&#24341;&#25806;&#30340;&#26680;&#24515;&#25512;&#29702;&#20219;&#21153;&#26159;&#26448;&#26009;&#21270;&#65292;&#21363;&#22312;&#25968;&#25454;&#24211;&#20013;&#35780;&#20272;Datalog&#31243;&#24207;&#24182;&#23558;&#20854;&#29289;&#29702;&#32435;&#20837;&#25968;&#25454;&#24211;&#26412;&#36523;&#12290;&#35745;&#31639;&#23427;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#36882;&#24402;&#24212;&#29992;&#25512;&#29702;&#35268;&#21017;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#25805;&#20316;&#65292;Datalog&#24341;&#25806;&#24517;&#39035;&#25552;&#20379;&#22686;&#37327;&#26448;&#26009;&#21270;&#65292;&#21363;&#23558;&#35745;&#31639;&#35843;&#25972;&#21040;&#26032;&#25968;&#25454;&#19978;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#32570;&#38519;&#23601;&#26159;&#21024;&#38500;&#25968;&#25454;&#27604;&#28155;&#21152;&#25968;&#25454;&#22797;&#26434;&#24471;&#22810;&#65292;&#22240;&#20026;&#24517;&#39035;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#20174;&#27491;&#22312;&#21024;&#38500;&#30340;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#30340;&#25968;&#25454;&#12290;&#24046;&#20998;&#25968;&#25454;&#27969;&#26159;&#19968;&#31181;&#25552;&#20379;&#26377;&#25928;&#30340;&#22686;&#37327;&#32500;&#25252;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#23588;&#20854;&#22312;&#28155;&#21152;&#21644;&#21024;&#38500;&#20043;&#38388;&#25317;&#26377;&#30456;&#31561;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36845;&#20195;&#25968;&#25454;&#27969;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#20010;&#21442;&#32771;Datalog&#23454;&#29616;&#36827;&#34892;&#26448;&#26009;&#21270;&#30340;&#24615;&#33021;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core reasoning task for datalog engines is materialization, the evaluation of a datalog program over a database alongside its physical incorporation into the database itself. The de-facto method of computing it, is through the recursive application of inference rules. Due to it being a costly operation, it is a must for datalog engines to provide incremental materialization, that is, to adjust the computation to new data, instead of restarting from scratch. One of the major caveats, is that deleting data is notoriously more involved than adding, since one has to take into account all possible data that has been entailed from what is being deleted. Differential Dataflow is a computational model that provides efficient incremental maintenance, notoriously with equal performance between additions and deletions, and work distribution, of iterative dataflows. In this paper we investigate the performance of materialization with three reference datalog implementations, out of which one is
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#37322;&#25216;&#26415;&#20135;&#21697;&#26102;&#24212;&#29992;&#30340;&#21452;&#37325;&#24615;&#29702;&#35770;&#65292;&#36890;&#36807;&#20851;&#27880;&#26550;&#26500;&#21644;&#30456;&#20851;&#24615;&#65292;&#23545;&#35299;&#37322;&#36827;&#34892;&#20102;&#24555;&#36895;&#32467;&#26500;&#21270;&#21644;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#35299;&#37322;&#20869;&#23481;&#21644;&#35270;&#39057;&#22238;&#24819;&#65292;&#25506;&#32034;&#20102;&#35299;&#37322;&#32773;&#22914;&#20309;&#36827;&#34892;&#35299;&#37322;&#30340;&#21512;&#29702;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;...</title><link>http://arxiv.org/abs/2308.04187</link><description>&lt;p&gt;
&#20026;&#20309;&#28155;&#21152;&#21040;&#20160;&#20040;&#20013;&#65311;&#23545;&#19968;&#31181;&#26085;&#24120;&#35299;&#37322;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adding Why to What? Analyses of an Everyday Explanation. (arXiv:2308.04187v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#37322;&#25216;&#26415;&#20135;&#21697;&#26102;&#24212;&#29992;&#30340;&#21452;&#37325;&#24615;&#29702;&#35770;&#65292;&#36890;&#36807;&#20851;&#27880;&#26550;&#26500;&#21644;&#30456;&#20851;&#24615;&#65292;&#23545;&#35299;&#37322;&#36827;&#34892;&#20102;&#24555;&#36895;&#32467;&#26500;&#21270;&#21644;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#35299;&#37322;&#20869;&#23481;&#21644;&#35270;&#39057;&#22238;&#24819;&#65292;&#25506;&#32034;&#20102;&#35299;&#37322;&#32773;&#22914;&#20309;&#36827;&#34892;&#35299;&#37322;&#30340;&#21512;&#29702;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#32771;&#34385;&#21040;&#65292;&#19982;&#19987;&#19994;&#35266;&#20247;&#30340;&#35299;&#37322;&#19981;&#21516;&#65292;&#24403;&#20026;&#19968;&#33324;&#20154;&#35299;&#37322;&#26102;&#65292;&#19981;&#33021;&#20551;&#35774;&#26377;&#20849;&#21516;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20294;&#26159;&#36825;&#26679;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#35299;&#37322;&#24046;&#24322;&#24456;&#22823;&#65292;&#24456;&#38590;&#30740;&#31350;&#35299;&#37322;&#30340;&#20849;&#21516;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21452;&#37325;&#24615;&#29702;&#35770;&#65292;&#36825;&#26159;&#19968;&#31181;&#25216;&#26415;&#21746;&#23398;&#26041;&#27861;&#65292;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26681;&#25454;&#35813;&#29702;&#35770;&#65292;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#26550;&#26500;&#65288;&#20363;&#22914;&#31639;&#27861;&#36923;&#36753;&#65289;&#25110;&#30456;&#20851;&#24615;&#65288;&#20363;&#22914;&#20915;&#31574;&#30340;&#20005;&#37325;&#24615;&#65292;&#25512;&#33616;&#30340;&#24433;&#21709;&#31561;&#65289;&#26469;&#35299;&#37322;XAI&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#29702;&#35770;&#20316;&#20026;&#20998;&#26512;&#26694;&#26550;&#30740;&#31350;&#20102;20&#31181;&#28216;&#25103;&#35299;&#37322;&#12290;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#29702;&#35770;&#24555;&#36895;&#32467;&#26500;&#21270;&#21644;&#27604;&#36739;&#25216;&#26415;&#20135;&#21697;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35299;&#37322;&#20869;&#23481;&#30340;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#35270;&#39057;&#22238;&#24819;&#30340;&#32467;&#26524;&#65292;&#25506;&#32034;&#35299;&#37322;&#32773;&#26159;&#22914;&#20309;&#20026;&#20182;&#20204;&#30340;&#35299;&#37322;&#36827;&#34892;&#21512;&#29702;&#21270;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35299;&#37322;&#32773;...
&lt;/p&gt;
&lt;p&gt;
In XAI it is important to consider that, in contrast to explanations for professional audiences, one cannot assume common expertise when explaining for laypeople. But such explanations between humans vary greatly, making it difficult to research commonalities across explanations. We used the dual nature theory, a techno-philosophical approach, to cope with these challenges. According to it, one can explain, for example, an XAI's decision by addressing its dual nature: by focusing on the Architecture (e.g., the logic of its algorithms) or the Relevance (e.g., the severity of a decision, the implications of a recommendation). We investigated 20 game explanations using the theory as an analytical framework. We elaborate how we used the theory to quickly structure and compare explanations of technological artifacts. We supplemented results from analyzing the explanation contents with results from a video recall to explore how explainers justified their explanation. We found that explainers
&lt;/p&gt;</description></item><item><title>&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#36807;&#21435;10&#24180;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#24739;&#32773;&#20449;&#20219;&#21644;&#19982;&#20154;&#31867;&#20132;&#20114;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.04178</link><description>&lt;p&gt;
&#21355;&#29983;&#20445;&#20581;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#19968;&#20010;&#31616;&#27905;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Assistive Chatbots for healthcare: a succinct review. (arXiv:2308.04178v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04178
&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#36807;&#21435;10&#24180;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#24739;&#32773;&#20449;&#20219;&#21644;&#19982;&#20154;&#31867;&#20132;&#20114;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25903;&#25345;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#20174;&#26368;&#36817;&#30340;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#26469;&#30475;&#65292;&#20174;&#26410;&#36798;&#21040;&#22914;&#27492;&#20043;&#39640;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36807;&#21435;10&#24180;&#65288;2013-2023&#24180;&#65289;&#25552;&#20986;&#30340;&#21307;&#30103;&#20445;&#20581;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20851;&#27880;&#26159;&#22240;&#20026;&#23427;&#20855;&#26377;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#36136;&#37327;&#12289;&#20943;&#23569;&#23545;&#20154;&#38469;&#20132;&#20114;&#30340;&#20381;&#36182;&#21644;&#33410;&#30465;&#20154;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#34920;&#26126;&#65292;&#30446;&#21069;&#26377;&#19968;&#20123;&#65288;&#21830;&#19994;&#21270;&#30340;&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#27491;&#22312;&#34987;&#29992;&#20110;&#24739;&#32773;&#25903;&#25345;&#65292;&#21516;&#26102;&#36824;&#26377;&#19968;&#20123;&#65288;&#38750;&#21830;&#19994;&#21270;&#30340;&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#27491;&#22312;&#20020;&#24202;&#35797;&#39564;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#39033;&#25216;&#26415;&#22312;&#24739;&#32773;&#23433;&#20840;&#21644;&#25968;&#25454;&#20445;&#25252;&#26041;&#38754;&#32570;&#20047;&#20449;&#20219;&#65292;&#20197;&#21450;&#21307;&#25252;&#20154;&#21592;&#23545;&#20854;&#22909;&#22788;&#32570;&#20047;&#26356;&#24191;&#27867;&#30340;&#35748;&#30693;&#12290;&#27492;&#22806;&#65292;&#24739;&#32773;&#22312;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#33021;&#34920;&#31034;&#19981;&#28385;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#20110;&#25913;&#21892;&#21307;&#30103;&#26381;&#21153;&#21644;&#24739;&#32773;&#25903;&#25345;&#30340;&#28508;&#21147;&#65292;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#20173;&#26377;&#35768;&#22810;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) for supporting healthcare services has never been more necessitated than by the recent global pandemic. Here, we review the state-of-the-art in AI-enabled Chatbots in healthcare proposed during the last 10 years (2013-2023). The focus on AI-enabled technology is because of its potential for enhancing the quality of human-machine interaction via Chatbots, reducing dependence on human-human interaction and saving man-hours. Our review indicates that there are a handful of (commercial) Chatbots that are being used for patient support, while there are others (non-commercial) that are in the clinical trial phases. However, there is a lack of trust on this technology regarding patient safety and data protection, as well as a lack of wider awareness on its benefits among the healthcare workers and professionals. Also, patients have expressed dissatisfaction with Natural Language Processing (NLP) skills of the Chatbots in comparison to humans. Notwithstanding the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;medicX&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#32452;&#21512;&#26159;ComplEx&#23884;&#20837;&#26041;&#27861;&#21644;LSTM&#32593;&#32476;&#65292;&#36798;&#21040;&#20102;95.19%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04172</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting Drug-Drug Interactions Using Knowledge Graphs. (arXiv:2308.04172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;medicX&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#32452;&#21512;&#26159;ComplEx&#23884;&#20837;&#26041;&#27861;&#21644;LSTM&#32593;&#32476;&#65292;&#36798;&#21040;&#20102;95.19%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#37324;&#65292;&#20154;&#20204;&#23545;&#33647;&#29289;&#30340;&#28040;&#36153;&#21644;&#32452;&#21512;&#27604;&#20197;&#21069;&#26356;&#22810;&#65292;&#23548;&#33268;&#20102;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#39044;&#27979;&#26410;&#30693;&#30340;DDIs&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#32435;&#20837;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20379;&#27604;&#21333;&#20010;&#33647;&#29289;&#23646;&#24615;&#26356;&#22909;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;medicX&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23427;&#23558;&#26469;&#33258;&#20844;&#20849;&#33647;&#29289;&#24211;&#30340;&#22810;&#20010;&#33647;&#29289;&#29305;&#24449;&#38598;&#25104;&#21040;KG&#20013;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32763;&#35793;&#12289;&#20998;&#35299;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;KG Embedding&#65288;KGE&#65289;&#26041;&#27861;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#23884;&#20837;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#39044;&#27979;&#26410;&#30693;&#30340;DDIs&#12290;&#22312;&#19981;&#21516;&#30340;&#32763;&#35793;&#21644;&#20998;&#35299;&#22411;KGE&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#26159;ComplEx&#23884;&#20837;&#26041;&#27861;&#21644;Long Short-Term Memory&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#23427;&#22312;&#22522;&#20110;DrugBank&#30340;DDIs&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;95.19%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decades, people have been consuming and combining more drugs than before, increasing the number of Drug-Drug Interactions (DDIs). To predict unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs) since they are able to capture the relationships among entities providing better drug representations than using a single drug property. In this paper, we propose the medicX end-to-end framework that integrates several drug features from public drug repositories into a KG and embeds the nodes in the graph using various translation, factorisation and Neural Network (NN) based KG Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm that predicts unknown DDIs. Among the different translation and factorisation-based KGE models, we found that the best performing combination was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network, which obtained an F1-score of 95.19% on a dataset based on the DDIs found in DrugBank vers
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#25512;&#29702;&#31561;&#26041;&#38754;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#19968;&#39033;&#20851;&#20110;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#30340;&#30740;&#35752;&#20250;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#36215;&#28304;&#12289;&#30446;&#26631;&#12289;&#37324;&#31243;&#30865;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#26410;&#26469;&#21313;&#24180;&#30340;&#37325;&#28857;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2308.04161</link><description>&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#20013;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Current and Future Challenges in Knowledge Representation and Reasoning. (arXiv:2308.04161v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04161
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#25512;&#29702;&#31561;&#26041;&#38754;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#19968;&#39033;&#20851;&#20110;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#30340;&#30740;&#35752;&#20250;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#36215;&#28304;&#12289;&#30446;&#26631;&#12289;&#37324;&#31243;&#30865;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#26410;&#26469;&#21313;&#24180;&#30340;&#37325;&#28857;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#12289;&#38271;&#26399;&#23384;&#22312;&#19988;&#27963;&#36291;&#30340;&#39046;&#22495;&#12290;&#22810;&#24180;&#26469;&#65292;&#23427;&#26377;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65307;&#36817;&#24180;&#26469;&#65292;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#25512;&#29702;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#34917;&#20805;&#19979;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;2022&#24180;7&#26376;&#65292;Dagstuhl&#30340;&#19968;&#20010;Perspectives&#30740;&#35752;&#20250;&#23601;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36825;&#20010;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25551;&#36848;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20854;&#19982;&#20854;&#20182;&#39046;&#22495;&#30340;&#20851;&#31995;&#12289;&#20854;&#20248;&#32570;&#28857;&#20197;&#21450;&#23545;&#26410;&#26469;&#36827;&#23637;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#22522;&#20110;Dagstuhl&#30740;&#35752;&#20250;&#19978;&#30340;&#28436;&#35762;&#12289;&#23567;&#32452;&#35752;&#35770;&#21644;&#35752;&#35770;&#24320;&#21457;&#20102;&#36825;&#20221;&#23459;&#35328;&#12290;&#23427;&#26159;&#25105;&#20204;&#23545;&#30693;&#35782;&#34920;&#31034;&#30340;&#35266;&#28857;&#30340;&#34920;&#36848;&#65306;&#23427;&#30340;&#36215;&#28304;&#12289;&#30446;&#26631;&#12289;&#37324;&#31243;&#30865;&#20197;&#21450;&#24403;&#21069;&#30340;&#37325;&#28857;&#65307;&#23427;&#19982;&#20854;&#20182;&#23398;&#31185;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65307;&#20197;&#21450;&#23427;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#26410;&#26469;&#21313;&#24180;&#30340;&#20851;&#38190;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Representation and Reasoning is a central, longstanding, and active area of Artificial Intelligence. Over the years it has evolved significantly; more recently it has been challenged and complemented by research in areas such as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl Perspectives workshop was held on Knowledge Representation and Reasoning. The goal of the workshop was to describe the state of the art in the field, including its relation with other areas, its shortcomings and strengths, together with recommendations for future progress. We developed this manifesto based on the presentations, panels, working groups, and discussions that took place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge Representation: its origins, goals, milestones, and current foci; its relation to other disciplines, especially to Artificial Intelligence; and on its challenges, along with key priorities for the next decade.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24322;&#26500;360&#24230;&#35270;&#39057;&#35774;&#35745;&#30340;&#26381;&#21153;&#36136;&#37327;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24046;&#24322;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#36880;&#24103;&#20248;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#31163;&#36755;&#20837;&#24046;&#24322;&#36755;&#20986;&#65288;SIDO&#65289;&#21644;&#21512;&#24182;&#36755;&#20837;&#24046;&#24322;&#36755;&#20986;&#65288;MIDO&#65289;&#20004;&#31181;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#26500;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04083</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#20013;&#30340;&#24322;&#26500;360&#24230;&#35270;&#39057;&#65306;&#24046;&#24322;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches. (arXiv:2308.04083v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24322;&#26500;360&#24230;&#35270;&#39057;&#35774;&#35745;&#30340;&#26381;&#21153;&#36136;&#37327;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24046;&#24322;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#36880;&#24103;&#20248;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#31163;&#36755;&#20837;&#24046;&#24322;&#36755;&#20986;&#65288;SIDO&#65289;&#21644;&#21512;&#24182;&#36755;&#20837;&#24046;&#24322;&#36755;&#20986;&#65288;MIDO&#65289;&#20004;&#31181;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#26500;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#35270;&#39057;&#25216;&#26415;&#25512;&#21160;&#30528;&#26410;&#26469;&#20803;&#23431;&#23449;&#30340;&#21457;&#23637;&#65292;&#26088;&#22312;&#36830;&#25509;&#20174;&#20219;&#20309;&#22320;&#26041;&#30340;&#29992;&#25143;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#30340;&#20351;&#29992;&#22330;&#26223;&#23558;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#23548;&#33268;&#20102;&#38750;VR&#21644;VR 360&#24230;&#35270;&#39057;&#30340;&#28151;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20026;&#20855;&#26377;&#19981;&#21516;&#24103;&#29575;&#21644;&#26197;&#21160;&#30151;&#35201;&#27714;&#30340;&#24322;&#26500;360&#24230;&#35270;&#39057;&#35774;&#35745;&#30340;&#26381;&#21153;&#36136;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24103;&#38553;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#33258;&#34892;&#35774;&#35745;&#30340;&#24046;&#24322;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#36880;&#24103;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#32467;&#26500;&#65292;&#21363;&#20998;&#31163;&#36755;&#20837;&#24046;&#24322;&#36755;&#20986;&#65288;SIDO&#65289;&#21644;&#21512;&#24182;&#36755;&#20837;&#24046;&#24322;&#36755;&#20986;&#65288;MIDO&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#24322;&#26500;&#22330;&#26223;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced video technologies are driving the development of the futuristic Metaverse, which aims to connect users from anywhere and anytime. As such, the use cases for users will be much more diverse, leading to a mix of 360-degree videos with two types: non-VR and VR 360-degree videos. This paper presents a novel Quality of Service model for heterogeneous 360-degree videos with different requirements for frame rates and cybersickness. We propose a frame-slotted structure and conduct frame-wise optimization using self-designed differentiated deep reinforcement learning algorithms. Specifically, we design two structures, Separate Input Differentiated Output (SIDO) and Merged Input Differentiated Output (MIDO), for this heterogeneous scenario. We also conduct comprehensive experiments to demonstrate their effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#20195;&#29702;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;&#20013;&#30340;&#26597;&#35810;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04077</link><description>&lt;p&gt;
&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#20195;&#29702;&#26799;&#24230;&#36827;&#34892;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#20195;&#29702;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;&#20013;&#30340;&#26597;&#35810;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#20248;&#21270;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#23427;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#65288;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#33021;&#22815;&#20849;&#21516;&#20248;&#21270;&#19968;&#20010;&#20840;&#23616;&#20989;&#25968;&#12290;&#36825;&#20123;&#23458;&#25143;&#31471;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#21482;&#20849;&#20139;&#26412;&#22320;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#32852;&#37030;&#20248;&#21270;&#24212;&#29992;&#20013;&#65292;&#26799;&#24230;&#20449;&#24687;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#24341;&#20986;&#20102;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;&#65288;ZOO&#65289;&#30340;&#33539;&#24335;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;ZOO&#31639;&#27861;&#23384;&#22312;&#26597;&#35810;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#38480;&#21046;&#65292;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#65306;&#65288;a&#65289;&#23427;&#20204;&#23545;&#26799;&#24230;&#20272;&#35745;&#38656;&#35201;&#22823;&#37327;&#30340;&#20989;&#25968;&#26597;&#35810;&#65307;&#65288;b&#65289;&#23427;&#20204;&#30340;&#23454;&#38469;&#26412;&#22320;&#26356;&#26032;&#19982;&#39044;&#26399;&#20840;&#23616;&#26356;&#26032;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36712;&#36857;&#20449;&#24687;&#30340;&#26799;&#24230;&#20195;&#29702;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20989;&#25968;&#26597;&#35810;&#21382;&#21490;&#36827;&#34892;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04071</link><description>&lt;p&gt;
&#36335;&#24452;&#31614;&#21517;&#22312;&#27010;&#29575;&#36712;&#36857;&#20248;&#21270;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Path Signatures for Diversity in Probabilistic Trajectory Optimisation. (arXiv:2308.04071v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25104;&#26412;&#34987;&#26368;&#23567;&#21270;&#65292;&#20197;&#29983;&#25104;&#36712;&#36857;&#20026;&#20989;&#25968;&#12290;&#22312;&#20855;&#26377;&#22810;&#20010;&#38556;&#30861;&#29289;&#21644;&#22797;&#26434;&#20960;&#20309;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#35299;&#20915;&#65292;&#24182;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35745;&#31639;&#30828;&#20214;&#30340;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#65292;&#20854;&#20013;&#21516;&#26102;&#24471;&#21040;&#22810;&#20010;&#35299;&#65292;&#27599;&#20010;&#35299;&#20174;&#19981;&#21516;&#30340;&#36215;&#22987;&#28857;&#21021;&#22987;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22914;&#26524;&#27809;&#26377;&#19968;&#20010;&#31574;&#30053;&#38450;&#27490;&#20004;&#20010;&#35299;&#22604;&#38519;&#22312;&#19968;&#36215;&#65292;&#31616;&#21333;&#30340;&#24182;&#34892;&#20248;&#21270;&#20250;&#36973;&#21463;&#27169;&#24335;&#22604;&#38519;&#30340;&#38382;&#39064;&#65292;&#38477;&#20302;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#25214;&#21040;&#20840;&#23616;&#35299;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#31895;&#36335;&#24452;&#29702;&#35770;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#27169;&#24335;&#22604;&#38519;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#39033;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04061</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#27491;&#21017;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;&#20302;&#26631;&#31614;&#29615;&#22659;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#39033;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#19982;&#26500;&#24314;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#19979;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#36825;&#20004;&#20010;&#19978;&#30028;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#39033;&#19982;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;&#23588;&#20854;&#26159;&#19982;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorit
&lt;/p&gt;</description></item><item><title>SODFormer&#26159;&#19968;&#20010;&#20351;&#29992;&#20107;&#20214;&#21644;&#24103;&#36827;&#34892;&#27969;&#24335;&#30446;&#26631;&#26816;&#27979;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#26102;&#31354;Transformer&#26550;&#26500;&#21644;&#24341;&#20837;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#30446;&#26631;&#30340;&#36830;&#32493;&#26816;&#27979;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04047</link><description>&lt;p&gt;
SODFormer&#65306;&#20351;&#29992;&#20107;&#20214;&#21644;&#24103;&#30340;Transformer&#36827;&#34892;&#27969;&#24335;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SODFormer: Streaming Object Detection with Transformer Using Events and Frames. (arXiv:2308.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04047
&lt;/p&gt;
&lt;p&gt;
SODFormer&#26159;&#19968;&#20010;&#20351;&#29992;&#20107;&#20214;&#21644;&#24103;&#36827;&#34892;&#27969;&#24335;&#30446;&#26631;&#26816;&#27979;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#26102;&#31354;Transformer&#26550;&#26500;&#21644;&#24341;&#20837;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#30446;&#26631;&#30340;&#36830;&#32493;&#26816;&#27979;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DAVIS&#30456;&#26426;&#36890;&#36807;&#24322;&#27493;&#20107;&#20214;&#21644;&#24103;&#30340;&#20004;&#31181;&#20114;&#34917;&#24863;&#30693;&#27169;&#24577;&#36880;&#28176;&#34987;&#29992;&#20110;&#35299;&#20915;&#20027;&#35201;&#30340;&#30446;&#26631;&#26816;&#27979;&#25361;&#25112;&#65288;&#20363;&#22914;&#65292;&#24555;&#36895;&#36816;&#21160;&#27169;&#31946;&#21644;&#20302;&#20809;&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#26102;&#38388;&#32447;&#32034;&#21644;&#34701;&#21512;&#20004;&#20010;&#24322;&#26500;&#30340;&#35270;&#35273;&#27969;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#21363;SODFormer&#65292;&#23427;&#39318;&#20808;&#25972;&#21512;&#20107;&#20214;&#21644;&#24103;&#20197;&#24322;&#27493;&#30340;&#26041;&#24335;&#36830;&#32493;&#22320;&#26816;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#24418;&#24577;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;&#21363;PKU-DAVIS-SOD&#65289;&#65292;&#25317;&#26377;1080.1k&#20010;&#25163;&#21160;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#31354;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26469;&#26816;&#27979;&#30446;&#26631;&#65292;&#20854;&#20013;&#26032;&#39062;&#30340;&#26102;&#24207;Transformer&#27169;&#22359;&#21033;&#29992;&#20004;&#20010;&#35270;&#35273;&#27969;&#30340;&#20016;&#23500;&#26102;&#38388;&#32447;&#32034;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24322;&#27493;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
DAVIS camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been used to address major object detection challenges (e.g., fast motion blur and low-light). However, how to effectively leverage rich temporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. To address this challenge, we propose a novel streaming object detector with Transformer, namely SODFormer, which first integrates events and frames to continuously detect objects in an asynchronous manner. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1k manual labels. Then, we design a spatiotemporal Transformer architecture to detect objects via an end-to-end sequence prediction problem, where the novel temporal Transformer module leverages rich temporal cues from two visual streams to improve the detection performance. Finally, an asynchronous attention-based fusion module is p
&lt;/p&gt;</description></item><item><title>InfeRE&#26159;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#38142;&#36880;&#27493;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#29983;&#25104;&#26368;&#32456;&#34920;&#36798;&#24335;&#32972;&#21518;&#30340;&#36880;&#27493;&#20869;&#37096;&#25991;&#26412;&#21305;&#37197;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#19968;&#33268;&#24615;&#35299;&#30721;&#26426;&#21046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfeRE&#22312;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.04041</link><description>&lt;p&gt;
InfeRE&#65306;&#36890;&#36807;&#25512;&#29702;&#38142;&#36880;&#27493;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
InfeRE: Step-by-Step Regex Generation via Chain of Inference. (arXiv:2308.04041v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04041
&lt;/p&gt;
&lt;p&gt;
InfeRE&#26159;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#38142;&#36880;&#27493;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#29983;&#25104;&#26368;&#32456;&#34920;&#36798;&#24335;&#32972;&#21518;&#30340;&#36880;&#27493;&#20869;&#37096;&#25991;&#26412;&#21305;&#37197;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#19968;&#33268;&#24615;&#35299;&#30721;&#26426;&#21046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfeRE&#22312;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;NL2RE&#65289;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#65288;regexes&#65289;&#30340;&#33258;&#21160;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#35270;&#20026;&#19968;&#20010;&#32447;&#24615;&#30340;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#21333;&#27425;&#33258;&#22238;&#24402;&#29983;&#25104;&#26368;&#32456;&#30340;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#29983;&#25104;&#26368;&#32456;&#32467;&#26524;&#32972;&#21518;&#36880;&#27493;&#30340;&#20869;&#37096;&#25991;&#26412;&#21305;&#37197;&#36807;&#31243;&#12290;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#25928;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;InfeRE&#65292;&#23427;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#36880;&#27493;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#24615;&#35299;&#30721;&#26426;&#21046;&#65292;&#23427;&#23558;&#20174;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;&#22810;&#20010;&#36755;&#20986;&#36827;&#34892;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;NL-RX-Turk&#21644;KB13&#19978;&#23545;InfeRE&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#27969;&#34892;&#30340;&#22522;&#20110;&#26641;&#30340;&#29983;&#25104;&#26041;&#27861;TRANX&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfeRE&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating regular expressions (abbrev. regexes) from natural language description (NL2RE) has been an emerging research area. Prior studies treat regex as a linear sequence of tokens and generate the final expressions autoregressively in a single pass. They did not take into account the step-by-step internal text-matching processes behind the final results. This significantly hinders the efficacy and interpretability of regex generation by neural language models. In this paper, we propose a new paradigm called InfeRE, which decomposes the generation of regexes into chains of step-by-step inference. To enhance the robustness, we introduce a self-consistency decoding mechanism that ensembles multiple outputs sampled from different models. We evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and compare the results with state-of-the-art approaches and the popular tree-based generation approach TRANX. Experimental results show that InfeRE substantially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#23427;&#37319;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12289;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#19988;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04033</link><description>&lt;p&gt;
&#36866;&#24212;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications. (arXiv:2308.04033v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#23427;&#37319;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12289;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#19988;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#12289;&#24320;&#21457;&#21644;&#30740;&#31350;&#29616;&#20195;&#26080;&#32447;&#36890;&#20449;&#25216;&#26415;&#30340;&#29616;&#26377;&#26041;&#27861;&#28041;&#21450;&#32791;&#26102;&#19988;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#31579;&#36873;&#22823;&#37327;&#30340;&#32593;&#39029;&#21644;&#25216;&#26415;&#35268;&#33539;&#25991;&#20214;&#65292;&#25910;&#38598;&#25152;&#38656;&#20449;&#24687;&#24182;&#36827;&#34892;&#32508;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#26368;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#23637;&#65292;&#24182;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#38468;&#21152;&#32452;&#20214;&#65306;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#26080;&#32447;&#25216;&#26415;&#35268;&#33539;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#31616;&#27905;&#30340;&#12289;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#20351;&#29992;&#30001;&#19987;&#23478;&#21019;&#24314;&#30340;&#26597;&#35810;&#21644;&#21442;&#32771;&#21709;&#24212;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#23637;&#31034;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23637;&#31034;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#20915;&#31574;&#30340;&#33258;&#20449;&#24230;&#12290; (This study investigated the impact of displaying more uncertainty information on human decision making in emotion classification, and the results showed that it can increase users' confidence in decision making.)</title><link>http://arxiv.org/abs/2308.04032</link><description>&lt;p&gt;
&#20154;&#31867;&#24773;&#32490;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measure of Uncertainty in Human Emotions. (arXiv:2308.04032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#23637;&#31034;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23637;&#31034;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#20915;&#31574;&#30340;&#33258;&#20449;&#24230;&#12290; (This study investigated the impact of displaying more uncertainty information on human decision making in emotion classification, and the results showed that it can increase users' confidence in decision making.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#35745;&#31639;&#26426;&#22914;&#20309;&#33021;&#22815;&#26816;&#27979;&#20154;&#31867;&#23637;&#31034;&#30340;&#24773;&#32490;&#24182;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#35745;&#31639;&#26426;&#29983;&#25104;&#24773;&#32490;&#20998;&#31867;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#20915;&#31574;&#25110;&#25191;&#34892;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#38656;&#35201;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#23545;&#20154;&#19982;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#21452;&#21521;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#24773;&#32490;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#23637;&#31034;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23637;&#31034;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#20570;&#20915;&#31574;&#26102;&#26356;&#21152;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many research explore how well computers are able to examine emotions displayed by humans and use that data to perform different tasks. However, there have been very few research which evaluate the computers ability to generate emotion classification information in an attempt to help the user make decisions or perform tasks. This is a crucial area to explore as it is paramount to the two way communication between humans and computers. This research conducted an experiment to investigate the impact of different uncertainty information displays of emotion classification on the human decision making process. Results show that displaying more uncertainty information can help users to be more confident when making decisions.
&lt;/p&gt;</description></item><item><title>Gentopia&#26159;&#19968;&#20010;&#21327;&#20316;&#24179;&#21488;&#65292;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#28789;&#27963;&#30340;&#23450;&#21046;&#12289;&#21327;&#20316;&#27665;&#20027;&#21270;&#21644;&#32508;&#21512;&#35780;&#20272;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04030</link><description>&lt;p&gt;
Gentopia: &#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Gentopia: A Collaborative Platform for Tool-Augmented LLMs. (arXiv:2308.04030v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04030
&lt;/p&gt;
&lt;p&gt;
Gentopia&#26159;&#19968;&#20010;&#21327;&#20316;&#24179;&#21488;&#65292;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#28789;&#27963;&#30340;&#23450;&#21046;&#12289;&#21327;&#20316;&#27665;&#20027;&#21270;&#21644;&#32508;&#21512;&#35780;&#20272;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;ALM&#65289;&#36171;&#20104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#33021;&#22815;&#36827;&#34892;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#30340;&#26234;&#33021;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;ALM&#26694;&#26550;&#22312;&#20197;&#19979;&#20851;&#38190;&#29305;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65306;&#28789;&#27963;&#30340;&#23450;&#21046;&#21270;&#12289;&#21327;&#20316;&#27665;&#20027;&#21270;&#21644;&#25972;&#20307;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Gentopia&#65292;&#19968;&#31181;ALM&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37197;&#32622;&#23454;&#29616;&#20195;&#29702;&#30340;&#28789;&#27963;&#23450;&#21046;&#65292;&#23558;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#12289;&#20219;&#21153;&#26684;&#24335;&#12289;&#25552;&#31034;&#27169;&#22359;&#21644;&#25554;&#20214;&#26080;&#32541;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#33539;&#24335;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Gentpool&#65292;&#19968;&#20010;&#20844;&#20849;&#24179;&#21488;&#65292;&#29992;&#20110;&#27880;&#20876;&#21644;&#20849;&#20139;&#29992;&#25143;&#23450;&#21046;&#30340;&#20195;&#29702;&#12290;&#22312;Gentpool&#20013;&#27880;&#20876;&#30340;&#20195;&#29702;&#21487;&#32452;&#21512;&#22312;&#19968;&#36215;&#36827;&#34892;&#20195;&#29702;&#21327;&#20316;&#65292;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20195;&#29702;&#65292;Gentbench&#26159;Gentpool&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29992;&#25143;&#23450;&#21046;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.04028</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#37327;&#25991;&#26723;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#38382;&#31572;&#20381;&#36182;&#20110;&#39640;&#25928;&#30340;&#27573;&#33853;&#26816;&#32034;&#26469;&#36873;&#25321;&#20505;&#36873;&#19978;&#19979;&#25991;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#65292;&#22914;TF-IDF&#25110;BM25&#65292;&#26159;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#22312;&#32593;&#32476;&#19978;&#65292;&#27809;&#26377;&#19968;&#31687;&#25991;&#31456;&#21487;&#20197;&#25552;&#20379;&#25152;&#26377;&#21487;&#33021;&#30340;&#31572;&#26696;&#65292;&#20197;&#22238;&#31572;&#29992;&#25143;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#31264;&#23494;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#23545;&#32500;&#22522;&#30334;&#31185;2018&#24180;12&#26376;20&#26085;&#30340;&#20542;&#38144;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#29992;&#20316;&#22238;&#31572;&#38382;&#39064;&#30340;&#28304;&#25991;&#26723;&#12290;&#38382;&#31572;&#31995;&#32479;&#22312;&#22810;&#20010;&#24320;&#25918;&#39046;&#22495;&#21644;&#26426;&#22120;&#29702;&#35299;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26681;&#25454;&#22810;&#39033;&#35843;&#26597;&#65292;&#26080;&#27861;&#20174;&#32500;&#22522;&#30334;&#31185;&#20934;&#30830;&#22238;&#31572;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia 
&lt;/p&gt;</description></item><item><title>AgentSims&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#30721;&#30340;&#27801;&#30418;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#20219;&#21153;&#39537;&#21160;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#20379;&#30740;&#31350;&#20154;&#21592;&#27979;&#35797;&#29305;&#23450;&#33021;&#21147;&#12290;&#28436;&#31034;&#29256;&#26412;&#21487;&#22312;https://agentsims.com&#33719;&#24471;&#12290;</title><link>http://arxiv.org/abs/2308.04026</link><description>&lt;p&gt;
AgentSims: &#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24320;&#28304;&#27801;&#30418;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. (arXiv:2308.04026v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04026
&lt;/p&gt;
&lt;p&gt;
AgentSims&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#30721;&#30340;&#27801;&#30418;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#20219;&#21153;&#39537;&#21160;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#20379;&#30740;&#31350;&#20154;&#21592;&#27979;&#35797;&#29305;&#23450;&#33021;&#21147;&#12290;&#28436;&#31034;&#29256;&#26412;&#21487;&#22312;https://agentsims.com&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312; ChatGPT &#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;1&#65289;&#26377;&#38480;&#30340;&#35780;&#20272;&#33021;&#21147;&#65292;&#65288;2&#65289;&#33030;&#24369;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#65288;3&#65289;&#19981;&#23458;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35753;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#36825;&#26159;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#19968;&#20010;&#32508;&#21512;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25512;&#20986;&#20102; AgentSims&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#20379;&#21508;&#23398;&#31185;&#30340;&#30740;&#31350;&#20154;&#21592;&#27979;&#35797;&#20182;&#20204;&#24863;&#20852;&#36259;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#20132;&#20114;&#24335;&#22270;&#24418;&#30028;&#38754;&#28155;&#21152;&#20195;&#29702;&#21644;&#24314;&#31569;&#29289;&#26469;&#26500;&#24314;&#35780;&#20272;&#20219;&#21153;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20960;&#34892;&#20195;&#30721;&#37096;&#32626;&#21644;&#27979;&#35797;&#26032;&#30340;&#25903;&#25345;&#26426;&#21046;&#65292;&#20363;&#22914;&#35760;&#24518;&#12289;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#29256;&#26412;&#21487;&#20197;&#22312; https://agentsims.com&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#25552;&#20986;&#20102;MSAC&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#39062;&#30340;CNN-based SER&#27169;&#22411;&#21644;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#21644;&#25429;&#25417;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;SER&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04025</link><description>&lt;p&gt;
MSAC&#65306;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition. (arXiv:2308.04025v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#25552;&#20986;&#20102;MSAC&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#39062;&#30340;CNN-based SER&#27169;&#22411;&#21644;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#21644;&#25429;&#25417;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;SER&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#24773;&#24863;&#23646;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#65292;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32780;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;SER&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21508;&#31181;&#35821;&#38899;&#23646;&#24615;&#30340;&#25968;&#25454;&#20998;&#24067;&#26469;&#24314;&#27169;&#35821;&#38899;&#24773;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN&#30340;SER&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#21152;&#24615;&#36793;&#30028;&#26368;&#22823;&#21270;&#36719;&#20214;&#26368;&#22823;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#25193;&#22823;&#20102;&#19981;&#21516;&#31867;&#21035;&#29305;&#24449;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#20197;&#26126;&#30830;&#25511;&#21046;&#35821;&#38899;&#23646;&#24615;&#65292;&#20351;&#27169;&#22411;&#21463;&#24773;&#24863;&#26080;&#20851;&#23646;&#24615;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#24182;&#25429;&#25417;&#21040;&#26356;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#30456;&#20851;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#27979;&#35797;&#21644;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;SER&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33539;&#22260;&#25439;&#22833;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#20998;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04024</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#21644;RL&#25506;&#32034;&#30340;&#33539;&#22260;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Scope Loss for Imbalanced Classification and RL Exploration. (arXiv:2308.04024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33539;&#22260;&#25439;&#22833;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#20998;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21644;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#22240;&#27492;&#23558;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#31561;&#21516;&#20110;&#30417;&#30563;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25214;&#20986;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#26041;&#24335;&#19978;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#20998;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;- &#33539;&#22260;&#25439;&#22833;&#12290;&#33539;&#22260;&#25439;&#22833;&#21487;&#20197;&#35843;&#25972;&#26799;&#24230;&#65292;&#20197;&#38450;&#27490;&#36807;&#24230;&#21033;&#29992;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#24615;&#33021;&#25439;&#22833;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#21644;&#19968;&#20010;&#20559;&#26012;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#33539;&#22260;&#25439;&#22833;&#65292;&#32467;&#26524;&#26174;&#31034;&#33539;&#22260;&#25439;&#22833;&#20248;&#20110;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26469;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04018</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Performance of Semi-Supervised Learning by Adversarial Attacks. (arXiv:2308.04018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26469;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26159;&#24314;&#31435;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;&#20551;&#35774;&#19978;&#65292;&#21363;&#35775;&#38382;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#24456;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#22914;&#20309;&#25104;&#21151;&#36873;&#25321;&#39640;&#32622;&#20449;&#24230;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19982;&#24403;&#21069;&#39044;&#27979;&#30340;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#65292;&#19977;&#20010;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#19982;SCAR&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
&lt;/p&gt;</description></item><item><title>&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#20026;&#32676;&#32452;&#25512;&#33616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#31890;&#24230;&#23618;&#27425;&#26469;&#25581;&#31034;&#32676;&#32452;&#25104;&#21592;&#30340;&#28508;&#22312;&#20559;&#22909;&#24182;&#20943;&#23569;&#25512;&#33616;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.04017</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#29992;&#20110;&#32676;&#32452;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-Granularity Attention Model for Group Recommendation. (arXiv:2308.04017v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04017
&lt;/p&gt;
&lt;p&gt;
&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#20026;&#32676;&#32452;&#25512;&#33616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#31890;&#24230;&#23618;&#27425;&#26469;&#25581;&#31034;&#32676;&#32452;&#25104;&#21592;&#30340;&#28508;&#22312;&#20559;&#22909;&#24182;&#20943;&#23569;&#25512;&#33616;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#32452;&#25512;&#33616;&#26159;&#22522;&#20110;&#20849;&#21516;&#20852;&#36259;&#12289;&#20559;&#22909;&#21644;&#29305;&#24449;&#20026;&#19968;&#32452;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#20010;&#20154;&#20559;&#22909;&#24182;&#20570;&#20986;&#26377;&#21033;&#20110;&#25972;&#20010;&#32676;&#32452;&#30340;&#38598;&#20307;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#34892;&#20026;&#20016;&#23500;&#30340;&#29992;&#25143;&#65292;&#24573;&#35270;&#20102;&#34892;&#20026;&#30456;&#23545;&#31232;&#30095;&#30340;&#29992;&#25143;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#23548;&#33268;&#20010;&#20154;&#20852;&#36259;&#30340;&#23398;&#20064;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;MGAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#31890;&#24230;&#23618;&#27425;&#65288;&#21363;&#23376;&#38598;&#12289;&#32676;&#32452;&#21644;&#36229;&#38598;&#65289;&#26469;&#25581;&#31034;&#32676;&#32452;&#25104;&#21592;&#28508;&#22312;&#20559;&#22909;&#21644;&#20943;&#23569;&#25512;&#33616;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#38598;&#20559;&#22909;&#25552;&#21462;&#27169;&#22359;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#20132;&#20114;&#20449;&#24687;&#21644;&#20351;&#29992;&#23618;&#27425;&#21270;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#29992;&#25143;&#28508;&#22312;&#30340;&#23376;&#38598;&#32423;&#21035;&#20559;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group recommendation provides personalized recommendations to a group of users based on their shared interests, preferences, and characteristics. Current studies have explored different methods for integrating individual preferences and making collective decisions that benefit the group as a whole. However, most of them heavily rely on users with rich behavior and ignore latent preferences of users with relatively sparse behavior, leading to insufficient learning of individual interests. To address this challenge, we present the Multi-Granularity Attention Model (MGAM), a novel approach that utilizes multiple levels of granularity (i.e., subsets, groups, and supersets) to uncover group members' latent preferences and mitigate recommendation noise. Specially, we propose a Subset Preference Extraction module that enhances the representation of users' latent subset-level preferences by incorporating their previous interactions with items and utilizing a hierarchical mechanism. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03999</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#29702;&#35299;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Explainable AI&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20934;&#30830;&#35299;&#37322;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65306;&#20934;&#30830;&#30340;&#35299;&#37322;&#23558;&#20026;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#37096;&#26816;&#27979;&#21040;&#30340;&#36755;&#20837;&#30456;&#20851;&#20869;&#23481;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#29616;&#26377;&#25216;&#26415;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#33410;&#28857;&#30340;&#28608;&#27963;&#21487;&#20197;&#34987;&#20154;&#31867;&#29702;&#35299;&#65292;&#20294;&#26159;&#23545;&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#35299;&#37322;&#36827;&#34892;&#20551;&#35774;&#21644;&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#33258;&#21160;&#21270;&#26041;&#27861;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20174;&#32500;&#22522;&#30334;&#31185;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#20013;&#31579;&#36873;&#20986;&#30340;&#32422;200&#19975;&#20010;&#31867;&#21035;&#65292;&#20197;&#21450;&#19968;&#20010;&#31216;&#20026;&#27010;&#24565;&#24402;&#32435;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#35821;&#20041;Web&#39046;&#22495;&#30340;&#24212;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, de-mystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#30340;&#36164;&#28304;&#31649;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.03995</link><description>&lt;p&gt;
&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks. (arXiv:2308.03995v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#30340;&#36164;&#28304;&#31649;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#65288;SAGIN&#65289;&#23558;&#21253;&#25324;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#65288;LEO&#65289;&#12289;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#21644;&#22320;&#38754;&#29992;&#25143;&#65288;GU&#65289;&#22312;&#20869;&#30340;&#24322;&#26500;&#35774;&#22791;&#25972;&#21512;&#36215;&#26469;&#65292;&#26377;&#26395;&#25512;&#21160;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;SAGIN&#30340;&#36164;&#28304;&#31649;&#29702;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#32039;&#24613;&#30740;&#31350;&#65292;&#22240;&#20026;&#19981;&#24688;&#24403;&#30340;&#36164;&#28304;&#31649;&#29702;&#20250;&#23548;&#33268;&#25968;&#25454;&#20256;&#36755;&#36136;&#37327;&#24046;&#65292;&#36827;&#32780;&#24433;&#21709;&#26234;&#33021;&#22478;&#24066;&#30340;&#26381;&#21153;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;SAGIN&#31995;&#32479;&#65292;&#21253;&#25324;&#20116;&#31181;&#19981;&#21516;&#30340;&#36890;&#20449;&#38142;&#36335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;CMT-MARL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;CMT-MARL&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#25972;&#20307;&#20256;&#36755;&#36895;&#29575;&#21644;&#20256;&#36755;&#25104;&#21151;&#29575;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#26410;&#26469;SAGIN&#23454;&#26045;&#30340;&#28508;&#22312;&#20215;&#20540;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#22810;&#35282;&#33394;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#25552;&#21319;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#23398;&#20064;&#20307;&#39564;&#21644;&#22686;&#24378;&#21442;&#19982;&#24230;&#30340;&#25163;&#27573;&#12290;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;&#21644;&#25506;&#31350;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#28385;&#36275;&#23398;&#29983;&#30340;&#20869;&#22312;&#24515;&#29702;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#19982;&#20154;&#31867;&#23548;&#24072;&#21644;&#21333;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26465;&#20214;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#23454;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03992</link><description>&lt;p&gt;
AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#22810;&#35282;&#33394;&#25945;&#32946;&#20195;&#29702;&#65306;&#25913;&#21464;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education. (arXiv:2308.03992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#22810;&#35282;&#33394;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#25552;&#21319;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#23398;&#20064;&#20307;&#39564;&#21644;&#22686;&#24378;&#21442;&#19982;&#24230;&#30340;&#25163;&#27573;&#12290;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;&#21644;&#25506;&#31350;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#28385;&#36275;&#23398;&#29983;&#30340;&#20869;&#22312;&#24515;&#29702;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#19982;&#20154;&#31867;&#23548;&#24072;&#21644;&#21333;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26465;&#20214;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#23454;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021; (AI) &#39537;&#21160;&#30340;&#22810;&#35282;&#33394;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#21644;&#20419;&#36827;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#21442;&#19982;&#24230;&#30340;&#25163;&#27573;&#12290;&#21033;&#29992;&#22522;&#20110;&#35774;&#35745;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#29615;&#22659;&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#19981;&#21516;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35282;&#33394;&#65306;&#23548;&#24072;&#26426;&#22120;&#20154;&#12289;&#21516;&#20276;&#26426;&#22120;&#20154;&#12289;&#32844;&#19994;&#21672;&#35810;&#26426;&#22120;&#20154;&#21644;&#24773;&#24863;&#25903;&#25345;&#26426;&#22120;&#20154;&#12290;&#36825;&#20123;&#35282;&#33394;&#22522;&#20110;&#33258;&#25105;&#20915;&#23450;&#29702;&#35770;&#30340;&#21407;&#21017;&#35774;&#35745;&#65292;&#28385;&#36275;&#23398;&#20064;&#32773;&#30340;&#19977;&#31181;&#20869;&#22312;&#24515;&#29702;&#38656;&#27714;&#65306;&#33021;&#21147;&#12289;&#33258;&#20027;&#24615;&#21644;&#20851;&#32852;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#25506;&#31350;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#40723;&#21169;&#23398;&#29983;&#25552;&#38382;&#12289;&#23547;&#27714;&#35299;&#20915;&#26041;&#26696;&#24182;&#25506;&#32034;&#20182;&#20204;&#30340;&#22909;&#22855;&#24515;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#39640;&#31561;&#25945;&#32946;&#29615;&#22659;&#20013;&#20197;200&#21517;&#21442;&#19982;&#23398;&#29983;&#20026;&#26399;&#19968;&#20010;&#26376;&#30340;&#26102;&#38388;&#27979;&#35797;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#28041;&#21450;&#20154;&#31867;&#23548;&#24072;&#21644;&#21333;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26465;&#20214;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the use of Artificial Intelligence (AI)-powered, multi-role chatbots as a means to enhance learning experiences and foster engagement in computer science education. Leveraging a design-based research approach, we develop, implement, and evaluate a novel learning environment enriched with four distinct chatbot roles: Instructor Bot, Peer Bot, Career Advising Bot, and Emotional Supporter Bot. These roles, designed around the tenets of Self-Determination Theory, cater to the three innate psychological needs of learners - competence, autonomy, and relatedness. Additionally, the system embraces an inquiry-based learning paradigm, encouraging students to ask questions, seek solutions, and explore their curiosities.  We test this system in a higher education context over a period of one month with 200 participating students, comparing outcomes with conditions involving a human tutor and a single chatbot. Our research utilizes a mixed-methods approach, encompassing quan
&lt;/p&gt;</description></item><item><title>NEOLAF&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#26550;&#26500;&#65292;&#30456;&#27604;&#20110;&#32431;&#36830;&#25509;&#20027;&#20041;&#21644;&#32431;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#22686;&#37327;&#23398;&#20064;&#12289;&#21327;&#20316;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12289;&#20154;&#24037;&#21327;&#21161;&#21644;&#33258;&#25105;&#25913;&#36827;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;NEOLAF&#20855;&#26377;&#20248;&#31168;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#25512;&#21160;&#35748;&#30693;&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.03990</link><description>&lt;p&gt;
NEOLAF,&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
NEOLAF, an LLM-powered neural-symbolic cognitive architecture. (arXiv:2308.03990v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03990
&lt;/p&gt;
&lt;p&gt;
NEOLAF&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#26550;&#26500;&#65292;&#30456;&#27604;&#20110;&#32431;&#36830;&#25509;&#20027;&#20041;&#21644;&#32431;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#22686;&#37327;&#23398;&#20064;&#12289;&#21327;&#20316;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12289;&#20154;&#24037;&#21327;&#21161;&#21644;&#33258;&#25105;&#25913;&#36827;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;NEOLAF&#20855;&#26377;&#20248;&#31168;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#25512;&#21160;&#35748;&#30693;&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27704;&#19981;&#20572;&#27490;&#30340;&#24320;&#25918;&#23398;&#20064;&#33258;&#36866;&#24212;&#26694;&#26550;(NEOLAF)&#65292;&#36825;&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;NEOLAF&#26694;&#26550;&#30456;&#27604;&#32431;&#36830;&#25509;&#20027;&#20041;&#21644;&#32431;&#31526;&#21495;&#26041;&#27861;&#26356;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#22686;&#37327;&#23398;&#20064;&#12289;&#39640;&#25928;&#24615;&#12289;&#21327;&#20316;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12289;&#20154;&#24037;&#21327;&#21161;&#21644;&#33258;&#25105;&#25913;&#36827;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#19968;&#20010;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#20195;&#29702;&#30340;NEOLAF&#20195;&#29702;&#34987;&#36755;&#20837;&#20102;&#26469;&#33258;&#24320;&#28304;MATH&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;NEOLAF&#20855;&#26377;&#20248;&#31168;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#35748;&#30693;&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Never Ending Open Learning Adaptive Framework (NEOLAF), an integrated neural-symbolic cognitive architecture that models and constructs intelligent agents. The NEOLAF framework is a superior approach to constructing intelligent agents than both the pure connectionist and pure symbolic approaches due to its explainability, incremental learning, efficiency, collaborative and distributed learning, human-in-the-loop enablement, and self-improvement. The paper further presents a compelling experiment where a NEOLAF agent, built as a problem-solving agent, is fed with complex math problems from the open-source MATH dataset. The results demonstrate NEOLAF's superior learning capability and its potential to revolutionize the field of cognitive architectures and self-improving adaptive instructional systems.
&lt;/p&gt;</description></item><item><title>SimplyRetrieve&#26159;&#19968;&#27454;&#31169;&#23494;&#19988;&#36731;&#37327;&#32423;&#30340;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#24494;&#35843;&#23558;&#31169;&#26377;&#25968;&#25454;&#38598;&#25104;&#36827;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#12289;&#36731;&#37327;&#32423;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20197;&#26041;&#20415;&#29992;&#25143;&#25506;&#32034;RCG&#22312;&#25552;&#21319;&#29983;&#25104;&#22411;AI&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03983</link><description>&lt;p&gt;
SimplyRetrieve: &#19968;&#27454;&#31169;&#23494;&#19988;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#26816;&#32034;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#22411;AI&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03983
&lt;/p&gt;
&lt;p&gt;
SimplyRetrieve&#26159;&#19968;&#27454;&#31169;&#23494;&#19988;&#36731;&#37327;&#32423;&#30340;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#24494;&#35843;&#23558;&#31169;&#26377;&#25968;&#25454;&#38598;&#25104;&#36827;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#12289;&#36731;&#37327;&#32423;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20197;&#26041;&#20415;&#29992;&#25143;&#25506;&#32034;RCG&#22312;&#25552;&#21319;&#29983;&#25104;&#22411;AI&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#23558;&#30693;&#35782;&#26816;&#32034;&#26550;&#26500;&#25972;&#21512;&#36827;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;LLM&#20013;&#65292;&#21487;&#20197;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#65292;&#23558;&#31169;&#26377;&#25968;&#25454;&#26080;&#32541;&#38598;&#25104;&#36827;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#65288;RCG&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#26126;&#30830;&#21306;&#20998;&#20102;LLM&#21644;&#26816;&#32034;&#22120;&#22312;&#19978;&#19979;&#25991;&#35299;&#37322;&#21644;&#30693;&#35782;&#35760;&#24518;&#20013;&#30340;&#20316;&#29992;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#23454;&#29616;&#12290;SimplyRetrieve&#26159;&#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#25552;&#20379;&#19968;&#20010;&#26412;&#22320;&#21270;&#12289;&#36731;&#37327;&#32423;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;SimplyRetrieve&#25552;&#20379;&#22522;&#20110;GUI&#21644;API&#30340;RCG&#24179;&#21488;&#65292;&#36741;&#20197;&#31169;&#23494;&#30693;&#35782;&#24211;&#26500;&#24314;&#22120;&#21644;&#26816;&#32034;&#35843;&#20248;&#27169;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#25506;&#32034;RCG&#22312;&#25913;&#36827;&#29983;&#25104;&#22411;AI&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CheXFusion&#30340;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21487;&#29992;&#20110;&#38271;&#23614;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#22810;&#35270;&#35282;&#29305;&#24449;&#24182;&#32771;&#34385;&#26631;&#31614;&#20849;&#29616;&#24615;&#12290;&#36890;&#36807;&#25968;&#25454;&#24179;&#34913;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;MIMIC-CXR&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;0.372&#30340;mAP&#65292;&#25104;&#20026;&#31454;&#36187;&#30340;&#20896;&#20891;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#32771;&#34385;&#22810;&#35270;&#35282;&#35774;&#32622;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#20849;&#29616;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03968</link><description>&lt;p&gt;
CheXFusion: &#29992;Transformer&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#35270;&#35282;&#29305;&#24449;&#36827;&#34892;&#38271;&#23614;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification. (arXiv:2308.03968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CheXFusion&#30340;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21487;&#29992;&#20110;&#38271;&#23614;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#22810;&#35270;&#35282;&#29305;&#24449;&#24182;&#32771;&#34385;&#26631;&#31614;&#20849;&#29616;&#24615;&#12290;&#36890;&#36807;&#25968;&#25454;&#24179;&#34913;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;MIMIC-CXR&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;0.372&#30340;mAP&#65292;&#25104;&#20026;&#31454;&#36187;&#30340;&#20896;&#20891;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#32771;&#34385;&#22810;&#35270;&#35282;&#35774;&#32622;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#20849;&#29616;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30001;&#20110;&#30142;&#30149;&#30340;&#38271;&#23614;&#20998;&#24067;&#12289;&#35786;&#26029;&#32467;&#26524;&#30340;&#20849;&#29616;&#24615;&#21644;&#27599;&#20010;&#30740;&#31350;&#25110;&#24739;&#32773;&#21487;&#29992;&#30340;&#22810;&#35270;&#35282;&#32780;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#23545;ICCV CVAMD 2023&#20849;&#20139;&#20219;&#21153;CX-LT&#65306;&#33016;&#37096;X&#23556;&#32447;&#30340;&#22810;&#26631;&#31614;&#38271;&#23614;&#20998;&#31867;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;CheXFusion&#65292;&#35813;&#27169;&#22359;&#33021;&#22815;&#26377;&#25928;&#22320;&#32858;&#21512;&#22810;&#35270;&#35282;&#29305;&#24449;&#65292;&#24182;&#22312;&#32771;&#34385;&#26631;&#31614;&#20849;&#29616;&#24615;&#30340;&#21516;&#26102;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#24341;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#25968;&#25454;&#24179;&#34913;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;MIMIC-CXR&#27979;&#35797;&#38598;&#20013;&#36798;&#21040;&#20102;0.372&#30340;mAP&#65292;&#33719;&#24471;&#20102;&#31454;&#36187;&#30340;&#31532;&#19968;&#21517;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#31361;&#26174;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#32771;&#34385;&#22810;&#35270;&#35282;&#35774;&#32622;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#20849;&#29616;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image classification poses unique challenges due to the long-tailed distribution of diseases, the co-occurrence of diagnostic findings, and the multiple views available for each study or patient. This paper introduces our solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays. Our approach introduces CheXFusion, a transformer-based fusion module incorporating multi-view images. The fusion module, guided by self-attention and cross-attention mechanisms, efficiently aggregates multi-view features while considering label co-occurrence. Furthermore, we explore data balancing and self-training methods to optimize the model's performance. Our solution achieves state-of-the-art results with 0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our success in the task underscores the significance of considering multi-view settings, class imbalance, and label co-occurrence in medical image classification. Publi
&lt;/p&gt;</description></item><item><title>ALFA&#20351;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#26469;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22495;&#23545;&#40784;&#23454;&#29616;&#20102;&#23545;&#19981;&#21464;&#29305;&#24449;&#30340;&#25552;&#21462;&#65292;&#20197;&#21450;&#23545;&#21442;&#19982;&#21307;&#38498;&#30340;&#39640;&#24230;&#29305;&#23450;&#29305;&#24449;&#30340;&#34920;&#31034;&#19982;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.03936</link><description>&lt;p&gt;
ALFA - &#21033;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#26469;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03936
&lt;/p&gt;
&lt;p&gt;
ALFA&#20351;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#26469;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22495;&#23545;&#40784;&#23454;&#29616;&#20102;&#23545;&#19981;&#21464;&#29305;&#24449;&#30340;&#25552;&#21462;&#65292;&#20197;&#21450;&#23545;&#21442;&#19982;&#21307;&#38498;&#30340;&#39640;&#24230;&#29305;&#23450;&#29305;&#24449;&#30340;&#34920;&#31034;&#19982;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#65292;&#26088;&#22312;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30149;&#29702;&#23398;&#22330;&#26223;&#20013;&#30340;&#24120;&#35265;&#20998;&#24067;&#20559;&#31227;&#20316;&#20026;&#21069;&#25552;&#20219;&#21153;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#22270;&#20687;&#20013;&#25552;&#21462;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#26631;&#31614;&#65292;&#20174;&#32780;&#28085;&#30422;&#19981;&#21516;&#30340;&#25277;&#35937;&#23618;&#27425;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#25277;&#35937;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#22495;&#23545;&#40784;&#27169;&#22359;&#26469;&#36827;&#19968;&#27493;&#25552;&#21462;&#22312;&#19981;&#21516;&#35757;&#32451;&#21307;&#38498;&#20013;&#30340;&#19981;&#21464;&#29305;&#24449;&#12290;&#20026;&#20102;&#34920;&#31034;&#21442;&#19982;&#21307;&#38498;&#30340;&#39640;&#24230;&#29305;&#23450;&#29305;&#24449;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#23545;&#21307;&#38498;&#26631;&#31614;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#32771;&#34385;&#35786;&#26029;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#27599;&#20010;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#36827;&#34892;&#35299;&#32544;&#20197;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#20998;&#31163;&#29305;&#24449;&#12290;&#36825;&#31181;&#34920;&#31034;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an exhaustive methodology that leverages all levels of feature abstraction, targeting an enhancement in the generalizability of image classification to unobserved hospitals. Our approach incorporates augmentation-based self-supervision with common distribution shifts in histopathology scenarios serving as the pretext task. This enables us to derive invariant features from training images without relying on training labels, thereby covering different abstraction levels. Moving onto the subsequent abstraction level, we employ a domain alignment module to facilitate further extraction of invariant features across varying training hospitals. To represent the highly specific features of participating hospitals, an encoder is trained to classify hospital labels, independent of their diagnostic labels. The features from each of these encoders are subsequently disentangled to minimize redundancy and segregate the features. This representation, which spans a broad spectrum of semanti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#21183;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#65292;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#26399;&#24453;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03908</link><description>&lt;p&gt;
ViLP&#65306;&#20351;&#29992;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#23039;&#21183;&#23884;&#20837;&#36827;&#34892;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#30693;&#35782;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#21183;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#65292;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#26399;&#24453;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#22823;&#37327;&#30340;&#20154;&#31867;&#21160;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;(MML)&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;2D&#39592;&#39612;&#25110;&#23039;&#21183;&#27169;&#24577;&#32463;&#24120;&#34987;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#35201;&#20040;&#29420;&#31435;&#20351;&#29992;&#65292;&#35201;&#20040;&#19982;&#35270;&#39057;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#20449;&#24687;&#65288;RGB&#27169;&#24577;&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25991;&#26412;&#21644;&#23039;&#21183;&#23646;&#24615;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#23578;&#26410;&#25506;&#32034;&#36807;&#23039;&#21183;&#12289;&#35270;&#35273;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;VAR&#30340;&#23039;&#21183;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#20154;&#31867;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;UCF-101&#21644;HMDB-51&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;92.81%&#21644;73.02%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#65292;&#35813;&#31995;&#32479;&#26356;&#21152;&#31169;&#23494;&#12289;&#21487;&#38752;&#12289;&#24555;&#36895;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.03905</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#30340;&#26234;&#33021;&#21161;&#25163;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#65292;&#35813;&#31995;&#32479;&#26356;&#21152;&#31169;&#23494;&#12289;&#21487;&#38752;&#12289;&#24555;&#36895;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#20010;&#20154;&#25968;&#23383;&#21161;&#25163;&#24212;&#29992;&#20110;&#25163;&#26426;&#21644;&#20854;&#20182;&#20010;&#20154;&#35774;&#22791;&#24050;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;&#19982;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#30456;&#27604;&#65292;&#35813;&#31995;&#32479;&#26356;&#20855;&#31169;&#23494;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#36895;&#24230;&#24555;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#26550;&#26500;&#21644;&#25216;&#26415;&#26041;&#38754;&#20570;&#20986;&#30340;&#20851;&#38190;&#36873;&#25321;&#12290;&#20363;&#22914;&#65292;&#23545;&#35805;&#31995;&#32479;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#29615;&#22659;&#20013;&#38590;&#20197;&#38271;&#26399;&#32500;&#25252;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03901</link><description>&lt;p&gt;
FLIPS: &#20351;&#29992;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#19982;&#32773;&#36873;&#25321;&#20013;&#30340;&#22909;&#22788;&#12290;FLIPS&#26681;&#25454;&#25968;&#25454;&#30340;&#26631;&#31614;&#20998;&#24067;&#39044;&#20808;&#23545;&#21442;&#19982;FL&#35757;&#32451;&#20316;&#19994;&#30340;&#21508;&#26041;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;FL&#35757;&#32451;&#26399;&#38388;&#30830;&#20445;&#27599;&#20010;&#32858;&#31867;&#22312;&#34987;&#36873;&#20013;&#30340;&#21442;&#19982;&#32773;&#20013;&#20844;&#24179;&#22320;&#34920;&#31034;&#12290;FLIPS&#21487;&#20197;&#25903;&#25345;&#26368;&#24120;&#35265;&#30340;FL&#31639;&#27861;&#65292;&#21253;&#25324;FedAvg&#65292;FedProx&#65292;FedDyn&#65292;FedOpt&#21644;FedYogi&#12290;&#20026;&#20102;&#31649;&#29702;&#24179;&#21488;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;FLIPS&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#22788;&#29702;&#20998;&#24067;&#24335;&#26234;&#33021;&#31038;&#21306;&#24212;&#29992;&#20013;&#23481;&#37327;&#21464;&#21270;&#30340;&#25302;&#32047;&#31649;&#29702;&#26426;&#21046;&#12290;&#26631;&#31614;&#20998;&#24067;&#12289;&#32858;&#31867;&#21644;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#38544;&#31169;&#36890;&#36807;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;(TEE)&#26469;&#30830;&#20445;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#23558;FLIPS&#19982;&#38543;&#26426;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.03882</link><description>&lt;p&gt;
&#36890;&#36807;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#22686;&#24378;&#21033;&#29992;&#24191;&#20041;&#21270;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#36827;&#34892;&#20445;&#23432;&#20215;&#20540;&#35780;&#20272;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#26080;&#27169;&#22411;&#26041;&#27861;&#20250;&#23545;&#25152;&#26377;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#32780;&#26377;&#27169;&#22411;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#27169;&#22411;&#23637;&#24320;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#36827;&#34892;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25214;&#21040;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#26102;&#23384;&#22312;&#22256;&#38590;&#65306;(a)&#30001;&#20110;&#32423;&#32852;&#27169;&#22411;&#35823;&#24046;&#65292;&#27169;&#22411;&#30340;&#23637;&#24320;&#33539;&#22260;&#38750;&#24120;&#30701;&#65292;(b)&#27169;&#22411;&#23637;&#24320;&#20165;&#20197;&#31163;&#32447;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#20026;&#36215;&#28857;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#31532;&#20108;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26410;&#35265;&#36807;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#20801;&#35768;&#23398;&#24471;&#30340;&#27169;&#22411;&#21644;&#20215;&#20540;&#20272;&#35745;&#22312;&#26410;&#35265;&#29366;&#24577;&#20013;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#36827;&#34892;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#26469;&#25214;&#21040;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#65292;&#28982;&#21518;&#36890;&#36807;&#36807;&#28388;&#20855;&#26377;&#36807;&#39640;&#30340;&#21551;&#21457;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;&#39640;&#35823;&#24046;&#65289;&#25110;&#36807;&#20302;&#30340;&#65288;&#36807;&#20110;&#30456;&#20284;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20840;&#38754;&#20998;&#26512;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#38477;&#20302;&#20102;&#25509;&#35302;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24182;&#21033;&#29992;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#25913;&#21892;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#29702;&#35299;&#65292;&#24110;&#21161;&#25191;&#27861;&#37096;&#38376;&#21644;&#20915;&#31574;&#32773;&#21046;&#23450;&#38024;&#23545;&#24615;&#30340;&#25919;&#31574;&#21644;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.03880</link><description>&lt;p&gt;
&#20445;&#25252;&#23432;&#25252;&#32773;&#65306;&#22312;&#32447;&#20799;&#31461;&#24615;&#34384;&#24453;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse. (arXiv:2308.03880v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03880
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20840;&#38754;&#20998;&#26512;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#38477;&#20302;&#20102;&#25509;&#35302;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24182;&#21033;&#29992;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#25913;&#21892;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#29702;&#35299;&#65292;&#24110;&#21161;&#25191;&#27861;&#37096;&#38376;&#21644;&#20915;&#31574;&#32773;&#21046;&#23450;&#38024;&#23545;&#24615;&#30340;&#25919;&#31574;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20799;&#31461;&#30340;&#22312;&#32447;&#26292;&#21147;&#20107;&#20214;&#26377;&#25152;&#22686;&#21152;&#65292;&#35201;&#27714;&#25105;&#20204;&#20104;&#20197;&#32039;&#24613;&#20851;&#27880;&#12290;&#26377;&#20851;&#37096;&#38376;&#38656;&#35201;&#25163;&#21160;&#20998;&#26512;&#28389;&#29992;&#25237;&#35785;&#20197;&#20102;&#35299;&#29359;&#32618;&#21160;&#24577;&#24182;&#35782;&#21035;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25237;&#35785;&#30340;&#25163;&#21160;&#20998;&#26512;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#26292;&#38706;&#20998;&#26512;&#21592;&#25509;&#35302;&#21040;&#26377;&#23475;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#19968;&#31181;&#19987;&#20026;&#20840;&#38754;&#20998;&#26512;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#36890;&#36807;&#23545;&#25253;&#21578;&#22312;&#20027;&#39064;&#12289;&#29359;&#32618;&#31243;&#24230;&#21644;&#20260;&#23475;&#31243;&#24230;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#25509;&#35302;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#25105;&#20204;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23545;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#23454;&#29616;&#23545;&#25253;&#21578;&#30340;&#26356;&#28145;&#20837;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#29702;&#35299;&#65292;&#20351;&#25191;&#27861;&#37096;&#38376;&#21644;&#20915;&#31574;&#32773;&#33021;&#22815;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
Online violence against children has increased globally recently, demanding urgent attention. Competent authorities manually analyze abuse complaints to comprehend crime dynamics and identify patterns. However, the manual analysis of these complaints presents a challenge because it exposes analysts to harmful content during the review process. Given these challenges, we present a novel solution, an automated tool designed to analyze children's sexual abuse reports comprehensively. By automating the analysis process, our tool significantly reduces the risk of exposure to harmful content by categorizing the reports on three dimensions: Subject, Degree of Criminality, and Damage. Furthermore, leveraging our multidisciplinary team's expertise, we introduce a novel approach to annotate the collected data, enabling a more in-depth analysis of the reports. This approach improves the comprehension of fundamental patterns and trends, enabling law enforcement agencies and policymakers to create 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#25945;&#32946;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;XGBoost&#21644;BERT&#30340;&#32467;&#21512;&#26469;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29305;&#24449;&#26469;&#36755;&#20986;&#26657;&#27491;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.03866</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#20013;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trusting Language Models in Education. (arXiv:2308.03866v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25945;&#32946;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;XGBoost&#21644;BERT&#30340;&#32467;&#21512;&#26469;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29305;&#24449;&#26469;&#36755;&#20986;&#26657;&#27491;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26377;&#26102;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#36991;&#20813;&#21521;&#23398;&#29983;&#23637;&#31034;&#38169;&#35823;&#31572;&#26696;&#65292;&#37325;&#35201;&#30340;&#26159;&#26657;&#20934;&#36825;&#20123;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230; - &#21363;&#39044;&#27979;&#27010;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29305;&#24449;&#30340;XGBoost&#22312;BERT&#20043;&#19978;&#36755;&#20986;&#26657;&#27491;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#27880;&#24847;&#21147;&#27969;&#20013;&#21253;&#21547;&#30340;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#19982;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models are being widely used in Education. Even though modern deep learning models achieve very good performance on question-answering tasks, sometimes they make errors. To avoid misleading students by showing wrong answers, it is important to calibrate the confidence - that is, the prediction probability - of these models. In our work, we propose to use an XGBoost on top of BERT to output the corrected probabilities, using features based on the attention mechanism. Our hypothesis is that the level of uncertainty contained in the flow of attention is related to the quality of the model's response itself.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.03855</link><description>&lt;p&gt;
&#31227;&#21160;&#20379;&#24212;&#65306;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#21518;&#19968;&#22359;&#25340;&#22270;
&lt;/p&gt;
&lt;p&gt;
Mobile Supply: The Last Piece of Jigsaw of Recommender System. (arXiv:2308.03855v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#22312;&#32447;&#24179;&#21488;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#38543;&#30528;&#25163;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#25512;&#33616;&#31639;&#27861;&#37096;&#32626;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#20998;&#39029;&#26426;&#21046;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36793;&#32536;&#31471;&#31227;&#21160;&#25490;&#21517;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#31227;&#21160;&#25490;&#21517;&#21482;&#33021;&#23545;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#65292;&#25152;&#20197;&#22914;&#26524;&#21482;&#35843;&#29992;&#19968;&#20004;&#27425;&#26159;&#19981;&#36215;&#20316;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29992;&#25143;&#26597;&#30475;&#20102;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#21518;&#65292;&#29992;&#25143;&#20250;&#21047;&#26032;&#39029;&#38754;&#33719;&#21462;&#26032;&#30340;&#39033;&#30446;&#12290;&#36825;&#20250;&#20351;&#31227;&#21160;&#25490;&#21517;&#27169;&#22411;&#20570;&#24456;&#22810;&#26080;&#29992;&#21151;&#65292;&#24433;&#21709;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#27969;&#27700;&#32447;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation system is a fundamental functionality of online platforms. With the development of computing power of mobile phones, some researchers have deployed recommendation algorithms on users' devices to solve the problems of data transmission delay and pagination mechanism. However, the existing edge-side mobile rankings cannot completely solve the problem of pagination mechanism. The mobile rankings can only sort the items on the current page, so it will not work if it is called once or twice. Besides, after the user has viewed the items of interest to the user on the current page, the user refresh to get a new page of items. This will make the mobile ranking model do a lot of useless work and affect the user's immersive experience. In order to solve the pagination mechanism problem, we propose a completely new module in the pipeline of recommender named Mobile Supply. The pipeline of recommender system is extended to "retrival-&gt;pre-ranking-&gt;ranking-&gt;re-ranking-&gt;Mobile Supply-&gt;
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#22768;&#26126;&#24335;&#20247;&#21253;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#26126;&#24335;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#36136;&#37327;&#20248;&#21270;&#21644;&#25104;&#26412;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03854</link><description>&lt;p&gt;
&#36890;&#36807;&#22768;&#26126;&#24335;&#20247;&#21253;&#37325;&#26032;&#23457;&#35270;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Revisiting Prompt Engineering via Declarative Crowdsourcing. (arXiv:2308.03854v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#22768;&#26126;&#24335;&#20247;&#21253;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#26126;&#24335;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#36136;&#37327;&#20248;&#21270;&#21644;&#25104;&#26412;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#26497;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#23481;&#26131;&#33030;&#24369;&#21644;&#38169;&#35823;&#12290;&#36817;&#24180;&#26469;&#28044;&#29616;&#20102;&#20197;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#20026;&#20013;&#24515;&#30340;&#24037;&#20855;&#21253;&#21644;&#25216;&#26415;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21521;LLM&#25552;&#20986;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#25968;&#25454;&#22788;&#29702;&#24037;&#20316;&#27969;&#20013;&#65292;&#20248;&#21270;&#36136;&#37327;&#24182;&#20445;&#25345;&#25104;&#26412;&#26377;&#38480;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22768;&#26126;&#24335;&#25552;&#31034;&#24037;&#31243;&#30340;&#24895;&#26223;&#12290;&#25105;&#20204;&#23558;LLM&#35270;&#20026;&#20247;&#21253;&#24037;&#20154;&#65292;&#24182;&#20511;&#37492;&#20102;&#22768;&#26126;&#24335;&#20247;&#21253;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#30830;&#20445;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#25506;&#32034;&#28151;&#21512;LLM&#38750;LLM&#26041;&#27861;&#65292;&#20197;&#20351;&#25552;&#31034;&#24037;&#31243;&#36807;&#31243;&#26356;&#21152;&#21407;&#21017;&#24615;&#12290;&#23545;&#25490;&#24207;&#12289;&#23454;&#20307;&#35299;&#26512;&#21644;&#25554;&#34917;&#30340;&#21021;&#27493;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;HRS10K&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#29992;&#20110;HRSOD&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.03826</link><description>&lt;p&gt;
&#12298;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection. (arXiv:2308.03826v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;HRS10K&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#29992;&#20110;HRSOD&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65288;SOD&#65289;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#21106;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#26368;&#26174;&#33879;&#30340;&#23545;&#35937;&#12290;&#38543;&#30528;&#25104;&#20687;&#35774;&#22791;&#30340;&#36827;&#27493;&#65292;&#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;SOD&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SOD&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#38590;&#20197;&#36866;&#24212;&#39640;&#20998;&#36776;&#29575;SOD&#65288;HRSOD&#65289;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#19968;&#20123;HRSOD&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;HRSOD&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#19981;&#23436;&#25972;&#30340;&#23545;&#35937;&#21306;&#22495;&#21644;&#19981;&#35268;&#21017;&#30340;&#23545;&#35937;&#36793;&#30028;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;HRS10K&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10500&#20010;2K-8K&#20998;&#36776;&#29575;&#30340;&#39640;&#36136;&#37327;&#27880;&#37322;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#30446;&#21069;&#29992;&#20110;HRSOD&#20219;&#21153;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#65292;&#23427;&#23558;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#26410;&#26469;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#26041;&#27861;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Salient Object Detection (SOD) aims to identify and segment the most conspicuous objects in an image or video. As an important pre-processing step, it has many potential applications in multimedia and vision tasks. With the advance of imaging devices, SOD with high-resolution images is of great demand, recently. However, traditional SOD methods are largely limited to low-resolution images, making them difficult to adapt to the development of High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no large enough datasets for training and evaluating. Besides, current HRSOD methods generally produce incomplete object regions and irregular object boundaries. To address above issues, in this work, we first propose a new HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K resolution. As far as we know, it is the largest dataset for the HRSOD task, which will significantly help future works in training and evaluating models. Furthermore, to improve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.03813</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#30340;&#20302;&#20998;&#36776;&#29575;&#28857;&#20113;&#23436;&#25104;&#21464;&#25442;&#22120;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#37117;&#26377;&#25104;&#21315;&#19978;&#19975;&#30340;&#20154;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#39045;&#39592;&#20260;&#23475;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#26893;&#20837;&#29289;&#65292;&#25163;&#24037;&#35774;&#35745;&#26114;&#36149;&#19988;&#36153;&#26102;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19987;&#29992;&#31995;&#32479;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#24517;&#35201;&#12290;&#33258;&#21160;&#39045;&#39592;&#32570;&#25439;&#37325;&#24314;&#30340;&#38382;&#39064;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#24418;&#29366;&#23436;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#19987;&#29992;&#28145;&#24230;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30446;&#21069;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20307;&#31215;&#34920;&#31034;&#27861;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#20998;&#36776;&#29575;&#20307;&#31215;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#20998;&#36776;&#29575;&#19979;&#37325;&#24314;&#39045;&#32570;&#25439;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22810;&#20010;&#34920;&#31034;&#31354;&#38388;&#24182;&#26681;&#25454;&#22810;&#20010;&#26041;&#38754;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03805</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#20998;&#26512;&#30340;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables. (arXiv:2308.03805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22810;&#20010;&#34920;&#31034;&#31354;&#38388;&#24182;&#26681;&#25454;&#22810;&#20010;&#26041;&#38754;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#29615;&#22659;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#12289;&#20154;&#21592;&#35782;&#21035;&#25110;&#20581;&#24247;&#30417;&#27979;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#27963;&#21160;&#21644;&#20256;&#24863;&#22120;&#27969;&#20998;&#26512;&#26041;&#38754;&#30340;&#22823;&#37096;&#20998;&#20808;&#21069;&#24037;&#20316;&#37117;&#20391;&#37325;&#20110;&#25968;&#25454;&#30340;&#26576;&#19968;&#26041;&#38754;&#65292;&#20363;&#22914;&#20165;&#35782;&#21035;&#27963;&#21160;&#31867;&#22411;&#25110;&#20165;&#35782;&#21035;&#25191;&#34892;&#27963;&#21160;&#30340;&#20154;&#21592;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#22810;&#36755;&#20986;&#23402;&#29983;&#32593;&#32476;&#65292;&#23398;&#20064;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22810;&#20010;&#34920;&#31034;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#34920;&#31034;&#31354;&#38388;&#20391;&#37325;&#20110;&#25968;&#25454;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#25968;&#25454;&#26679;&#26412;&#30340;&#34920;&#31034;&#21521;&#37327;&#22312;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#20351;&#24471;&#22312;&#35813;&#26041;&#38754;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#24847;&#20041;&#30340;&#25968;&#25454;&#24444;&#27492;&#38752;&#36817;&#12290;&#22240;&#27492;&#65292;&#27491;&#22914;&#19968;&#31995;&#21015;&#23454;&#39564;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#22810;&#20010;&#26041;&#38754;&#20026;&#25968;&#25454;&#32858;&#31867;&#25552;&#20379;&#24230;&#37327;&#25351;&#26631;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and eve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#30740;&#31350;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>http://arxiv.org/abs/2308.03800</link><description>&lt;p&gt;
&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#25991;&#26412;&#25968;&#25454;&#25366;&#25496;&#65306;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#30740;&#31350;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;&#20197;&#19979;&#31616;&#31216;NLP&#65289;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#12290;&#39318;&#20808;&#65292;&#25105;&#25628;&#32034;&#20102;&#28207;&#20132;&#25152;&#26032;&#38395;&#30340;&#30417;&#31649;&#20844;&#21578;&#21644;&#25191;&#27861;&#20844;&#21578;&#65292;&#20197;&#23450;&#20041;&#27450;&#35784;&#20844;&#21496;&#24182;&#25552;&#21462;&#20854;MD&#65286;A&#25253;&#21578;&#65292;&#28982;&#21518;&#25972;&#29702;&#20102;&#25253;&#21578;&#20013;&#30340;&#21477;&#23376;&#65292;&#24182;&#26631;&#35760;&#20102;&#25253;&#21578;&#26102;&#38388;&#12290;&#25105;&#30340;&#26041;&#27861;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#20855;&#26377;&#23884;&#20837;&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#22522;&#26412;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#38480;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#25105;&#26088;&#22312;&#20840;&#38754;&#27604;&#36739;&#23427;&#20204;&#22312;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#30340;&#32467;&#26524;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#36825;&#39033;&#24037;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#65292;NLP&#21644;&#37329;&#34701;&#20132;&#21449;&#30740;&#31350;&#30340;&#19981;&#26029;&#22686;&#38271;&#36129;&#29486;&#20102;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&amp;A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#20041;&#20449;&#36947;&#22343;&#34913;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#29992;&#25143;&#35821;&#20041;&#36890;&#20449;&#20013;&#30001;&#20110;&#19981;&#21516;&#35821;&#35328;&#23548;&#33268;&#30340;&#35821;&#20041;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03789</link><description>&lt;p&gt;
&#35821;&#20041;&#20449;&#36947;&#22343;&#34913;&#22120;: &#22312;&#22810;&#29992;&#25143;&#35821;&#20041;&#36890;&#20449;&#20013;&#24314;&#27169;&#35821;&#35328;&#19981;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications. (arXiv:2308.03789v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#20041;&#20449;&#36947;&#22343;&#34913;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#29992;&#25143;&#35821;&#20041;&#36890;&#20449;&#20013;&#30001;&#20110;&#19981;&#21516;&#35821;&#35328;&#23548;&#33268;&#30340;&#35821;&#20041;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22810;&#29992;&#25143;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#20195;&#29702;&#65288;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#65289;&#36890;&#36807;&#20132;&#25442;&#35821;&#20041;&#28040;&#24687;&#36827;&#34892;&#20132;&#27969;&#24182;&#20256;&#36798;&#24847;&#20041;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35821;&#35328;&#22312;&#26500;&#24314;&#21644;&#24041;&#22266;&#30693;&#35782;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#24433;&#21709;&#27010;&#24565;&#34920;&#31034;&#21644;&#35821;&#20041;&#25552;&#21462;&#21644;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#20041;&#36890;&#20449;&#20013;&#65292;&#35821;&#35328;&#30340;&#20851;&#38190;&#20316;&#29992;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#24403;&#19981;&#32771;&#34385;&#36825;&#19968;&#28857;&#26102;&#65292;&#20195;&#29702;&#30340;&#35821;&#35328;&#34987;&#20551;&#35774;&#20026;&#20860;&#23481;&#21644;&#19968;&#33268;&#30340;&#65292;&#24573;&#35270;&#20102;&#30001;&#20110;&#35821;&#35328;&#19981;&#21305;&#37197;&#21487;&#33021;&#24341;&#36215;&#30340;&#23454;&#38469;&#38480;&#21046;&#12290;&#36825;&#26159;&#26412;&#25991;&#30340;&#37325;&#28857;&#12290;&#24403;&#20195;&#29702;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#26102;&#65292;&#28040;&#24687;&#35299;&#37322;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#35821;&#20041;&#36890;&#36947;&#24341;&#20837;&#30340;&#20005;&#37325;&#22833;&#30495;&#32780;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;&#20449;&#36947;&#22343;&#34913;&#22120;&#65292;&#20197;&#25269;&#28040;&#24182;&#38480;&#21046;&#28040;&#24687;&#35299;&#37322;&#20013;&#30340;&#20005;&#37325;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#27169;&#25311;&#20102;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a multi-user semantic communications system in which agents (transmitters and receivers) interact through the exchange of semantic messages to convey meanings. In this context, languages are instrumental in structuring the construction and consolidation of knowledge, influencing conceptual representation and semantic extraction and interpretation. Yet, the crucial role of languages in semantic communications is often overlooked. When this is not the case, agent languages are assumed compatible and unambiguously interoperable, ignoring practical limitations that may arise due to language mismatching. This is the focus of this work. When agents use distinct languages, message interpretation is prone to semantic noise resulting from critical distortion introduced by semantic channels. To address this problem, this paper proposes a new semantic channel equalizer to counteract and limit the critical ambiguity in message interpretation. Our proposed solution models the mismatch o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#20215;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT Base&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;11%&#30340;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.03782</link><description>&lt;p&gt;
Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#35770;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#20215;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT Base&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;11%&#30340;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#20998;&#26512;&#24739;&#32773;&#33647;&#29289;&#35780;&#35770;&#24182;&#20934;&#30830;&#20998;&#31867;&#28385;&#24847;&#31243;&#24230;&#20026;&#31215;&#26497;&#12289;&#20013;&#24615;&#25110;&#28040;&#26497;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#35265;&#35299;&#65292;&#36825;&#26159;&#27835;&#30103;&#25928;&#26524;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#22810;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#21253;&#25324;BERT base&#27169;&#22411;&#12289;Bio+Clinical BERT&#20197;&#21450;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;CNN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT base&#27169;&#22411;&#65292;&#22914;&#34920;2&#25152;&#31034;&#65292;&#23427;&#22312;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#19978;&#25552;&#21319;&#20102;11%&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#20197;&#25506;&#32034;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#27169;&#22411;&#30340;&#29305;&#23450;&#20248;&#21183;&#12290;Bio+Clinical BERT&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#21307;&#23398;&#34892;&#35805;&#26041;&#38754;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;CNN&#21017;&#23637;&#29616;&#20986;&#35782;&#21035;&#20851;&#38190;&#35789;&#21644;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#27010;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;&#65292;&#24182;&#20197;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#20026;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.03773</link><description>&lt;p&gt;
&#23646;&#24615;&#27010;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Goodness-of-Fit of Attributed Probabilistic Graph Generative Models. (arXiv:2308.03773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#27010;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;&#65292;&#24182;&#20197;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#20026;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26159;&#33021;&#22815;&#36827;&#34892;&#34920;&#31034;&#21644;&#25277;&#26679;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21019;&#24314;&#20102;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#23646;&#24615;&#30340;&#22270;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#38543;&#26426;&#23646;&#24615;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30830;&#23450;&#20854;&#25311;&#21512;&#20248;&#24230;&#30340;&#19968;&#33324;&#26465;&#20214;&#24182;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#38543;&#26426;&#20108;&#36827;&#21046;&#32593;&#32476;&#30340;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#20026;&#22522;&#20934;&#26469;&#23450;&#20041;&#25311;&#21512;&#20248;&#24230;&#12290;&#23545;&#20110;&#36825;&#20010;&#32479;&#35745;&#37327;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#23646;&#24615;&#22270;&#32467;&#26500;&#36136;&#37327;&#30340;&#36807;&#31243;&#65292;&#30830;&#20445;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#30340;&#20559;&#24046;&#26368;&#23567;&#65292;&#24182;&#19988;&#39640;&#27010;&#29575;&#19979;&#20445;&#25345;&#24658;&#23450;&#25110;&#38543;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20934;&#21017;&#24212;&#29992;&#20110;&#39564;&#35777;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#27969;&#34892;&#30340;&#22270;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic generative models of graphs are important tools that enable representation and sampling. Many recent works have created probabilistic models of graphs that are capable of representing not only entity interactions but also their attributes. However, given a generative model of random attributed graph(s), the general conditions that establish goodness of fit are not clear a-priori. In this paper, we define goodness of fit in terms of the mean square contingency coefficient for random binary networks. For this statistic, we outline a procedure for assessing the quality of the structure of a learned attributed graph by ensuring that the discrepancy of the mean square contingency coefficient (constant, or random) is minimal with high probability. We apply these criteria to verify the representation capability of a probabilistic generative model for various popular types of graph models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#32534;&#30721;&#20307;&#21644;&#25552;&#20379;&#22810;&#23610;&#24230;&#20960;&#20309;&#20449;&#24687;&#65292;&#21516;&#26102;&#36827;&#34892;&#28145;&#24230;&#39044;&#27979;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#65292;&#20197;&#23454;&#29616;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#20934;&#30830;&#24314;&#27169;&#21644;&#35270;&#35282;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.03772</link><description>&lt;p&gt;
&#20351;&#29992;&#20266;&#28145;&#24230;&#21644;&#34701;&#21512;&#25216;&#26415;&#25913;&#36827;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Improved Neural Radiance Fields Using Pseudo-depth and Fusion. (arXiv:2308.03772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#32534;&#30721;&#20307;&#21644;&#25552;&#20379;&#22810;&#23610;&#24230;&#20960;&#20309;&#20449;&#24687;&#65292;&#21516;&#26102;&#36827;&#34892;&#28145;&#24230;&#39044;&#27979;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#65292;&#20197;&#23454;&#29616;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#20934;&#30830;&#24314;&#27169;&#21644;&#35270;&#35282;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#25216;&#26415;&#38382;&#19990;&#20197;&#26469;&#65292;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#36752;&#23556;&#22330;&#37325;&#24314;&#36890;&#24120;&#36890;&#36807;&#20174;&#38468;&#36817;&#30340;&#28304;&#22270;&#20687;&#20013;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#20307;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#32534;&#30721;&#30495;&#23454;&#22330;&#26223;&#20013;&#21508;&#31181;&#35268;&#27169;&#30340;&#29289;&#20307;/&#32467;&#26500;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#22810;&#23610;&#24230;&#32534;&#30721;&#20307;&#24182;&#20026;NeRF&#27169;&#22411;&#25552;&#20379;&#22810;&#23610;&#24230;&#20960;&#20309;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20026;&#20351;&#26500;&#24314;&#30340;&#32534;&#30721;&#20307;&#23613;&#21487;&#33021;&#25509;&#36817;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#34920;&#38754;&#21644;&#28210;&#26579;&#28145;&#24230;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#36827;&#34892;&#28145;&#24230;&#39044;&#27979;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#12290;&#39044;&#27979;&#30340;&#28145;&#24230;&#22270;&#23558;&#29992;&#20110;&#30417;&#30563;&#28210;&#26579;&#28145;&#24230;&#12289;&#32553;&#23567;&#28145;&#24230;&#33539;&#22260;&#24182;&#24341;&#23548;&#28857;&#37319;&#26679;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#36974;&#25377;&#12289;&#20809;&#29031;&#31561;&#21407;&#22240;&#65292;&#28857;&#20307;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#20960;&#20309;&#20449;&#24687;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the advent of Neural Radiance Fields, novel view synthesis has received tremendous attention. The existing approach for the generalization of radiance field reconstruction primarily constructs an encoding volume from nearby source images as additional inputs. However, these approaches cannot efficiently encode the geometric information of real scenes with various scale objects/structures. In this work, we propose constructing multi-scale encoding volumes and providing multi-scale geometry information to NeRF models. To make the constructed volumes as close as possible to the surfaces of objects in the scene and the rendered depth more accurate, we propose to perform depth prediction and radiance field reconstruction simultaneously. The predicted depth map will be used to supervise the rendered depth, narrow the depth range, and guide points sampling. Finally, the geometric information contained in point volume features may be inaccurate due to occlusion, lighting, etc. To this en
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#20013;&#32467;&#21512;&#39550;&#39542;&#21592;&#21980;&#30561;&#26816;&#27979;&#21644;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#22330;&#26223;&#29702;&#35299;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#29305;&#27530;&#30340;3D&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#24182;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#23454;&#29616;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#35782;&#21035;&#39550;&#39542;&#21592;&#30340;&#27880;&#24847;&#21147;&#21644;&#29615;&#22659;&#20013;&#30340;&#26174;&#33879;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.03770</link><description>&lt;p&gt;
&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#20013;&#30340;&#35270;&#35273;&#26174;&#33879;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Saliency Detection in Advanced Driver Assistance Systems. (arXiv:2308.03770v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03770
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#20013;&#32467;&#21512;&#39550;&#39542;&#21592;&#21980;&#30561;&#26816;&#27979;&#21644;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#22330;&#26223;&#29702;&#35299;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#29305;&#27530;&#30340;3D&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#24182;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#23454;&#29616;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#35782;&#21035;&#39550;&#39542;&#21592;&#30340;&#27880;&#24847;&#21147;&#21644;&#29615;&#22659;&#20013;&#30340;&#26174;&#33879;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26174;&#33879;&#24615;&#26159;&#25351;&#20154;&#31867;&#20174;&#35266;&#23519;&#29615;&#22659;&#20013;&#32858;&#28966;&#24182;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#20272;&#35745;&#35270;&#35273;&#26174;&#33879;&#24615;&#30340;&#27773;&#36710;&#30740;&#31350;&#39046;&#22495;&#20986;&#29616;&#20102;&#26126;&#26174;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#22312;&#39550;&#39542;&#27773;&#36710;&#26102;&#65292;&#39550;&#39542;&#21592;&#33258;&#28982;&#32780;&#28982;&#22320;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#29305;&#23450;&#23545;&#35937;&#19978;&#65292;&#21033;&#29992;&#22823;&#33041;&#39537;&#21160;&#30340;&#26174;&#33879;&#24615;&#26426;&#21046;&#26469;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#20803;&#32032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#31995;&#32479;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#21980;&#30561;&#26816;&#27979;&#31995;&#32479;&#19982;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#22330;&#26223;&#29702;&#35299;&#31649;&#32447;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;3D&#28145;&#24230;&#32593;&#32476;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#39044;&#35757;&#32451;&#21644;&#23450;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#27773;&#36710;&#32423;&#22806;&#37096;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#24103;&#12290;&#25152;&#25552;&#20986;&#30340;&#31649;&#32447;&#36816;&#34892;&#22312;&#19968;&#20010;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#65292;&#37319;&#29992;STA1295&#26680;&#24515;&#65292;&#20855;&#26377;ARM A7&#21452;&#26680;&#65292;&#24182;&#23884;&#20837;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Saliency refers to the innate human mechanism of focusing on and extracting important features from the observed environment. Recently, there has been a notable surge of interest in the field of automotive research regarding the estimation of visual saliency. While operating a vehicle, drivers naturally direct their attention towards specific objects, employing brain-driven saliency mechanisms that prioritize certain elements over others. In this investigation, we present an intelligent system that combines a drowsiness detection system for drivers with a scene comprehension pipeline based on saliency. To achieve this, we have implemented a specialized 3D deep network for semantic segmentation, which has been pretrained and tailored for processing the frames captured by an automotive-grade external camera. The proposed pipeline was hosted on an embedded platform utilizing the STA1295 core, featuring ARM A7 dual-cores, and embeds an hardware accelerator. Additionally, we employ a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26435;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#20132;&#36890;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#35782;&#21644;&#28608;&#21169;&#26426;&#21046;&#23454;&#29616;&#20102;&#33021;&#28304;&#28040;&#32791;&#25928;&#29575;&#30340;&#20840;&#23616;&#20849;&#35782;&#65292;&#24182;&#20248;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20195;&#29702;&#30340;&#26412;&#22320;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;DAO&#20013;&#30340;&#32467;&#26500;&#20725;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25805;&#20316;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#22522;&#20110;DAO&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03769</link><description>&lt;p&gt;
&#23454;&#29616;&#19982;&#25805;&#20316;&#20998;&#26435;&#33258;&#27835;&#32452;&#32455;&#38598;&#25104;&#30340;&#20132;&#36890;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Integrated Traffic Control with Operating Decentralized Autonomous Organization. (arXiv:2308.03769v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26435;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#20132;&#36890;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#35782;&#21644;&#28608;&#21169;&#26426;&#21046;&#23454;&#29616;&#20102;&#33021;&#28304;&#28040;&#32791;&#25928;&#29575;&#30340;&#20840;&#23616;&#20849;&#35782;&#65292;&#24182;&#20248;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20195;&#29702;&#30340;&#26412;&#22320;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;DAO&#20013;&#30340;&#32467;&#26500;&#20725;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25805;&#20316;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#22522;&#20110;DAO&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#26085;&#30410;&#22797;&#26434;&#21270;&#65292;&#20154;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#21040;&#21508;&#31181;&#24322;&#26500;&#26234;&#33021;&#20195;&#29702;&#30340;&#38598;&#25104;&#25511;&#21046;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25511;&#21046;&#26041;&#27861;&#22312;&#21516;&#26102;&#32771;&#34385;&#21040;&#26368;&#20248;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#24182;&#26410;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26435;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#25511;&#21046;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20849;&#35782;&#21644;&#28608;&#21169;&#26426;&#21046;&#65292;&#22312;&#36798;&#21040;&#33021;&#28304;&#28040;&#32791;&#25928;&#29575;&#65288;ECE&#65289;&#30340;&#20840;&#23616;&#20849;&#35782;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#28041;&#21450;&#30340;&#25152;&#26377;&#26234;&#33021;&#20195;&#29702;&#30340;&#26412;&#22320;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;DAO&#20013;&#30340;&#32467;&#26500;&#20725;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25805;&#20316;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#25805;&#20316;&#26041;&#27861;&#35782;&#21035;&#20986;&#22312;DAO&#20013;&#25191;&#34892;&#26234;&#33021;&#21512;&#32422;&#30340;&#20851;&#38190;&#20195;&#29702;&#65292;&#20174;&#32780;&#26368;&#32456;&#25193;&#23637;&#20102;&#22522;&#20110;DAO&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a growing complexity of the intelligent traffic system (ITS), an integrated control of ITS that is capable of considering plentiful heterogeneous intelligent agents is desired. However, existing control methods based on the centralized or the decentralized scheme have not presented their competencies in considering the optimality and the scalability simultaneously. To address this issue, we propose an integrated control method based on the framework of Decentralized Autonomous Organization (DAO). The proposed method achieves a global consensus on energy consumption efficiency (ECE), meanwhile to optimize the local objectives of all involved intelligent agents, through a consensus and incentive mechanism. Furthermore, an operation algorithm is proposed regarding the issue of structural rigidity in DAO. Specifically, the proposed operation approach identifies critical agents to execute the smart contract in DAO, which ultimately extends the capability of DAO-based control. In additi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#21477;&#23383;&#24149;&#25551;&#36848;3D&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#22330;&#26223;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#19979;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.03767</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#22686;&#24378;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#21477;&#23383;&#24149;&#25551;&#36848;3D&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#22330;&#26223;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#19979;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#12290;&#34429;&#28982;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20248;&#31168;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35813;&#39046;&#22495;&#20027;&#35201;&#38598;&#20013;&#20110;&#20026;2D&#22270;&#20687;&#29983;&#25104;&#21333;&#20010;&#21477;&#23376;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#23558;&#28145;&#24230;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#38598;&#25104;&#65292;&#33021;&#21542;&#22686;&#24378;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#24182;&#29983;&#25104;&#26356;&#22909;&#30340;&#25551;&#36848;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#22330;&#26223;&#30340;&#22810;&#21477;&#23383;&#24149;&#25551;&#36848;&#12290;RGB&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#28145;&#24230;&#22270;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#22330;&#26223;&#12290;&#28145;&#24230;&#22270;&#21487;&#20197;&#26159;&#30495;&#23454;&#20540;&#25110;&#20272;&#35745;&#20540;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20219;&#20309;RGB&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#26469;&#34701;&#21512;RGB&#21644;&#28145;&#24230;&#22270;&#20687;&#12290;&#23454;&#39564;&#22312;NYU-v2&#25968;&#25454;&#38598;&#21644;Stanford&#22270;&#20687;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image para
&lt;/p&gt;</description></item><item><title>AMaizeD&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#33719;&#21462;&#30340;&#22810;&#20809;&#35889;&#22270;&#20687;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#21106;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#29577;&#31859;&#20316;&#29289;&#30149;&#23475;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.03766</link><description>&lt;p&gt;
AMaizeD: &#19968;&#31181;&#33258;&#21160;&#29577;&#31859;&#30149;&#23475;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection. (arXiv:2308.03766v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03766
&lt;/p&gt;
&lt;p&gt;
AMaizeD&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#33719;&#21462;&#30340;&#22810;&#20809;&#35889;&#22270;&#20687;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#21106;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#29577;&#31859;&#20316;&#29289;&#30149;&#23475;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AMaizeD&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#26469;&#33258;&#26080;&#20154;&#26426;&#30340;&#22810;&#20809;&#35889;&#22270;&#20687;&#23545;&#29577;&#31859;&#20316;&#29289;&#20013;&#30149;&#23475;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#19987;&#23478;&#30740;&#31350;&#20154;&#21592;&#21644;&#20892;&#23398;&#23478;&#36890;&#36807;&#31934;&#24515;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#32858;&#28966;&#20110;&#29577;&#31859;&#20316;&#29289;&#65292;&#25910;&#38598;&#20102;&#21508;&#31181;&#29577;&#31859;&#21697;&#31181;&#12289;&#26685;&#22521;&#23454;&#36341;&#21644;&#29615;&#22659;&#26465;&#20214;&#65292;&#25429;&#25417;&#20102;&#29577;&#31859;&#29983;&#38271;&#21644;&#30149;&#23475;&#21457;&#23637;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20809;&#35889;&#22270;&#20687;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#20809;&#35889;&#20998;&#36776;&#29575;&#21644;&#23545;&#26893;&#29289;&#20581;&#24247;&#24494;&#23567;&#21464;&#21270;&#30340;&#22686;&#24378;&#25935;&#24863;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#21106;&#25216;&#26415;&#65292;&#35782;&#21035;&#29577;&#31859;&#26893;&#26666;&#21450;&#20854;&#30456;&#20851;&#30149;&#23475;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#21508;&#31181;&#29577;&#31859;&#30149;&#23475;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, incl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#26234;&#33021;&#24037;&#20316;&#21306;&#24212;&#29992;&#20013;&#37096;&#32626;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#38431;&#30340;&#20132;&#36890;&#20998;&#37197;&#26041;&#27861;&#26469;&#20248;&#21270;ATMA&#36710;&#36742;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#20174;&#32780;&#20943;&#23569;&#24930;&#36895;&#36816;&#34892;&#23548;&#33268;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.03764</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#24037;&#20316;&#21306;&#24212;&#29992;&#20013;&#37096;&#32626;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31995;&#32479;&#24182;&#37319;&#29992;&#22522;&#20110;&#25490;&#38431;&#30340;&#20132;&#36890;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach. (arXiv:2308.03764v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#26234;&#33021;&#24037;&#20316;&#21306;&#24212;&#29992;&#20013;&#37096;&#32626;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#38431;&#30340;&#20132;&#36890;&#20998;&#37197;&#26041;&#27861;&#26469;&#20248;&#21270;ATMA&#36710;&#36742;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#20174;&#32780;&#20943;&#23569;&#24930;&#36895;&#36816;&#34892;&#23548;&#33268;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#25216;&#26415;&#33258;&#20027;&#21345;&#36710;&#35013;&#37197;&#22120;(ATMA)&#26159;&#19968;&#31181;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#39118;&#26684;&#30340;&#36710;&#36742;&#31995;&#32479;&#65292;&#21033;&#29992;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#26469;&#22686;&#24378;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#32500;&#25252;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;ATMA&#36710;&#36742;&#19982;&#26222;&#36890;&#36710;&#36742;&#20043;&#38388;&#30340;&#36895;&#24230;&#24046;&#24322;&#36896;&#25104;&#20102;&#31227;&#21160;&#29942;&#39048;&#65292;&#38477;&#20302;&#20102;&#36890;&#34892;&#33021;&#21147;&#24182;&#22686;&#21152;&#20102;&#25490;&#38431;&#38271;&#24230;&#65292;&#36827;&#32780;&#23548;&#33268;&#39069;&#22806;&#30340;&#24310;&#35823;&#12290;ATMA&#37319;&#21462;&#19981;&#21516;&#36335;&#32447;&#23548;&#33268;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#19981;&#21516;&#23481;&#37327;&#38477;&#20302;&#27169;&#24335;&#65292;&#21487;&#33021;&#24433;&#21709;&#29992;&#25143;&#22343;&#34913;&#20132;&#36890;&#20998;&#37197;&#24182;&#23548;&#33268;&#19981;&#21516;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;&#26412;&#25991;&#26088;&#22312;&#20248;&#21270;ATMA&#36710;&#36742;&#22312;&#32593;&#32476;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#20197;&#26368;&#23567;&#21270;&#19982;&#24930;&#36895;&#36816;&#34892;&#30456;&#20851;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#38431;&#30340;&#20132;&#36890;&#20998;&#37197;&#26041;&#27861;&#26469;&#35782;&#21035;&#30001;ATMA&#31995;&#32479;&#24341;&#36215;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;&#24341;&#20837;&#20102;&#32771;&#34385;&#23481;&#37327;&#38477;&#20302;&#30340;&#22522;&#20110;&#25490;&#38431;&#30340;&#26102;&#38388;&#30456;&#20851;(QBTD)&#34892;&#31243;&#26102;&#38388;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging technology of the Autonomous Truck Mounted Attenuator (ATMA), a leader-follower style vehicle system, utilizes connected and automated vehicle capabilities to enhance safety during transportation infrastructure maintenance in work zones. However, the speed difference between ATMA vehicles and general vehicles creates a moving bottleneck that reduces capacity and increases queue length, resulting in additional delays. The different routes taken by ATMA cause diverse patterns of time-varying capacity drops, which may affect the user equilibrium traffic assignment and lead to different system costs. This manuscript focuses on optimizing the routing for ATMA vehicles in a network to minimize the system cost associated with the slow-moving operation.  To achieve this, a queuing-based traffic assignment approach is proposed to identify the system cost caused by the ATMA system. A queuing-based time-dependent (QBTD) travel time function, considering capacity drop, is introduced a
&lt;/p&gt;</description></item><item><title>MedMine&#36890;&#36807;&#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.03629</link><description>&lt;p&gt;
MedMine: &#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03629
&lt;/p&gt;
&lt;p&gt;
MedMine&#36890;&#36807;&#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#36827;&#34892;&#33647;&#29289;&#25366;&#25496;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#23545;&#21307;&#30103;&#24212;&#29992;&#30340;&#30495;&#23454;&#24433;&#21709;&#20197;&#21450;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20840;&#33258;&#21160;&#25552;&#21462;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#38556;&#30861;&#65292;&#20197;&#20415;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#20020;&#24202;&#23454;&#36341;&#20013;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#38556;&#30861;&#21253;&#25324;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#65292;&#21253;&#25324;&#22522;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;Med7&#21644;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa&#30340;&#26041;&#24335;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#25361;&#25112;&#36187;&#30340;&#21382;&#21490;&#33647;&#29289;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23427;&#20204;&#30340;&#20248;&#21155;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#24494;&#35843;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#20197;&#20415;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#22914;&#20309;&#32467;&#21512;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25110;&#32773;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or impr
&lt;/p&gt;</description></item><item><title>RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03421</link><description>&lt;p&gt;
RecycleGPT&#65306;&#19968;&#31181;&#20855;&#26377;&#21487;&#22238;&#25910;&#27169;&#22359;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03421
&lt;/p&gt;
&lt;p&gt;
RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24517;&#39035;&#36816;&#34892;K&#27425;&#25165;&#33021;&#29983;&#25104;K&#20010;&#20196;&#29260;&#30340;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RecycleGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#23558;&#25972;&#20010;&#27169;&#22411;&#36816;&#34892;&#22810;&#27425;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#24207;&#21015;&#20013;&#30456;&#37051;&#30340;&#20196;&#29260;&#36890;&#24120;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#21069;&#38754;&#30340;&#20196;&#29260;&#21512;&#29702;&#29468;&#27979;&#25110;&#25512;&#26029;&#20986;&#19979;&#19968;&#20010;&#20196;&#29260;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;1.4&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02632</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27169;&#25311;FMCW&#38647;&#36798;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22522;&#20110;&#23556;&#32447;&#36861;&#36394;&#65292;&#36890;&#24120;&#35745;&#31639;&#23494;&#38598;&#19988;&#19981;&#33021;&#32771;&#34385;&#32972;&#26223;&#22122;&#22768;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;FMCW&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#21512;&#25104;&#30340;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;&#35813;&#26041;&#27861;&#29983;&#25104;&#20102;16&#20010;&#21516;&#26102;&#30340;&#33033;&#20914;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#24320;&#21457;&#29992;&#20110;&#22788;&#29702;&#38647;&#36798;&#25968;&#25454;&#65288;&#28388;&#27874;&#21644;&#32858;&#31867;&#65289;&#30340;&#31639;&#27861;&#12290;&#36825;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#19981;&#21487;&#22797;&#29616;&#30340;&#19981;&#23384;&#22312;&#25110;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25705;&#25176;&#36710;&#30340;&#38647;&#36798;&#27979;&#37327;&#25968;&#25454;&#23545;GAN&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#29983;&#25104;&#25705;&#25176;&#36710;&#30452;&#32447;&#34892;&#39542;&#30340;&#21512;&#25104;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#26102;&#65292;&#20351;&#29992;&#20102;&#25705;&#25176;&#36710;&#30340;&#36317;&#31163;&#21644;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#23558;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#21644;&#22914;&#20309;&#20135;&#29983;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01941</link><description>&lt;p&gt;
&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;&#65306;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Digital twin brain: a bridge between biological intelligence and artificial intelligence. (arXiv:2308.01941v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#23558;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#21644;&#22914;&#20309;&#20135;&#29983;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#20026;&#29702;&#35299;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;&#31995;&#32479;&#26469;&#27169;&#25311;&#22823;&#33041;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#21069;&#27839;&#36827;&#23637;&#25581;&#31034;&#20102;&#22823;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#24378;&#35843;&#20102;&#32593;&#32476;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#22312;&#26159;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#26356;&#22909;&#22320;&#25581;&#31034;&#26234;&#33021;&#26159;&#22914;&#20309;&#20174;&#22823;&#33041;&#30340;&#22810;&#23610;&#24230;&#23384;&#20648;&#24211;&#20013;&#20135;&#29983;&#30340;&#26102;&#20505;&#20102;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;(DTB)&#20316;&#20026;&#19968;&#20010;&#23558;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#32852;&#31995;&#36215;&#26469;&#30340;&#36716;&#21464;&#24615;&#24179;&#21488;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#26680;&#24515;&#20803;&#32032;&#65306;&#23545;&#21452;&#32990;&#32974;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#33041;&#32467;&#26500;&#12289;&#29992;&#20110;&#29983;&#25104;&#22823;&#33041;&#21151;&#33021;&#30340;&#24213;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#20851;&#38190;&#26159;&#65292;&#22823;&#33041;&#22270;&#35889;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20445;&#25345;&#20102;&#22823;&#33041;&#30340;&#32593;&#32476;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in neuroscience and artificial intelligence have paved the way for unprecedented opportunities for understanding the complexity of the brain and its emulation by computational systems. Cutting-edge advancements in neuroscience research have revealed the intricate relationship between brain structure and function, while the success of artificial neural networks highlights the importance of network architecture. Now is the time to bring them together to better unravel how intelligence emerges from the brain's multiscale repositories. In this review, we propose the Digital Twin Brain (DTB) as a transformative platform that bridges the gap between biological and artificial intelligence. It consists of three core elements: the brain structure that is fundamental to the twinning process, bottom-layer models to generate brain functions, and its wide spectrum of applications. Crucially, brain atlases provide a vital constraint, preserving the brain's network organizat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01681</link><description>&lt;p&gt;
NBIAS: &#29992;&#20110;&#25991;&#26412;&#20013;&#20559;&#35265;&#35782;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20351;&#29992;&#26102;&#20135;&#29983;&#20542;&#26012;&#30340;&#35299;&#37322;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#25345;&#32493;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#12289;&#27495;&#35270;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31639;&#27861;&#26368;&#32456;&#20250;&#20570;&#20986;&#19981;&#24179;&#31561;&#24433;&#21709;&#26576;&#20010;&#32676;&#20307;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#20844;&#24179;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;"NBIAS"&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#35821;&#26009;&#24211;&#26500;&#24314;&#12289;&#27169;&#22411;&#24320;&#21457;&#23618;&#21644;&#35780;&#20272;&#23618;&#12290;&#25968;&#25454;&#38598;&#30001;&#20174;&#21508;&#20010;&#39046;&#22495;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#38376;&#25143;&#32593;&#31449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21807;&#19968;&#30340;&#21629;&#21517;&#23454;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
&lt;/p&gt;</description></item><item><title>FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01006</link><description>&lt;p&gt;
FusionAD: &#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01006
&lt;/p&gt;
&lt;p&gt;
FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24615;&#33021;&#24050;&#25104;&#20026;&#38081;&#26495;&#19968;&#22359;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#32852;&#21512;&#20248;&#21270;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#20173;&#28982;&#20960;&#20046;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FusionAD&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#20004;&#20010;&#26368;&#20851;&#38190;&#20256;&#24863;&#22120;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#36229;&#36234;&#24863;&#30693;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;&#22522;&#20110;&#34701;&#21512;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#19982;&#22522;&#20110;&#30456;&#26426;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;UniAD&#30456;&#27604;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#34701;&#21512;&#36741;&#21161;&#30340;&#27169;&#24577;&#24863;&#30693;&#39044;&#27979;&#21644;&#29366;&#24577;&#24863;&#30693;&#35268;&#21010;&#27169;&#22359;&#65292;&#31216;&#20026;FMSPnP&#65292;&#20805;&#20998;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;FusionAD&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#32447;&#24179;&#22343;15%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce
&lt;/p&gt;</description></item><item><title>MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00352</link><description>&lt;p&gt;
MetaGPT: &#20803;&#32534;&#31243;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00352
&lt;/p&gt;
&lt;p&gt;
MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#65292;&#33258;&#21160;&#20219;&#21153;&#35299;&#20915;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#32570;&#20047;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#25506;&#32034;&#21644;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#24187;&#35273;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#26102;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#23548;&#33268;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#37319;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#20316;&#20026;&#20803;&#32534;&#31243;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaGPT&#39318;&#20808;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOPs&#65289;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#12290;&#28982;&#21518;&#65292;&#23427;&#36827;&#19968;&#27493;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#24179;&#34892;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MetaGPT&#21033;&#29992;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ProtoFL&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#27491;&#35268;&#27969;&#30340;&#26412;&#22320;&#21333;&#31867;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#20808;&#21069;&#26041;&#27861;&#20855;&#26377;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12450</link><description>&lt;p&gt;
ProtoFL: &#36890;&#36807;&#21407;&#22411;&#33976;&#39311;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ProtoFL&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#27491;&#35268;&#27969;&#30340;&#26412;&#22320;&#21333;&#31867;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#20808;&#21069;&#26041;&#27861;&#20855;&#26377;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#25552;&#39640;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#26377;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36523;&#20221;&#39564;&#35777;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#36890;&#20449;&#36718;&#27425;&#12289;&#31232;&#32570;&#30340;&#34920;&#31034;&#21644;&#21487;&#25193;&#23637;&#24615;&#32473;&#20854;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20854;&#21457;&#25381;&#20840;&#37096;&#28508;&#21147;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"ProtoFL"&#65292;&#22522;&#20110;&#21407;&#22411;&#34920;&#31034;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#20840;&#23616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27491;&#35268;&#27969;&#30340;&#26412;&#22320;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#25506;&#35752;&#20102;&#20351;&#29992;FL&#26469;&#25552;&#39640;&#21333;&#31867;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;MNIST&#12289;CIFAR-10&#12289;CIFAR-100&#12289;ImageNet-30&#21644;Keystroke-Dynamics&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#26041;&#27861;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33016;&#36879;&#35786;&#26029;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#20116;&#31181;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#21644;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;SHAP&#21644;Attri-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#36825;&#31181;&#31867;&#22411;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.12344</link><description>&lt;p&gt;
&#38169;&#35823;&#30340;&#21407;&#22240;&#32780;&#27491;&#30830;&#30340;&#65306;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26816;&#27979;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33016;&#36879;&#35786;&#26029;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#20116;&#31181;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#21644;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;SHAP&#21644;Attri-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#36825;&#31181;&#31867;&#22411;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24456;&#23481;&#26131;&#23398;&#20064;&#21040;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#37027;&#20040;&#36825;&#31181;&#23545;&#28151;&#28102;&#20449;&#24687;&#30340;&#20381;&#36182;&#20250;&#24456;&#38590;&#36890;&#36807;&#24615;&#33021;&#25351;&#26631;&#26469;&#26816;&#27979;&#20986;&#26469;&#12290;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#20107;&#21518;&#35299;&#37322;&#25110;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#25215;&#35834;&#21487;&#20197;&#35782;&#21035;&#20986;&#38169;&#35823;&#30340;&#27169;&#22411;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#20570;&#21040;&#36825;&#19968;&#28857;&#23384;&#22312;&#30528;&#19968;&#20123;&#28151;&#21512;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#35299;&#37322;&#25216;&#26415;&#27491;&#30830;&#35782;&#21035;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#31181;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#21644;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#23545;&#20110;&#22312;&#33016;&#36879;&#35786;&#26029;&#20219;&#21153;&#20013;&#26816;&#27979;&#19977;&#31181;&#20154;&#20026;&#28155;&#21152;&#30340;&#28151;&#28102;&#22240;&#23376;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;SHAP&#20197;&#21450;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;Attri-Net&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11661</link><description>&lt;p&gt;
&#29992;GPT-4&#22686;&#24378;CLIP&#65306;&#21033;&#29992;&#35270;&#35273;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;VLMs&#36890;&#36807;&#35774;&#35745;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25552;&#31034;&#26469;0-shot&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25552;&#31034;&#24037;&#31243;&#21033;&#29992;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#20808;&#36827;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24037;&#20855;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25805;&#20316;&#20197;&#25552;&#20379;&#20219;&#20309;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#65288;&#22914;EuroSAT&#65288;~7&#65285;&#65289;&#12289;DTD&#65288;~7&#65285;&#65289;&#12289;SUN397&#65288;~4.6&#65285;&#65289;&#21644;CUB&#65288;~3.3&#65285;&#65289;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23569;&#37327;&#26679;&#26412;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#30340;s
&lt;/p&gt;
&lt;p&gt;
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26102;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#24179;&#34913;&#38544;&#31169;&#21644;&#36827;&#27493;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.09426</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24179;&#34913;&#38544;&#31169;&#21644;&#36827;&#27493;&#65306;&#22522;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#21307;&#23398;&#30740;&#31350;&#21644;&#25945;&#32946;&#20013;&#30340;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education. (arXiv:2307.09426v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26102;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#24179;&#34913;&#38544;&#31169;&#21644;&#36827;&#27493;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#30340;&#36827;&#23637;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#21307;&#23398;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#24773;&#20917;&#19979;&#65292;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#21644;&#20020;&#24202;&#30149;&#29702;&#23398;&#20449;&#24687;&#23545;&#20110;&#24320;&#21457;&#25968;&#23383;&#30149;&#29702;&#23398;&#65288;DP&#65289;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23558;&#21307;&#23398;&#25968;&#25454;&#20256;&#36755;&#8220;&#23613;&#37327;&#24320;&#25918;&#8221;&#25552;&#39640;&#20102;&#25968;&#25454;&#29992;&#20110;&#27425;&#32423;&#30446;&#30340;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#23545;&#24739;&#32773;&#38544;&#31169;&#26500;&#25104;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#27861;&#35268;&#35201;&#27714;&#20445;&#25345;&#21307;&#23398;&#25968;&#25454;&#8220;&#23613;&#37327;&#23553;&#38381;&#8221;&#20197;&#36991;&#20813;&#20877;&#35782;&#21035;&#39118;&#38505;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27861;&#35268;&#35201;&#27714;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#19981;&#32771;&#34385;&#29616;&#20195;&#22270;&#20687;&#21305;&#37197;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#30340;&#25968;&#25454;&#38142;&#25509;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;DP&#30340;&#26631;&#20934;&#21270;&#19981;&#36275;&#20351;&#24471;&#22312;&#25152;&#26377;WSI&#26684;&#24335;&#19978;&#24314;&#31435;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#32473;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#32773;&#22312;&#24320;&#21457;AI&#31639;&#27861;&#26102;&#22312;&#38544;&#31169;&#19982;&#36827;&#27493;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#24102;&#26469;&#20102;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of biomedical research heavily relies on access to large amounts of medical data. In the case of histopathology, Whole Slide Images (WSI) and clinicopathological information are valuable for developing Artificial Intelligence (AI) algorithms for Digital Pathology (DP). Transferring medical data "as open as possible" enhances the usability of the data for secondary purposes but poses a risk to patient privacy. At the same time, existing regulations push towards keeping medical data "as closed as necessary" to avoid re-identification risks. Generally, these legal regulations require the removal of sensitive data but do not consider the possibility of data linkage attacks due to modern image-matching algorithms. In addition, the lack of standardization in DP makes it harder to establish a single solution for all formats of WSIs. These challenges raise problems for bio-informatics researchers in balancing privacy and progress while developing AI algorithms. This paper explo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#25935;&#24863;&#30340;&#31181;&#26063;&#21644;&#26063;&#35028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#31181;&#26063;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;BiLSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#22312;&#22806;&#26679;&#26412;&#65288;OOS&#65289;F1&#24471;&#20998;&#19978;&#27604;&#25991;&#29486;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;36.8%&#12290;&#27492;&#22806;&#65292;&#36824;&#26500;&#24314;&#20102;&#32654;&#22269;&#26368;&#20840;&#38754;&#30340;&#22995;&#27663;&#21644;&#21517;&#23383;&#20998;&#24067;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20844;&#27491;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#24110;&#21161;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#32773;&#12290;</title><link>http://arxiv.org/abs/2307.08496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;&#31181;&#26063;&#39044;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Trust Race Prediction?. (arXiv:2307.08496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#25935;&#24863;&#30340;&#31181;&#26063;&#21644;&#26063;&#35028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#31181;&#26063;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;BiLSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#22312;&#22806;&#26679;&#26412;&#65288;OOS&#65289;F1&#24471;&#20998;&#19978;&#27604;&#25991;&#29486;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;36.8%&#12290;&#27492;&#22806;&#65292;&#36824;&#26500;&#24314;&#20102;&#32654;&#22269;&#26368;&#20840;&#38754;&#30340;&#22995;&#27663;&#21644;&#21517;&#23383;&#20998;&#24067;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20844;&#27491;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#24110;&#21161;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#25935;&#24863;&#30340;&#31181;&#26063;&#21644;&#26063;&#35028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20844;&#21496;&#37117;&#20511;&#21161;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20351;&#29992;&#26469;&#33258;&#32654;&#22269;50&#20010;&#24030;&#30340;&#36873;&#27665;&#27880;&#20876;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;BiLSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#22312;&#22806;&#26679;&#26412;&#65288;OOS&#65289;F1&#24471;&#20998;&#19978;&#27604;&#25991;&#29486;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;36.8%&#12290;&#27492;&#22806;&#65292;&#25105;&#26500;&#24314;&#20102;&#32654;&#22269;&#26368;&#20840;&#38754;&#30340;&#22995;&#27663;&#21644;&#21517;&#23383;&#20998;&#24067;&#25968;&#25454;&#24211;&#65292;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#25913;&#36827;&#22995;&#27663;&#22320;&#29702;&#32534;&#30721;&#65288;BISG&#65289;&#21644;&#36125;&#21494;&#26031;&#25913;&#36827;&#21517;&#23383;&#22995;&#27663;&#22320;&#29702;&#32534;&#30721;&#65288;BIFSG&#65289;&#30340;&#35206;&#30422;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20844;&#27491;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#24110;&#21161;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#23454;&#20540;&#35266;&#27979;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#23618;&#20107;&#20214;&#39537;&#21160;&#32858;&#31867;&#12289;&#26102;&#38388;&#24046;&#20998;&#35823;&#24046;&#35843;&#21046;&#21644;&#36164;&#26684;&#30165;&#36857;&#31561;&#26041;&#27861;&#65292;&#24182;&#22312;&#32463;&#20856;RL&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#34920;&#26684;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.02947</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23454;&#20540;&#35266;&#27979;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#23454;&#20540;&#35266;&#27979;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#23618;&#20107;&#20214;&#39537;&#21160;&#32858;&#31867;&#12289;&#26102;&#38388;&#24046;&#20998;&#35823;&#24046;&#35843;&#21046;&#21644;&#36164;&#26684;&#30165;&#36857;&#31561;&#26041;&#27861;&#65292;&#24182;&#22312;&#32463;&#20856;RL&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#34920;&#26684;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20197;&#39640;&#25928;&#19988;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#24335;&#23454;&#29616;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#20540;&#35266;&#27979;&#30340;RL&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#22810;&#23618;&#20107;&#20214;&#39537;&#21160;&#32858;&#31867;&#65292;&#22686;&#21152;&#20102;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#35823;&#24046;&#35843;&#21046;&#21644;&#36164;&#26684;&#30165;&#36857;, &#24182;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#25913;&#36827;&#12290;&#28040;&#34701;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#32463;&#20856;RL&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#19981;&#26029;&#20248;&#20110;&#34920;&#26684;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#21457;&#29616;&#20102;&#31283;&#23450;&#30340;&#25511;&#21046;&#31574;&#30053;&#65306;&#23665;&#36710;&#12289;&#20498;&#31435;&#25670;&#21644;&#25670;&#33218;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#25240;&#20013;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) provides a powerful framework for decision-making in complex environments. However, implementing RL in hardware-efficient and bio-inspired ways remains a challenge. This paper presents a novel Spiking Neural Network (SNN) architecture for solving RL problems with real-valued observations. The proposed model incorporates multi-layered event-based clustering, with the addition of Temporal Difference (TD)-error modulation and eligibility traces, building upon prior work. An ablation study confirms the significant impact of these components on the proposed model's performance. A tabular actor-critic algorithm with eligibility traces and a state-of-the-art Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our network consistently outperforms the tabular approach and successfully discovers stable control policies on classic RL environments: mountain car, cart-pole, and acrobot. The proposed model offers an appealing trade-off in terms of computa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;MAE-DFER&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#26174;&#24335;&#26102;&#38388;&#38754;&#37096;&#36816;&#21160;&#24314;&#27169;&#65292;&#25512;&#36827;&#20102;&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.02227</link><description>&lt;p&gt;
MAE-DFER&#65306;&#39640;&#25928;&#30340;&#38754;&#20855;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#33258;&#30417;&#30563;&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition. (arXiv:2307.02227v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;MAE-DFER&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#26174;&#24335;&#26102;&#38388;&#38754;&#37096;&#36816;&#21160;&#24314;&#27169;&#65292;&#25512;&#36827;&#20102;&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;DFER&#65289;&#23545;&#20110;&#26234;&#33021;&#21644;&#31227;&#24773;&#30340;&#26426;&#22120;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#21463;&#21040;&#26368;&#36817;&#38754;&#20855;&#33258;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;VideoMAE&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;MAE-DFER&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#20016;&#23500;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#22823;&#21147;&#25512;&#36827;DFER&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;VideoMAE&#20013;&#20351;&#29992;&#30340;Vanilla Vision Transformer&#65288;ViT&#65289;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#22240;&#27492;MAE-DFER&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;Transformer&#65288;LGI-Former&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#22312;VideoMAE&#30340;&#29420;&#31435;&#22806;&#35266;&#20869;&#23481;&#37325;&#26500;&#22522;&#30784;&#19978;&#65292;MAE-DFER&#36824;&#24341;&#20837;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#38754;&#37096;&#36816;&#21160;&#24314;&#27169;&#65292;&#20197;&#40723;&#21169;LGI-Former&#25366;&#25496;&#38745;&#24577;&#22806;&#35266;&#21644;&#21160;&#24577;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic facial expression recognition (DFER) is essential to the development of intelligent and empathetic machines. Prior efforts in this field mainly fall into supervised learning paradigm, which is severely restricted by the limited labeled data in existing datasets. Inspired by recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method which leverages large-scale self-supervised pre-training on abundant unlabeled data to largely advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>InstructZero&#36890;&#36807;&#20248;&#21270;&#20302;&#32500;&#36719;&#25552;&#31034;&#32780;&#38750;&#31163;&#25955;&#25351;&#20196;&#65292;&#23454;&#29616;&#20102;&#23545;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25351;&#20196;&#20248;&#21270;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#29983;&#25104;&#25913;&#21892;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#26032;&#36719;&#25552;&#31034;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#33258;&#21160;&#25351;&#20196;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03082</link><description>&lt;p&gt;
&#12298;InstructZero: &#38024;&#23545;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25351;&#20196;&#20248;&#21270;&#12299;
&lt;/p&gt;
&lt;p&gt;
InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models. (arXiv:2306.03082v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03082
&lt;/p&gt;
&lt;p&gt;
InstructZero&#36890;&#36807;&#20248;&#21270;&#20302;&#32500;&#36719;&#25552;&#31034;&#32780;&#38750;&#31163;&#25955;&#25351;&#20196;&#65292;&#23454;&#29616;&#20102;&#23545;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25351;&#20196;&#20248;&#21270;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#29983;&#25104;&#25913;&#21892;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#26032;&#36719;&#25552;&#31034;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#33258;&#21160;&#25351;&#20196;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#25351;&#20196;&#36319;&#38543;&#32773;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#24773;&#20917;&#19979;&#26368;&#20339;&#25351;&#20196;&#30340;&#36873;&#25321;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31105;&#27490;&#21453;&#21521;&#20256;&#25773;&#30340;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#20248;&#21270;&#31163;&#25955;&#25351;&#20196;&#65292;&#32780;&#26159;&#20248;&#21270;&#19968;&#20010;&#24212;&#29992;&#20110;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#32500;&#36719;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;InstructZero&#26041;&#27861;&#30340;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#36719;&#25552;&#31034;&#34987;&#36716;&#25442;&#20026;&#19968;&#20010;&#25351;&#20196;&#65292;&#28982;&#21518;&#36890;&#36807;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#25552;&#20132;&#32473;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#24182;&#23558;&#24615;&#33021;&#21457;&#36865;&#32473;&#36125;&#21494;&#26031;&#20248;&#21270;&#20197;&#29983;&#25104;&#25913;&#21892;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#26032;&#36719;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21644;API&#32452;&#21512;&#19978;&#35780;&#20272;&#20102;InstructZero&#65292;&#21253;&#25324;Vicuna&#21644;ChatGPT&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;InstructZero&#20248;&#20110;&#29616;&#26377;&#33258;&#21160;&#25351;&#20196;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#21487;&#29992;&#20110;https://gith&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) are instruction followers, but it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. On each iteration of the proposed method, which we call InstructZero, a soft prompt is converted into an instruction using the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, and the performance is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our results show that InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks. Our code and data are publicly available at https://gith
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#24037;&#20855;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;33K&#20010;&#26679;&#26412;&#21644;22.5M&#20010;&#26631;&#35760;&#30340;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#29702;&#35299;&#36825;&#31867;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#35821;&#35328;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02864</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20844;&#20849;&#20107;&#21153;&#39046;&#22495;&#36827;&#34892;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. (arXiv:2306.02864v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02864
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#24037;&#20855;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;33K&#20010;&#26679;&#26412;&#21644;22.5M&#20010;&#26631;&#35760;&#30340;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#29702;&#35299;&#36825;&#31867;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#35821;&#35328;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#23545;&#20844;&#27665;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#20419;&#36827;&#36879;&#26126;&#24230;&#12289;&#38382;&#36131;&#21046;&#21644;&#20915;&#31574;&#12290;&#23427;&#20351;&#20844;&#27665;&#33021;&#22815;&#20102;&#35299;&#25919;&#24220;&#25919;&#31574;&#65292;&#21442;&#19982;&#20844;&#20849;&#35805;&#35821;&#65292;&#24182;&#36861;&#31350;&#20195;&#34920;&#30340;&#36131;&#20219;&#12290;&#23545;&#20110;&#20381;&#38752;&#26576;&#20123;&#35268;&#23450;&#36816;&#33829;&#30340;&#20844;&#21496;&#26469;&#35828;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#26377;&#26102;&#29978;&#33267;&#20851;&#20046;&#29983;&#27515;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#26377;&#25928;&#22788;&#29702;&#21644;&#29702;&#35299;&#36825;&#31867;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#35821;&#35328;&#65292;&#22823;&#22823;&#22686;&#24378;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#22312;&#20998;&#31867;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#20013;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#39033;&#33258;&#28982;&#30340;&#22810;&#26631;&#31614;&#20219;&#21153;&#65292;&#36825;&#20123;&#25991;&#20214;&#30340;&#20998;&#31867;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#24037;&#20855;&#26469;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;33K&#20010;&#26679;&#26412;&#21644;22.5M&#20010;&#26631;&#35760;&#30340;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;4&#20010;&#19981;&#21516;&#30340;&#35199;&#29677;&#29273;&#35821;LLM&#22312;&#26368;&#22810;30&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#20214;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 diff
&lt;/p&gt;</description></item><item><title>&#22312;&#24403;&#21069;LLMs&#35805;&#39064;&#30340;&#35752;&#35770;&#20013;&#65292;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#35201;&#32032;--&#36879;&#26126;&#24230;--&#34987;&#24573;&#35270;&#20102;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#19979;&#65292;&#36861;&#27714;LLMs&#30340;&#36879;&#26126;&#24230;&#23545;&#20110;&#25903;&#25345;&#36866;&#24403;&#30340;&#20154;&#31867;&#29702;&#35299;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.01941</link><description>&lt;p&gt;
LLM&#26102;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#36879;&#26126;&#24230;&#65306;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap. (arXiv:2306.01941v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01941
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;LLMs&#35805;&#39064;&#30340;&#35752;&#35770;&#20013;&#65292;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#35201;&#32032;--&#36879;&#26126;&#24230;--&#34987;&#24573;&#35270;&#20102;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#19979;&#65292;&#36861;&#27714;LLMs&#30340;&#36879;&#26126;&#24230;&#23545;&#20110;&#25903;&#25345;&#36866;&#24403;&#30340;&#20154;&#31867;&#29702;&#35299;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20852;&#36215;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21019;&#26032;&#26426;&#20250;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30830;&#20445;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#21644;&#37096;&#32626;LLMs&#21644;&#22522;&#20110;LLMs&#30340;&#24212;&#29992;&#31243;&#24207;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#25903;&#26609;--&#36879;&#26126;&#24230;--&#22312;&#24403;&#21069;&#20851;&#20110;LLMs&#30340;&#35752;&#35770;&#20013;&#22823;&#37096;&#20998;&#37117;&#32570;&#22833;&#20102;&#12290;&#36861;&#27714;&#26032;&#30340;&#36879;&#26126;&#24230;&#26041;&#27861;&#23545;&#20110;LLMs&#33267;&#20851;&#37325;&#35201;&#65292;&#22810;&#24180;&#26469;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;(HCI)&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#24517;&#39035;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65306;&#36879;&#26126;&#24230;&#22522;&#26412;&#19978;&#26159;&#25903;&#25345;&#36866;&#24403;&#20154;&#31867;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#36861;&#27714;&#19981;&#21516;&#30446;&#26631;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;LLM&#26102;&#20195;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#26032;&#20852;LLM&#29983;&#24577;&#31995;&#32479;&#20013;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38656;&#27714;&#65292;&#24320;&#21457;&#21644;&#35774;&#35745;&#36879;&#26126;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of powerful large language models (LLMs) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. We have reached a pivotal moment for ensuring that LLMs and LLM-infused applications are developed and deployed responsibly. However, a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs. It is paramount to pursue new approaches to provide transparency for LLMs, and years of research at the intersection of AI and human-computer interaction (HCI) highlight that we must do so with a human-centered perspective: Transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. In this new era of LLMs, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging LLM ecosystem, the novel types of LLM-infused 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.13452</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#24314;&#27169;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#26377;&#39537;&#21160;&#21147;&#30340;&#20114;&#21160;&#24615;&#20195;&#29702;&#65292;&#20182;&#20204;&#36861;&#27714;&#26377;&#36259;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#24773;&#22659;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24418;&#24335;&#21270;&#30340;&#29289;&#29702;&#20869;&#22312;&#21160;&#26426;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#22810;&#31181;&#29289;&#29702;&#24773;&#22659;&#30340;&#35780;&#20998;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#20381;&#36182;&#20110;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#30340;&#27169;&#22411;&#21040;&#20381;&#36182;&#20110;&#21069;&#21521;&#29289;&#29702;&#39044;&#27979;&#30340;&#27169;&#22411;&#30340;&#21508;&#31181;&#20869;&#22312;&#21160;&#26426;&#20551;&#35774;&#26469;&#24314;&#27169;&#20154;&#31867;&#30340;&#36259;&#21619;&#21453;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#31867;&#21453;&#24212;&#30340;&#21333;&#19968;&#26368;&#20339;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#29289;&#29702;&#39044;&#27979;&#25439;&#22833;&#25512;&#23548;&#20986;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#25512;&#24191;&#20182;&#20204;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#19982;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#20542;&#21521;&#20110;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.08275</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#26045;&#65306;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects. (arXiv:2304.08275v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#28389;&#29992;&#21644;&#19981;&#24403;&#20351;&#29992;&#24341;&#36215;&#30340;&#25285;&#24551;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#30340;&#22522;&#26412;&#26041;&#38754;&#21253;&#25324;&#38544;&#31169;&#12289;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#36825;&#32473;&#23547;&#27714;&#36981;&#24490;&#36825;&#20123;&#21407;&#21017;&#30340;AI/ML&#24320;&#21457;&#32773;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#25552;&#39640;AI/ML&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#27719;&#32534;&#21644;&#35752;&#35770;10&#20010;&#31361;&#20986;&#30340;&#32039;&#24352;&#20851;&#31995;&#12289;&#26435;&#34913;&#21644;&#20854;&#20182;&#22522;&#26412;&#26041;&#38754;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20415;&#22312;&#25345;&#32493;&#21162;&#21147;&#23558;&#36825;&#20123;&#21407;&#21017;&#36716;&#21270;&#20026;&#23454;&#36341;&#30340;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#20262;&#29702;&#21407;&#21017;&#26041;&#38754;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#65292;&#24182;&#36890;&#36807;&#22312;&#24191;&#27867;&#25991;&#29486;&#20013;&#30340;&#25903;&#25345;&#36827;&#34892;&#21452;&#38754;&#20114;&#21160;&#30340;&#37325;&#28857;&#35752;&#35770;&#12290;&#36825;&#20010;&#30446;&#24405;&#23545;&#20110;&#25552;&#39640;&#20154;&#20204;&#23545;&#20262;&#29702;&#20934;&#21017;&#26041;&#38754;&#20043;&#38388;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#20197;&#21450;&#20419;&#36827;&#35774;&#35745;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20570;&#20986;&#26377;&#20805;&#20998;&#20381;&#25454;&#30340;&#21028;&#26029;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and develo
&lt;/p&gt;</description></item><item><title>GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.16459</link><description>&lt;p&gt;
GNNBuilder&#65306;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#29983;&#25104;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16459
&lt;/p&gt;
&lt;p&gt;
GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#24456;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21152;&#36895;&#22120;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#30828;&#20214;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#23454;&#38469;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNBuilder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#12290;&#23427;&#20855;&#26377;&#22235;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;GNNBuilder&#21487;&#20197;&#33258;&#21160;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#29983;&#25104;GNN&#21152;&#36895;&#22120;&#65307;&#65288;2&#65289;GNNBuilder&#37319;&#29992;&#26631;&#20934;&#30340;PyTorch&#32534;&#31243;&#25509;&#21475;&#65292;&#20026;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#38646;&#24320;&#38144;&#65307;&#65288;3&#65289;GNNBuilder&#25903;&#25345;&#31471;&#21040;&#31471;&#30340;&#20195;&#30721;&#29983;&#25104;&#12289;&#27169;&#25311;&#12289;&#21152;&#36895;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#37096;&#32626;&#65292;&#23454;&#29616;&#20102;GNN&#21152;&#36895;&#22120;&#35774;&#35745;&#30340;&#19968;&#38190;&#24335;&#25805;&#20316;&#65307;&#65288;4&#65289;GNNBuilder&#37197;&#22791;&#20102;&#20854;&#25152;&#29983;&#25104;&#30340;&#21152;&#36895;&#22120;&#30340;&#20934;&#30830;&#24615;&#33021;&#27169;&#22411;&#65292;&#20351;&#24471;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#24555;&#36895;&#32780;&#28789;&#27963;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21152;&#36895;&#22120;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#21152;&#36895;&#22120;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#65292;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#20102;&#22810;&#36798;12.95&#20493;&#12290;&#20854;&#27425;&#65292;&#23545;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GNNBuilder&#30340;&#20986;&#33394;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#21333;&#20010;GPU&#30340;&#26381;&#21153;&#22120;&#19978;&#65292;&#25105;&#20204;&#23545;400&#20010;GNN&#27169;&#22411;&#36827;&#34892;&#20102;30&#20998;&#38047;&#30340;DSE&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;GNNBuilder&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are plenty of graph neural network (GNN) accelerators being proposed. However, they highly rely on users' hardware expertise and are usually optimized for one specific GNN model, making them challenging for practical use . Therefore, in this work, we propose GNNBuilder, the first automated, generic, end-to-end GNN accelerator generation framework. It features four advantages: (1) GNNBuilder can automatically generate GNN accelerators for a wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch programming interface, introducing zero overhead for algorithm developers; (3) GNNBuilder supports end-to-end code generation, simulation, accelerator optimization, and hardware deployment, realizing a push-button fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate performance models of its generated accelerator, enabling fast and flexible design space exploration (DSE). In the experiments, first, we show that our accelerator pe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.00286</link><description>&lt;p&gt;
&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#65292;&#23545;&#24453;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#26377;&#24046;&#24322;&#24615;&#65306;&#21033;&#29992;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#20016;&#23500;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#29992;&#20110;&#19982;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#20204;&#20351;&#29992;&#32771;&#34385;&#20102;&#19968;&#25209;&#24471;&#20998;&#19977;&#20803;&#32452;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20256;&#32479;&#26041;&#27861;&#35748;&#20026;&#19977;&#20803;&#32452;&#30340;&#26631;&#31614;&#35201;&#20040;&#20026;&#30495;&#65292;&#35201;&#20040;&#20026;&#20551;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36127;&#26679;&#26412;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#12290;&#19982;&#36825;&#19968;&#26368;&#36817;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#22312;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#23558;&#23427;&#20204;&#19982;&#35821;&#20041;&#19978;&#26080;&#25928;&#30340;&#36127;&#26679;&#26412;&#21306;&#21035;&#23545;&#24453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#30340;&#19977;&#20010;&#20027;&#35201;&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#21644;&#21463;&#25511;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#20844;&#20849;&#22522;&#20934;KG&#19978;&#31995;&#32479;&#22320;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;SPTS v2&#26694;&#26550;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2301.01635</link><description>&lt;p&gt;
SPTS v2: &#21333;&#28857;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;SPTS v2&#26694;&#26550;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#20043;&#38388;&#30340;&#20869;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#31471;&#21040;&#31471;&#30340;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#25163;&#21160;&#26631;&#27880;&#65288;&#22914;&#27700;&#24179;&#30697;&#24418;&#12289;&#26059;&#36716;&#30697;&#24418;&#12289;&#22235;&#36793;&#24418;&#21644;&#22810;&#36793;&#24418;&#65289;&#35270;&#20026;&#24517;&#35201;&#26465;&#20214;&#65292;&#32780;&#36825;&#27604;&#20351;&#29992;&#21333;&#28857;&#35201;&#26114;&#36149;&#24471;&#22810;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36890;&#36807;&#25552;&#20986;&#30340;&#21517;&#20026;SPTS v2&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#12290;SPTS v2&#36890;&#36807;&#20197;&#39034;&#24207;&#39044;&#27979;&#21516;&#19968;&#39044;&#27979;&#24207;&#21015;&#20013;&#25152;&#26377;&#25991;&#26412;&#23454;&#20363;&#30340;&#20013;&#24515;&#28857;&#65292;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;&#36825;&#20004;&#20010;&#35299;&#30721;&#22120;&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#36807;&#31243;&#36827;&#34892;&#20132;&#20114;&#36830;&#25509;&#65292;&#20197;&#20256;&#36882;&#26799;&#24230;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. For the first time, we demonstrate that training scene text spotting models can be achieved with an extremely low-cost single-point annotation by the proposed framework, termed SPTS v2. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Compre
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;&#12290;&#36890;&#36807;&#32416;&#27491;&#20256;&#32479;&#25490;&#24207;&#26694;&#26550;&#20013;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25151;&#28304;&#21305;&#37197;&#65292;&#24182;&#21152;&#20837;&#20102;&#32771;&#34385;&#25151;&#28304;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2210.07774</link><description>&lt;p&gt;
&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning To Rank Diversely At Airbnb. (arXiv:2210.07774v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;&#12290;&#36890;&#36807;&#32416;&#27491;&#20256;&#32479;&#25490;&#24207;&#26694;&#26550;&#20013;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25151;&#28304;&#21305;&#37197;&#65292;&#24182;&#21152;&#20837;&#20102;&#32771;&#34385;&#25151;&#28304;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Airbnb&#26159;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#65292;&#23558;&#20986;&#31199;&#25151;&#28304;&#30340;&#25151;&#19996;&#19982;&#26469;&#33258;&#20840;&#29699;&#30340;&#28508;&#22312;&#23458;&#20154;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#24212;&#29992;&#21040;&#21305;&#37197;&#23458;&#20154;&#19982;&#25151;&#19996;&#30340;&#36807;&#31243;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#25490;&#24207;&#25913;&#36827;&#26159;&#36890;&#36807;&#19968;&#31181;&#26680;&#24515;&#31574;&#30053;&#39537;&#21160;&#30340;&#65306;&#25353;&#29031;&#20854;&#39044;&#35745;&#30340;&#39044;&#35746;&#27010;&#29575;&#23545;&#25151;&#28304;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#25913;&#36827;&#36825;&#20123;&#39044;&#35746;&#27010;&#29575;&#20272;&#35745;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31574;&#30053;&#26263;&#21547;&#30340;&#19968;&#20010;&#20551;&#35774;&#26159;&#65292;&#25628;&#32034;&#32467;&#26524;&#20013;&#30340;&#27599;&#20010;&#25151;&#28304;&#30340;&#39044;&#35746;&#27010;&#29575;&#21487;&#20197;&#29420;&#31435;&#30830;&#23450;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20010;&#20551;&#35774;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32416;&#27491;&#36825;&#20010;&#20551;&#35774;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#21450;&#22522;&#20110;&#35813;&#29702;&#35770;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#36890;&#36807;&#26174;&#24335;&#32771;&#34385;&#25151;&#28304;&#20043;&#38388;&#30340;&#21487;&#33021;&#30456;&#20284;&#24615;&#24182;&#20943;&#23567;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Airbnb is a two-sided marketplace, bringing together hosts who own listings for rent, with prospective guests from around the globe. Applying neural network-based learning to rank techniques has led to significant improvements in matching guests with hosts. These improvements in ranking were driven by a core strategy: order the listings by their estimated booking probabilities, then iterate on techniques to make these booking probability estimates more and more accurate. Embedded implicitly in this strategy was an assumption that the booking probability of a listing could be determined independently of other listings in search results. In this paper we discuss how this assumption, pervasive throughout the commonly-used learning to rank frameworks, is false. We provide a theoretical foundation correcting this assumption, followed by efficient neural network architectures based on the theory. Explicitly accounting for possible similarities between listings, and reducing them to diversify
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2208.00085</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#34588;&#34562;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#35299;&#20915;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#34588;&#34562;&#32676;&#20307;&#24182;&#26816;&#26597;&#20854;&#20581;&#24247;&#29366;&#20917;&#65292;&#20174;&#32780;&#22312;&#24773;&#20917;&#21464;&#24471;&#20005;&#37325;&#20043;&#21069;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#21361;&#38505;&#29366;&#24577;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35745;&#21010;&#23450;&#26399;&#34588;&#34562;&#32676;&#20307;&#26816;&#26597;&#65292;&#20174;&#32780;&#33410;&#30465;&#37325;&#35201;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29992;&#20110;&#34588;&#34562;&#30417;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#20026;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38754;&#21521;&#20861;&#21307;&#23398;&#21644;&#34588;&#34562;&#23398;&#19987;&#19994;&#20154;&#21592;&#21644;&#19987;&#23478;&#65292;&#26088;&#22312;&#21521;&#20182;&#20204;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#27599;&#20010;&#24212;&#29992;&#31867;&#21035;&#37117;&#20197;&#31616;&#35201;&#30340;&#29702;&#35770;&#20171;&#32461;&#21644;&#19982;&#20854;&#22522;&#26412;&#26041;&#27861;&#30456;&#20851;&#30340;&#21160;&#26426;&#24320;&#31687;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#33021;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;...
&lt;/p&gt;
&lt;p&gt;
Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23398;&#20064;&#24335;&#22810;&#35270;&#22270;&#31435;&#20307;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20165;&#26377;&#19968;&#37096;&#20998;&#24102;&#26377;&#28145;&#24230;&#20934;&#30830;&#20540;&#30340;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21322;&#30417;&#30563;&#20998;&#24067;&#22686;&#24378;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;MVS&#20013;&#30340;&#20998;&#24067;&#38388;&#38553;&#22810;&#20041;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.11699</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#28145;&#24230;&#22810;&#35270;&#22270;&#31435;&#20307;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23398;&#20064;&#24335;&#22810;&#35270;&#22270;&#31435;&#20307;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20165;&#26377;&#19968;&#37096;&#20998;&#24102;&#26377;&#28145;&#24230;&#20934;&#30830;&#20540;&#30340;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21322;&#30417;&#30563;&#20998;&#24067;&#22686;&#24378;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;MVS&#20013;&#30340;&#20998;&#24067;&#38388;&#38553;&#22810;&#20041;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#65292;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#22810;&#35270;&#22270;&#31435;&#20307;(MVS)&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20026;&#20102;&#23558;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#28857;(&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;)&#32467;&#21512;&#36215;&#26469;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#26114;&#36149;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20165;&#26377;&#19968;&#23567;&#37096;&#20998;MVS&#25968;&#25454;&#38468;&#24102;&#31264;&#23494;&#28145;&#24230;&#20934;&#30830;&#20540;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#23398;&#20064;&#24335;MVS&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#22270;&#20013;&#30340;&#22330;&#26223;&#24040;&#22823;&#21464;&#21270;&#21644;&#28789;&#27963;&#35774;&#32622;&#65292;&#36825;&#21487;&#33021;&#30772;&#22351;&#20102;&#32463;&#20856;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#65292;&#21363;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#26631;&#31614;&#25968;&#25454;&#20849;&#20139;&#30456;&#21516;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;MVS&#38382;&#39064;&#20013;&#31216;&#20026;&#21322;&#30417;&#30563;&#20998;&#24067;&#38388;&#38553;&#22810;&#20041;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#20998;&#24067;&#22686;&#24378;MVS&#26694;&#26550;&#65292;&#21363;SDA-MVS&#12290;&#23545;&#20110;MVS&#25968;&#25454;&#20013;&#22522;&#26412;&#20551;&#35774;&#36866;&#29992;&#30340;&#31616;&#21333;&#24773;&#20917;&#65292;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#40723;&#21169;&#27169;&#22411;&#39044;&#27979;&#22312;&#21407;&#22987;&#26679;&#26412;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Significant progress has been witnessed in learning-based Multi-view Stereo (MVS) under supervised and unsupervised settings. To combine their respective merits in accuracy and completeness, meantime reducing the demand for expensive labeled data, this paper explores the problem of learning-based MVS in a semi-supervised setting that only a tiny part of the MVS data is attached with dense depth ground truth. However, due to huge variation of scenarios and flexible settings in views, it may break the basic assumption in classic semi-supervised learning, that unlabeled data and labeled data share the same label space and data distribution, named as semi-supervised distribution-gap ambiguity in the MVS problem. To handle these issues, we propose a novel semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the simple case that the basic assumption works in MVS data, consistency regularization encourages the model predictions to be consistent between original sample and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#30340;&#32593;&#32476;&#32676;&#32452;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#25104;&#25945;&#24072;&#32452;&#21644;&#23398;&#29983;&#32452;&#65292;&#23398;&#29983;&#32452;&#23398;&#20064;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#20351;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#31574;&#30053;&#23558;&#23398;&#29983;&#32452;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#32593;&#32476;&#26187;&#21319;&#20026;&#25945;&#24072;&#32452;&#65292;&#36890;&#36807;&#35757;&#32451;&#25945;&#24072;&#32452;&#20197;&#25913;&#36827;&#20854;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.01186</link><description>&lt;p&gt;
ORC: &#21033;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#30340;&#32593;&#32476;&#32676;&#32452;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
ORC: Network Group-based Knowledge Distillation using Online Role Change. (arXiv:2206.01186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#30340;&#32593;&#32476;&#32676;&#32452;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#25104;&#25945;&#24072;&#32452;&#21644;&#23398;&#29983;&#32452;&#65292;&#23398;&#29983;&#32452;&#23398;&#20064;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#20351;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#31574;&#30053;&#23558;&#23398;&#29983;&#32452;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#32593;&#32476;&#26187;&#21319;&#20026;&#25945;&#24072;&#32452;&#65292;&#36890;&#36807;&#35757;&#32451;&#25945;&#24072;&#32452;&#20197;&#25913;&#36827;&#20854;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#30001;&#20110;&#21333;&#19968;&#30340;&#20840;&#33021;&#25945;&#24072;&#32593;&#32476;&#26080;&#27861;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#65292;&#26368;&#36817;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#20010;&#25945;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#23427;&#20204;&#30340;&#25913;&#36827;&#25928;&#26524;&#24182;&#19981;&#22914;&#39044;&#26399;&#65292;&#22240;&#20026;&#19968;&#20123;&#19981;&#25104;&#29087;&#30340;&#25945;&#24072;&#21487;&#33021;&#20250;&#23558;&#38169;&#35823;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23398;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#24182;&#21033;&#29992;&#22810;&#20010;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#26412;&#25991;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#25104;&#25945;&#24072;&#32452;&#21644;&#23398;&#29983;&#32452;&#12290;&#23398;&#29983;&#32452;&#30001;&#38656;&#35201;&#23398;&#20064;&#25945;&#24072;&#30693;&#35782;&#30340;&#19981;&#25104;&#29087;&#32593;&#32476;&#32452;&#25104;&#65292;&#32780;&#25945;&#24072;&#32452;&#21017;&#30001;&#33021;&#22815;&#25104;&#21151;&#25945;&#25480;&#30340;&#36873;&#20013;&#32593;&#32476;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#31574;&#30053;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23558;&#23398;&#29983;&#32452;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#32593;&#32476;&#26187;&#21319;&#20026;&#25945;&#24072;&#32452;&#12290;&#36890;&#36807;&#20351;&#29992;&#23398;&#29983;&#32452;&#30340;&#38169;&#35823;&#26679;&#26412;&#35757;&#32451;&#25945;&#24072;&#32452;&#20197;&#25913;&#36827;&#20854;&#30693;&#35782;&#65292;&#25105;&#20204;&#36716;&#31227;&#21327;&#20316;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In knowledge distillation, since a single, omnipotent teacher network cannot solve all problems, multiple teacher-based knowledge distillations have been studied recently. However, sometimes their improvements are not as good as expected because some immature teachers may transfer the false knowledge to the student. In this paper, to overcome this limitation and take the efficacy of the multiple networks, we divide the multiple networks into teacher and student groups, respectively. That is, the student group is a set of immature networks that require learning the teacher's knowledge, while the teacher group consists of the selected networks that are capable of teaching successfully. We propose our online role change strategy where the top-ranked networks in the student group are able to promote to the teacher group at every iteration. After training the teacher group using the error samples of the student group to refine the teacher group's knowledge, we transfer the collaborative kno
&lt;/p&gt;</description></item></channel></rss>