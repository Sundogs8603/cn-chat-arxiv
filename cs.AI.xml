<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SoftZoo&#26159;&#19968;&#20010;&#38754;&#21521;&#22810;&#31181;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#24179;&#21488;&#65292;&#25903;&#25345;&#24191;&#27867;&#12289;&#33258;&#28982;&#21551;&#21457;&#30340;&#26448;&#26009;&#32452;&#21512;&#21644;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#21487;&#24494;&#20998;&#35774;&#35745;&#34920;&#31034;&#12290;&#36890;&#36807;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;SoftZoo&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20307;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09555</link><description>&lt;p&gt;
SoftZoo&#65306;&#38754;&#21521;&#22810;&#26679;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#22522;&#20934;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments. (arXiv:2303.09555v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09555
&lt;/p&gt;
&lt;p&gt;
SoftZoo&#26159;&#19968;&#20010;&#38754;&#21521;&#22810;&#31181;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#24179;&#21488;&#65292;&#25903;&#25345;&#24191;&#27867;&#12289;&#33258;&#28982;&#21551;&#21457;&#30340;&#26448;&#26009;&#32452;&#21512;&#21644;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#21487;&#24494;&#20998;&#35774;&#35745;&#34920;&#31034;&#12290;&#36890;&#36807;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;SoftZoo&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20307;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#25511;&#21046;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#22312;&#21516;&#26102;&#21327;&#21516;&#20248;&#21270;&#24418;&#24577;&#26102;&#20986;&#29616;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#26159;&#20026;&#29305;&#23450;&#29615;&#22659;&#25110;&#34920;&#31034;&#37327;&#37327;&#36523;&#23450;&#21046;&#30340;&#12290;&#20026;&#20102;&#26356;&#20805;&#20998;&#22320;&#29702;&#35299;&#22266;&#26377;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#26435;&#34913;&#65292;&#24182;&#21152;&#36895;&#24320;&#21457;&#26032;&#22411;&#36719;&#20307;&#26426;&#22120;&#20154;&#65292;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;&#24050;&#24314;&#31435;&#22909;&#30340;&#20219;&#21153;&#12289;&#29615;&#22659;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20840;&#38754;&#34394;&#25311;&#24179;&#21488;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SoftZoo&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#24179;&#21488;&#12290;SoftZoo&#25903;&#25345;&#24191;&#27867;&#12289;&#33258;&#28982;&#21551;&#21457;&#30340;&#26448;&#26009;&#32452;&#21512;&#65292;&#21253;&#25324;&#33021;&#22815;&#27169;&#25311;&#24179;&#22320;&#12289;&#27801;&#28448;&#12289;&#28287;&#22320;&#12289;&#40655;&#22303;&#12289;&#20912;&#12289;&#38634;&#12289;&#27973;&#27700;&#21644;&#28023;&#27915;&#31561;&#22810;&#31181;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#22810;&#20010;&#19982;&#36719;&#20307;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24555;&#36895;&#31227;&#21160;&#12289;&#28789;&#27963;&#36716;&#21521;&#21644;&#36335;&#24452;&#36319;&#38543;&#65292;&#20197;&#21450;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#21487;&#24494;&#20998;&#35774;&#35745;&#34920;&#31034;&#12290;&#32467;&#21512;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;SoftZoo&#20801;&#35768;&#22312;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#36719;&#20307;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While significant research progress has been made in robot learning for control, unique challenges arise when simultaneously co-optimizing morphology. Existing work has typically been tailored for particular environments or representations. In order to more fully understand inherent design and performance tradeoffs and accelerate the development of new breeds of soft robots, a comprehensive virtual platform with well-established tasks, environments, and evaluation metrics is needed. In this work, we introduce SoftZoo, a soft robot co-design platform for locomotion in diverse environments. SoftZoo supports an extensive, naturally-inspired material set, including the ability to simulate environments such as flat ground, desert, wetland, clay, ice, snow, shallow water, and ocean. Further, it provides a variety of tasks relevant for soft robotics, including fast locomotion, agile turning, and path following, as well as differentiable design representations for morphology and control. Combi
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27969;&#30340;&#32534;&#31243;&#65288;FBP&#65289;&#65292;&#24182;&#24378;&#35843;&#20102;FBP&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#22270;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#21487;&#20197;&#25913;&#36827;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#21253;&#25324;&#25925;&#38556;&#23450;&#20301;&#12289;&#19994;&#21153;&#20998;&#26512;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.09552</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#22270;&#20316;&#20026;&#23436;&#25972;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Dataflow graphs as complete causal graphs. (arXiv:2303.09552v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27969;&#30340;&#32534;&#31243;&#65288;FBP&#65289;&#65292;&#24182;&#24378;&#35843;&#20102;FBP&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#22270;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#21487;&#20197;&#25913;&#36827;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#21253;&#25324;&#25925;&#38556;&#23450;&#20301;&#12289;&#19994;&#21153;&#20998;&#26512;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#20214;&#21270;&#24320;&#21457;&#26159;&#29616;&#20195;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#30340;&#26680;&#24515;&#21407;&#21017;&#20043;&#19968;&#12290;&#29702;&#35299;&#36719;&#20214;&#31995;&#32479;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#20026;&#24320;&#21457;&#20154;&#21592;&#24102;&#26469;&#26174;&#33879;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#20351;&#24471;&#22312;&#31995;&#32479;&#35268;&#27169;&#19979;&#36319;&#36394;&#21644;&#21457;&#29616;&#36825;&#31181;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#26234;&#21147;&#36127;&#25285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27969;&#30340;&#32534;&#31243;&#65288;FBP&#65289;&#65292;&#24182;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;FBP&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#22270;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#35828;&#26126;&#24615;&#31034;&#20363;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#26469;&#25913;&#36827;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#21253;&#25324;&#25925;&#38556;&#23450;&#20301;&#12289;&#19994;&#21153;&#20998;&#26512;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Component-based development is one of the core principles behind modern software engineering practices. Understanding of causal relationships between components of a software system can yield significant benefits to developers. Yet modern software design approaches make it difficult to track and discover such relationships at system scale, which leads to growing intellectual debt. In this paper we consider an alternative approach to software design, flow-based programming (FBP), and draw the attention of the community to the connection between dataflow graphs produced by FBP and structural causal models. With expository examples we show how this connection can be leveraged to improve day-to-day tasks in software projects, including fault localisation, business analysis and experimentation.
&lt;/p&gt;</description></item><item><title>WebSHAP&#26159;&#31532;&#19968;&#20010;&#23558;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;SHAP&#36866;&#24212;Web&#29615;&#22659;&#30340;&#27983;&#35272;&#22120;&#20869;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;ML&#30340;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.09545</link><description>&lt;p&gt;
WebSHAP: &#38754;&#21521;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
WebSHAP: Towards Explaining Any Machine Learning Models Anywhere. (arXiv:2303.09545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09545
&lt;/p&gt;
&lt;p&gt;
WebSHAP&#26159;&#31532;&#19968;&#20010;&#23558;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;SHAP&#36866;&#24212;Web&#29615;&#22659;&#30340;&#27983;&#35272;&#22120;&#20869;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;ML&#30340;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#25105;&#20204;&#26085;&#24120;&#30340;&#32593;&#32476;&#20307;&#39564;&#20013;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#38656;&#35201;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Web&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#24448;&#24448;&#38656;&#35201;&#19987;&#29992;&#30340;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21521;&#36739;&#20302;&#24310;&#36831;&#21644;&#26356;&#39640;&#38544;&#31169;&#24615;&#30340;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;Web ML&#36801;&#31227;&#26102;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23458;&#25143;&#31471;&#35299;&#37322;&#26041;&#26696;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WebSHAP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;SHAP&#36866;&#24212;Web&#29615;&#22659;&#30340;&#27983;&#35272;&#22120;&#20869;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#24037;&#20855;&#26159;&#20351;&#29992;&#29616;&#20195;Web&#25216;&#26415;&#65288;&#22914;WebGL&#65289;&#24320;&#21457;&#30340;&#65292;&#21033;&#29992;&#23458;&#25143;&#31471;&#30828;&#20214;&#33021;&#21147;&#65292;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;Web ML&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;WebSHAP&#22312;&#35299;&#37322;&#22522;&#20110;ML&#30340;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290; &#22238;&#39038;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36879;&#26126;Web ML&#30340;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;WebSHAP&#29616;&#24050;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) is increasingly integrated into our everyday Web experience, there is a call for transparent and explainable web-based ML. However, existing explainability techniques often require dedicated backend servers, which limit their usefulness as the Web community moves toward in-browser ML for lower latency and greater privacy. To address the pressing need for a client-side explainability solution, we present WebSHAP, the first in-browser tool that adapts the state-of-the-art model-agnostic explainability technique SHAP to the Web environment. Our open-source tool is developed with modern Web technologies such as WebGL that leverage client-side hardware capabilities and make it easy to integrate into existing Web ML applications. We demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval decisions to loan applicants. Reflecting on our work, we discuss the opportunities and challenges for future research on transparent Web ML. WebSHAP is available
&lt;/p&gt;</description></item><item><title>SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09540</link><description>&lt;p&gt;
SemDeDup:&#36890;&#36807;&#35821;&#20041;&#21435;&#37325;&#23454;&#29616;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;arXiv:2303.09540v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09540
&lt;/p&gt;
&lt;p&gt;
SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#28023;&#37327;&#25968;&#25454;&#30340;&#22686;&#21152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#22411;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#38500;&#26597;&#25214;&#31934;&#30830;&#37325;&#22797;&#39033;&#22806;&#65292;&#22823;&#37096;&#20998;&#26410;&#32463;&#31934;&#24515;&#31579;&#36873;&#65292;&#21487;&#33021;&#23384;&#22312;&#24456;&#22810;&#20887;&#20313;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SemDeDup&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#65306;&#21363;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#25968;&#25454;&#23545;&#12290;&#21435;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#21487;&#20197;&#20445;&#25345;&#24615;&#33021;&#24182;&#21152;&#36895;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#20197;&#22806;&#24471;&#21040;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#37096;&#20998;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;C4&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;SemDeDup&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#36136;&#37327;&#23884;&#20837;&#31616;&#21333;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#26356;&#24555;&#22320;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
&lt;/p&gt;</description></item><item><title>ROBOSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#38543;&#26426;&#23376;&#38598;&#30340;&#38431;&#21451;&#26469;&#23545;&#27604;&#21327;&#21516;&#24863;&#30693;&#21644;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#65292;&#20197;&#25490;&#38500;&#28508;&#22312;&#25915;&#20987;&#32773;&#65292;&#24182;&#25512;&#23548;&#20986;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#25152;&#38656;&#30340;&#37319;&#26679;&#35797;&#39564;&#20010;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09495</link><description>&lt;p&gt;
Among Us: &#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Among Us: Adversarially Robust Collaborative Perception by Consensus. (arXiv:2303.09495v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09495
&lt;/p&gt;
&lt;p&gt;
ROBOSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#38543;&#26426;&#23376;&#38598;&#30340;&#38431;&#21451;&#26469;&#23545;&#27604;&#21327;&#21516;&#24863;&#30693;&#21644;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#65292;&#20197;&#25490;&#38500;&#28508;&#22312;&#25915;&#20987;&#32773;&#65292;&#24182;&#25512;&#23548;&#20986;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#25152;&#38656;&#30340;&#37319;&#26679;&#35797;&#39564;&#20010;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#33021;&#22815;&#27604;&#21333;&#20010;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#24863;&#30693;&#22330;&#26223;(&#20363;&#22914;&#65292;&#26816;&#27979;&#29289;&#20307;)&#65292;&#20294;&#22312;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26102;&#24456;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#12290;&#36825;&#19968;&#38382;&#39064;&#21487;&#36890;&#36807;&#23545;&#25239;&#24615;&#38450;&#24481;&#26469;&#35299;&#20915;&#65292;&#20294;&#35757;&#32451;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#26426;&#21046;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ROBOSAC&#65292;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26032;&#22411;&#38450;&#24481;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#24212;&#23545;&#26410;&#30693;&#30340;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#21327;&#21516;&#24863;&#30693;&#24212;&#35813;&#27604;&#21333;&#20010;&#24863;&#30693;&#26356;&#33021;&#36798;&#25104;&#19968;&#33268;&#65292;&#32780;&#19981;&#24212;&#30456;&#20114;&#20135;&#29983;&#20998;&#27495;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35828;&#21644;&#39564;&#35777;&#30340;&#26694;&#26550;&#65306;&#21033;&#29992;&#19968;&#32452;&#38543;&#26426;&#36873;&#25321;&#30340;&#38431;&#21451;&#65292;&#23545;&#21327;&#21516;&#24863;&#30693;&#19982;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#12290;&#22312;&#36825;&#26679;&#30340;&#26694;&#26550;&#19979;&#65292;&#26356;&#22810;&#30340;&#38431;&#21451;&#36890;&#24120;&#24847;&#21619;&#30528;&#26356;&#22909;&#30340;&#24863;&#30693;&#34920;&#29616;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#37319;&#26679;&#26102;&#38388;&#26469;&#25490;&#38500;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#38656;&#35201;&#22810;&#23569;&#20010;&#37319;&#26679;&#35797;&#39564;&#25165;&#33021;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#30340;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple robots could perceive a scene (e.g., detect objects) collaboratively better than individuals, although easily suffer from adversarial attacks when using deep learning. This could be addressed by the adversarial defense, but its training requires the often-unknown attacking mechanism. Differently, we propose ROBOSAC, a novel sampling-based defense strategy generalizable to unseen attackers. Our key idea is that collaborative perception should lead to consensus rather than dissensus in results compared to individual perception. This leads to our hypothesize-and-verify framework: perception results with and without collaboration from a random subset of teammates are compared until reaching a consensus. In such a framework, more teammates in the sampled subset often entail better perception performance but require longer sampling time to reject potential attackers. Thus, we derive how many sampling trials are needed to ensure the desired size of an attacker-free subset, or equival
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SpaceTime&#30340;&#29366;&#24577;&#31354;&#38388;&#26102;&#38388;&#24207;&#21015;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#30697;&#38453;&#30340;&#26032;&#30340;SSM&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#33258;&#22238;&#24402;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#36828;&#26399;&#12290;</title><link>http://arxiv.org/abs/2303.09489</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Effectively Modeling Time Series with Simple Discrete State Spaces. (arXiv:2303.09489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09489
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SpaceTime&#30340;&#29366;&#24577;&#31354;&#38388;&#26102;&#38388;&#24207;&#21015;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#30697;&#38453;&#30340;&#26032;&#30340;SSM&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#33258;&#22238;&#24402;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#36828;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26159;&#19968;&#20010;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#26041;&#27861;&#20855;&#22791;&#20197;&#19979;&#19977;&#20010;&#29305;&#24615;&#65306;&#34920;&#36798;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#39044;&#27979;&#36828;&#26399;&#65292;&#20197;&#21450;&#26377;&#25928;&#35757;&#32451;&#38271;&#24207;&#21015;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26159;&#26102;&#38388;&#24207;&#21015;&#30340;&#32463;&#20856;&#27169;&#22411;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#23558;SSMs&#19982;&#28145;&#24230;&#23398;&#20064;&#23618;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20808;&#21069;&#26041;&#27861;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;SSM&#34920;&#31034;&#19981;&#33021;&#34920;&#36798;&#33258;&#22238;&#24402;&#30340;&#26102;&#38388;&#24207;&#21015;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SpaceTime&#65292;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#26102;&#38388;&#24207;&#21015;&#26550;&#26500;&#65292;&#25913;&#36827;&#20102;&#25152;&#26377;&#19977;&#20010;&#29305;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#34920;&#29616;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20276;&#38543;&#30697;&#38453;&#30340;&#26032;&#30340;SSM&#21442;&#25968;&#21270;&#26041;&#27861;&#8212;&#8212;&#31163;&#25955;&#26102;&#38388;&#36807;&#31243;&#30340;&#35268;&#33539;&#34920;&#31034;&#8212;&#8212;&#23427;&#20351;&#24471;SpaceTime&#30340;SSM&#23618;&#33021;&#22815;&#23398;&#20064;&#31216;&#24515;&#30340;&#33258;&#22238;&#24402;&#36807;&#31243;&#12290;&#20026;&#20102;&#39044;&#27979;&#36828;&#26399;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#8220;&#38381;&#29615;&#8221;&#21464;&#20307;&#30340;&#20276;&#38543;SSM&#65292;&#20351;&#24471;SpaceTime&#33021;&#22815;&#39044;&#27979;&#35768;&#22810;&#23558;&#26469;&#30340;&#26102;&#38388;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a "closed-loop" variation of the companion SSM, which enables SpaceTime to predict many future time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#20248;&#21270;&#20219;&#24847;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#65292;&#20174;&#32780;&#21152;&#36895;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.09478</link><description>&lt;p&gt;
&#31616;&#21333;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#30340;&#20219;&#24847;&#38454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Arbitrary Order Meta-Learning with Simple Population-Based Evolution. (arXiv:2303.09478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#20248;&#21270;&#20219;&#24847;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#65292;&#20174;&#32780;&#21152;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#20351;&#23398;&#20064;&#31995;&#32479;&#36805;&#36895;&#12289;&#28789;&#27963;&#22320;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#36890;&#24120;&#38656;&#35201;&#23450;&#20041;&#19968;&#32452;&#22806;&#24490;&#29615;&#20803;&#21442;&#25968;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#26356;&#26032;&#19968;&#32452;&#20869;&#37096;&#24490;&#29615;&#21442;&#25968;&#12290;&#22823;&#22810;&#25968;&#20803;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#12289;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#26696;&#26469;&#26356;&#26032;&#36825;&#20123;&#20803;&#21442;&#25968;&#65292;&#20294;&#26631;&#20934;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#24448;&#24448;&#19981;&#36866;&#29992;&#20110;&#26356;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#65292;&#22240;&#20026;&#20803;&#20248;&#21270;&#36807;&#31243;&#21464;&#24471;&#22826;&#22797;&#26434;&#25110;&#19981;&#31283;&#23450;&#12290;&#21463;&#21040;&#30495;&#23454;&#19990;&#30028;&#36827;&#21270;&#20013;&#39640;&#38454;&#20803;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#20351;&#29992;&#31616;&#21333;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#21487;&#20197;&#38544;&#24335;&#22320;&#20248;&#21270;&#20219;&#24847;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#38544;&#24335;&#22320;&#20248;&#21270;&#20102;&#20219;&#24847;&#38454;&#20803;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning, the notion of learning to learn, enables learning systems to quickly and flexibly solve new tasks. This usually involves defining a set of outer-loop meta-parameters that are then used to update a set of inner-loop parameters. Most meta-learning approaches use complicated and computationally expensive bi-level optimisation schemes to update these meta-parameters. Ideally, systems should perform multiple orders of meta-learning, i.e. to learn to learn to learn and so on, to accelerate their own learning. Unfortunately, standard meta-learning techniques are often inappropriate for these higher-order meta-parameters because the meta-optimisation procedure becomes too complicated or unstable. Inspired by the higher-order meta-learning we observe in real-world evolution, we show that using simple population-based evolution implicitly optimises for arbitrarily-high order meta-parameters. First, we theoretically prove and empirically show that population-based evolution implici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615; &#12290;</title><link>http://arxiv.org/abs/2303.09470</link><description>&lt;p&gt;
&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels. (arXiv:2303.09470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615; &#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#29289;&#21697;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24039;&#22937;&#22320;&#20351;&#29992;&#36317;&#31163;&#31867;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20877;&#32467;&#21512;&#25240;&#25187;&#31574;&#30053;&#20197;&#20943;&#23569;&#36317;&#31163;&#25152;&#26377;&#31867;&#20013;&#24515;&#65288;&#21363;&#24322;&#24120;&#20540;&#65289;&#36828;&#30340;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#36317;&#31163;&#21508;&#33258;&#31867;&#20013;&#24515;&#26356;&#36828;&#30340;&#26679;&#26412;&#26356;&#21487;&#33021;&#26159;&#22122;&#22768;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#39046;&#22495;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#21644;&#35777;&#26126;&#25968;&#25628;&#32034;&#30456;&#32467;&#21512;&#30340;PN-MCTS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#26368;&#32456;&#36208;&#27493;&#36873;&#25321;&#12289;&#35299;&#20915;&#23376;&#26641;&#20197;&#21450;UCT&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.09449</link><description>&lt;p&gt;
&#22522;&#20110;&#35777;&#26126;&#25968;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Proof Number Based Monte-Carlo Tree Search. (arXiv:2303.09449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#21644;&#35777;&#26126;&#25968;&#25628;&#32034;&#30456;&#32467;&#21512;&#30340;PN-MCTS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#26368;&#32456;&#36208;&#27493;&#36873;&#25321;&#12289;&#35299;&#20915;&#23376;&#26641;&#20197;&#21450;UCT&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28216;&#25103;&#25628;&#32034;&#31639;&#27861;PN-MCTS&#65292;&#23427;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#35777;&#26126;&#25968;&#25628;&#32034;&#65288;PNS&#65289;&#30456;&#32467;&#21512;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#39046;&#22495;&#65292;&#21363;&#26368;&#32456;&#36208;&#27493;&#36873;&#25321;&#65292;&#35299;&#20915;&#23376;&#26641;&#20197;&#21450;UCT&#20844;&#24335;&#65292;&#36825;&#20123;&#39046;&#22495;&#21487;&#20197;&#21033;&#29992;&#22312;MCTS&#26641;&#20013;&#25910;&#38598;&#21040;&#30340;&#35777;&#26126;&#25968;&#21644;&#35777;&#20266;&#25968;&#25552;&#20379;&#30340;&#38468;&#21152;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#32452;&#21512;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#20960;&#20010;&#28216;&#25103;&#20013;&#19982;vanilla UCT MCTS&#36827;&#34892;&#23545;&#20915;&#65306;&#21160;&#20316;&#32447;&#65288;$7$$\times$$7$&#21644;$8$$\times$$8$&#65289;&#65292;MiniShogi&#65292; Knightthrough&#65292; Awari&#21644;Gomoku&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#20986;&#29616;&#24179;&#23616;&#30340;&#28216;&#25103;&#65292;&#22914;Awari&#65292;&#36890;&#36807;&#22312;MCTS&#26641;&#30340;&#39030;&#37096;&#28155;&#21152;&#19968;&#20010;&#38468;&#21152;&#30340;PNS&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PN-MCTS&#22312;6&#20010;&#28216;&#25103;&#39046;&#22495;&#20013;&#26377;5&#20010;&#30340;&#32988;&#29575;&#20248;&#20110;MCTS&#65288;&#38500;&#20102;Gomoku&#65289;&#65292;&#20854;&#20013;&#22312;&#21160;&#20316;&#32447;&#28216;&#25103;&#20013;&#33719;&#24471;&#20102;&#39640;&#36798;96.2%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new game search algorithm, PN-MCTS, that combines Monte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These two algorithms have been successfully applied for decision making in a range of domains. We define three areas where the additional knowledge provided by the proof and disproof numbers gathered in MCTS trees might be used: final move selection, solving subtrees, and the UCT formula. We test all possible combinations on different time settings, playing against vanilla UCT MCTS on several games: Lines of Action ($7$$\times$$7$ and $8$$\times$$8$), MiniShogi, Knightthrough, Awari, and Gomoku. Furthermore, we extend this new algorithm to properly address games with draws, like Awari, by adding an additional layer of PNS on top of the MCTS tree. The experiments show that PN-MCTS confidently outperforms MCTS in 5 out of 6 game domains (all except Gomoku), achieving win rates up to 96.2% for Lines of Action.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09447</link><description>&lt;p&gt;
&#20351;&#29992;Prompt-Tuning&#30340;&#21407;&#22411;&#36716;&#21521;&#38024;&#23545;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20316;&#20026;&#31867;&#21035;&#23884;&#20837;&#30340;&#19968;&#31181;&#34920;&#31034;&#65292;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#30340;&#20869;&#23384;&#21344;&#29992;&#25110;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#23548;&#33268;&#30340;&#24615;&#33021;&#24613;&#21095;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65288;CPP&#65289;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#35843;&#25972;&#65292;&#24403;&#22312;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38556;&#30861;&#24182;&#26174;&#30528;&#25552;&#39640;&#21407;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPP&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;CPP&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#23427;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#19979;&#36830;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09446</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#36755;&#20837;&#25511;&#21046;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#29983;&#25104;&#39640;&#24230;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#25509;&#21475;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29992;&#25143;&#25110;&#25163;&#21160;&#25506;&#32034;&#19981;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25110;&#32773;&#36153;&#21147;&#22320;&#27880;&#37322;&#25968;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23454;&#20363;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MICVAE)&#65292;&#23427;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32534;&#30721;&#31232;&#30095;&#30340;&#38901;&#24459;&#29305;&#24449;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MICVAE&#34920;&#29616;&#20986;&#20102;&#31232;&#30095;&#30340;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#26426;&#21046;&#25152;&#38656;&#30340;&#33391;&#22909;&#21697;&#36136;&#65306;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#12290;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;(~4)&#65292;MICVAE&#20063;&#33021;&#35753;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#27604;&#29305;&#24065;&#20215;&#26684;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;3.6&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.09397</link><description>&lt;p&gt;
&#21033;&#29992;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrency Price Prediction using Twitter Sentiment Analysis. (arXiv:2303.09397v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#27604;&#29305;&#24065;&#20215;&#26684;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21152;&#23494;&#36135;&#24065;&#30340;&#27874;&#21160;&#24615;&#21450;&#22810;&#26679;&#21270;&#30340;&#24847;&#35265;&#65292;&#22312;&#35768;&#22810;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#37117;&#25104;&#20026;&#20102;&#35752;&#35770;&#30340;&#20013;&#24515;&#35805;&#39064;&#12290;Twitter &#36805;&#36895;&#25104;&#20026;&#26032;&#38395;&#26469;&#28304;&#21644;&#27604;&#29305;&#24065;&#35752;&#35770;&#30340;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;&#25512;&#25991;&#24773;&#24863;&#26469;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21487;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#36716;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25512;&#25991;&#38598;&#30340;&#24773;&#24863;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;&#24773;&#24863;&#20197;&#21450;&#21382;&#21490;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#25968;&#25454;&#12289;&#25512;&#25991;&#25968;&#37327;&#12289;&#29992;&#25143;&#30340;&#36861;&#38543;&#32773;&#25968;&#37327;&#20197;&#21450;&#29992;&#25143;&#26159;&#21542;&#36890;&#36807;&#39564;&#35777;&#26469;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#21453;&#24212;&#20102;&#23454;&#26102;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;&#32780;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#21017;&#20026;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cryptocurrency ecosystem has been the centre of discussion on many social media platforms, following its noted volatility and varied opinions. Twitter is rapidly being utilised as a news source and a medium for bitcoin discussion. Our algorithm seeks to use historical prices and sentiment of tweets to forecast the price of Bitcoin. In this study, we develop an end-to-end model that can forecast the sentiment of a set of tweets (using a Bidirectional Encoder Representations from Transformers - based Neural Network Model) and forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted sentiment and other metrics like historical cryptocurrency price data, tweet volume, a user's following, and whether or not a user is verified. The sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average of real-time data, and test data. The mean absolute percent error for the price prediction was 3.6%.
&lt;/p&gt;</description></item><item><title>HAT&#26159;&#19968;&#31181;&#35745;&#31639;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.09383</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#27880;&#24847;&#21147;&#39044;&#27979;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Human Attention using Computational Attention. (arXiv:2303.09383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09383
&lt;/p&gt;
&lt;p&gt;
HAT&#26159;&#19968;&#31181;&#35745;&#31639;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#26088;&#22312;&#39044;&#27979;&#33258;&#19978;&#32780;&#19979;&#25110;&#33258;&#19979;&#32780;&#19978;&#25511;&#21046;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#35270;&#35273;&#25628;&#32034;&#21644;&#33258;&#30001;&#35266;&#30475;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#36825;&#20004;&#31181;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#25511;&#21046;&#12290;HAT&#22312;&#39044;&#27979;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#36827;&#34892;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#26041;&#38754;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#26080;&#20219;&#21153;&#33258;&#30001;&#35266;&#30475;&#27880;&#35270;&#36335;&#24452;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36807;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;HAT&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#20849;&#21516;&#21019;&#24314;&#31867;&#20284;&#20110;&#20154;&#31867;&#21160;&#24577;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#30340;&#26102;&#31354;&#24847;&#35782;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#19982;&#20197;&#21069;&#20381;&#36182;&#20110;&#31895;&#31961;&#30340;&#27880;&#35270;&#21333;&#20803;&#26684;&#32593;&#26684;&#24182;&#30001;&#20110;&#31163;&#25955;&#21270;&#22266;&#23450;&#32780;&#32463;&#21382;&#20449;&#24687;&#20002;&#22833;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;HAT&#20855;&#26377;&#23494;&#38598;&#39044;&#27979;&#26550;&#26500;&#65292;&#24182;&#20026;&#27599;&#20010;&#27880;&#35270;&#36755;&#20986;&#23494;&#38598;&#28909;&#22270;&#65292;&#20174;&#32780;&#36991;&#20813;&#31163;&#25955;&#27880;&#35270;&#12290;HAT&#22312;&#35745;&#31639;&#35270;&#35273;&#27880;&#24847;&#21147;&#26041;&#38754;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most models of visual attention are aimed at predicting either top-down or bottom-up control, as studied using different visual search and free-viewing tasks. We propose Human Attention Transformer (HAT), a single model predicting both forms of attention control. HAT is the new state-of-the-art (SOTA) in predicting the scanpath of fixations made during target-present and target-absent search, and matches or exceeds SOTA in the prediction of taskless free-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel transformer-based architecture and a simplified foveated retina that collectively create a spatio-temporal awareness akin to the dynamic visual working memory of humans. Unlike previous methods that rely on a coarse grid of fixation cells and experience information loss due to fixation discretization, HAT features a dense-prediction architecture and outputs a dense heatmap for each fixation, thus avoiding discretizing fixations. HAT sets a new standard in computati
&lt;/p&gt;</description></item><item><title>&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09354</link><description>&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65306;&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09354
&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#23558;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CompPath&#65289;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36716;&#21270;&#20026;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25253;&#21578;&#38590;&#20197;&#37325;&#22797; ML &#32467;&#26524;&#30340;&#22256;&#38590;&#12290;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;IDC&#65289;&#26159;&#19968;&#20010;&#20844;&#20849;&#24211;&#65292;&#21253;&#21547; &gt;120 &#20010;&#30284;&#30151;&#22270;&#20687;&#25910;&#38598;&#65292;&#21253;&#25324; &gt;38,000 &#24352;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#26088;&#22312;&#19982;&#20113;&#31471; ML &#26381;&#21153;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102; IDC &#20419;&#36827; CompPath &#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#21147;&#12290; &#26448;&#26009;&#21644;&#26041;&#27861;&#65306;IDC &#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65306;&#25152;&#26377;&#22270;&#20687;&#37117;&#26681;&#25454; DICOM &#26631;&#20934;&#36827;&#34892;&#32534;&#30721;&#65292;&#20855;&#26377;&#25345;&#20037;&#21270;&#26631;&#35782;&#31526;&#12289;&#21487;&#36890;&#36807;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#36827;&#34892;&#21457;&#29616;&#65292;&#24182;&#21487;&#36890;&#36807;&#24320;&#25918;&#24335;&#24037;&#20855;&#35775;&#38382;&#12290;&#20511;&#27492;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312; IDC &#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#38024;&#23545;&#32954;&#30284;&#32452;&#32455;&#20998;&#31867;&#30340;&#19968;&#31181;&#20195;&#34920;&#24615;&#22522;&#20110; ML &#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#12290;&#20026;&#35780;&#20272;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#39564;&#34987;&#22810;&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of &gt;120 cancer image collections, including &gt;38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20276;&#38543;&#26041;&#27861;&#26469;&#23454;&#29616;&#23454;&#26102;&#24377;&#24615;&#30340;&#37096;&#20998;&#24418;&#29366;&#21305;&#37197;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#27714;&#35299;&#36229;&#24377;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#33719;&#24471;&#20276;&#38543;&#38382;&#39064;&#65292;&#32467;&#21512;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26469;&#27714;&#35299;&#26410;&#30693;&#37327;&#24212;&#29992;&#20110;&#29289;&#20307;&#30340;&#34920;&#38754;&#21147;&#20998;&#24067;&#12290;&#35813;&#27969;&#31243;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#25509;&#21463;&#30340;&#27880;&#20876;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.09343</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20276;&#38543;&#26041;&#27861;&#23454;&#26102;&#24377;&#24615;&#37096;&#20998;&#24418;&#29366;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Real-time elastic partial shape matching using a neural network-based adjoint method. (arXiv:2303.09343v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20276;&#38543;&#26041;&#27861;&#26469;&#23454;&#29616;&#23454;&#26102;&#24377;&#24615;&#30340;&#37096;&#20998;&#24418;&#29366;&#21305;&#37197;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#27714;&#35299;&#36229;&#24377;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#33719;&#24471;&#20276;&#38543;&#38382;&#39064;&#65292;&#32467;&#21512;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26469;&#27714;&#35299;&#26410;&#30693;&#37327;&#24212;&#29992;&#20110;&#29289;&#20307;&#30340;&#34920;&#38754;&#21147;&#20998;&#24067;&#12290;&#35813;&#27969;&#31243;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#25509;&#21463;&#30340;&#27880;&#20876;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#21305;&#37197;&#36890;&#24120;&#20250;&#25552;&#20379;&#26174;&#33879;&#30340;&#21464;&#24418;&#65292;&#21487;&#33021;&#20250;&#30001;&#20110;&#32570;&#20047;&#29289;&#29702;&#31574;&#30053;&#23548;&#33268;&#32467;&#26500;&#25925;&#38556;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38750;&#32447;&#24615;&#21487;&#21464;&#24418;&#20307;&#30340;&#37096;&#20998;&#34920;&#38754;&#21305;&#37197;&#23545;&#20110;&#25511;&#21046;&#32467;&#26500;&#21464;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#27880;&#20876;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26469;&#34920;&#36798;&#65292;&#20854;&#20013;&#26410;&#30693;&#37327;&#26159;&#24212;&#29992;&#20110;&#29289;&#20307;&#30340;&#34920;&#38754;&#21147;&#20998;&#24067;&#65292;&#36890;&#36807;&#36229;&#24377;&#27169;&#22411;&#35745;&#31639;&#24471;&#21040;&#30340;&#21464;&#24418;&#12290;&#37319;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#36229;&#24377;&#24615;&#38382;&#39064;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#65292;&#20276;&#38543;&#38382;&#39064;&#36890;&#36807;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#25509;&#21463;&#30340;&#27880;&#20876;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface matching usually provides significant deformations that can lead to structural failure due to the lack of physical policy. In this context, partial surface matching of non-linear deformable bodies is crucial in engineering to govern structure deformations. In this article, we propose to formulate the registration problem as an optimal control problem using an artificial neural network where the unknown is the surface force distribution that applies to the object and the resulting deformation computed using a hyper-elastic model. The optimization problem is solved using an adjoint method where the hyper-elastic problem is solved using the feed-forward neural network and the adjoint problem is obtained through the backpropagation of the network. Our process improves the computation speed by multiple orders of magnitude while providing acceptable registration errors.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#30340;&#21021;&#32423;&#21644;&#20013;&#32423;&#35780;&#20272;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32534;&#31243;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09325</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#33021;&#36890;&#36807;&#39640;&#31561;&#25945;&#32946;&#32534;&#31243;&#35838;&#31243;&#30340;&#35780;&#20272;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?. (arXiv:2303.09325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09325
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#30340;&#21021;&#32423;&#21644;&#20013;&#32423;&#35780;&#20272;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32534;&#31243;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#30340;&#21021;&#32423;&#21644;&#20013;&#32423;&#35780;&#20272;&#20013;&#30340;&#33021;&#21147;&#12290;&#20154;&#20204;&#23545;&#36825;&#31181;&#26032;&#20852;&#25216;&#26415;&#22312;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#65288;&#20363;&#22914;&#65292;&#32451;&#20064;&#29983;&#25104;&#65292;&#20195;&#30721;&#35299;&#37322;&#65289;&#21644;&#19981;&#33391;&#29992;&#36884;&#65288;&#20363;&#22914;&#65292;&#20316;&#24330;&#65289;&#30340;&#35752;&#35770;&#24050;&#32463;&#21152; intens &#20102;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#31181;&#35780;&#20272;&#24037;&#20855;&#30340;&#24191;&#27867;&#32534;&#31243;&#35838;&#31243;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20005;&#26684;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#35780;&#20272;&#20174;&#31616;&#21333;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;&#19981;&#28041;&#21450;&#20195;&#30721;&#65289;&#21040;&#20195;&#30721;&#20998;&#24067;&#22312;&#22810;&#20010;&#25991;&#20214;&#20013;&#30340;&#22797;&#26434;&#32534;&#31243;&#39033;&#30446;&#30340;&#19977;&#20010;Python&#35838;&#31243;&#19978;&#35780;&#20272;&#20102;GPT&#65288;&#24635;&#20849;599&#20010;&#32451;&#20064;&#39064;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GPT&#27169;&#22411;&#22914;&#20309;&#25104;&#21151;&#22320;&#21033;&#29992;&#33258;&#21160;&#35780;&#20998;&#22120;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#19981;&#33021;&#36890;&#36807;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#36890;&#24120;&#28041;&#21450;&#30340;&#23436;&#25972;&#35780;&#20272;&#24037;&#20855;&#30340;&#20840;&#35889;&#12290;&#34429;&#28982;GPT&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#29983;&#25104;&#31616;&#21333;&#32451;&#20064;&#30340;&#35821;&#27861;&#27491;&#30830;&#20195;&#30721;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#32451;&#20064;&#65292;&#20182;&#20204;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#24182;&#19988;&#26080;&#27861;&#29983;&#25104;&#28385;&#36275;&#22810;&#25991;&#20214;&#32534;&#31243;&#39033;&#30446;&#35201;&#27714;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;GPT&#27169;&#22411;&#26174;&#31034;&#20102;&#26377;&#38480;&#30340;&#21033;&#29992;&#33258;&#21160;&#35780;&#20998;&#22120;&#25552;&#20379;&#30340;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;GPT&#27169;&#22411;&#21487;&#33021;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Pyth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31616;&#21333;&#23646;&#24615;&#39640;&#25928;&#25512;&#26029;NFA&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09311</link><description>&lt;p&gt;
&#21033;&#29992;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#23646;&#24615;&#39640;&#25928;&#25512;&#26029;NFA
&lt;/p&gt;
&lt;p&gt;
Taking advantage of a very simple property to efficiently infer NFAs. (arXiv:2303.09311v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31616;&#21333;&#23646;&#24615;&#39640;&#25928;&#25512;&#26029;NFA&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#27861;&#25512;&#26029;&#26159;&#23398;&#20064;&#24418;&#24335;&#35821;&#27861;&#20316;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#25110;&#19968;&#32452;&#37325;&#20889;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20851;&#27880;&#25512;&#26029;&#24517;&#39035;&#25509;&#21463;&#19968;&#20123;&#35789;&#27719;&#24182;&#25298;&#32477;&#32473;&#23450;&#26679;&#26412;&#20013;&#30340;&#20854;&#20182;&#35789;&#27719;&#30340;NFA&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#22312;SAT&#20013;&#33258;&#28982;&#24314;&#27169;&#12290;&#35813;&#26631;&#20934;&#27169;&#22411;&#38750;&#24120;&#24040;&#22823;&#65292;&#22240;&#27492;&#22522;&#20110;&#21069;&#32512;&#12289;&#21518;&#32512;&#21644;&#28151;&#21512;&#30340;&#27169;&#22411;&#34987;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#26356;&#23567;&#30340;SAT&#23454;&#20363;&#12290;&#26377;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#21644;&#26126;&#26174;&#30340;&#23646;&#24615;&#22768;&#31216;&#65306;&#22914;&#26524;&#32473;&#23450;&#26679;&#26412;&#26377;&#22823;&#23567;&#20026;k&#30340;NFA&#65292;&#37027;&#20040;&#20063;&#26377;&#22823;&#23567;&#20026;k+1&#30340;NFA&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32473;k+1&#30340;NFA&#28155;&#21152;&#19968;&#20123;&#29305;&#24449;&#26469;&#21152;&#24378;&#36825;&#20010;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27492;&#23646;&#24615;&#26469;&#38480;&#21046;&#32473;&#23450;&#26679;&#26412;&#30340;&#26368;&#23567;NFA&#30340;&#22823;&#23567;&#33539;&#22260;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#23567;&#20026;k+1&#30340;NFA&#30340;&#31616;&#21270;&#21644;&#31934;&#31616;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#22823;&#23567;&#20026;k&#30340;&#21021;&#22987;&#27169;&#22411;&#35201;&#23567;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#31616;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#29305;&#23450;&#26679;&#26412;&#20013;&#26500;&#24314;&#22823;&#23567;&#20026;k&#30340;NFA&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical inference consists in learning a formal grammar as a finite state machine or as a set of rewrite rules. In this paper, we are concerned with inferring Nondeterministic Finite Automata (NFA) that must accept some words, and reject some other words from a given sample. This problem can naturally be modeled in SAT. The standard model being enormous, some models based on prefixes, suffixes, and hybrids were designed to generate smaller SAT instances. There is a very simple and obvious property that says: if there is an NFA of size k for a given sample, there is also an NFA of size k+1. We first strengthen this property by adding some characteristics to the NFA of size k+1. Hence, we can use this property to tighten the bounds of the size of the minimal NFA for a given sample. We then propose simplified and refined models for NFA of size k+1 that are smaller than the initial models for NFA of size k. We also propose a reduction algorithm to build an NFA of size k from a specific
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.09306</link><description>&lt;p&gt;
&#26500;&#24314;&#40065;&#26834;&#30340;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#21253;&#25324;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#21629;&#21517;&#23454;&#20307;&#12290;&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;CNER &#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#22797;&#26434;&#21644;&#22797;&#21512;&#23454;&#20307;&#65292;&#32780;&#36825;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#19981;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915; BanglaCoNER &#25968;&#25454;&#38598;&#19978;&#30340; CNER &#20219;&#21153;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21363;&#26465;&#20214;&#38543;&#26426;&#22330; (CRF) &#21644;&#22522;&#20110; finetuning transformer &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; BanglaBERT&#65289;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324; 15300 &#20010;&#29992;&#20110;&#35757;&#32451;&#30340;&#21477;&#23376;&#21644; 800 &#20010;&#29992;&#20110;&#39564;&#35777;&#30340;&#21477;&#23376;&#65292;&#26684;&#24335;&#20026; .conll&#12290;&#23545;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512; (EDA) &#25581;&#31034;&#20986;&#25968;&#25454;&#38598;&#26377; 7 &#31181;&#19981;&#21516;&#30340; NER &#26631;&#31614;&#65292;&#20854;&#20013;&#26377;&#33521;&#35821;&#21333;&#35789;&#30340;&#26126;&#26174;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying named entities in text. But much work hasn't been done for complex named entity recognition in Bangla, despite being the seventh most spoken language globally. CNER is a more challenging task than traditional NER as it involves identifying and classifying complex and compound entities, which are not common in Bangla language. In this paper, we present the winning solution of Bangla Complex Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER dataset using two different approaches, namely Conditional Random Fields (CRF) and finetuning transformer based Deep Learning models such as BanglaBERT.  The dataset consisted of 15300 sentences for training and 800 sentences for validation, in the .conll format. Exploratory Data Analysis (EDA) on the dataset revealed that the dataset had 7 different NER tags, with notable presence of English words, s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#65292;&#20351;&#29992;&#8220;&#32676;&#20307;&#21453;&#20107;&#23454;&#8221;&#38598;&#20307;&#35299;&#37322;&#31867;&#20284;&#23454;&#20363;&#30340;&#32452;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32676;&#20307;&#21453;&#20107;&#23454;&#31639;&#27861;&#26469;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;&#35299;&#37322;&#65292;&#36866;&#21512;&#20154;&#31867;&#23545;&#36830;&#36143;&#12289;&#24191;&#27867;&#35299;&#37322;&#30340;&#21916;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.09297</link><description>&lt;p&gt;
&#35299;&#37322;&#32676;&#20307;&#23454;&#20363;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#32676;&#20307;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20351;&#29992;&#26696;&#20363;&#12289;&#31639;&#27861;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals. (arXiv:2303.09297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#65292;&#20351;&#29992;&#8220;&#32676;&#20307;&#21453;&#20107;&#23454;&#8221;&#38598;&#20307;&#35299;&#37322;&#31867;&#20284;&#23454;&#20363;&#30340;&#32452;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32676;&#20307;&#21453;&#20107;&#23454;&#31639;&#27861;&#26469;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;&#35299;&#37322;&#65292;&#36866;&#21512;&#20154;&#31867;&#23545;&#36830;&#36143;&#12289;&#24191;&#27867;&#35299;&#37322;&#30340;&#21916;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#20107;&#21518;&#35299;&#37322;&#24418;&#24335;&#65292;&#22240;&#20026;&#23427;&#20204;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#39046;&#22495;&#12289;&#25552;&#20379;&#20102;&#27861;&#24459;&#21512;&#35268;&#24615;&#65288;&#20363;&#22914;&#31526;&#21512;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65289;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#23545;&#27604;&#24615;&#36136;&#12290;&#34429;&#28982;&#21453;&#20107;&#23454;&#35299;&#37322;&#36890;&#24120;&#29992;&#20110;&#35299;&#37322;&#21333;&#20010;&#39044;&#27979;&#23454;&#20363;&#65292;&#20294;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20363;&#65292;&#21363;&#20351;&#29992;&#8220;&#32676;&#20307;&#21453;&#20107;&#23454;&#8221;&#26469;&#38598;&#20307;&#35299;&#37322;&#31867;&#20284;&#23454;&#20363;&#30340;&#32452;&#65288;&#20363;&#22914;&#31361;&#20986;&#26174;&#31034;&#19968;&#32452;&#24739;&#32773;&#20013;&#30142;&#30149;&#37325;&#22797;&#20986;&#29616;&#30340;&#27169;&#24335;&#65289;&#12290;&#36825;&#20123;&#32676;&#20307;&#21453;&#20107;&#23454;&#28385;&#36275;&#20154;&#20204;&#23545;&#21253;&#21547;&#22810;&#20010;&#20107;&#20214;/&#23454;&#20363;&#30340;&#36830;&#36143;&#12289;&#24191;&#27867;&#35299;&#37322;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32676;&#20307;&#21453;&#20107;&#23454;&#31639;&#27861;&#26469;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;&#35299;&#37322;&#65292;&#36825;&#31181;&#35299;&#37322;&#21448;&#24544;&#23454;&#20110;&#24453;&#35299;&#37322;&#27169;&#22411;&#12290;&#36824;&#20351;&#29992;&#23458;&#35266;&#65288;&#21363;&#20934;&#30830;&#24615;&#65289;&#21644;&#20027;&#35266;&#65288;&#21363;&#20449;&#24515;&#12289;&#35299;&#37322;&#28385;&#24847;&#24230;&#65289;&#35780;&#20272;&#20102;&#35813;&#35299;&#37322;&#31574;&#30053;&#22312;&#22823;&#22411;&#25511;&#21046;&#29992;&#25143;&#30740;&#31350;&#65288;N=207&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are an increasingly popular form of post hoc explanation due to their (i) applicability across problem domains, (ii) proposed legal compliance (e.g., with GDPR), and (iii) reliance on the contrastive nature of human explanation. Although counterfactual explanations are normally used to explain individual predictive-instances, we explore a novel use case in which groups of similar instances are explained in a collective fashion using ``group counterfactuals'' (e.g., to highlight a repeating pattern of illness in a group of patients). These group counterfactuals meet a human preference for coherent, broad explanations covering multiple events/instances. A novel, group-counterfactual algorithm is proposed to generate high-coverage explanations that are faithful to the to-be-explained model. This explanation strategy is also evaluated in a large, controlled user study (N=207), using objective (i.e., accuracy) and subjective (i.e., confidence, explanation satisfa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;&#38598;&#21512;&#30340;&#22810;&#26679;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23454;&#29616;&#35774;&#35745;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#38477;&#20302;&#25925;&#38556;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.09283</link><description>&lt;p&gt;
&#20351;&#29992;&#35774;&#35745;&#22810;&#26679;&#24615;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Resiliency to Natural Image Corruptions in Deep Learning using Design Diversity. (arXiv:2303.09283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;&#38598;&#21512;&#30340;&#22810;&#26679;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23454;&#29616;&#35774;&#35745;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#38477;&#20302;&#25925;&#38556;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;&#38598;&#21512;&#30340;&#22810;&#26679;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#24402;&#22240;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#28508;&#21147;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#22522;&#20110;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#24050;&#30693;&#30340;&#20934;&#30830;&#24615;-&#22810;&#26679;&#24615;&#25240;&#34935;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#22522;&#20110;&#35774;&#35745;&#22810;&#26679;&#24615;&#30340;&#20998;&#26512;&#30740;&#31350;&#65292;&#20854;&#26174;&#31034;&#22914;&#26524;&#23454;&#29616;&#35774;&#35745;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#21017;&#21487;&#20197;&#38477;&#20302;&#24120;&#35265;&#25925;&#38556;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;ResNet50&#29992;&#20316;&#27604;&#36739;&#22522;&#32447;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#21333;&#29420;&#30340;DL&#27169;&#22411;&#32467;&#26500;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#24341;&#36215;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#20559;&#31227;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#25216;&#26415;&#29420;&#31435;&#35757;&#32451;&#25110;&#35757;&#32451;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#27169;&#22411;&#32467;&#26500;&#30340;&#38598;&#21512;&#65292;&#24182;&#35780;&#20272;&#20102;&#22522;&#20110;&#39044;&#27979;&#21644;&#22522;&#20110;&#24402;&#22240;&#30340;&#22810;&#26679;&#24615;&#19982;&#26368;&#32456;&#38598;&#21512;&#20934;&#30830;&#24615;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#35774;&#35745;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the relationship between diversity metrics, accuracy, and resiliency to natural image corruptions of Deep Learning (DL) image classifier ensembles. We investigate the potential of an attribution-based diversity metric to improve the known accuracy-diversity trade-off of the typical prediction-based diversity. Our motivation is based on analytical studies of design diversity that have shown that a reduction of common failure modes is possible if diversity of design choices is achieved.  Using ResNet50 as a comparison baseline, we evaluate the resiliency of multiple individual DL model architectures against dataset distribution shifts corresponding to natural image corruptions. We compare ensembles created with diverse model architectures trained either independently or through a Neural Architecture Search technique and evaluate the correlation of prediction-based and attribution-based diversity to the final ensemble accuracy. We evaluate a set of diversity 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.09271</link><description>&lt;p&gt;
&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles. (arXiv:2303.09271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20851;&#38190;&#31995;&#32479;&#30340;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#33021;&#22815;&#35299;&#37322;&#20026;&#20309;&#27169;&#22411;&#20570;&#20986;&#29305;&#23450;&#39044;&#27979;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#20379;&#30340;&#35299;&#37322;&#24517;&#39035;&#26159;&#21487;&#35777;&#26126;&#30340;&#65292;&#24182;&#19988;&#26368;&#22909;&#19981;&#21253;&#21547;&#20887;&#20313;&#20449;&#24687;&#65292;&#21363;&#26368;&#23567;&#35299;&#37322;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#26159;&#26368;&#23567;&#30340;&#65292;&#32780;&#19988;&#22312;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#26368;&#23567;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31070;&#35861;&#8221;&#31995;&#32479;&#65292;&#21487;&#20197;&#30830;&#23450;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#65292;&#22312;&#35745;&#31639;&#26368;&#23567;&#35299;&#37322;&#26102;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#26469;&#33258;&#30456;&#20851;&#24037;&#20316;&#30340;&#21483;&#20570;MARCO&#30340;&#31639;&#27861;&#65288;&#23558;&#20854;&#31216;&#20026;m-MARCO&#65289;&#65292;&#30446;&#30340;&#26159;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain why a machine learning model arrives at a particular prediction is crucial when used as decision support by human operators of critical systems. The provided explanations must be provably correct, and preferably without redundant information, called minimal explanations. In this paper, we aim at finding explanations for predictions made by tree ensembles that are not only minimal, but also minimum with respect to a cost function.  To this end, we first present a highly efficient oracle that can determine the correctness of explanations, surpassing the runtime performance of current state-of-the-art alternatives by several orders of magnitude when computing minimal explanations.  Secondly, we adapt an algorithm called MARCO from related works (calling it m-MARCO) for the purpose of computing a single minimum explanation per prediction, and demonstrate an overall speedup factor of two compared to the MARCO algorithm which enumerates all minimal explanations.  Final
&lt;/p&gt;</description></item><item><title>SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09266</link><description>&lt;p&gt;
SmartBERT&#65306;&#29992;&#20110;&#21152;&#36895;BERT&#25512;&#29702;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09266
&lt;/p&gt;
&lt;p&gt;
SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26679;&#26412;&#22312;&#26089;&#26399;&#36864;&#20986;&#20043;&#21069;&#37117;&#24517;&#39035;&#32463;&#36807;&#25152;&#26377;&#36830;&#32493;&#23618;&#65292;&#36739;&#22797;&#26434;&#30340;&#26679;&#26412;&#36890;&#24120;&#20250;&#32463;&#21382;&#26356;&#22810;&#30340;&#23618;&#65292;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SmartBERT&#30340;Bert&#25512;&#29702;&#30340;&#26032;&#22411;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#30456;&#32467;&#21512;&#30340;&#26426;&#21046;&#65292;&#23427;&#23558;&#36339;&#36807;&#38376;&#21644;&#36864;&#20986;&#31639;&#23376;&#21152;&#20837;&#21040;BERT&#30340;&#27599;&#19968;&#23618;&#20013;&#12290;SmartBERT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#23618;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#25105;&#20204;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#25552;&#39640;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;&#36864;&#20986;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36339;&#36807;&#38376;&#30340;&#19968;&#33268;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#26435;&#37325;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20843;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20915;&#23450;&#26159;&#21542;&#20026;&#20219;&#21153;&#30340;&#23454;&#20363;&#39044;&#27979;&#25110;&#23558;&#20854;&#22996;&#27966;&#32473;&#20154;&#31867;&#65292;&#30740;&#31350;&#26174;&#31034;AI&#22996;&#27966;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#28385;&#24847;&#24230;&#65292;&#26080;&#35770;&#20154;&#31867;&#26159;&#21542;&#24847;&#35782;&#21040;&#22996;&#27966;&#12290;&#20154;&#31867;&#39640;&#27700;&#24179;&#30340;&#33258;&#25105;&#25928;&#33021;&#24863;&#26159;&#36825;&#20123;&#25913;&#21892;&#30340;&#22522;&#30784;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.09224</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#65306;AI&#22996;&#27966;&#23545;&#20154;&#31867;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#28385;&#24847;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction. (arXiv:2303.09224v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09224
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20915;&#23450;&#26159;&#21542;&#20026;&#20219;&#21153;&#30340;&#23454;&#20363;&#39044;&#27979;&#25110;&#23558;&#20854;&#22996;&#27966;&#32473;&#20154;&#31867;&#65292;&#30740;&#31350;&#26174;&#31034;AI&#22996;&#27966;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#28385;&#24847;&#24230;&#65292;&#26080;&#35770;&#20154;&#31867;&#26159;&#21542;&#24847;&#35782;&#21040;&#22996;&#27966;&#12290;&#20154;&#31867;&#39640;&#27700;&#24179;&#30340;&#33258;&#25105;&#25928;&#33021;&#24863;&#26159;&#36825;&#20123;&#25913;&#21892;&#30340;&#22522;&#30784;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#23398;&#20064;&#20915;&#23450;&#26159;&#21542;&#36890;&#36807;&#32771;&#34385;&#21452;&#26041;&#30340;&#33021;&#21147;&#26469;&#20026;&#20219;&#21153;&#30340;&#23454;&#20363;&#39044;&#27979;&#25110;&#23558;&#20854;&#22996;&#27966;&#32473;&#20154;&#31867;&#12290;&#22312;&#20351;&#29992;&#21512;&#25104;&#29983;&#25104;&#25110;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#20154;&#39044;&#27979;&#36827;&#34892;&#30340;&#27169;&#25311;&#20013;&#65292;&#30456;&#23545;&#20110;&#20154;&#25110;AI&#27169;&#22411;&#21333;&#29420;&#23436;&#25104;&#20219;&#21153;&#65292;&#22996;&#27966;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#20154;&#26426;&#22242;&#38431;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20154;&#20204;&#22312;&#24847;&#35782;&#21040;AI&#27169;&#22411;&#22996;&#27966;&#20219;&#21153;&#23454;&#20363;&#32473;&#20182;&#20204;&#26102;&#22914;&#20309;&#25191;&#34892;&#21644;&#24863;&#30693;&#20219;&#21153;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#19968;&#39033;&#26377;196&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AI&#22996;&#27966;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#28385;&#24847;&#24230;&#65292;&#26080;&#35770;&#20154;&#31867;&#26159;&#21542;&#24847;&#35782;&#21040;&#22996;&#27966;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20154;&#31867;&#39640;&#27700;&#24179;&#30340;&#33258;&#25105;&#25928;&#33021;&#24863;&#26159;&#36825;&#20123;&#34920;&#29616;&#21644;&#28385;&#24847;&#24230;&#25913;&#21892;&#30340;&#22522;&#30784;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20801;&#35768;AI&#27169;&#22411;&#25509;&#31649;&#20219;&#21153;&#23454;&#20363;&#21487;&#20197;&#22312;&#20154;&#26426;&#21327;&#20316;&#20013;&#25552;&#39640;&#34920;&#29616;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for an instance of a task or to delegate it to a human by considering both parties' capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams -- compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them. In an experimental study with 196 participants, we show that task performance and task satisfaction improve through AI delegation, regardless of whether humans are aware of the delegation. Additionally, we identify humans' increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction. Our findings provide initial evidence that allowing AI models to take over m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#20998;&#25903;&#30340;&#22810;&#36755;&#20986;&#22238;&#24402;&#27169;&#22411;&#65292;&#21033;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;Mel-Frequency Cepstral Coefficients&#25216;&#26415;&#26469;&#25552;&#21462;&#35270;&#35273;&#21644;&#22768;&#23398;&#29305;&#24449;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#31181;&#27169;&#24577;&#20002;&#22833;&#26041;&#27861;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.09210</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#30340;&#21452;&#20998;&#25903;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Dual Branch Network for Emotional Reaction Intensity Estimation. (arXiv:2303.09210v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#20998;&#25903;&#30340;&#22810;&#36755;&#20986;&#22238;&#24402;&#27169;&#22411;&#65292;&#21033;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;Mel-Frequency Cepstral Coefficients&#25216;&#26415;&#26469;&#25552;&#21462;&#35270;&#35273;&#21644;&#22768;&#23398;&#29305;&#24449;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#31181;&#27169;&#24577;&#20002;&#22833;&#26041;&#27861;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;(ERI)&#30340;&#20272;&#35745;&#26159;&#22810;&#27169;&#24577;&#22330;&#26223;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;&#21307;&#23398;&#12289;&#23433;&#20840;&#39550;&#39542;&#21644;&#20854;&#20182;&#39046;&#22495;&#26377;&#26681;&#26412;&#24615;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#31532;&#20116;&#23626;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;(ABAW)&#20013;ERI&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#20998;&#25903;&#30340;&#22810;&#36755;&#20986;&#22238;&#24402;&#27169;&#22411;&#12290;&#31354;&#38388;&#27880;&#24847;&#21147;&#34987;&#29992;&#20110;&#26356;&#22909;&#22320;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;&#65292;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#25216;&#26415;&#25552;&#21462;&#22768;&#23398;&#29305;&#24449;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#24577;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23448;&#26041;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Reaction Intensity(ERI) estimation is an important task in multimodal scenarios, and has fundamental applications in medicine, safe driving and other fields. In this paper, we propose a solution to the ERI challenge of the fifth Affective Behavior Analysis in-the-wild(ABAW), a dual-branch based multi-output regression model. The spatial attention is used to better extract visual features, and the Mel-Frequency Cepstral Coefficients technology extracts acoustic features, and a method named modality dropout is added to fusion multimodal features. Our method achieves excellent results on the official validation set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36807;&#21435;&#30340;&#25191;&#34892;&#35760;&#24405;&#20013;&#23398;&#20064;KPI&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#20197;&#25512;&#33616;&#20248;&#21270;KPI&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.09209</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#31639;&#27861;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#23398;&#20064;&#34892;&#21160;&#20197;&#25512;&#33616;&#26368;&#20339;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Recommending the optimal policy by learning to act from temporal data. (arXiv:2303.09209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36807;&#21435;&#30340;&#25191;&#34892;&#35760;&#24405;&#20013;&#23398;&#20064;KPI&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#20197;&#25512;&#33616;&#20248;&#21270;KPI&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20010;&#36807;&#31243;&#25366;&#25496;&#20013;&#26368;&#31361;&#20986;&#30340;&#38382;&#39064;&#26159;&#35268;&#33539;&#24615;&#27969;&#31243;&#30417;&#25511;&#65292;&#23427;&#21253;&#25324;&#30830;&#23450;&#19968;&#32452;&#25805;&#20316;&#20197;&#20248;&#21270;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#27979;&#37327;&#25110;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;KPI&#65289;&#20026;&#30446;&#26631;&#30340;&#25512;&#33616;&#36807;&#31243;&#12290;&#36825;&#19968;&#38382;&#39064;&#30340;&#25361;&#25112;&#22312;&#20110;&#24517;&#39035;&#20165;&#22522;&#20110;&#23384;&#20648;&#22312;&#25152;&#35859;&#30340;&#25191;&#34892;&#26085;&#24535;&#20013;&#30340;&#26377;&#26102;&#26631;&#27880;&#65288;&#27969;&#31243;&#65289;&#25191;&#34892;&#25968;&#25454;&#25552;&#20379;&#35268;&#33539;&#24615;&#27969;&#31243;&#30417;&#25511;&#25216;&#26415;&#65292;&#22240;&#32570;&#20047;&#31934;&#24515;&#21046;&#20316;&#21644;&#20154;&#24037;&#39564;&#35777;&#30340;&#26126;&#30830;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#24335;&#20174;&#36807;&#21435;&#30340;&#25191;&#34892;&#35760;&#24405;&#20013;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#24182;&#25512;&#33616;&#20248;&#21270;&#24863;&#20852;&#36259;&#30340;KPI&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;&#39318;&#20808;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#38024;&#23545;&#29305;&#23450;KPI&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;RL&#35757;&#32451;&#26469;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#35813;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive Process Monitoring is a prominent problem in Process Mining, which consists in identifying a set of actions to be recommended with the goal of optimising a target measure of interest or Key Performance Indicator (KPI). One challenge that makes this problem difficult is the need to provide Prescriptive Process Monitoring techniques only based on temporally annotated (process) execution data, stored in, so-called execution logs, due to the lack of well crafted and human validated explicit models. In this paper we aim at proposing an AI based approach that learns, by means of Reinforcement Learning (RL), an optimal policy (almost) only from the observation of past executions and recommends the best activities to carry on for optimizing a KPI of interest. This is achieved first by learning a Markov Decision Process for the specific KPIs from data, and then by using RL training to learn the optimal policy. The approach is validated on real and synthetic datasets and compared wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25277;&#35937;&#35770;&#35777;&#20013;&#30340;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#26080;&#29615;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#27010;&#24565;&#37325;&#26032;&#32534;&#20889;&#25104;&#19968;&#20010;&#34892;&#21160;&#35821;&#35328;&#65292;&#24182;&#24314;&#31435;&#36215;&#35770;&#25454;&#38472;&#36848;&#21644;&#23427;&#20204;&#30452;&#25509;&#25110;&#38388;&#25509;&#21518;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09197</link><description>&lt;p&gt;
&#25277;&#35937;&#35770;&#35777;&#20013;&#30340;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporality and Causality in Abstract Argumentation. (arXiv:2303.09197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25277;&#35937;&#35770;&#35777;&#20013;&#30340;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#26080;&#29615;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#27010;&#24565;&#37325;&#26032;&#32534;&#20889;&#25104;&#19968;&#20010;&#34892;&#21160;&#35821;&#35328;&#65292;&#24182;&#24314;&#31435;&#36215;&#35770;&#25454;&#38472;&#36848;&#21644;&#23427;&#20204;&#30452;&#25509;&#25110;&#38388;&#25509;&#21518;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25277;&#35937;&#35770;&#35777;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32771;&#34385;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#22909;&#22788;&#65292;&#21363;&#35770;&#25454;&#34987;&#38472;&#36848;&#30340;&#39034;&#24207;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#26080;&#29615;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#27010;&#24565;&#37325;&#26032;&#32534;&#20889;&#25104;&#19968;&#20010;&#34892;&#21160;&#35821;&#35328;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#19990;&#30028;&#30340;&#28436;&#21270;&#65292;&#24182;&#24314;&#31435;&#35770;&#25454;&#38472;&#36848;&#21644;&#23427;&#20204;&#30340;&#30452;&#25509;&#25110;&#38388;&#25509;&#21518;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;Answer Set&#32534;&#31243;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of abstract argumentation, we present the benefits of considering temporality, i.e. the order in which arguments are enunciated, as well as causality. We propose a formal method to rewrite the concepts of acyclic abstract argumentation frameworks into an action language, that allows us to model the evolution of the world, and to establish causal relationships between the enunciation of arguments and their consequences, whether direct or indirect. An Answer Set Programming implementation is also proposed, as well as perspectives towards explanations.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;POMCP&#25191;&#34892;&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#36719;&#25919;&#31574;&#25351;&#23548;&#65292;&#20195;&#26367;&#25163;&#21160;&#23450;&#20041;&#30340;&#31574;&#30053;&#30456;&#20851;&#35268;&#21017;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;&#29615;&#22659;&#29366;&#24577;&#31354;&#38388;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09172</link><description>&lt;p&gt;
POMCP&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#20197;&#23454;&#29616;&#36719;&#25919;&#31574;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Specifications for Soft Policy Guidance in POMCP. (arXiv:2303.09172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09172
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;POMCP&#25191;&#34892;&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#36719;&#25919;&#31574;&#25351;&#23548;&#65292;&#20195;&#26367;&#25163;&#21160;&#23450;&#20041;&#30340;&#31574;&#30053;&#30456;&#20851;&#35268;&#21017;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;&#29615;&#22659;&#29366;&#24577;&#31354;&#38388;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#65288;POMCP&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#35299;&#20915;&#22120;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#31574;&#30053;&#65292;&#22312;&#26412;&#22320;&#21644;&#22312;&#32447;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#65292;&#20174;&#32780;&#20351;&#24471;&#35268;&#27169;&#19978;&#30340;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;POMCP&#22312;&#31232;&#30095;&#22870;&#21169;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#21363;&#20165;&#22312;&#36798;&#21040;&#26368;&#32456;&#30446;&#26631;&#26102;&#33719;&#24471;&#22870;&#21169;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#29615;&#22659;&#20013;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#23558;&#36923;&#36753;&#35268;&#33539;&#38598;&#25104;&#21040;POMCP&#20013;&#65292;&#20197;&#25351;&#23548;&#25506;&#32034;&#24182;&#28385;&#36275;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#19982;&#31574;&#30053;&#30456;&#20851;&#30340;&#35268;&#21017;&#38656;&#35201;&#30001;&#39046;&#22495;&#19987;&#23478;&#25163;&#21160;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#20174;POMCP&#25191;&#34892;&#30340;&#36319;&#36394;&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#65292;&#21363;&#30001;&#35268;&#21010;&#22120;&#29983;&#25104;&#30340;&#20449;&#24565;-&#34892;&#20026;&#23545;&#38598;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#33539;&#24335;&#34920;&#31034;&#30340;&#35268;&#21017;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;POMCP&#20013;&#21033;&#29992;&#23427;&#20197;&#23454;&#29616;&#36719;&#25351;&#23548;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for Partially Observable Markov Decision Processes (POMDPs). It allows scaling to large state spaces by computing an approximation of the optimal policy locally and online, using a Monte Carlo Tree Search based strategy. However, POMCP suffers from sparse reward function, namely, rewards achieved only when the final goal is reached, particularly in environments with large state spaces and long horizons. Recently, logic specifications have been integrated into POMCP to guide exploration and to satisfy safety requirements. However, such policy-related rules require manual definition by domain experts, especially in real-world scenarios. In this paper, we use inductive logic programming to learn logic specifications from traces of POMCP executions, i.e., sets of belief-action pairs generated by the planner. Specifically, we learn rules expressed in the paradigm of answer set programming. We then integrate them inside
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25152;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;Pearson&#30456;&#20851;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#29305;&#27530;&#25216;&#33021;&#22788;&#29702;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09167</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Emotional Reaction Intensity Estimation Based on Multimodal Data. (arXiv:2303.09167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25152;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;Pearson&#30456;&#20851;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#29305;&#27530;&#25216;&#33021;&#22788;&#29702;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#25105;&#20204;&#22312;CVPR 2023 ABAW&#20013;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#24037;&#20316;&#22346;&#21450;&#31454;&#36187;&#20013;&#25552;&#20986;&#30340;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#65288;ERI&#65289;&#20272;&#35745;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#38899;&#39057;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;Transformer&#32534;&#30721;&#22120;&#21644;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#28151;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#29305;&#27530;&#22788;&#29702;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces our method for the Emotional Reaction Intensity (ERI) Estimation Challenge, in CVPR 2023: 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Based on the multimodal data provided by the originazers, we extract acoustic and visual features with different pretrained models. The multimodal features are mixed together by Transformer Encoders with cross-modal attention mechnism. In this paper, 1. better features are extracted with the SOTA pretrained models. 2. Compared with the baseline, we improve the Pearson's Correlations Coefficient a lot. 3. We process the data with some special skills to enhance performance ability of our model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25968;&#23398;&#19978;&#28548;&#28165;&#20102;&#24102;&#26377;&#27010;&#24565;&#29942;&#39048;&#32467;&#26500;&#21644;&#22810;&#20219;&#21153;&#32452;&#25104;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#21644;&#33258;&#30001;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09154</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#24565;&#29942;&#39048;&#32467;&#26500;&#21644;&#22810;&#20219;&#21153;&#32452;&#25104;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation. (arXiv:2303.09154v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25968;&#23398;&#19978;&#28548;&#28165;&#20102;&#24102;&#26377;&#27010;&#24565;&#29942;&#39048;&#32467;&#26500;&#21644;&#22810;&#20219;&#21153;&#32452;&#25104;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#21644;&#33258;&#30001;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#27010;&#24565;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;CBM&#20013;&#65292;&#27010;&#24565;&#34987;&#25554;&#20837;&#21040;&#36755;&#20986;&#23618;&#21644;&#26368;&#21518;&#19968;&#20010;&#20013;&#38388;&#23618;&#20043;&#38388;&#20316;&#20026;&#21487;&#35266;&#23519;&#20540;&#12290;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36755;&#20986;&#30340;&#21407;&#22240;&#65306;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#21040;&#36755;&#20986;&#23618;&#30340;&#27010;&#24565;&#23545;&#24212;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#22312;CBM&#20013;&#29702;&#35299;&#27867;&#21270;&#35823;&#24046;&#34892;&#20026;&#23578;&#19981;&#21487;&#33021;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#22855;&#24322;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#24403;&#27169;&#22411;&#26159;&#22855;&#24322;&#30340;&#26102;&#65292;&#20174;&#21442;&#25968;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#19968;&#26144;&#23556;&#19981;&#33021;&#21019;&#24314;&#12290;&#36825;&#31181;&#19981;&#21487;&#35782;&#21035;&#24615;&#20351;&#24471;&#20998;&#26512;&#27867;&#21270;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#27425;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25968;&#23398;&#19978;&#28548;&#28165;&#20102;CBM&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#21644;&#33258;&#30001;&#33021;&#65292;&#24403;&#20854;&#26550;&#26500;&#26159;&#19977;&#23618;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#19981;&#20877;&#21482;&#26159;&#19968;&#20010;&#26631;&#31614;&#65292;&#32780;&#26159;&#19968;&#32452;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck model (CBM) is a ubiquitous method that can interpret neural networks using concepts. In CBM, concepts are inserted between the output layer and the last intermediate layer as observable values. This helps in understanding the reason behind the outputs generated by the neural networks: the weights corresponding to the concepts from the last hidden layer to the output layer. However, it has not yet been possible to understand the behavior of the generalization error in CBM since a neural network is a singular statistical model in general. When the model is singular, a one to one map from the parameters to probability distributions cannot be created. This non-identifiability makes it difficult to analyze the generalization performance. In this study, we mathematically clarify the Bayesian generalization error and free energy of CBM when its architecture is three-layered linear neural networks. We also consider a multitask problem where the neural network outputs not on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20256;&#32479;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.09124</link><description>&lt;p&gt;
&#32420;&#32500;&#26463;&#24418;&#29366;&#27979;&#37327;&#20449;&#24687;&#21487;&#29992;&#20110;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes. (arXiv:2303.09124v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20256;&#32479;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30333;&#36136;&#36830;&#25509;&#30340;&#31070;&#32463;&#24433;&#20687;&#23398;&#27979;&#37327;&#21487;&#20197;&#39044;&#27979;&#35832;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#35748;&#30693;&#27979;&#37327;&#31561;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#22312;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#38271;&#24230;&#12289;&#30452;&#24452;&#21644;&#20280;&#38271;&#27604;&#31561;&#19977;&#20010;&#22522;&#26412;&#24418;&#29366;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#20256;&#32479;&#22238;&#24402;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24494;&#32467;&#26500;&#12289;&#36830;&#25509;&#21644;&#24418;&#29366;&#27979;&#37327;&#30340;&#39640;&#25928;&#20004;&#38454;&#27573;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20026;&#20256;&#32479;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroimaging measures of the brain's white matter connections can enable the prediction of non-imaging phenotypes, such as demographic and cognitive measures. Existing works have investigated traditional microstructure and connectivity measures from diffusion MRI tractography, without considering the shape of the connections reconstructed by tractography. In this paper, we investigate the potential of fiber tract shape features for predicting non-imaging phenotypes, both individually and in combination with traditional features. We focus on three basic shape features: length, diameter, and elongation. Two different prediction methods are used, including a traditional regression method and a deep-learning-based prediction method. Experiments use an efficient two-stage fusion strategy for prediction using microstructure, connectivity, and shape measures. To reduce predictive bias due to brain size, normalized shape features are also investigated. Experimental results on the Human Connect
&lt;/p&gt;</description></item><item><title>SigVIC&#26159;&#19968;&#31181;&#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#25351;&#23548;&#29305;&#24449;&#32553;&#25918;&#21644;&#27604;&#29305;&#20998;&#37197;&#65292;&#36873;&#25321;Top-K&#27973;&#23618;&#29305;&#24449;&#26469;&#31934;&#32454;&#35843;&#25972;&#35299;&#30721;&#29305;&#24449;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09112</link><description>&lt;p&gt;
SigVIC: &#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
SigVIC: Spatial Importance Guided Variable-Rate Image Compression. (arXiv:2303.09112v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09112
&lt;/p&gt;
&lt;p&gt;
SigVIC&#26159;&#19968;&#31181;&#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#25351;&#23548;&#29305;&#24449;&#32553;&#25918;&#21644;&#27604;&#29305;&#20998;&#37197;&#65292;&#36873;&#25321;Top-K&#27973;&#23618;&#29305;&#24449;&#26469;&#31934;&#32454;&#35843;&#25972;&#35299;&#30721;&#29305;&#24449;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#27604;&#26426;&#21046;&#25552;&#39640;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#20026;&#19981;&#21516;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#21464;&#27604;&#29575;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#20043;&#19968;&#26159;&#25353;&#36890;&#36947;&#25110;&#31354;&#38388;&#22343;&#21248;&#32553;&#25918;&#20869;&#37096;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#31354;&#38388;&#37325;&#35201;&#24615;&#30340;&#22810;&#26679;&#24615;&#23545;&#20110;&#22270;&#20687;&#21387;&#32553;&#30340;&#27604;&#29305;&#20998;&#37197;&#26159;&#26377;&#25351;&#23548;&#24847;&#20041;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65288;SigVIC&#65289;&#65292;&#20854;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#31354;&#38388;&#38376;&#25511;&#21333;&#20803;&#65288;SGU&#65289;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#23398;&#20064;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#31354;&#38388;&#32553;&#25918;&#32593;&#32476;&#65288;SSN&#65289;&#20351;&#29992;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#26469;&#25351;&#23548;&#29305;&#24449;&#32553;&#25918;&#21644;&#21487;&#21464;&#27604;&#29575;&#30340;&#27604;&#29305;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#35299;&#30721;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#36873;&#25321;Top-K&#27973;&#23618;&#29305;&#24449;&#36890;&#36807;&#27973;&#23618;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65288;SFFM&#65289;&#26469;&#31934;&#32454;&#22320;&#35843;&#25972;&#35299;&#30721;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#26080;&#35770;&#26159;&#21464;&#27604;&#29575;&#36824;&#26159;&#38750;&#21464;&#27604;&#29575;&#65289;&#65292;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable-rate mechanism has improved the flexibility and efficiency of learning-based image compression that trains multiple models for different rate-distortion tradeoffs. One of the most common approaches for variable-rate is to channel-wisely or spatial-uniformly scale the internal features. However, the diversity of spatial importance is instructive for bit allocation of image compression. In this paper, we introduce a Spatial Importance Guided Variable-rate Image Compression (SigVIC), in which a spatial gating unit (SGU) is designed for adaptively learning a spatial importance mask. Then, a spatial scaling network (SSN) takes the spatial importance mask to guide the feature scaling and bit allocation for variable-rate. Moreover, to improve the quality of decoded image, Top-K shallow features are selected to refine the decoded features through a shallow feature fusion module (SFFM). Experiments show that our method outperforms other learning-based methods (whether variable-rate or 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09065</link><description>&lt;p&gt;
&#22522;&#20110;t-SPN&#21644;&#28388;&#27874;&#30340;&#32454;&#32990;&#20998;&#31867;&#30340;&#26368;&#22823;&#38388;&#38548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#27010;&#29575;&#20307;&#31995;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26641;&#24418;&#27714;&#21644;&#20135;&#21697;&#32593;&#32476;(t-SPN)&#65292;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#12290;&#26500;&#24314;t-SPN&#30340;&#30446;&#30340;&#26159;&#34920;&#31034;&#26410;&#24402;&#19968;&#21270;&#27010;&#29575;&#20316;&#20026;&#26368;&#30456;&#20284;&#30340;&#32454;&#32990;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#26469;&#23398;&#20064;&#26500;&#24314;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#36793;&#32536;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#26368;&#26377;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#12290;&#20026;&#20102;&#22686;&#24378;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;L2&#27491;&#21017;&#21270;&#65288;REG&#65289;&#21644;&#26368;&#22823;&#38388;&#38548;&#65288;MM&#65289;&#26631;&#20934;&#12290;&#20026;&#20102;&#31361;&#20986;&#32454;&#32990;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#30340;&#26377;&#25928;&#24615;&#65306;&#29702;&#24819;&#39640;&#36890;&#28388;&#27874;&#21644;&#25289;&#26222;&#25289;&#26031;&#28388;&#27874;(Log)&#12290;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#26368;&#22823;&#38388;&#38548;&#20934;&#21017;&#19982;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#22238;&#35793;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#30693;&#35782;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#29992;&#20110;&#20266;&#20195;&#30721;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32570;&#23569;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;-&#20266;&#20195;&#30721;&#24179;&#34892;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09062</link><description>&lt;p&gt;
&#26469;&#33258;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#20266;&#20195;&#30721;&#29983;&#25104;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language. (arXiv:2303.09062v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#22238;&#35793;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#30693;&#35782;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#29992;&#20110;&#20266;&#20195;&#30721;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32570;&#23569;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;-&#20266;&#20195;&#30721;&#24179;&#34892;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#36951;&#30041;&#28304;&#20195;&#30721;&#30340;&#20266;&#20195;&#30721;&#25551;&#36848;&#20197;&#23454;&#29616;&#36719;&#20214;&#32500;&#25252;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#33258;&#21160;&#21270;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;C++&#65289;&#30340;&#20266;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#26159;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20195;&#30721;-&#20266;&#20195;&#30721;&#35821;&#26009;&#24211;&#30340;&#21487;&#29992;&#24615;&#12290;&#38024;&#23545;&#22312;&#36951;&#30041;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#20013;&#32534;&#20889;&#20195;&#30721;&#30340;&#20266;&#20195;&#30721;&#27880;&#37322;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#28304;PL&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36890;&#36807;&#20351;&#29992;&#24179;&#34892;&#20195;&#30721;-&#20266;&#20195;&#30721;&#25968;&#25454;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#27169;&#22411;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#27809;&#26377;PL-&#20266;&#20195;&#30721;&#24179;&#34892;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#30340;&#36951;&#30041;PL&#65288;C&#65289;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#29992;&#20363;&#30340;&#36807;&#28388;&#31574;&#30053;&#30340;&#36845;&#20195;&#22238;&#35793;&#65288;IBT&#65289;&#26041;&#27861;&#65292;&#20197;&#23558;&#32463;&#36807;&#35757;&#32451;&#30340;C++-to-pseudocode&#27169;&#22411;&#35843;&#25972;&#20026;C-to-pseudocode&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of pseudo-code descriptions of legacy source code for software maintenance is a manually intensive task. Recent encoder-decoder language models have shown promise for automating pseudo-code generation for high resource programming languages such as C++, but are heavily reliant on the availability of a large code-pseudocode corpus. Soliciting such pseudocode annotations for codes written in legacy programming languages (PL) is a time consuming and costly affair requiring a thorough understanding of the source PL. In this paper, we focus on transferring the knowledge acquired by the code-to-pseudocode neural model trained on a high resource PL (C++) using parallel code-pseudocode data. We aim to transfer this knowledge to a legacy PL (C) with no PL-pseudocode parallel data for training. To achieve this, we utilize an Iterative Back Translation (IBT) approach with a novel test-cases based filtration strategy, to adapt the trained C++-to-pseudocode model to C-to-pseudocode model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20280;&#32553;&#20215;&#20540;&#20998;&#35299;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24322;&#27493;&#21487;&#20280;&#32553;&#35757;&#32451;&#26426;&#21046;&#12289;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#21644;&#25506;&#32034;&#24615;&#32463;&#39564;&#22238;&#25918;&#35299;&#20915;&#20102;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21644;&#32570;&#20047;&#20027;&#21160;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24615;&#33021;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.09058</link><description>&lt;p&gt;
SVDE: &#38754;&#21521;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20280;&#32553;&#20215;&#20540;&#20998;&#35299;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2303.09058v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20280;&#32553;&#20215;&#20540;&#20998;&#35299;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24322;&#27493;&#21487;&#20280;&#32553;&#35757;&#32451;&#26426;&#21046;&#12289;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#21644;&#25506;&#32034;&#24615;&#32463;&#39564;&#22238;&#25918;&#35299;&#20915;&#20102;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21644;&#32570;&#20047;&#20027;&#21160;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#36890;&#36807;&#23558;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20998;&#35299;&#20026;&#26412;&#22320;&#35266;&#23519;-&#21160;&#20316;&#31354;&#38388;&#26469;&#38477;&#20302;&#31995;&#32479;&#38590;&#24230;&#65292;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21644;&#32570;&#20047;&#20027;&#21160;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#20215;&#20540;&#20998;&#35299;&#25506;&#32034;&#65288;SVDE&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20280;&#32553;&#35757;&#32451;&#26426;&#21046;&#12289;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#21644;&#25506;&#32034;&#24615;&#32463;&#39564;&#22238;&#25918;&#12290;&#21487;&#20280;&#32553;&#35757;&#32451;&#26426;&#21046;&#24322;&#27493;&#22320;&#23558;&#31574;&#30053;&#23398;&#20064;&#19982;&#29615;&#22659;&#20132;&#20114;&#35299;&#32806;&#65292;&#20197;MapReduce&#30340;&#26041;&#24335;&#21152;&#36895;&#26679;&#26412;&#29983;&#25104;&#12290;&#38024;&#23545;&#32570;&#20047;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#21644;&#25506;&#32034;&#24615;&#32463;&#39564;&#22238;&#25918;&#65292;&#20197;&#22686;&#24378;&#25506;&#32034;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#21644;&#36807;&#28388;&#38750;&#26032;&#39062;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-decomposition methods, which reduce the difficulty of a multi-agent system by decomposing the joint state-action space into local observation-action spaces, have become popular in cooperative multi-agent reinforcement learning (MARL). However, value-decomposition methods still have the problems of tremendous sample consumption for training and lack of active exploration. In this paper, we propose a scalable value-decomposition exploration (SVDE) method, which includes a scalable training mechanism, intrinsic reward design, and explorative experience replay. The scalable training mechanism asynchronously decouples strategy learning with environmental interaction, so as to accelerate sample generation in a MapReduce manner. For the problem of lack of exploration, an intrinsic reward design and explorative experience replay are proposed, so as to enhance exploration to produce diverse samples and filter non-novel samples, respectively. Empirically, our method achieves the best perfo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#29983;&#25104;&#22810;&#32500;&#20998;&#23376;&#20171;&#36136;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#30340;&#22256;&#38590;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;&#30142;&#30149;&#36712;&#36857;&#21644;&#33647;&#29289;&#30740;&#21457;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#21644;&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09056</link><description>&lt;p&gt;
&#29983;&#25104;&#22810;&#32500;&#20998;&#23376;&#20171;&#36136;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29992;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30142;&#30149;&#36712;&#36857;&#39044;&#27979;&#21644;&#33647;&#29289;&#24320;&#21457;&#25968;&#23383;&#23402;&#29983;&#65306;&#32771;&#34385;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic multi-dimensional molecular-mediator time series data for artificial intelligence-based disease trajectory forecasting and drug development digital twins: Considerations. (arXiv:2303.09056v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09056
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#29983;&#25104;&#22810;&#32500;&#20998;&#23376;&#20171;&#36136;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#30340;&#22256;&#38590;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;&#30142;&#30149;&#36712;&#36857;&#21644;&#33647;&#29289;&#30740;&#21457;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#21644;&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#30340;&#20351;&#29992;&#34987;&#35748;&#20026;&#26159;&#31070;&#32463;&#32593;&#32476;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#34429;&#28982;&#29983;&#25104;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#20855;&#26377;&#20316;&#29992;&#65292;&#20027;&#35201;&#28041;&#21450;&#22270;&#20687;&#22788;&#29702;&#12290;&#20294;&#22312;&#38656;&#35201;&#20102;&#35299;&#31995;&#32479;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29983;&#25104;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#26368;&#20026;&#31361;&#20986;&#30340;&#26159;&#22312;&#29983;&#25104;&#22810;&#32500;&#20998;&#23376;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#39044;&#27979;&#21508;&#31181;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#35760;&#21644;&#20171;&#36136;&#29305;&#24449;&#30740;&#31350;&#30340;&#22522;&#30784;&#25968;&#25454;&#65292;&#24182;&#19988;&#26159;&#33647;&#29289;&#30740;&#21457;&#27969;&#31243;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#19981;&#36275;&#20197;&#21450;&#32479;&#35745;&#21644;&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#19981;&#36866;&#29992;&#24615;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of synthetic data is recognized as a crucial step in the development of neural network-based Artificial Intelligence (AI) systems. While the methods for generating synthetic data for AI applications in other domains have a role in certain biomedical AI systems, primarily related to image processing, there is a critical gap in the generation of time series data for AI tasks where it is necessary to know how the system works. This is most pronounced in the ability to generate synthetic multi-dimensional molecular time series data (SMMTSD); this is the type of data that underpins research into biomarkers and mediator signatures for forecasting various diseases and is an essential component of the drug development pipeline. We argue the insufficiency of statistical and data-centric machine learning (ML) means of generating this type of synthetic data is due to a combination of factors: perpetual data sparsity due to the Curse of Dimensionality, the inapplicability of the Central Li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;VoIP&#36890;&#20449;&#24179;&#21488;&#36827;&#34892;DNS&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#23558;&#22122;&#22768;&#25233;&#21046;&#21644;VoIP&#29305;&#26377;&#30340;&#22768;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#20248;&#20110;&#34892;&#19994;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09048</link><description>&lt;p&gt;
&#22312;VoIP&#24179;&#21488;&#19978;&#25552;&#39640;&#30693;&#35273;&#36136;&#37327;&#12289;&#21487;&#25026;&#24230;&#21644;&#22768;&#23398;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms. (arXiv:2303.09048v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;VoIP&#36890;&#20449;&#24179;&#21488;&#36827;&#34892;DNS&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#23558;&#22122;&#22768;&#25233;&#21046;&#21644;VoIP&#29305;&#26377;&#30340;&#22768;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#20248;&#20110;&#34892;&#19994;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;Deep Noise Suppression (DNS) 2020 Challenge&#27169;&#22411;&#22312;VoIP&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;DNS 2020&#27169;&#22411;&#36866;&#24212;&#20110;VoIP&#36890;&#20449;&#30340;&#29305;&#23450;&#22768;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;&#22240;&#21387;&#32553;&#12289;&#20256;&#36755;&#21644;&#24179;&#21488;&#29305;&#23450;&#22788;&#29702;&#32780;&#24341;&#36215;&#30340;&#22833;&#30495;&#21644;&#20266;&#24433;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#38899;&#22686;&#24378;&#30340;VoIP-DNS&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20849;&#21516;&#20248;&#21270;&#38477;&#22122;&#21644;VoIP&#29305;&#23450;&#30340;&#22768;&#23398;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;VoIP&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;VoIP&#24212;&#29992;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#21033;&#29992;VoIP-DNS&#33021;&#22815;&#25552;&#39640;&#21644;&#23450;&#21046;DNS-2020&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;VoIP&#24179;&#21488;&#19978;&#30340;&#28508;&#21147;&#65292;&#36825;&#39033;&#21457;&#29616;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21161;&#29702;&#31561;&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.09038</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Prompt Learning&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65306;&#32467;&#26524;&#12289;&#38480;&#21046;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20854;&#31867;&#20284;&#20154;&#31867;&#34920;&#36798;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20415;&#24739;&#32773;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#24471;&#21040;&#26356;&#22909;&#30340;&#21307;&#30103;&#25945;&#32946;&#12290;&#30740;&#31350;&#37319;&#38598;&#20102;62&#20221;&#20302;&#21058;&#37327;&#33016;&#37096;CT&#32954;&#30284;&#31579;&#26597;&#25195;&#25551;&#21644;76&#20221;&#33041;MRI&#36716;&#31227;&#24615;&#31579;&#26597;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#35780;&#20215;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;0.07&#22788;&#65292;&#20449;&#24687;&#38169;&#35823;0.11&#22788;&#12290;&#23601;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#32780;&#35328;&#65292;&#23427;&#20204;&#26159;&#19968;&#33324;&#24615;&#30340;&#30456;&#20851;&#24314;&#35758;&#65292;&#20363;&#22914;&#20445;&#25345;&#19982;&#21307;&#29983;&#30340;&#38543;&#35775;&#21644;&#23494;&#20999;&#30417;&#27979;&#20219;&#20309;&#30151;&#29366;&#65292;&#23545;&#20110;&#20849;138&#20010;&#30149;&#20363;&#20013;&#30340;&#32422;37&#65285;&#65292;ChatGPT&#25552;&#20379;&#20102;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#26377;&#20851;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20174;&#20687;&#32032;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.09031</link><description>&lt;p&gt;
&#19968;&#24133;&#22270;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#35821;&#35328;&#27169;&#22411;&#20174;&#20687;&#32032;&#20013;&#35268;&#21010;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
A Picture is Worth a Thousand Words: Language Models Plan from Pixels. (arXiv:2303.09031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20174;&#20687;&#32032;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25191;&#34892;&#23454;&#38469;&#29615;&#22659;&#20013;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#25512;&#29702;&#20986;&#35268;&#21010;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#20043;&#21069;&#36890;&#36807;PLM&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#23450;&#35266;&#23519;&#32467;&#26524;&#20197;&#25991;&#26412;&#24418;&#24335;&#21487;&#33719;&#24471;&#65288;&#20363;&#22914;&#30001;&#23383;&#24149;&#27169;&#22411;&#25552;&#20379;&#65289;&#65292;&#35201;&#20040;&#20165;&#20174;&#25351;&#20196;&#20013;&#29702;&#35299;&#35268;&#21010;&#65292;&#25110;&#32773;&#21482;&#26377;&#26377;&#38480;&#26041;&#24335;&#22320;&#25972;&#21512;&#20102;&#26377;&#20851;&#35270;&#35273;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#39044;&#35757;&#32451;&#30340;&#21487;&#20379;&#24615;&#20989;&#25968;&#65289;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#35266;&#23519;&#32467;&#26524;&#30452;&#25509;&#32534;&#30721;&#20026;PLM&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;PLM&#20063;&#33021;&#22815;&#20934;&#30830;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#26469;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#32452;&#32676;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#25968;&#25454;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09007</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Flow Cytometry Data Analysis. (arXiv:2303.09007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#26469;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#32452;&#32676;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#25968;&#25454;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#32454;&#32990;&#26415;&#20027;&#35201;&#29992;&#20110;&#26816;&#27979;&#32454;&#32990;&#20013;&#29305;&#23450;&#26631;&#35760;&#29289;&#30340;&#34920;&#36798;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26816;&#27979;&#33180;&#34920;&#38754;&#21463;&#20307;&#12289;&#25239;&#21407;&#12289;&#31163;&#23376;&#25110;DNA/RNA &#34920;&#36798;&#36807;&#31243;&#20013;&#12290;&#29616;&#20195;&#30340;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#22120;&#21487;&#20197;&#21516;&#26102;&#24555;&#36895;&#20998;&#26512;&#25968;&#20197;&#19975;&#35745;&#30340;&#32454;&#32990;&#65292;&#24182;&#20174;&#21333;&#20010;&#32454;&#32990;&#20013;&#27979;&#37327;&#22810;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#38590;&#20197;&#35299;&#37322;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#26469;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#32452;&#32676;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#25968;&#25454;&#22797;&#26434;&#20851;&#31995;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow cytometry mainly used for detecting the characteristics of a number of biochemical substances based on the expression of specific markers in cells. It is particularly useful for detecting membrane surface receptors, antigens, ions, or during DNA/RNA expression. Not only can it be employed as a biomedical research tool for recognising distinctive types of cells in mixed populations, but it can also be used as a diagnostic tool for classifying abnormal cell populations connected with disease. Modern flow cytometers can rapidly analyse tens of thousands of cells at the same time while also measuring multiple parameters from a single cell. However, the rapid development of flow cytometers makes it challenging for conventional analysis methods to interpret flow cytometry data. Researchers need to be able to distinguish interesting-looking cell populations manually in multi-dimensional data collected from millions of cells. Thus, it is essential to find a robust approach for analysing f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08983</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#65306;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08983
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#29992;&#25143;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#31574;&#30053;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;CNN&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#24102;&#26377;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33976;&#39311;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;ImageDataNet+&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20197;&#21450;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;CIFAR-100+&#65292;Flowers-102+&#21644;Food-101+&#12290;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12289;&#26356;&#26377;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20855;&#26377;&#24456;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;&#22312;ImageDataNet+&#19978;&#27979;&#37327;&#30340;Expected Calibration Error&#65288;ECE&#65289;&#20063;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;&#35270;&#35273;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#30456;&#23545;&#22352;&#26631;&#23545;&#20110;&#27010;&#24565;&#24418;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#21160;&#29289;&#35782;&#21035;&#29305;&#24449;&#26102;&#20063;&#24456;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.08969</link><description>&lt;p&gt;
&#30456;&#23545;&#22352;&#26631;&#23545;&#20110;&#20044;&#25289;&#22982;&#30340;"&#24605;&#32500;&#25216;&#24039;"&#33267;&#20851;&#37325;&#35201;(arXiv:2303.08969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Relative coordinates are crucial for Ulam's "trick to the train of thought". (arXiv:2303.08969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;&#35270;&#35273;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#30456;&#23545;&#22352;&#26631;&#23545;&#20110;&#27010;&#24565;&#24418;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#21160;&#29289;&#35782;&#21035;&#29305;&#24449;&#26102;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#39044;&#35774;&#30340;&#22352;&#26631;&#31995;&#26469;&#26631;&#35760;&#20687;&#32032;&#20301;&#32622;&#12290;&#36825;&#20123;&#22788;&#29702;&#31639;&#27861;&#22240;&#27492;&#36127;&#25285;&#30528;&#22806;&#37096;&#21442;&#32771;&#32593;&#26684;&#65292;&#20351;&#24471;&#33719;&#21462;&#30456;&#23545;&#20869;&#37096;&#29305;&#24449;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#19982;&#21160;&#29289;&#30340;&#35270;&#35273;&#21644;&#35748;&#30693;&#19981;&#21516;&#65306;&#21160;&#29289;&#21487;&#20197;&#35782;&#21035;&#27809;&#26377;&#22806;&#37096;&#22352;&#26631;&#31995;&#32479;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#19968;&#20010;&#29420;&#31435;&#20110;&#22352;&#26631;&#31995;&#30340;&#35270;&#35273;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#19981;&#20165;&#23545;&#20110;&#21160;&#29289;&#35270;&#35273;&#37325;&#35201;&#65292;&#32780;&#19988;&#20063;&#26159;&#27010;&#24565;&#24418;&#25104;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#24320;&#22987;&#23637;&#31034;&#20102;&#19968;&#20010;&#35270;&#35273;&#29289;&#20307;&#21464;&#24418;&#20256;&#36882;&#23454;&#39564;&#65292;&#25509;&#30528;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#30456;&#23545;&#22352;&#26631;&#23454;&#29616;&#21464;&#24418;&#19981;&#21464;&#24615;&#30340;&#31639;&#27861;&#12290;&#25991;&#31456;&#26368;&#21518;&#35752;&#35770;&#20102;&#36890;&#29992;&#27010;&#24565;&#24418;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial signal processing algorithms often use pre-given coordinate systems to label pixel positions. These processing algorithms are thus burdened by an external reference grid, making the acquisition of relative, intrinsic features difficult. This is in contrast to animal vision and cognition: animals recognize features without an external coordinate system. We show that a coordinate system-independent algorithm for visual signal processing is not only important for animal vision, but also fundamental for concept formation. In this paper we start with a visual object deformation transfer experiment. We then formulate an algorithm that achieves deformation-invariance with relative coordinates. The paper concludes with implications for general concept formation.
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#25968;&#25454;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;AI&#27835;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#30446;&#26631;&#35270;&#20026;&#31995;&#32479;&#20449;&#24687;&#27969;&#65292;&#24378;&#35843;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08956</link><description>&lt;p&gt;
&#25506;&#31350;&#25968;&#25454;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#22312;AI&#27835;&#29702;&#24212;&#29992;&#20013;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relevance of Data Privacy-Enhancing Technologies for AI Governance Use Cases. (arXiv:2303.08956v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08956
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#25968;&#25454;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;AI&#27835;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#30446;&#26631;&#35270;&#20026;&#31995;&#32479;&#20449;&#24687;&#27969;&#65292;&#24378;&#35843;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20132;&#25442;&#21644;&#20998;&#26512;&#20013;&#38544;&#31169;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#24050;&#32463;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25913;&#21892;&#12290;&#31867;&#20284;&#30340;&#36879;&#26126;&#24230;&#32467;&#26500;&#24037;&#20855;&#23545;&#20110;AI&#27835;&#29702;&#20063;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#22806;&#37096;&#23457;&#26597;&#12289;&#23457;&#35745;&#21644;&#28304;&#39564;&#35777;&#31561;&#33021;&#21147;&#12290;&#20026;&#20102;&#36991;&#20813;&#27835;&#29702;&#19978;&#30340;&#37325;&#22823;&#28431;&#27934;&#21644;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#23558;&#36825;&#20123;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#30446;&#26631;&#35270;&#20026;&#20449;&#24687;&#27969;&#31995;&#32479;&#12290;&#30001;&#20110;&#22312;&#26412;&#25991;&#25552;&#21040;&#30340;AI&#27835;&#29702;&#29992;&#20363;&#25152;&#38656;&#30340;&#36719;&#20214;&#26632;&#20013;&#21487;&#33021;&#23384;&#22312;&#37325;&#21472;&#65292;&#22240;&#27492;&#22312;&#25972;&#20307;&#19978;&#30475;&#24453;&#31995;&#32479;&#65292;&#20102;&#35299;&#36825;&#20123;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#26631;&#20934;&#12289;&#23457;&#35745;&#31243;&#24207;&#12289;&#36719;&#20214;&#21644;&#35268;&#33539;&#23450;&#22411;&#20043;&#21069;&#65292;&#39318;&#20808;&#32039;&#24613;&#38656;&#35201;&#23558;AI&#27835;&#29702;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#20316;&#20026;&#31995;&#32479;&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of privacy-enhancing technologies has made immense progress in reducing trade-offs between privacy and performance in data exchange and analysis. Similar tools for structured transparency could be useful for AI governance by offering capabilities such as external scrutiny, auditing, and source verification. It is useful to view these different AI governance objectives as a system of information flows in order to avoid partial solutions and significant gaps in governance, as there may be significant overlap in the software stacks needed for the AI governance use cases mentioned in this text. When viewing the system as a whole, the importance of interoperability between these different AI governance solutions becomes clear. Therefore, it is imminently important to look at these problems in AI governance as a system, before these standards, auditing procedures, software, and norms settle into place.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Answer Set Programming&#23454;&#29616;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#20154;&#31867;&#23545;&#35805;&#30340;AutoConcierge&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26377;&#20851;&#38468;&#36817;&#39184;&#21381;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08941</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20132;&#20114;&#24335;&#29305;&#23450;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#65292;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs. (arXiv:2303.08941v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Answer Set Programming&#23454;&#29616;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#20154;&#31867;&#23545;&#35805;&#30340;AutoConcierge&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26377;&#20851;&#38468;&#36817;&#39184;&#21381;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23454;&#29616;&#19982;&#20154;&#31867;&#30340;&#20154;&#31867;&#23545;&#35805;&#30456;&#20284;&#30340;&#36890;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#32463;&#20856;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20381;&#36182;&#20110;&#27169;&#24335;&#21305;&#37197;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#29702;&#35299;&#21477;&#23376;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#21709;&#24212;&#12290;&#35201;&#29983;&#25104;&#20445;&#35777;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#24517;&#39035;&#8220;&#29702;&#35299;&#8221;&#21477;&#23376;&#30340;&#35821;&#20041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#8220;&#29702;&#35299;&#8221;&#65292;&#38656;&#35201;&#22522;&#20110;&#36923;&#36753;&#30340;&#65288;&#24120;&#35782;&#65289;&#25512;&#29702;&#26041;&#27861;&#65292;&#20363;&#22914;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;AutoConcierge&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;LLMs&#21644;ASP&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#35805;&#20195;&#29702;&#65292;&#21487;&#20197;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#21463;&#38480;&#39046;&#22495;&#20869;&#30340;&#20154;&#31867;&#23545;&#35805;&#12290;AutoConcierge&#19987;&#27880;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#39046;&#22495;-&#26681;&#25454;&#29992;&#25143;&#30340;&#21916;&#22909;&#24314;&#35758;&#20182;&#20204;&#38468;&#36817;&#30340;&#39184;&#21381;&#12290;AutoConcierge&#23558;&#20132;&#20114;&#24335;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#35805;&#35821;&#65292;&#30830;&#23450;&#20854;&#20013;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-like communication with machines remains a classic, challenging topic in the field of Knowledge Representation and Reasoning and Natural Language Processing. These Large Language Models (LLMs) rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses. To generate an assuredly correct response, one has to "understand" the semantics of a sentence. To achieve this "understanding", logic-based (commonsense) reasoning methods such as Answer Set Programming (ASP) are arguably needed. In this paper, we describe the AutoConcierge system that leverages LLMs and ASP to develop a conversational agent that can truly "understand" human dialogs in restricted domains. AutoConcierge is focused on a specific domain-advising users about restaurants in their local area based on their preferences. AutoConcierge will interactively understand a user's utterances, identify the missing information in them
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#35843;&#26597;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#23545;&#29983;&#25104;AI&#30340;&#20851;&#27880;&#28857;&#21644;&#26399;&#26395;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#21442;&#19982;&#24335;AI&#26469;&#36171;&#20104;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#22312;&#19982;AI&#30340;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#21327;&#20316;&#20013;&#30340;&#33258;&#20027;&#26435;&#12290;</title><link>http://arxiv.org/abs/2303.08931</link><description>&lt;p&gt;
&#35774;&#35745;&#21442;&#19982;&#24335;AI&#65306;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#23545;&#29983;&#25104;AI&#30340;&#24551;&#34385;&#21644;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing Participatory AI: Creative Professionals' Worries and Expectations about Generative AI. (arXiv:2303.08931v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#35843;&#26597;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#23545;&#29983;&#25104;AI&#30340;&#20851;&#27880;&#28857;&#21644;&#26399;&#26395;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#21442;&#19982;&#24335;AI&#26469;&#36171;&#20104;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#22312;&#19982;AI&#30340;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#21327;&#20316;&#20013;&#30340;&#33258;&#20027;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35270;&#35273;&#25110;&#20070;&#38754;&#20869;&#23481;&#30340;&#29983;&#25104;AI&#24050;&#32463;&#22312;&#30701;&#30701;&#20960;&#24180;&#20869;&#32463;&#21382;&#20102;&#22797;&#26434;&#24615;&#30340;&#39134;&#36291;&#24182;&#24191;&#27867;&#26222;&#21450;&#12290;&#36825;&#20123;&#25216;&#26415;&#28508;&#22312;&#22320;&#23545;&#21019;&#24847;&#39046;&#22495;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#20914;&#20987;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23450;&#24615;&#35843;&#30740;&#65288;$N$ = 23&#65289;&#65292;&#35843;&#26597;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#22914;&#20309;&#24605;&#32771;&#29983;&#25104;AI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;AI&#27169;&#22411;&#30340;&#36827;&#27493;&#20419;&#20351;&#23545;&#23450;&#20041;&#21019;&#36896;&#21147;&#30340;&#37325;&#35201;&#21453;&#24605;&#65292;&#24182;&#25506;&#35752;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#22914;&#20309;&#24819;&#35937;&#20351;&#29992;AI&#26469;&#25903;&#25345;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22522;&#20110;&#36825;&#20123;&#24605;&#32771;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#35774;&#35745;&#22312;&#21019;&#24847;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;\textit{&#21442;&#19982;&#24335; AI}&#65292;&#20197;&#36171;&#20104;&#21019;&#24847;&#19987;&#19994;&#20154;&#22763;&#22312;&#20182;&#20204;&#19982;AI&#30340;&#29616;&#22312;&#21644;&#26410;&#26469;&#20849;&#23384;&#20013;&#30340;&#33258;&#20027;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI, i.e., the group of technologies that automatically generate visual or written content based on text prompts, has undergone a leap in complexity and become widely available within just a few years. Such technologies potentially introduce a massive disruption to creative fields. This paper presents the results of a qualitative survey ($N$ = 23) investigating how creative professionals think about generative AI. The results show that the advancement of these AI models prompts important reflections on what defines creativity and how creatives imagine using AI to support their workflows. Based on these reflections, we discuss how we might design \textit{participatory AI} in the domain of creative expertise with the goal of empowering creative professionals in their present and future coexistence with AI.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;EvalAttAI&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#24402;&#22240;&#26144;&#23556;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#30340;&#24615;&#33021;&#24182;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08866</link><description>&lt;p&gt;
EvalAttAI&#65306;&#19968;&#31181;&#32508;&#21512;&#35780;&#20272;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#20013;&#30340;&#24402;&#22240;&#26144;&#23556;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (arXiv:2303.08866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;EvalAttAI&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#24402;&#22240;&#26144;&#23556;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#30340;&#24615;&#33021;&#24182;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#25193;&#24352;&#65292;&#24050;&#32463;&#20135;&#29983;&#20102;&#35768;&#22810;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40657;&#30418;&#30340;&#26041;&#27861;&#12290;&#24402;&#22240;&#26144;&#23556;&#36890;&#24120;&#29992;&#20110;&#31361;&#20986;&#26174;&#31034;&#24433;&#21709;&#27169;&#22411;&#20570;&#20986;&#29305;&#23450;&#20915;&#31574;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#37096;&#20998;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#33258;&#28982;&#22122;&#22768;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#20063;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#28857;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#26356;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#25506;&#32034;&#22312;&#21307;&#23398;&#25104;&#20687;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#24050;&#32463;&#38519;&#20837;&#20102;&#20725;&#23616;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#24402;&#22240;&#26144;&#23556;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#20849;&#35782;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#24182;&#30830;&#23450;&#26368;&#22909;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;&#33258;&#28982;&#21644;&#21307;&#23398;&#25104;&#20687;&#65289;&#21644;&#21508;&#31181;&#24402;&#22240;&#26041;&#27861;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20004;&#31181;&#27969;&#34892;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21024;&#38500;&#21644;&#25554;&#20837;&#31283;&#20581;&#24615;&#65292;&#19981;&#36275;&#20197;&#35780;&#20272;&#40065;&#26834;&#27169;&#22411;&#20013;&#30340;&#24402;&#22240;&#26144;&#23556;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;EvalAttAI&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#32771;&#34385;&#24402;&#22240;&#26144;&#23556;&#30340;&#31283;&#20581;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;EvalAttAI&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#30340;&#24615;&#33021;&#65292;&#24182;&#36873;&#25321;&#36866;&#21512;&#20854;&#29305;&#23450;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion of explainable artificial intelligence as a field of research has generated numerous methods of visualizing and understanding the black box of a machine learning model. Attribution maps are generally used to highlight the parts of the input image that influence the model to make a specific decision. On the other hand, the robustness of machine learning models to natural noise and adversarial attacks is also being actively explored. This paper focuses on evaluating methods of attribution mapping to find whether robust neural networks are more explainable. We explore this problem within the application of classification for medical imaging. Explainability research is at an impasse. There are many methods of attribution mapping, but no current consensus on how to evaluate them and determine the ones that are the best. Our experiments on multiple datasets (natural and medical imaging) and various attribution methods reveal that two popular evaluation metrics, Deletion and Ins
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#35762;&#36848;&#20102;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#20256;&#24863;&#25968;&#25454;&#20013;&#24322;&#24120;&#27169;&#24335;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.08823</link><description>&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Wireless Sensor Networks anomaly detection using Machine Learning: A Survey. (arXiv:2303.08823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#35762;&#36848;&#20102;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#20256;&#24863;&#25968;&#25454;&#20013;&#24322;&#24120;&#27169;&#24335;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#20844;&#27665;/&#20891;&#20107;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#65292;&#22914;&#24037;&#19994;&#36807;&#31243;&#25511;&#21046;&#12289;&#24314;&#31569;&#32467;&#26500;&#24378;&#24230;&#30417;&#27979;&#12289;&#29615;&#22659;&#30417;&#27979;&#12289;&#36793;&#22659;&#20837;&#20405;&#12289;&#29289;&#32852;&#32593;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#12290;&#28982;&#32780;&#65292;WSN&#20135;&#29983;&#30340;&#20256;&#24863;&#25968;&#25454;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35786;&#26029;&#24322;&#24120;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#35782;&#21035;&#20256;&#24863;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#27010;&#36848;&#20102;ML&#25216;&#26415;&#22312;WSN&#39046;&#22495;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;WSN&#30340;&#29305;&#24449;&#21644;WSN&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#21508;&#31181;&#24212;&#29992;&#20110;WSN&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#30417;&#30563;&#65292;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31561;ML&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;ML&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless Sensor Networks (WSNs) have become increasingly valuable in various civil/military applications like industrial process control, civil engineering applications such as buildings structural strength monitoring, environmental monitoring, border intrusion, IoT (Internet of Things), and healthcare. However, the sensed data generated by WSNs is often noisy and unreliable, making it a challenge to detect and diagnose anomalies. Machine learning (ML) techniques have been widely used to address this problem by detecting and identifying unusual patterns in the sensed data. This survey paper provides an overview of the state of the art applications of ML techniques for data anomaly detection in WSN domains. We first introduce the characteristics of WSNs and the challenges of anomaly detection in WSNs. Then, we review various ML techniques such as supervised, unsupervised, and semi-supervised learning that have been applied to WSN data anomaly detection. We also compare different ML-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08806</link><description>&lt;p&gt;
&#29702;&#35299;&#20107;&#21518;&#35299;&#37322;&#22120;&#65306;&#20197;Anchors&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#39640;&#24230;&#35201;&#27714;&#20294;&#38590;&#20197;&#23454;&#29616;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#20010;&#20307;&#39044;&#27979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26412;&#22320;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#35299;&#37322;&#30340;&#36807;&#31243;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#21487;&#33021;&#19982;&#35201;&#35299;&#37322;&#30340;&#39044;&#27979;&#19968;&#26679;&#31070;&#31192;&#12290;&#27492;&#22806;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#32463;&#24120;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#31616;&#21333;&#27169;&#22411;&#19978;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#23545;Anchors&#65288;Ribeiro&#31561;&#20154;&#65292;2018&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#24378;&#35843;&#19968;&#23567;&#32452;&#21333;&#35789;&#20197;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.08335</link><description>&lt;p&gt;
FactReranker&#65306;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#29992;&#20110;&#24544;&#23454;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08335
&lt;/p&gt;
&lt;p&gt;
FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20445;&#25345;&#25152;&#20135;&#29983;&#30340;&#25688;&#35201;&#21644;&#22320;&#38754;&#23454;&#20917;&#25918;&#23556;&#23398;&#21457;&#29616;&#20043;&#38388;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#30452;&#25509;&#20248;&#21270;&#27491;&#30830;&#35748;&#30693;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;CheXBert&#25110;RadGraph&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#25110;&#26463;&#25628;&#32034;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#39033;&#26102;&#27809;&#26377;&#32771;&#34385;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23454;&#38469;&#19968;&#33268;&#24615;&#30340;&#25913;&#21892;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31532;&#20108;&#38454;&#27573;&#25688;&#35201;&#26041;&#27861;FactReranker&#65292;&#23427;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22522;&#20110;&#23427;&#20204;&#20272;&#35745;&#30340;&#23454;&#38469;&#19968;&#33268;&#24615;&#24471;&#20998;&#26469;&#23398;&#20064;&#20174;&#25152;&#26377;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;RadGraph&#27169;&#24335;&#25552;&#21462;&#36755;&#20837;&#21307;&#30103;&#25253;&#21578;&#12289;&#20854;&#40644;&#37329;&#25688;&#35201;&#21644;&#20505;&#36873;&#25688;&#35201;&#30340;&#21307;&#30103;&#20107;&#23454;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#25552;&#21462;&#30340;&#21307;&#30103;&#20107;&#23454;&#26469;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#20998;&#35299;&#20102;&#20107;&#23454;-
&lt;/p&gt;
&lt;p&gt;
Automatic radiology report summarization is a crucial clinical task, whose key challenge is to maintain factual accuracy between produced summaries and ground truth radiology findings. Existing research adopts reinforcement learning to directly optimize factual consistency metrics such as CheXBert or RadGraph score. However, their decoding method using greedy search or beam search considers no factual consistency when picking the optimal candidate, leading to limited factual consistency improvement. To address it, we propose a novel second-stage summarizing approach FactReranker, the first attempt that learns to choose the best summary from all candidates based on their estimated factual consistency score. We propose to extract medical facts of the input medical report, its gold summary, and candidate summaries based on the RadGraph schema and design the fact-guided reranker to efficiently incorporate the extracted medical facts for selecting the optimal summary. We decompose the fact-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#22495;TSG&#35774;&#32622;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#30721;&#21387;&#32553;&#20301;&#27969;&#26469;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21644;&#25552;&#39640;&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07863</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#21387;&#32553;&#35270;&#39057;&#30340;&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos. (arXiv:2303.07863v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#22495;TSG&#35774;&#32622;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#30721;&#21387;&#32553;&#20301;&#27969;&#26469;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21644;&#25552;&#39640;&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21477;&#23376;&#23545;&#40784;&#26088;&#22312;&#26681;&#25454;&#21477;&#23376;&#26597;&#35810;&#36890;&#36807;&#35821;&#20041;&#23450;&#20301;&#30446;&#26631;&#30636;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#22495;TSG&#65288;Temporal Sentence Grounding&#65289;&#35774;&#32622;&#65292;&#30452;&#25509;&#20351;&#29992;&#21387;&#32553;&#35270;&#39057;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#12290;&#38024;&#23545;&#21407;&#22987;&#35270;&#39057;&#27604;&#29305;&#27969;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#19977;&#25903;&#36335;&#21387;&#32553;&#31354;&#38388;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65288;TCSF&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23450;&#20301;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21387;&#32553;&#20266;&#24433;&#26469;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#32534;&#30721;&#21387;&#32553;&#20301;&#27969;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20808;&#35299;&#30721;&#25972;&#20010;&#24103;&#30340;&#26041;&#27861;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a target moment semantically according to a sentence query. Although previous respectable works have made decent success, they only focus on high-level visual features extracted from the consecutive decoded frames and fail to handle the compressed videos for query modelling, suffering from insufficient representation capability and significant computational complexity during training and testing. In this paper, we pose a new setting, compressed-domain TSG, which directly utilizes compressed videos rather than fully-decompressed frames as the visual input. To handle the raw video bit-stream input, we propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) framework, which extracts and aggregates three kinds of low-level visual features (I-frame, motion vector and residual features) for effective and efficient grounding. Particularly, instead of encoding the whole decoded frames like previous
&lt;/p&gt;</description></item><item><title>TriDet&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#21449;&#22836;&#27169;&#22411;&#21644;SGP&#29305;&#24449;&#37329;&#23383;&#22612;&#26469;&#25552;&#39640;&#26102;&#38388;&#34892;&#20026;&#26816;&#27979;&#30340;&#36793;&#30028;&#39044;&#27979;&#21644;&#32858;&#21512;&#22810;&#31181;&#26102;&#38388;&#23610;&#24230;&#20449;&#24687;&#12290;&#22312;&#19977;&#20010;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;TriDet&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07347</link><description>&lt;p&gt;
TriDet: &#20351;&#29992;&#30456;&#23545;&#36793;&#30028;&#24314;&#27169;&#30340;&#26102;&#38388;&#34892;&#20026;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TriDet: Temporal Action Detection with Relative Boundary Modeling. (arXiv:2303.07347v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07347
&lt;/p&gt;
&lt;p&gt;
TriDet&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#21449;&#22836;&#27169;&#22411;&#21644;SGP&#29305;&#24449;&#37329;&#23383;&#22612;&#26469;&#25552;&#39640;&#26102;&#38388;&#34892;&#20026;&#26816;&#27979;&#30340;&#36793;&#30028;&#39044;&#27979;&#21644;&#32858;&#21512;&#22810;&#31181;&#26102;&#38388;&#23610;&#24230;&#20449;&#24687;&#12290;&#22312;&#19977;&#20010;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;TriDet&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#38454;&#27573;&#26694;&#26550;TriDet&#65292;&#29992;&#20110;&#26102;&#38388;&#34892;&#20026;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#30001;&#20110;&#35270;&#39057;&#20013;&#27169;&#31946;&#30340;&#21160;&#20316;&#36793;&#30028;&#32780;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#36793;&#30028;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#21449;&#22836;&#65288;Trident-head&#65289;&#26469;&#36890;&#36807;&#30456;&#23545;&#27010;&#29575;&#20998;&#24067;&#26469;&#24314;&#27169;&#21160;&#20316;&#36793;&#30028;&#12290;&#22312;TriDet&#30340;&#29305;&#24449;&#37329;&#23383;&#22612;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#31890;&#24230;&#24863;&#30693;&#65288;SGP&#65289;&#23618;&#26469;&#32531;&#35299;&#33258;&#27880;&#24847;&#21147;&#22312;&#35270;&#39057;&#29305;&#24449;&#20013;&#21457;&#29983;&#30340;&#25490;&#21517;&#25439;&#22833;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#31890;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#12290;&#30001;&#20110;&#19977;&#21449;&#22836;&#21644;&#22522;&#20110;SGP&#30340;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20248;&#21183;&#65292;TriDet&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;THUMOS14&#12289;HACS&#21644;EPIC-KITCHEN 100&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;&#20363;&#22914;&#65292;&#22312;THUMOS14&#19978;&#65292;TriDet&#30340;&#24179;&#22343;mAP&#36798;&#21040;$69.3\%$&#65292;&#36229;&#36807;&#20808;&#21069;&#26368;&#20248;&#32467;&#26524;$2.5\%$&#65292;&#20294;&#21482;&#26377;$74.6\%$&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a one-stage framework TriDet for temporal action detection. Existing methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer to mitigate the rank loss problem of self-attention that takes place in the video features and aggregate information across different temporal granularities. Benefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. For example, TriDet hits an average mAP of $69.3\%$ on THUMOS14, outperforming the previous best by $2.5\%$, but with only $74.6\%$ o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>ODIN&#37319;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#29983;&#25104;&#25353;&#38656;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#32422;&#26463;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23398;&#20064;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.06832</link><description>&lt;p&gt;
ODIN&#65306;&#24212;&#23545;&#25968;&#25454;&#38145;&#23450;&#30340;&#25353;&#38656;&#25968;&#25454;&#21046;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in. (arXiv:2303.06832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06832
&lt;/p&gt;
&lt;p&gt;
ODIN&#37319;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#29983;&#25104;&#25353;&#38656;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#32422;&#26463;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23398;&#20064;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#32422;&#26463;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#21463;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#36739;&#22823;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;ODIN&#35797;&#22270;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#29983;&#25104;&#25353;&#38656;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#25968;&#25454;&#38598;&#32422;&#26463;&#12290;ODIN&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#25552;&#31034;&#29983;&#25104;&#22120;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#21518;&#22788;&#29702;&#22120;&#12290;&#20026;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#21644;&#22270;&#20687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;Stable Diffusion&#65289;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;ODIN&#36827;&#34892;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#35780;&#20272;&#20197;&#23637;&#31034;&#20854;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#21518;&#22788;&#29702;&#23454;&#39564;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;ODIN&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23398;&#20064;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ODIN is an innovative approach that addresses the problem of dataset constraints by integrating generative AI models. Traditional zero-shot learning methods are constrained by the training dataset. To fundamentally overcome this limitation, ODIN attempts to mitigate the dataset constraints by generating on-demand datasets based on user requirements. ODIN consists of three main modules: a prompt generator, a text-to-image generator, and an image post-processor. To generate high-quality prompts and images, we adopted a large language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g., Stable Diffusion), respectively. We evaluated ODIN on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation. Overall, ODIN is a feasible approach that enables Al to learn unseen knowledge beyond the training dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.03770</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation. (arXiv:2303.03770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#20551;&#23450;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21516;&#26102;&#21487;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;UDA&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;SF-UDA&#35774;&#32622;&#65292;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#23545;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#20998;&#31867;&#25439;&#22833;&#22522;&#20110;&#20272;&#35745;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#37325;&#26032;&#21152;&#26435;&#65292;&#20197;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#38598;&#30456;&#37051;&#26679;&#26412;&#30340;&#30693;&#35782;&#26469;&#36880;&#27493;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#26469;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#26679;&#26412;&#23545;&#25490;&#38500;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#38500;&#30001;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#26679;&#26412;&#26500;&#25104;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples shari
&lt;/p&gt;</description></item><item><title>BIFRNet&#26159;&#19968;&#31181;&#33041;&#21551;&#21457;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#29305;&#27530;&#30340;&#32972;&#20391;&#35270;&#35273;&#36890;&#36335;&#26469;&#35782;&#21035;&#37096;&#20998;&#36974;&#25377;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#29305;&#24449;&#37325;&#24314;&#26469;&#22635;&#34917;&#34987;&#36974;&#25377;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.01309</link><description>&lt;p&gt;
BIFRNet: &#19968;&#31181;&#33041;&#21551;&#21457;&#30340;&#29305;&#24449;&#37325;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#37096;&#20998;&#36974;&#25377;&#22270;&#20687;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
BIFRNet: A Brain-Inspired Feature Restoration DNN for Partially Occluded Image Recognition. (arXiv:2303.01309v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01309
&lt;/p&gt;
&lt;p&gt;
BIFRNet&#26159;&#19968;&#31181;&#33041;&#21551;&#21457;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#29305;&#27530;&#30340;&#32972;&#20391;&#35270;&#35273;&#36890;&#36335;&#26469;&#35782;&#21035;&#37096;&#20998;&#36974;&#25377;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#29305;&#24449;&#37325;&#24314;&#26469;&#22635;&#34917;&#34987;&#36974;&#25377;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#36974;&#25377;&#22270;&#20687;&#35782;&#21035;&#65288;POIR&#65289;&#19968;&#30452;&#20197;&#26469;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38590;&#39064;&#12290;&#35299;&#20915;POIR&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#31574;&#30053;&#26159;&#20351;&#29992;&#26410;&#36974;&#25377;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#22270;&#20687;&#20005;&#37325;&#36974;&#25377;&#26102;&#65292;&#35813;&#31574;&#30053;&#23558;&#22833;&#21435;&#25928;&#26524;&#65292;&#22240;&#20026;&#21487;&#35265;&#37096;&#20998;&#21482;&#33021;&#25552;&#20379;&#26377;&#38480;&#30340;&#20449;&#24687;&#12290;&#31070;&#32463;&#31185;&#23398;&#30340;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#35201;&#35782;&#21035;&#37096;&#20998;&#36974;&#25377;&#30340;&#22270;&#20687;&#65292;&#29305;&#24449;&#37325;&#24314;&#65288;&#22635;&#34917;&#34987;&#36974;&#25377;&#30340;&#20449;&#24687;&#65289;&#38750;&#24120;&#37325;&#35201;&#65292;&#34987;&#31216;&#20026;&#26080;&#27169;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;CNN&#24120;&#24120;&#24573;&#30053;&#29305;&#24449;&#37325;&#24314;&#65292;&#36825;&#21487;&#33021;&#26159;CNN&#23545;POIR&#38382;&#39064;&#26080;&#25928;&#30340;&#21407;&#22240;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33041;&#21551;&#21457;&#29305;&#24449;&#37325;&#24314;&#32593;&#32476;&#65288;BIFRNet&#65289;&#26469;&#35299;&#20915;POIR&#38382;&#39064;&#12290;&#23427;&#27169;&#20223;&#20102;&#33145;&#20391;&#35270;&#35273;&#36890;&#36335;&#26469;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#21644;&#32972;&#20391;&#35270;&#35273;&#36890;&#36335;&#26469;&#21306;&#20998;&#34987;&#36974;&#25377;&#21644;&#21487;&#35265;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partially occluded image recognition (POIR) problem has been a challenge for artificial intelligence for a long time. A common strategy to handle the POIR problem is using the non-occluded features for classification. Unfortunately, this strategy will lose effectiveness when the image is severely occluded, since the visible parts can only provide limited information. Several studies in neuroscience reveal that feature restoration which fills in the occluded information and is called amodal completion is essential for human brains to recognize partially occluded images. However, feature restoration is commonly ignored by CNNs, which may be the reason why CNNs are ineffective for the POIR problem. Inspired by this, we propose a novel brain-inspired feature restoration network (BIFRNet) to solve the POIR problem. It mimics a ventral visual pathway to extract image features and a dorsal visual pathway to distinguish occluded and visible image regions. In addition, it also uses a knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00085</link><description>&lt;p&gt;
AR3n: &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AR3n&#65288;&#21457;&#38899;&#20026;Aaron&#65289;&#65292;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#65292;&#21487;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#20070;&#20889;&#24247;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#36866;&#24212;&#24615;&#36741;&#21161;&#12290;&#19982;&#20197;&#24448;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24739;&#32773;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#25110;&#29289;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#26469;&#20351;AR3n&#25512;&#24191;&#21040;&#22810;&#20010;&#21463;&#35797;&#32773;&#12290;&#35813;&#31995;&#32479;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#22522;&#20110;&#34987;&#35797;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#36890;&#36807;&#19968;&#32452;&#20223;&#30495;&#23454;&#39564;&#21644;&#20154;&#20307;&#21463;&#35797;&#23454;&#39564;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#25511;&#21046;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#20004;&#31181;&#25511;&#21046;&#22120;&#30340;&#36741;&#21161;&#26426;&#21046;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.14368</link><description>&lt;p&gt;
&#23454;&#29616;&#25193;&#23637;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Controllability of Diffusion Models. (arXiv:2302.14368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#21487;&#25511;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#21463;&#22522;&#20110;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#25805;&#32437;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#20110;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12289;&#19968;&#20010;&#31354;&#38388;&#20869;&#23481;&#25513;&#30721;&#21644;&#19968;&#20010;&#25153;&#24179;&#30340;&#26679;&#24335;&#23884;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#25193;&#25955;&#27169;&#22411;&#28176;&#36827;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#24615;&#20559;&#32622;&#65292;&#22312;&#31354;&#38388;&#32467;&#26500;&#25513;&#30721;&#20013;&#32534;&#30721;&#23039;&#21183;/&#24067;&#23616;&#20449;&#24687;&#65292;&#22312;&#26679;&#24335;&#20195;&#30721;&#20013;&#32534;&#30721;&#35821;&#20041;/&#26679;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20801;&#35768;&#37096;&#20998;&#20381;&#36182;&#20110;&#26465;&#20214;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#23545;&#27599;&#20010;&#28508;&#22312;&#20195;&#30721;&#21644;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#20869;&#23481;&#21644;&#26679;&#24335;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25511;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#24182;&#25506;&#31350;&#19981;&#21516;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21487;&#20197;&#36827;&#34892;&#22240;&#23376;&#20998;&#35299;&#65292;&#32780;&#24102;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21017;&#38656;&#35201;&#29305;&#21035;&#32771;&#34385;&#19981;&#21516;&#30340;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2302.14146</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#19982;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#20013;&#30340;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Markov Conditions and Factorization in Logical Credal Networks. (arXiv:2302.14146v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#24182;&#25506;&#31350;&#19981;&#21516;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21487;&#20197;&#36827;&#34892;&#22240;&#23376;&#20998;&#35299;&#65292;&#32780;&#24102;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21017;&#38656;&#35201;&#29305;&#21035;&#32771;&#34385;&#19981;&#21516;&#30340;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#35843;&#26597;&#19981;&#21516;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#30340;&#32467;&#26500;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#27809;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32467;&#26500;&#23558;&#23548;&#33268;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#22240;&#23376;&#20998;&#35299;&#32467;&#26524;&#12290;&#23545;&#20110;&#24102;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#12289;&#22240;&#23376;&#20998;&#35299;&#32467;&#26524;&#21644;&#35268;&#33539;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the recently proposed language of Logical Credal Networks, in particular investigating the consequences of various Markov conditions. We introduce the notion of structure for a Logical Credal Network and show that a structure without directed cycles leads to a well-known factorization result. For networks with directed cycles, we analyze the differences between Markov conditions, factorization results, and specification requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11137</link><description>&lt;p&gt;
Fairguard: &#22312;&#26234;&#24935;&#22478;&#24066;&#20013;&#21033;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#20844;&#27491;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#36816;&#34892;&#22312;&#35745;&#31639;&#39044;&#27979;&#26694;&#26550;&#19978;&#65292;&#25910;&#38598;&#12289;&#25972;&#21512;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#30740;&#31350;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#30340;&#30495;&#23454;&#22478;&#24066;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20559;&#35265;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Fairguard&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#24494;&#35266;&#23618;&#38754;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20844;&#27491;&#30340;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#12290;Fairguard&#26694;&#26550;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38745;&#24577;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#36873;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#26465;&#20214;&#26469;&#20943;&#23569;&#25968;&#25454;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#30830;&#20445;&#39044;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#32452;&#20214;&#26469;&#35843;&#33410;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#29983;&#25104;&#26410;&#26469;&#30340;&#20844;&#27491;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#35273;&#24773;&#24863;&#35299;&#35835;&#20219;&#21153;(VEIT)&#65292;&#26088;&#22312;&#36890;&#36807;AI&#23545;&#21019;&#20316;&#32773;&#30340;&#24515;&#29702;&#29366;&#24577;&#36827;&#34892;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24471;&#21040;&#20102;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#19987;&#19994;&#30340;&#27880;&#37322;&#12290;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;VEIT&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#23383;&#24149;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10276</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#21019;&#20316;&#35299;&#35835;&#24515;&#29702;&#29366;&#24577;&#65306;&#12298;&#30475;&#35265;&#20320;&#30340;&#20869;&#24515;&#12299;
&lt;/p&gt;
&lt;p&gt;
See Your Heart: Psychological states Interpretation through Visual Creations. (arXiv:2302.10276v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#35273;&#24773;&#24863;&#35299;&#35835;&#20219;&#21153;(VEIT)&#65292;&#26088;&#22312;&#36890;&#36807;AI&#23545;&#21019;&#20316;&#32773;&#30340;&#24515;&#29702;&#29366;&#24577;&#36827;&#34892;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24471;&#21040;&#20102;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#19987;&#19994;&#30340;&#27880;&#37322;&#12290;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;VEIT&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#23383;&#24149;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#31070;&#20998;&#26512;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#21019;&#20316;&#29983;&#25104;&#23545;&#20010;&#20307;&#24515;&#29702;&#29366;&#24577;&#30340;&#35299;&#37322;&#27491;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#20027;&#35201;&#21253;&#25324;&#24773;&#32490;&#20998;&#31867;&#21644;&#24433;&#21709;&#26631;&#27880;&#20004;&#20010;&#20219;&#21153;&#65292;&#38590;&#20197;&#28385;&#36275;&#24515;&#29702;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#28385;&#36275;&#31934;&#31070;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#35273;&#24773;&#24863;&#35299;&#35835;&#20219;&#21153;(VEIT)&#12290;VEIT&#35201;&#27714;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#35270;&#35273;&#21019;&#20316;&#29983;&#25104;&#21512;&#29702;&#30340;&#21019;&#20316;&#32773;&#24515;&#29702;&#29366;&#24577;&#30340;&#35299;&#37322;&#12290;&#20026;&#25903;&#25345;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;Sandplay Interpretation Dataset (SpyIn)&#65292;&#35813;&#25968;&#25454;&#38598;&#24471;&#21040;&#20102;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#19987;&#19994;&#30340;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#20998;&#26512;&#34920;&#26126;&#65292;SpyIn&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;VEIT&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#23383;&#24149;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22522;&#20110;SpyIn&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#39033;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In psychoanalysis, generating interpretations to one's psychological state through visual creations is facing significant demands. The two main tasks of existing studies in the field of computer vision, sentiment/emotion classification and affective captioning, can hardly satisfy the requirement of psychological interpreting. To meet the demands for psychoanalysis, we introduce a challenging task, \textbf{V}isual \textbf{E}motion \textbf{I}nterpretation \textbf{T}ask (VEIT). VEIT requires AI to generate reasonable interpretations of creator's psychological state through visual creations. To support the task, we present a multimodal dataset termed SpyIn (\textbf{S}and\textbf{p}la\textbf{y} \textbf{In}terpretation Dataset), which is psychological theory supported and professional annotated. Dataset analysis illustrates that SpyIn is not only able to support VEIT, but also more challenging compared with other captioning datasets. Building on SpyIn, we conduct experiments of several image 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20132;&#20114;&#24335;&#24352;&#37327;&#20998;&#35299;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;&#30142;&#30149;&#21644;&#22522;&#22240;&#20026;&#20013;&#24515;&#30340;&#29983;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#30142;&#30149;&#22522;&#22240;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411; KDGene&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KDGene&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#23545;&#20110;&#31958;&#23615;&#30149;&#26696;&#20363;&#30340;&#29983;&#29289;&#23398;&#20998;&#26512;&#20063;&#35777;&#23454;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09335</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#30142;&#30149;&#22522;&#22240;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion based on Tensor Decomposition for Disease Gene Prediction. (arXiv:2302.09335v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20132;&#20114;&#24335;&#24352;&#37327;&#20998;&#35299;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;&#30142;&#30149;&#21644;&#22522;&#22240;&#20026;&#20013;&#24515;&#30340;&#29983;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#30142;&#30149;&#22522;&#22240;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411; KDGene&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KDGene&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#23545;&#20110;&#31958;&#23615;&#30149;&#26696;&#20363;&#30340;&#29983;&#29289;&#23398;&#20998;&#26512;&#20063;&#35777;&#23454;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#37492;&#23450;&#30142;&#30149;&#22522;&#22240;&#19968;&#30452;&#26159;&#35299;&#30721;&#30142;&#30149;&#20998;&#23376;&#26426;&#21046;&#30340;&#20851;&#38190;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#22312;&#26500;&#24314;&#29983;&#29289;&#32593;&#32476;&#21644;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#26469;&#37492;&#23450;&#30142;&#30149;&#22522;&#22240;&#65292;&#20294;&#24573;&#30053;&#20102;&#29983;&#29289;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;&#30142;&#30149;&#21644;&#22522;&#22240;&#20026;&#20013;&#24515;&#30340;&#29983;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#21033;&#29992;&#20132;&#20114;&#24335;&#24352;&#37327;&#20998;&#35299; (KDGene) &#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#30142;&#30149;&#22522;&#22240;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411;&#12290;KDGene&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#24341;&#20837;&#24352;&#37327;&#20998;&#35299;&#30340;&#20132;&#20114;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;&#29983;&#29289;&#30693;&#35782;&#20013;&#30340;&#20449;&#24687;&#20132;&#20114;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KDGene&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#31958;&#23615;&#30149;&#26696;&#20363;&#30340;&#32508;&#21512;&#29983;&#29289;&#23398;&#20998;&#26512;&#35777;&#23454;&#20102; KDGene &#37492;&#23450;&#28508;&#22312;&#30142;&#30149;&#22522;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate identification of disease genes has consistently been one of the keys to decoding a disease's molecular mechanism. Most current approaches focus on constructing biological networks and utilizing machine learning, especially, deep learning to identify disease genes, but ignore the complex relations between entities in the biological knowledge graph. In this paper, we construct a biological knowledge graph centered on diseases and genes, and develop an end-to-end Knowledge graph completion model for Disease Gene Prediction using interactional tensor decomposition (called KDGene). KDGene introduces an interaction module between the embeddings of entities and relations to tensor decomposition, which can effectively enhance the information interaction in biological knowledge. Experimental results show that KDGene significantly outperforms state-of-the-art algorithms. Furthermore, the comprehensive biological analysis of the case of diabetes mellitus confirms KDGene's ability for id
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35780;&#20272;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gscore&#30340;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#36825;&#23545;&#20110;&#27809;&#26377;&#30495;&#23454;&#26631;&#31614;&#30340;&#27979;&#35797;&#26102;&#21487;&#20316;&#20026;&#23454;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08287</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35780;&#20272;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#65306;&#22522;&#20110;&#25968;&#25454;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective. (arXiv:2302.08287v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35780;&#20272;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gscore&#30340;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#36825;&#23545;&#20110;&#27809;&#26377;&#30495;&#23454;&#26631;&#31614;&#30340;&#27979;&#35797;&#26102;&#21487;&#20316;&#20026;&#23454;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#38656;&#35201;&#30830;&#23450;&#27979;&#35797;&#26679;&#26412;&#23646;&#20110;&#20869;&#37096;&#20998;&#24067;&#36824;&#26159;&#22806;&#37096;&#20998;&#24067;&#65292;&#20294;&#29616;&#23454;&#20013;&#25105;&#20204;&#24182;&#19981;&#24635;&#26159;&#25317;&#26377;&#36825;&#20123;&#27979;&#35797;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35780;&#20272;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#35745;&#31639;Gscore&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#34913;&#37327;&#26080;&#30417;&#30563;&#27979;&#35797;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Gbench&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;200&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;Gscore&#19982;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#23450;&#37327;&#30456;&#20851;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;AUROC&#31561;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;Gscore&#30340;&#32467;&#26524;&#19968;&#33268;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#23454;&#29992;&#30340;&#26377;&#25928;&#35780;&#20272;&#26041;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#29615;&#22659;&#19979;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection methods assume that they have test ground truths, i.e., whether individual test samples are in-distribution (IND) or OOD. However, in the real world, we do not always have such ground truths, and thus do not know which sample is correctly detected and cannot compute the metric like AUROC to evaluate the performance of different OOD detection methods. In this paper, we are the first to introduce the unsupervised evaluation problem in OOD detection, which aims to evaluate OOD detection methods in real-world changing environments without OOD labels. We propose three methods to compute Gscore as an unsupervised indicator of OOD detection performance. We further introduce a new benchmark Gbench, which has 200 real-world OOD datasets of various label spaces to train and evaluate our method. Through experiments, we find a strong quantitative correlation betwwen Gscore and the OOD detection performance. Extensive experiments demonstrate that our Gscore achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;</title><link>http://arxiv.org/abs/2302.07268</link><description>&lt;p&gt;
AI&#32842;&#22825;&#21161;&#25163;&#21487;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#32447;&#20132;&#27969;&#25968;&#37327;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#21644;&#20854;&#20182;&#25968;&#23383;&#35770;&#22363;&#19978;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#21487;&#33021;&#20250;&#20135;&#29983;&#20998;&#35010;&#21644;&#20914;&#31361;&#12290;&#36825;&#31181;&#26377;&#27602;&#24615;&#22686;&#21152;&#20102;&#26497;&#21270;&#30340;&#31243;&#24230;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#20405;&#34432;&#20102;&#22810;&#20803;&#21270;&#31038;&#20250;&#21457;&#23637;&#35299;&#20915;&#24433;&#21709;&#25152;&#26377;&#20154;&#30340;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23398;&#32773;&#21644;&#27665;&#38388;&#31038;&#20250;&#32452;&#32455;&#25512;&#21160;&#24178;&#39044;&#25514;&#26045;&#65292;&#20351;&#38754;&#23545;&#38754;&#30340;&#23545;&#35805;&#19981;&#37027;&#20040;&#20855;&#26377;&#20998;&#35010;&#24615;&#25110;&#26356;&#20855;&#29983;&#20135;&#21147;&#65292;&#20294;&#23558;&#36825;&#20123;&#21162;&#21147;&#25193;&#23637;&#33267;&#22312;&#32447;&#21457;&#29983;&#30340;&#35768;&#22810;&#35805;&#35821;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35813;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;&#20309;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#24314;&#35758;&#30830;&#23454;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#25913;&#21892;&#23545;&#35805;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; QIKT &#30340;&#22522;&#20110;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20197;&#38382;&#39064;&#20026;&#20013;&#24515;&#30340;&#35748;&#30693;&#34920;&#31034;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#21516;&#36136;&#21270;&#38382;&#39064;&#30340;&#20551;&#35774;&#23545;&#23454;&#38469;&#24773;&#20917;&#19981;&#20934;&#30830;&#20197;&#21450;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06885</link><description>&lt;p&gt;
&#37319;&#29992;&#20197;&#38382;&#39064;&#20026;&#20013;&#24515;&#30340;&#35748;&#30693;&#34920;&#31034;&#25913;&#36827;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations. (arXiv:2302.06885v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; QIKT &#30340;&#22522;&#20110;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20197;&#38382;&#39064;&#20026;&#20013;&#24515;&#30340;&#35748;&#30693;&#34920;&#31034;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#21516;&#36136;&#21270;&#38382;&#39064;&#30340;&#20551;&#35774;&#23545;&#23454;&#38469;&#24773;&#20917;&#19981;&#20934;&#30830;&#20197;&#21450;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#26159;&#39044;&#27979;&#23398;&#29983;&#26410;&#26469;&#34920;&#29616;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#35266;&#23519;&#20182;&#20204;&#30340;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22823;&#37096;&#20998;&#37117;&#20381;&#36182;&#20110;&#8220;&#21516;&#36136;&#21270;&#38382;&#39064;&#8221;&#30340;&#20551;&#35774;&#65292;&#21363;&#22914;&#26524;&#38382;&#39064;&#20855;&#26377;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#38598;&#65292;&#21017;&#23427;&#20204;&#30340;&#36129;&#29486;&#26159;&#31561;&#20215;&#30340;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#38469;&#25945;&#32946;&#22330;&#26223;&#20013;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#27492;&#22806;&#65292;&#20174;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20013;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QIKT&#30340;&#22522;&#20110;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#26469;&#24212;&#23545;&#20197;&#19978;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;QIKT&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#38382;&#39064;&#30340;&#35748;&#30693;&#34920;&#31034;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#20174;&#38382;&#39064;-&#31572;&#26696;&#20132;&#20114;&#20013;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#25935;&#24863;&#30340;&#35748;&#30693;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) is a crucial technique to predict students' future performance by observing their historical learning processes. Due to the powerful representation ability of deep neural networks, remarkable progress has been made by using deep learning techniques to solve the KT problem. The majority of existing approaches rely on the \emph{homogeneous question} assumption that questions have equivalent contributions if they share the same set of knowledge components. Unfortunately, this assumption is inaccurate in real-world educational scenarios. Furthermore, it is very challenging to interpret the prediction results from the existing deep learning based KT models. Therefore, in this paper, we present QIKT, a question-centric interpretable KT model to address the above challenges. The proposed QIKT approach explicitly models students' knowledge state variations at a fine-grained level with question-sensitive cognitive representations that are jointly learned from a question-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38656;&#27714;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20351;&#29992;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.04723</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#29992;&#20110;&#38656;&#27714;&#20998;&#31867;&#65306;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Learning for Requirements Classification: An Exploratory Study. (arXiv:2302.04723v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38656;&#27714;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20351;&#29992;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#24037;&#31243;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#23581;&#35797;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#23436;&#25104;&#19968;&#31995;&#21015;RE&#20219;&#21153;&#65292;&#20363;&#22914;&#38656;&#27714;&#20998;&#31867;&#12289;&#38656;&#27714;&#36861;&#36394;&#12289;&#27495;&#20041;&#26816;&#27979;&#21644;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#20170;&#22825;&#22823;&#37096;&#20998;&#30340;ML / DL&#26041;&#27861;&#37117;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20010;&#38480;&#21046;&#23545;RE&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#20808;&#36827;&#30340;ML / DL&#25216;&#26415;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38656;&#27714;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30528;&#37325;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#22240;&#20026;&#35768;&#22810;RE&#20219;&#21153;&#21487;&#20197;&#34987;&#26694;&#23450;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;ZSL&#26041;&#27861;&#37319;&#29992;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Requirements engineering researchers have been experimenting with machine learning and deep learning approaches for a range of RE tasks, such as requirements classification, requirements tracing, ambiguity detection, and modelling. However, most of today's ML/DL approaches are based on supervised learning techniques, meaning that they need to be trained using a large amount of task-specific labelled training data. This constraint poses an enormous challenge to RE researchers, as the lack of labelled data makes it difficult for them to fully exploit the benefit of advanced ML/DL technologies. Objective: This paper addresses this problem by showing how a zero-shot learning approach can be used for requirements classification without using any labelled training data. We focus on the classification task because many RE tasks can be framed as classification problems. Method: The ZSL approach used in our study employs contextual word-embeddings and transformer-based language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Meta-SN&#65292;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36830;&#23545;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;Meta-SN&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#32534;&#30721;&#31867;&#21035;&#26631;&#31614;&#30340;&#20302;&#32500;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#20449;&#24565;&#32593;&#32476;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03507</link><description>&lt;p&gt;
&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#36830;&#23545;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Siamese Network for Few-Shot Text Classification. (arXiv:2302.03507v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Meta-SN&#65292;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36830;&#23545;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;Meta-SN&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#32534;&#30721;&#31867;&#21035;&#26631;&#31614;&#30340;&#20302;&#32500;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#20449;&#24565;&#32593;&#32476;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#25968;&#25454;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#20803;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;PROTO&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#36830;&#23545;&#32593;&#32476;&#65292;Meta-SN&#65292;&#26469;&#35299;&#20915;PROTO&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#24573;&#30053;&#20102;&#35745;&#31639;&#21407;&#22411;&#21521;&#37327;&#26102;&#37319;&#26679;&#25903;&#25345;&#38598;&#30340;&#38543;&#26426;&#24615;&#65307;&#65288;2&#65289;&#24573;&#30053;&#20102;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65307;&#65288;3&#65289;&#20197;&#32431;&#38543;&#26426;&#26041;&#24335;&#26500;&#24314;&#20803;&#20219;&#21153;&#12290;Meta-SN&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65288;&#20363;&#22914;&#31867;&#21035;&#21517;&#31216;&#21644;&#25551;&#36848;&#25991;&#26412;&#65289;&#26469;&#32534;&#30721;&#31867;&#21035;&#26631;&#31614;&#30340;&#20302;&#32500;&#23884;&#20837;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20174;&#37319;&#26679;&#25903;&#25345;&#38598;&#20013;&#35745;&#31639;&#21407;&#22411;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;Meta-SN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#21363;&#22686;&#21152;&#20102;&#23545;&#38590;&#20197;&#20998;&#31867;&#26679;&#26412;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Meta-SN&#20248;&#20110;&#21253;&#25324;PROTO&#22312;&#20869;&#30340;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning has been used to tackle the problem of label scarcity in text classification, of which meta-learning based methods have shown to be effective, such as the prototypical networks (PROTO). Despite the success of PROTO, there still exist three main problems: (1) ignore the randomness of the sampled support sets when computing prototype vectors; (2) disregard the importance of labeled samples; (3) construct meta-tasks in a purely random manner. In this paper, we propose a Meta-Learning Siamese Network, namely, Meta-SN, to address these issues. Specifically, instead of computing prototype vectors from the sampled support sets, Meta-SN utilizes external knowledge (e.g. class names and descriptive texts) for class labels, which is encoded as the low-dimensional embeddings of prototype vectors. In addition, Meta-SN presents a novel sampling strategy for constructing meta-tasks, which gives higher sampling probabilities to hard-to-classify samples. Extensive experiments are con
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;GAN&#21453;&#28436;&#32467;&#26524;&#20013;&#30340;&#32534;&#36753;&#33021;&#21147;&#20943;&#24369;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#20026;&#22495;&#20869;&#21644;&#22495;&#22806;&#37096;&#20998;&#65292;&#24182;&#23545;&#36825;&#20004;&#37096;&#20998;&#36827;&#34892;&#26435;&#37325;&#35843;&#33410;&#32454;&#21270;&#65292;&#20197;&#20445;&#25345;&#22495;&#20869;&#21306;&#22495;&#30340;&#32534;&#36753;&#33021;&#21147;&#24182;&#25552;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.12141</link><description>&lt;p&gt;
&#20160;&#20040;&#20250;&#20943;&#24369;&#32534;&#36753;&#33021;&#21147;&#65311;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#26041;&#27861;&#29992;&#20110;&#25913;&#21892;GAN&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion. (arXiv:2301.12141v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;GAN&#21453;&#28436;&#32467;&#26524;&#20013;&#30340;&#32534;&#36753;&#33021;&#21147;&#20943;&#24369;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#20026;&#22495;&#20869;&#21644;&#22495;&#22806;&#37096;&#20998;&#65292;&#24182;&#23545;&#36825;&#20004;&#37096;&#20998;&#36827;&#34892;&#26435;&#37325;&#35843;&#33410;&#32454;&#21270;&#65292;&#20197;&#20445;&#25345;&#22495;&#20869;&#21306;&#22495;&#30340;&#32534;&#36753;&#33021;&#21147;&#24182;&#25552;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21453;&#28436;&#26041;&#27861;&#20851;&#27880;&#29983;&#25104;&#22120;&#20013;&#30340;&#39069;&#22806;&#39640;&#27604;&#29575;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#26435;&#37325;&#25110;&#20013;&#38388;&#29305;&#24449;&#65289;&#65292;&#20197;&#20248;&#21270;&#20174;&#23884;&#20837;&#24335;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#21453;&#28436;&#21644;&#32534;&#36753;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#22312;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#21512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#20250;&#20943;&#24369;&#32534;&#36753;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#22797;&#26434;&#22270;&#20687;&#65288;&#20363;&#22914;&#21253;&#21547;&#36974;&#25377;&#65292;&#35814;&#32454;&#32972;&#26223;&#21644;&#20266;&#24433;&#30340;&#22270;&#20687;&#65289;&#19978;&#12290;&#20851;&#38190;&#22312;&#20110;&#31934;&#32454;&#21270;&#21453;&#28436;&#32467;&#26524;&#65292;&#36991;&#20813;&#32534;&#36753;&#33021;&#21147;&#38477;&#32423;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#65288;DHR&#65289;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#20027;&#27969;&#32454;&#21270;&#25216;&#26415;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#20445;&#25345;&#32534;&#36753;&#33021;&#21147;&#21644;&#20445;&#35777;&#20445;&#30495;&#24230;&#30340;&#25552;&#39640;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65306;&#22495;&#20869;&#21644;&#22495;&#22806;&#37096;&#20998;&#12290;&#32454;&#21270;&#36807;&#31243;&#26088;&#22312;&#20445;&#25345;&#22495;&#20869;&#21306;&#22495;&#30340;&#21487;&#32534;&#36753;&#24615;&#24182;&#25552;&#39640;&#20004;&#20010;&#22495;&#30340;&#20445;&#30495;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#26435;&#37325;&#35843;&#33410;&#26469;&#32454;&#21270;&#36825;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, inversion methods have focused on additional high-rate information in the generator (e.g., weights or intermediate features) to refine inversion and editing results from embedded latent codes. Although these techniques gain reasonable improvement in reconstruction, they decrease editing capability, especially on complex images (e.g., containing occlusions, detailed backgrounds, and artifacts). A vital crux is refining inversion results, avoiding editing capability degradation. To tackle this problem, we introduce Domain-Specific Hybrid Refinement (DHR), which draws on the advantages and disadvantages of two mainstream refinement techniques to maintain editing ability with fidelity improvement. Specifically, we first propose Domain-Specific Segmentation to segment images into two parts: in-domain and out-of-domain parts. The refinement process aims to maintain the editability for in-domain areas and improve two domains' fidelity. We refine these two parts by weight modulation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#30340;&#39592;&#24178;&#65292;&#20849;&#20139;&#23884;&#20837;&#31867;&#21521;&#37327;&#26469;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#24615;&#33021;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#38544;&#31169;&#20445;&#25252;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2301.11705</link><description>&lt;p&gt;
FedPH: &#38544;&#31169;&#22686;&#24378;&#22411;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedPH: Privacy-enhanced Heterogeneous Federated Learning. (arXiv:2301.11705v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#30340;&#39592;&#24178;&#65292;&#20849;&#20139;&#23884;&#20837;&#31867;&#21521;&#37327;&#26469;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#24615;&#33021;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#38544;&#31169;&#20445;&#25252;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#65292;&#36890;&#36807;&#20132;&#25442;&#21442;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35745;&#31639;&#36164;&#28304;&#24046;&#24322;&#20351;&#24471;&#30456;&#20851;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#30340;&#39592;&#24178;&#65292;&#23436;&#20840;&#36830;&#25509;&#30340;&#23618;&#26500;&#25104;&#22836;&#37096;&#12290;&#39592;&#24178;&#25552;&#21462;&#22836;&#37096;&#29305;&#24449;&#65292;&#31867;&#30340;&#23884;&#20837;&#21521;&#37327;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#65292;&#20197;&#25913;&#21892;&#22836;&#37096;&#24182;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20849;&#20139;&#31867;&#30340;&#23884;&#20837;&#21521;&#37327;&#32780;&#19981;&#26159;&#26799;&#24230;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#31169;&#26377;&#25968;&#25454;&#65292;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#26356;&#21152;&#26377;&#25928;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21521;&#31867;&#30340;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a distributed machine-learning environment that allows clients to learn collaboratively without sharing private data. This is accomplished by exchanging parameters. However, the differences in data distributions and computing resources among clients make related studies difficult. To address these heterogeneous problems, we propose a novel Federated Learning method. Our method utilizes a pre-trained model as the backbone of the local model, with fully connected layers comprising the head. The backbone extracts features for the head, and the embedding vector of classes is shared between clients to improve the head and enhance the performance of the local model. By sharing the embedding vector of classes instead of gradient-based parameters, clients can better adapt to private data, and communication between the server and clients is more effective. To protect privacy, we propose a privacy-preserving hybrid method that adds noise to the embedding vector of classes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEAR&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;&#19982;&#33258;&#36866;&#24212;&#37325;&#21551;&#65288;HEAR&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#27969;&#24335;&#36755;&#20837;&#20013;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#30340;&#19981;&#24517;&#35201;&#28014;&#28857;&#25805;&#20316;&#21644;&#19981;&#24517;&#35201;&#26631;&#31614;&#32763;&#36716;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27969;&#24335;&#36755;&#20837;&#19978;&#30340;&#26631;&#35760;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09244</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24335;&#24207;&#21015;&#26631;&#35760;&#30340;&#39640;&#25928;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient Encoders for Streaming Sequence Tagging. (arXiv:2301.09244v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEAR&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;&#19982;&#33258;&#36866;&#24212;&#37325;&#21551;&#65288;HEAR&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#27969;&#24335;&#36755;&#20837;&#20013;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#30340;&#19981;&#24517;&#35201;&#28014;&#28857;&#25805;&#20316;&#21644;&#19981;&#24517;&#35201;&#26631;&#31614;&#32763;&#36716;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27969;&#24335;&#36755;&#20837;&#19978;&#30340;&#26631;&#35760;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#37327;&#27969;&#36755;&#20837;&#65288;&#20363;&#22914;&#36716;&#24405;&#35821;&#38899;&#65289;&#20013;&#65292;&#20026;&#20102;&#23545;&#27969;&#24335;&#24207;&#21015;&#26631;&#35760;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#26631;&#35760;&#20174;&#22836;&#24320;&#22987;&#23545;&#27599;&#20010;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#12290;&#20808;&#21069;&#35745;&#31639;&#30340;&#19981;&#21487;&#37325;&#29992;&#24615;&#23548;&#33268;&#20102;&#26356;&#39640;&#25968;&#37327;&#30340;&#28014;&#28857;&#25805;&#20316;&#65288;&#25110;FLOP&#65289;&#21644;&#26356;&#39640;&#25968;&#37327;&#30340;&#19981;&#24517;&#35201;&#26631;&#31614;&#32763;&#36716;&#12290;&#22686;&#21152;&#30340;FLOP&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#25346;&#38047;&#26102;&#38388;&#65292;&#32780;&#22686;&#21152;&#30340;&#26631;&#31614;&#32763;&#36716;&#20250;&#23548;&#33268;&#27969;&#24335;&#24615;&#33021;&#26356;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEAR&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;&#19982;&#33258;&#36866;&#24212;&#37325;&#21551;&#65288;HEAR&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#22312;&#31163;&#32447;&#65288;&#25110;&#23436;&#25972;&#65289;&#36755;&#20837;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27969;&#36755;&#20837;&#65288;&#25110;&#19981;&#23436;&#25972;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;HEAR&#20855;&#26377;&#28151;&#21512;&#21333;&#21521; - &#21452;&#21521;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#25191;&#34892;&#24207;&#21015;&#26631;&#35760;&#65292;&#20197;&#21450;&#33258;&#36866;&#24212;&#37325;&#21551;&#27169;&#22359;&#65288;ARM&#65289;&#20197;&#26377;&#36873;&#25321;&#22320;&#24341;&#23548;&#32534;&#30721;&#22120;&#30340;&#21452;&#21521;&#37096;&#20998;&#30340;&#37325;&#26032;&#21551;&#21160;&#12290;&#22312;&#22235;&#20010;&#24207;&#21015;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;HEAR&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#27969;&#24335;&#34920;&#29616;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31163;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A naive application of state-of-the-art bidirectional encoders for streaming sequence tagging would require encoding each token from scratch for each new token in an incremental streaming input (like transcribed speech). The lack of re-usability of previous computation leads to a higher number of Floating Point Operations (or FLOPs) and higher number of unnecessary label flips. Increased FLOPs consequently lead to higher wall-clock time and increased label flipping leads to poorer streaming performance. In this work, we present a Hybrid Encoder with Adaptive Restart (HEAR) that addresses these issues while maintaining the performance of bidirectional encoders over the offline (or complete) inputs while improving performance on streaming (or incomplete) inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to perform sequence tagging, along with an Adaptive Restart Module (ARM) to selectively guide the restart of bidirectional portion of the encoder. Across four se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#24182;&#32467;&#21512;&#32676;&#20247;&#22806;&#21253;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#35757;&#32451;&#19968;&#20010;&#26144;&#23556;&#35821;&#20041;&#30456;&#20284;&#24615;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24357;&#21512;&#20154;&#31867;&#30452;&#35273;&#21644;&#20844;&#24179;&#20998;&#31867;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10154</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#31867;&#23548;&#21521;&#30340;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#24182;&#32467;&#21512;&#32676;&#20247;&#22806;&#21253;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#35757;&#32451;&#19968;&#20010;&#26144;&#23556;&#35821;&#20041;&#30456;&#20284;&#24615;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24357;&#21512;&#20154;&#31867;&#30452;&#35273;&#21644;&#20844;&#24179;&#20998;&#31867;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#31616;&#21382;&#31579;&#36873;&#21644;&#20869;&#23481;&#23457;&#26680;&#31561;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#24517;&#39035;&#26159;&#20844;&#24179;&#30340;&#65292;&#24182;&#36890;&#36807;&#23545;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#30340;&#25200;&#21160;&#19981;&#21464;&#26469;&#36991;&#20813;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23545;&#36825;&#20123;&#25200;&#21160;&#30340;&#30452;&#35273;&#19982;&#25429;&#25417;&#23427;&#20204;&#30340;&#24418;&#24335;&#30456;&#20284;&#24230;&#35268;&#33539;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#30828;&#32534;&#30721;&#21333;&#35789;&#26367;&#25442;&#65292;&#23548;&#33268;&#35268;&#33539;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#25110;&#32773;&#26080;&#27861;&#20805;&#20998;&#22320;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#19968;&#33268;&#65288;&#20363;&#22914;&#65292;&#22312;&#19981;&#23545;&#31216;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#19979;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#21457;&#29616;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#30452;&#35273;&#20844;&#24179;&#35268;&#33539;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#30417;&#30563;&#24335;&#36716;&#25442;&#21644;GPT-3&#30340;&#38646;-shot&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#19978;&#31867;&#20284;&#20294;&#22312;&#25935;&#24863;&#23646;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#32676;&#20247;&#22806;&#21253;&#33719;&#24471;&#36825;&#20123;&#20505;&#36873;&#20154;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#35821;&#20041;&#30456;&#20284;&#24615;&#26144;&#23556;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2212.08189</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#22312;&#32447;&#30830;&#23450;&#24615;&#36864;&#28779;&#65306;&#19968;&#31181;&#20998;&#23618;&#21644;&#28176;&#36827;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#36880;&#27493;&#36924;&#36817;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#23618;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#20915;&#31574;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#21487;&#33021;&#30340;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#31354;&#38388;&#30340;&#28176;&#36827;&#20998;&#21306;&#12290;&#26368;&#20248;&#20998;&#21306;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#20248;&#21270;&#23376;&#38382;&#39064;&#36880;&#27493;&#36924;&#36817;&#65292;&#29983;&#25104;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#23545;&#27599;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#20998;&#21306;&#30340;&#27599;&#20010;&#23376;&#38598;&#20013;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#12290;&#36825;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36880;&#27493;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25104;&#20687;&#36870;&#38382;&#39064;&#20013;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26102;&#20986;&#29616;&#30340;&#20005;&#37325;&#20266;&#20687;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#21407;&#22240;&#26159;&#30001;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#26465;&#20214;&#36755;&#20837;&#23548;&#33268;&#26465;&#20214;&#20223;&#23556;&#32806;&#21512;&#23618;&#8220;&#29190;&#28856;&#21453;&#28436;&#8221;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#26465;&#20214;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.04319</link><description>&lt;p&gt;
&#20851;&#20110;&#24402;&#19968;&#21270;&#27969;&#22312;&#25104;&#20687;&#36870;&#38382;&#39064;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Normalizing Flows for Inverse Problems in Imaging. (arXiv:2212.04319v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25104;&#20687;&#36870;&#38382;&#39064;&#20013;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26102;&#20986;&#29616;&#30340;&#20005;&#37325;&#20266;&#20687;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#21407;&#22240;&#26159;&#30001;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#26465;&#20214;&#36755;&#20837;&#23548;&#33268;&#26465;&#20214;&#20223;&#23556;&#32806;&#21512;&#23618;&#8220;&#29190;&#28856;&#21453;&#28436;&#8221;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#26465;&#20214;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#26679;&#26412;&#20197;&#35299;&#20915;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29992;&#20110;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#24402;&#19968;&#21270;&#27969;&#37319;&#29992;&#21487;&#21464;&#25442;&#20223;&#23556;&#32806;&#21512;&#23618;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#65292;&#20294;&#20598;&#23572;&#20250;&#35266;&#23519;&#21040;&#24847;&#22806;&#30340;&#20005;&#37325;&#20266;&#20687;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#20266;&#20687;&#30340;&#26469;&#28304;&#24182;&#25552;&#20986;&#36991;&#20813;&#23427;&#20204;&#30340;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32463;&#39564;&#21644;&#29702;&#35770;&#25581;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20110;&#26576;&#20123;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#26465;&#20214;&#36755;&#20837;&#20013;&#30340;&#26465;&#20214;&#20223;&#23556;&#32806;&#21512;&#23618;&#8220;&#29190;&#28856;&#21453;&#28436;&#8221;&#25152;&#36896;&#25104;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#22312;&#25104;&#20687;&#36870;&#38382;&#39064;&#20013;&#65292;&#23548;&#33268;&#20687;&#32032;&#38169;&#35823;&#20266;&#20687;&#30340;&#27010;&#29575;&#19982;&#20197;Mahalanobis&#36317;&#31163;&#20026;&#22522;&#30784;&#30340;&#36870;&#38382;&#39064;&#30340;&#24322;&#24120;&#26679;&#26412;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#22312;&#25105;&#20204;&#30340;&#35843;&#26597;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#27880;&#37322;&#20197;&#36991;&#20813;&#8220;&#29190;&#28856;&#21453;&#28436;&#8221;&#65292;&#28982;&#21518;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#29992;&#20854;&#20182;&#21487;&#21464;&#25442;&#20223;&#23556;&#32806;&#21512;&#23618;&#26367;&#25442;&#26576;&#20123;&#32806;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional normalizing flows can generate diverse image samples for solving inverse problems. Most normalizing flows for inverse problems in imaging employ the conditional affine coupling layer that can generate diverse images quickly. However, unintended severe artifacts are occasionally observed in the output of them. In this work, we address this critical issue by investigating the origins of these artifacts and proposing the conditions to avoid them. First of all, we empirically and theoretically reveal that these problems are caused by "exploding inverse" in the conditional affine coupling layer for certain out-of-distribution (OOD) conditional inputs. Then, we further validated that the probability of causing erroneous artifacts in pixels is highly correlated with a Mahalanobis distance-based OOD score for inverse problems in imaging. Lastly, based on our investigations, we propose a remark to avoid exploding inverse and then based on it, we suggest a simple remedy that substitu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#23413;&#21270;&#30340;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#25104;&#36739;&#23567;&#30340;&#23376;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#24182;&#26080;&#32541;&#22320;&#32452;&#35013;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#39640;&#25928;&#12289;&#26377;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2212.04129</link><description>&lt;p&gt;
&#28145;&#24230;&#23413;&#21270;: &#20998;&#32780;&#27835;&#20043;&#22320;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Incubation: Training Large Models by Divide-and-Conquering. (arXiv:2212.04129v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#23413;&#21270;&#30340;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#25104;&#36739;&#23567;&#30340;&#23376;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#24182;&#26080;&#32541;&#22320;&#32452;&#35013;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#39640;&#25928;&#12289;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#12289;&#32531;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#36807;&#24230;&#25311;&#21512;&#31561;&#38382;&#39064;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#23413;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#25104;&#36739;&#23567;&#30340;&#23376;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#24182;&#26080;&#32541;&#22320;&#32452;&#35013;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#39640;&#25928;&#12289;&#26377;&#25928;&#35757;&#32451;&#12290;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#20445;&#29420;&#31435;&#35757;&#32451;&#30340;&#23376;&#27169;&#22359;&#30340;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#23616;&#30340;&#20849;&#20139;&#20803;&#27169;&#22411;&#65292;&#23427;&#34987;&#29992;&#26469;&#38544;&#24335;&#22320;&#23558;&#25152;&#26377;&#27169;&#22359;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#24182;&#19988;&#21487;&#20197;&#35774;&#35745;&#20026;&#19968;&#20010;&#20855;&#26377;&#21487;&#24573;&#30053;&#35745;&#31639;&#24320;&#38144;&#30340;&#26497;&#23567;&#32593;&#32476;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#23413;&#21270;&#31639;&#27861;&#65292;&#23427;&#35757;&#32451;&#27599;&#20010;&#23376;&#27169;&#22359;&#26469;&#26367;&#25442;&#20803;&#27169;&#22411;&#30340;&#30456;&#24212;&#37096;&#20998;&#24182;&#23436;&#25104;&#32473;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a remarkable success of large deep learning models. However, training these models is challenging due to high computational costs, painfully slow convergence, and overfitting issues. In this paper, we present Deep Incubation, a novel approach that enables the efficient and effective training of large models by dividing them into smaller sub-modules that can be trained separately and assembled seamlessly. A key challenge for implementing this idea is to ensure the compatibility of the independently trained sub-modules. To address this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the modules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task. Despite the simplicity, our approach effectively enc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20010;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#20013;&#35745;&#31639;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#36716;&#25442;&#35299;&#20915;&#20102;&#36890;&#29992;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20063;&#25552;&#31034;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#20197;&#33719;&#24471;&#20449;&#24687;&#20016;&#23500;&#30340;&#36793;&#30028;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2212.02932</link><description>&lt;p&gt;
&#20174;&#35266;&#27979;&#12289;&#26377;&#20559;&#21644;&#38543;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#36793;&#30028;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Learning to Bound Counterfactual Inference from Observational, Biased and Randomised Data. (arXiv:2212.02932v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20010;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#20013;&#35745;&#31639;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#36716;&#25442;&#35299;&#20915;&#20102;&#36890;&#29992;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20063;&#25552;&#31034;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#20197;&#33719;&#24471;&#20449;&#24687;&#20016;&#23500;&#30340;&#36793;&#30028;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#26469;&#33258;&#22810;&#20010;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#38598;&#25104;&#36215;&#26469;&#65292;&#26368;&#32456;&#35745;&#31639;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#20855;&#26377;&#36873;&#25321;&#20559;&#24046;&#30340;&#21333;&#20010;&#35266;&#27979;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#20284;&#28982;&#20989;&#25968;&#27809;&#26377;&#23616;&#37096;&#26368;&#22823;&#20540;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#22240;&#26524;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#26696;&#35745;&#31639;&#37096;&#20998;&#21487;&#35782;&#21035;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#36817;&#20284;&#36793;&#30028;&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#37325;&#28857;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22270;&#24418;&#21464;&#25442;&#23558;&#36890;&#29992;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;&#26080;&#35770;&#26159;&#24178;&#39044;&#36824;&#26159;&#35266;&#27979;&#25968;&#25454;&#65292;&#26377;&#20559;&#25110;&#26080;&#20559;&#65289;&#26144;&#23556;&#21040;&#21069;&#32773;&#20174;&#32780;&#35299;&#20915;&#21516;&#26679;&#30340;&#38382;&#39064;&#12290;&#31995;&#32479;&#30340;&#25968;&#23383;&#23454;&#39564;&#21644;&#23545;&#22993;&#24687;&#27835;&#30103;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20063;&#25552;&#31034;&#22312;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#20197;&#33719;&#24471;&#20449;&#24687;&#20016;&#23500;&#30340;&#36793;&#30028;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of integrating data from multiple, possibly biased, observational and interventional studies, to eventually compute counterfactuals in structural causal models. We start from the case of a single observational dataset affected by a selection bias. We show that the likelihood of the available data has no local maxima. This enables us to use the causal expectation-maximisation scheme to compute approximate bounds for partially identifiable counterfactual queries, which are the focus of this paper. We then show how the same approach can solve the general case of multiple datasets, no matter whether interventional or observational, biased or unbiased, by remapping it into the former one via graphical transformations. Systematic numerical experiments and a case study on palliative care show the effectiveness and accuracy of our approach, while hinting at the benefits of integrating heterogeneous data to get informative bounds in case of partial identifiability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2212.02710</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#35937;&#35782;&#21035;&#65306;&#38754;&#21521;&#23545;&#35937;&#27010;&#24565;&#23398;&#20064;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23545;&#35937;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#20307;&#39564;&#30340;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#26426;&#22120;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#26356;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#23545;&#35937;&#20855;&#26377;&#21738;&#20123;&#23646;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#23545;&#35937;&#20570;&#20160;&#20040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#20197;&#25512;&#21160;&#23545;&#35937;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;&#23427;&#35201;&#27714;&#26426;&#22120;&#25512;&#29702;&#20986;&#23545;&#35937;&#30340;&#20316;&#29992;&#65292;&#24182;&#21516;&#26102;&#32473;&#20986;&#21407;&#22240;&#65306;&#26159;&#21738;&#20123;&#23646;&#24615;&#20351;&#24471;&#19968;&#20010;&#23545;&#35937;&#20855;&#26377;&#36825;&#20123;&#20316;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345; OCL&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#65292;&#21253;&#25324;&#19977;&#20010;&#23618;&#27425;&#30340;&#23545;&#35937;&#27010;&#24565;&#65288;&#31867;&#21035;&#12289;&#23646;&#24615;&#12289;&#20316;&#29992;&#65289;&#65292;&#20197;&#21450;&#19977;&#20010;&#23618;&#27425;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512; OCL &#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#65306;Object Concept Reasoning Network (OCRN)&#12290;&#23427;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#21644;&#27010;&#24565;&#23454;&#20363;&#21270;&#26469;&#25512;&#26029;&#19977;&#20010;&#23618;&#27425;&#65292;&#36981;&#24490;&#23427;&#20204;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELICIT&#65292;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#31867;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2212.02469</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#27169;&#22411;&#20808;&#39564;&#30340;&#19968;&#27425;&#24615;&#38544;&#24335;&#21160;&#30011;&#21270;&#22836;&#20687;&#21046;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Implicit Animatable Avatars with Model-based Priors. (arXiv:2212.02469v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELICIT&#65292;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#31867;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21019;&#24314;&#20154;&#31867;&#22836;&#20687;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#38656;&#35201;&#31264;&#23494;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;&#35270;&#39057;&#25110;&#22810;&#35270;&#35282;&#22270;&#20687;&#65289;&#65292;&#35201;&#20040;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#29305;&#23450;3D&#20154;&#20307;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#20808;&#39564;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#31232;&#30095;&#35270;&#35282;&#36755;&#20837;&#36827;&#34892;&#37325;&#24314;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#22312;&#20165;&#26377;&#19968;&#24352;&#22270;&#20687;&#26102;&#26080;&#27861;&#23454;&#29616;&#36924;&#30495;&#37325;&#24314;&#12290;&#20026;&#20102;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELICIT&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#19968;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#20307;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26032;&#26041;&#27861;&#12290;&#21463;&#21040;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#36523;&#20307;&#20960;&#20309;&#24418;&#29366;&#24182;&#20174;&#19968;&#24352;&#22270;&#29255;&#20013;&#24819;&#35937;&#36896;&#22411;&#23436;&#25972;&#30340;&#34915;&#26588;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#22312;ELICIT&#20013;&#21033;&#29992;&#20102;&#20004;&#20010;&#20808;&#39564;&#65306;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ELICIT&#21033;&#29992;&#19968;&#20010;&#33945;&#30382;&#39030;&#28857;&#27169;&#26495;&#27169;&#22411;&#65288;&#21363;SMPL&#65289;&#30340;3D&#36523;&#20307;&#24418;&#29366;&#20960;&#20309;&#20808;&#39564;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;CLIP&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#20102;&#35270;&#35273;&#26381;&#35013;&#35821;&#20041;&#20808;&#39564;&#12290;&#36825;&#20004;&#20010;&#20808;&#39564;&#22343;&#29992;&#20110;&#20174;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#36924;&#30495;&#30340;&#21487;&#21160;3D&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22495;&#27867;&#21270;&#30340;&#30452;&#25509;&#24433;&#21709;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#27010;&#24565;&#35299;&#20915;&#20102;&#30456;&#20851;&#24615;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#19982;&#29616;&#26377;&#22495;&#27867;&#21270;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14594</link><description>&lt;p&gt;
&#38024;&#23545;&#22495;&#27867;&#21270;&#30340;&#30452;&#25509;&#24433;&#21709;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct-Effect Risk Minimization for Domain Generalization. (arXiv:2211.14594v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22495;&#27867;&#21270;&#30340;&#30452;&#25509;&#24433;&#21709;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#27010;&#24565;&#35299;&#20915;&#20102;&#30456;&#20851;&#24615;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#19982;&#29616;&#26377;&#22495;&#27867;&#21270;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22495;&#22806;&#25512; generalization &#20013;&#23646;&#24615;&#34394;&#20551;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#22495;&#38388;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#21363;&#30456;&#20851;&#24615;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#36896;&#25104;&#20102;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#27010;&#24565;&#26469;&#35299;&#20915;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#33021;&#22815;&#23398;&#20064;&#21040;&#30452;&#25509;&#24433;&#21709;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#20351;&#29992;&#34920;&#31034;&#21644;&#31867;&#26631;&#31614;&#39044;&#27979;&#22495;&#26631;&#31614;&#30340;&#38169;&#35823;&#26469;&#23398;&#20064;&#38388;&#25509;&#24433;&#21709;&#34920;&#31034;&#65307;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#21644;&#39564;&#35777;&#38454;&#27573;&#20013;&#20855;&#26377;&#30456;&#20284;&#38388;&#25509;&#24433;&#21709;&#34920;&#31034;&#20294;&#26631;&#31614;&#19981;&#21516;&#30340;&#25968;&#25454;&#30456;&#21305;&#37197;&#26469;&#28040;&#38500;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#23398;&#20064;&#21040;&#30340;&#38388;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#20854;&#21487;&#19982;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#31639;&#27861;&#30456;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2211.02408</link><description>&lt;p&gt;
&#25554;&#20837;&#21518;&#38376;&#20803;&#32032;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#25216;&#26415;&#22312;&#30740;&#31350;&#32773;&#21644;&#20844;&#20247;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20381;&#36182;&#20110;&#22806;&#37096;&#26469;&#28304;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29992;&#25143;&#30456;&#20449;&#26816;&#32034;&#21040;&#30340;&#27169;&#22411;&#20250;&#20687;&#25215;&#35834;&#30340;&#37027;&#26679;&#36816;&#34892;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#21521;&#38376;&#25915;&#20987;&#25991;&#26412;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21482;&#26159;&#36731;&#24494;&#22320;&#25913;&#21464;&#20102;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#23545;&#20110;&#24102;&#26377;&#24178;&#20928;&#25552;&#31034;&#30340;&#22270;&#20687;&#29983;&#25104;&#27809;&#26377;&#21487;&#30097;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#25554;&#20837;&#25552;&#31034;&#20013;&#65292;&#20363;&#22914;&#19968;&#20010;&#38750;&#25289;&#19969;&#23383;&#31526;&#25110;&#34920;&#24773;&#31526;&#21495;&#65292;&#25915;&#20987;&#32773;&#23601;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;Stable Diffusion&#21644;highligh&#19978;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highligh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17505</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Space-fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#35843;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#31649;&#29702;&#65288;&#20272;&#35745;&#12289;&#39044;&#27979;&#25110;&#25511;&#21046;&#65289;&#38543;&#31354;&#38388;&#21464;&#21270;&#30340;&#20449;&#21495;&#65292;&#20363;&#22914;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#21644;&#29983;&#38271;/&#32553;&#23567;&#30340;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#26041;&#27861;&#65292;&#21327;&#21516;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#20272;&#35745;&#31354;&#38388;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26694;&#26550;&#20013;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#33258;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20272;&#35745;&#22797;&#26434;&#20989;&#25968;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#36319;&#36394;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26102;&#31354;&#29616;&#35937;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recurrent task in coordinated systems is managing (estimating, predicting, or controlling) signals that vary in space, such as distributed sensed data or computation outcomes. Especially in large-scale settings, the problem can be addressed through decentralised and situated computing systems: nodes can locally sense, process, and act upon signals, and coordinate with neighbours to implement collective strategies. Accordingly, in this work we devise distributed coordination strategies for the estimation of a spatial phenomenon through collaborative adaptive sampling. Our design is based on the idea of dynamically partitioning space into regions that compete and grow/shrink to provide accurate aggregate sampling. Such regions hence define a sort of virtualised space that is "fluid", since its structure adapts in response to pressure forces exerted by the underlying phenomenon. We provide an adaptive sampling algorithm in the field-based coordination framework, and prove it is self-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2210.01162</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#23567;&#36829;&#21453;&#36830;&#32493;&#25511;&#21046;&#20197;&#23454;&#29616;&#19981;&#21487;&#34892;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#32508;&#21512;&#65292;&#20197;&#23454;&#29616;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;(LTL)&#34920;&#36798;&#30340;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20854;&#20013;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#26410;&#30693;&#65288;&#36879;&#26126;&#30418;&#23376;&#65289;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#26080;&#27861;&#20840;&#23616;&#23436;&#25104;&#12290;&#25105;&#20204;&#19981;&#20462;&#25913;&#32473;&#23450;&#30340;LTL&#20844;&#24335;&#65292;&#32780;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;DRL&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#36829;&#35268;&#28385;&#36275;&#23427;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35201;&#27714;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#26426;&#28385;&#36275;&#21644;&#26368;&#23567;&#36829;&#35268;&#20195;&#20215;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;LTL&#20219;&#21153;&#30340;DRL&#26234;&#33021;&#20307;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;DRL&#30340;&#36817;&#35270;&#20542;&#21521;&#65292;&#36825;&#22312;&#23398;&#20064;&#21487;&#20197;&#20855;&#26377;&#38271;&#25110;&#26080;&#38480;&#25345;&#32493;&#26102;&#38388;&#30340;&#19968;&#33324;LTL&#20219;&#21153;&#26102;&#32463;&#24120;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13476</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;:&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30740;&#31350;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32972;&#26223;&#19979;&#20165;&#20973;&#20511;&#20960;&#20010;&#26631;&#31614;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#20307;&#21306;&#20998;&#21644;&#19981;&#21464;&#26144;&#23556;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#19977;&#20010;&#24120;&#35265;&#29942;&#39048;&#65306;(1)&#23614;&#37096;&#20998;&#24067;&#65306;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#36981;&#24490;&#38544;&#21547;&#30340;&#38271;&#23614;&#31867;&#20998;&#24067;&#12290;&#30450;&#30446;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#20687;&#32032;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#24615;&#33021;&#24694;&#21270;&#65307;(2)&#19968;&#33268;&#24615;&#65306;&#30001;&#20110;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#20043;&#38388;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#20998;&#21106;&#27169;&#22411;&#26159;&#21542;&#23398;&#20250;&#20102;&#26377;&#24847;&#20041;&#19988;&#19968;&#33268;&#30340;&#35299;&#21078;&#29305;&#24449;&#20173;&#19981;&#28165;&#26970;&#65307;&#20197;&#21450;(3)&#22810;&#26679;&#24615;&#65306;&#25972;&#20010;&#25968;&#25454;&#38598;&#20869;&#37096;&#20999;&#29255;&#30340;&#30456;&#20851;&#24615;&#21463;&#21040;&#30340;&#20851;&#27880;&#26174;&#33879;&#36739;&#23569;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#38598;&#26412;&#36523;&#30340;&#31574;&#30053;&#26041;&#27861;&#65292;&#20174;&#19981;&#21516;&#30340;&#35299;&#21078;&#35270;&#22270;&#20013;&#21457;&#29616;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MOAN&#65292;&#29992;&#20110;&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290; MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#21069;&#32773;&#25552;&#21462;&#20849;&#21516;&#20381;&#36182;&#30340;&#20687;&#32032;&#21306;&#22495;&#65292;&#32780;&#21518;&#32773;&#24378;&#21046;&#32593;&#32476;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35299;&#21078;&#23398;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we i
&lt;/p&gt;</description></item><item><title>LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12746</link><description>&lt;p&gt;
LSAP: &#37325;&#26032;&#24605;&#32771;GAN&#28508;&#31354;&#38388;&#20013;&#21453;&#28436;&#30340;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;
&lt;/p&gt;
&lt;p&gt;
LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space. (arXiv:2209.12746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12746
&lt;/p&gt;
&lt;p&gt;
LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21453;&#28436;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#22270;&#20687;&#23884;&#20837;&#65292;&#22312;&#36825;&#20010;&#27493;&#39588;&#20013;&#65292;&#32534;&#30721;&#22120;&#25110;&#32773;&#20248;&#21270;&#36807;&#31243;&#23884;&#20837;&#22270;&#20687;&#20197;&#33719;&#21462;&#30456;&#24212;&#30340;&#28508;&#22312;&#30721;&#12290;&#20043;&#21518;&#65292;&#31532;&#20108;&#27493;&#26088;&#22312;&#25913;&#21892;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32467;&#26524;&#32454;&#21270;&#12290;&#23613;&#31649;&#31532;&#20108;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#20445;&#30495;&#24230;&#65292;&#20294;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;&#28145;&#24230;&#20381;&#36182;&#20110;&#22312;&#31532;&#19968;&#27493;&#20013;&#33719;&#24471;&#30340;&#21453;&#21521;&#28508;&#22312;&#30721;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#20855;&#26377;&#26356;&#22909;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#28508;&#22312;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#36825;&#20004;&#20010;&#29305;&#24449;&#19982;&#21453;&#21521;&#30721;&#19982;&#21512;&#25104;&#20998;&#24067;&#30340;&#23545;&#40784;&#65288;&#25110;&#19981;&#23545;&#40784;&#65289;&#31243;&#24230;&#26377;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#31354;&#38388;&#23545;&#40784;&#21453;&#28436;&#33539;&#20363;&#65288;LSAP&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#39118;&#26684;&#31354;&#38388;&#65288;$\mathcal{S^N}$&#65289;&#21644;&#26631;&#20934;&#21270;&#20869;&#23481;&#31354;&#38388;&#65288;$\mathcal{C^N}$&#65289;&#65292;&#20998;&#21035;&#22312;&#39118;&#26684;&#21644;&#20869;&#23481;&#19978;&#23545;&#40784;&#27491;&#21521;&#21644;&#36127;&#21521;&#28508;&#22312;&#30721;&#21644;&#21512;&#25104;&#20998;&#24067;&#12290; LSAP&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#36716;&#25442;&#21644;&#22270;&#20687;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LSAP&#20855;&#26377;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29305;&#24615;&#65292;&#22914;&#25913;&#36827;&#30340;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#27169;&#24335;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the methods evolve, inversion is mainly divided into two steps. The first step is Image Embedding, in which an encoder or optimization process embeds images to get the corresponding latent codes. Afterward, the second step aims to refine the inversion and editing results, which we named Result Refinement. Although the second step significantly improves fidelity, perception and editability are almost unchanged, deeply dependent on inverse latent codes attained in the first step. Therefore, a crucial problem is gaining the latent codes with better perception and editability while retaining the reconstruction fidelity. In this work, we first point out that these two characteristics are related to the degree of alignment (or disalignment) of the inverse codes with the synthetic distribution. Then, we propose Latent Space Alignment Inversion Paradigm (LSAP), which consists of evaluation metric and solution for this problem. Specifically, we introduce Normalized Style Space ($\mathcal{S^N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25991;&#26412;&#25968;&#25454;&#19978;&#20004;&#31867;&#26041;&#27861;&#65306;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#21644;&#25552;&#21462;&#31616;&#21333;&#36923;&#36753;&#35268;&#21017;&#65292;&#21457;&#29616;&#22312;&#30456;&#21516;&#27169;&#22411;&#19979;&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35299;&#37322;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.01420</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#27604;&#36739;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#35268;&#21017;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comparing Feature Importance and Rule Extraction for Interpretability on Text Data. (arXiv:2207.01420v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25991;&#26412;&#25968;&#25454;&#19978;&#20004;&#31867;&#26041;&#27861;&#65306;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#21644;&#25552;&#21462;&#31616;&#21333;&#36923;&#36753;&#35268;&#21017;&#65292;&#21457;&#29616;&#22312;&#30456;&#21516;&#27169;&#22411;&#19979;&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35299;&#37322;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#28041;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#36825;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#22312;&#23616;&#37096;&#26041;&#27861;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#26063;&#32676;&#65306;&#19968;&#31181;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#21478;&#19968;&#31181;&#21017;&#25552;&#21462;&#31616;&#21333;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#24847;&#22806;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#37027;&#20123;&#25105;&#20204;&#39044;&#35745;&#20250;&#26377;&#23450;&#24615;&#24039;&#21512;&#30340;&#31616;&#21333;&#27169;&#22411;&#19978;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex machine learning algorithms are used more and more often in critical tasks involving text data, leading to the development of interpretability methods. Among local methods, two families have emerged: those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations, even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect, we propose a new approach to compare explanations produced by different methods.
&lt;/p&gt;</description></item><item><title>WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2205.14375</link><description>&lt;p&gt;
WaveMix: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;&#36164;&#28304;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14375
&lt;/p&gt;
&lt;p&gt;
WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WaveMix&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26082;&#20855;&#26377;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#21448;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;WaveMix&#32593;&#32476;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#21253;&#25324;Cityscapes&#20013;&#30340;&#20998;&#21106;&#21644;Places-365&#12289;&#20116;&#20010;EMNIST&#25968;&#25454;&#38598;&#21644;iNAT-mini&#20013;&#30340;&#20998;&#31867;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;&#20196;&#20154;&#24778;&#22855;&#30340;&#26159;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;WaveMix&#32467;&#26500;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#24403;&#25511;&#21046;&#21442;&#25968;&#25968;&#37327;&#26102;&#65292;WaveMix&#25152;&#38656;&#30340;GPU RAM&#26356;&#23569;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#30465;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#25910;&#30410;&#65292;&#25105;&#20204;&#22312;WaveMix&#22359;&#20013;&#20351;&#29992;&#20102;&#22810;&#32423;&#20108;&#32500;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;2D-DWT&#65289;&#65292;&#23427;&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;:(1)&#23427;&#22522;&#20110;&#19977;&#31181;&#24378;&#22270;&#20687;&#20808;&#39564;&#26465;&#20214;&#37325;&#26032;&#32452;&#32455;&#31354;&#38388;&#20449;&#24687;&#8212;&#8212;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20301;&#31227;&#19981;&#21464;&#24615;&#21644;&#36793;&#32536;&#30340;&#31232;&#30095;&#24615;,(2) i
&lt;/p&gt;
&lt;p&gt;
We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
&lt;/p&gt;</description></item><item><title>Anchors &#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#35268;&#21017;&#24615;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20984;&#26174;&#20986;&#19968;&#20010;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#26469;&#24378;&#35843;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#65292;&#24182;&#36890;&#36807; TF-IDF &#21521;&#37327;&#21270;&#27493;&#39588;&#20197;&#21450;&#27169;&#22411;&#23618;&#27425;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20854;&#22312;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#27719;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#29992;&#20316; Anchors &#35789;&#27719;&#12290;</title><link>http://arxiv.org/abs/2205.13789</link><description>&lt;p&gt;
&#19968;&#29255;&#25991;&#23383;&#28023;&#65306;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#30340; Anchors &#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v2 [stat.ML] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13789
&lt;/p&gt;
&lt;p&gt;
Anchors &#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#35268;&#21017;&#24615;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20984;&#26174;&#20986;&#19968;&#20010;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#26469;&#24378;&#35843;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#65292;&#24182;&#36890;&#36807; TF-IDF &#21521;&#37327;&#21270;&#27493;&#39588;&#20197;&#21450;&#27169;&#22411;&#23618;&#27425;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20854;&#22312;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#27719;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#29992;&#20316; Anchors &#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Anchors &#26159;&#19968;&#31181;&#22522;&#20110;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#24182;&#24378;&#35843;&#19968;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#65292;&#36825;&#20123;&#35789;&#35821;&#23384;&#22312;&#20110;&#25991;&#26723;&#20013;&#26102;&#65292;&#27169;&#22411;&#36755;&#20986;&#31867;&#20284;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#31867;&#30340;&#31639;&#27861;&#24418;&#24335;&#21270;&#21518;&#65292;&#32467;&#21512;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20102; Anchors &#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#20998;&#21035;&#35206;&#30422;&#20102;&#22522;&#26412; if-then &#35268;&#21017;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#39033;&#20998;&#26512;&#65292;&#27934;&#35265;&#20219;&#20309;&#21487;&#24494;&#20998;&#20998;&#31867;&#22120;&#30340; Anchors &#34892;&#20026;&#29305;&#24449;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#35821;&#65292;&#36890;&#36807;&#21453;&#21521;&#25991;&#20214;&#37325;&#26032;&#21152;&#26435;&#65292;&#21487;&#20197;&#20316;&#20026; Anchors &#35789;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anchors (Ribeiro et al., 2018) is a post-hoc, rule-based interpretability method. For text data, it proposes to explain a decision by highlighting a small set of words (an anchor) such that the model to explain has similar outputs when they are present in a document. In this paper, we present the first theoretical analysis of Anchors, considering that the search for the best anchor is exhaustive. After formalizing the algorithm for text classification, we present explicit results on different classes of models when the vectorization step is TF-IDF, and words are replaced by a fixed out-of-dictionary token when removed. Our inquiry covers models such as elementary if-then rules and linear classifiers. We then leverage this analysis to gain insights on the behavior of Anchors for any differentiable classifiers. For neural networks, we empirically show that the words corresponding to the highest partial derivatives of the model with respect to the input, reweighted by the inverse document
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#22686;&#24378;&#32593;&#32476;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2202.03382</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Corrupted Image Modeling for Self-Supervised Visual Pre-Training. (arXiv:2202.03382v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#22686;&#24378;&#32593;&#32476;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;&#65292;&#20351;&#29992;&#19968;&#20010;&#36741;&#21161;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#23567;&#22411;&#30340;BEiT&#65288;Vision Transformer&#27169;&#22411;&#65289;&#65292;&#23558;&#36755;&#20837;&#30340;&#22270;&#20687;&#36827;&#34892;&#30772;&#22351;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20154;&#24037;&#30340;[MASK]&#20196;&#29260;&#65292;&#29983;&#25104;&#22120;&#22312;&#36755;&#20986;&#20998;&#24067;&#20013;&#37319;&#26679;&#24688;&#24403;&#30340;&#22791;&#36873;&#39033;&#29992;&#20110;&#26367;&#25442;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#20123;&#22270;&#20687;&#29255;&#27573;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#19968;&#20010;&#22686;&#24378;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#24674;&#22797;&#21407;&#22987;&#22270;&#20687;&#25110;&#39044;&#27979;&#27599;&#20010;&#35270;&#35273;&#20196;&#29260;&#26159;&#21542;&#34987;&#29983;&#25104;&#22120;&#37319;&#26679;&#26367;&#25442;&#12290;&#29983;&#25104;&#22120;&#21644;&#22686;&#24378;&#32593;&#32476;&#21516;&#26102;&#36827;&#34892;&#35757;&#32451;&#65292;&#21327;&#21516;&#26356;&#26032;&#12290;&#39044;&#35757;&#32451;&#21518;&#65292;&#22686;&#24378;&#32593;&#32476;&#21487;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23481;&#37327;&#35270;&#35273;&#32534;&#30721;&#22120;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;ViT&#21644;CNN&#21487;&#20197;&#20351;&#29992;&#32479;&#19968;&#30340;&#38750;&#23402;&#29983;&#26694;&#26550;&#23398;&#20064;&#20016;&#23500;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#26041;&#27861;&#22312;ImageNet&#12289;COIL-100&#21644;PASCAL VOC 2007&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our appro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#38750;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#26469;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20915;&#31574;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.05818</link><description>&lt;p&gt;
&#27979;&#37327;&#38750;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#65306;&#24050;&#30693;&#21644;&#26410;&#30693;&#26410;&#30693;&#30340;&#35748;&#30693;&#12289;&#36923;&#36753;&#21644;&#35745;&#31639;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Measuring Non-Probabilistic Uncertainty: A cognitive, logical and computational assessment of known and unknown unknowns. (arXiv:2201.05818v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#38750;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#26469;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20915;&#31574;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#26080;&#27861;&#29992;&#27010;&#29575;&#35770;&#20805;&#20998;&#25551;&#36848;&#30340;&#21407;&#22240;&#26377;&#20004;&#20010;&#12290;&#31532;&#19968;&#20010;&#21407;&#22240;&#26159;&#30001;&#20110;&#29420;&#29305;&#25110;&#20960;&#20046;&#29420;&#29305;&#30340;&#20107;&#20214;&#65292;&#36825;&#20123;&#20107;&#20214;&#20174;&#26410;&#21457;&#29983;&#25110;&#21457;&#29983;&#24471;&#22826;&#23569;&#65292;&#20197;&#33267;&#20110;&#26080;&#27861;&#21487;&#38752;&#22320;&#27979;&#37327;&#39057;&#29575;&#12290;&#31532;&#20108;&#20010;&#21407;&#22240;&#26159;&#24403;&#20154;&#20204;&#25285;&#24515;&#21487;&#33021;&#21457;&#29983;&#26576;&#20123;&#20107;&#24773;&#65292;&#32780;&#33258;&#24049;&#29978;&#33267;&#26080;&#27861;&#24819;&#35937;&#65292;&#20363;&#22914;&#65306; "&#27668;&#20505;&#21464;&#21270;&#12289;&#37329;&#34701;&#21361;&#26426;&#12289;&#22823;&#27969;&#34892;&#12289;&#25112;&#20105;&#65292;&#19979;&#19968;&#20010;&#26159;&#20160;&#20040;&#65311;" &#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#35748;&#30693;&#22320;&#22270;&#23558;&#26368;&#32456;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30772;&#22351;&#20174;&#20855;&#20307;&#12289;&#21487;&#35782;&#21035;&#21644;&#24046;&#24322;&#21270;&#30340;&#26041;&#24335;&#24433;&#21709;&#21040;&#20225;&#19994;&#39640;&#31649;&#12289;&#21592;&#24037;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#19981;&#21516;&#35762;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#35832;&#22914;&#21672;&#35810;&#25253;&#21578;&#25110;&#21521;&#32929;&#19996;&#30340;&#20449;&#20989;&#31561;&#25991;&#26412;&#65292;&#20197;&#26816;&#27979;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#36890;&#24120;&#25351;&#23548;&#20915;&#31574;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two reasons why uncertainty may not be adequately described by Probability Theory. The first one is due to unique or nearly-unique events, that either never realized or occurred too seldom for frequencies to be reliably measured. The second one arises when one fears that something may happen, that one is not even able to figure out, e.g., if one asks: "Climate change, financial crises, pandemic, war, what next?"  In both cases, simple one-to-one cognitive maps between available alternatives and possible consequences eventually melt down. However, such destructions reflect into the changing narratives of business executives, employees and other stakeholders in specific, identifiable and differential ways. In particular, texts such as consultants' reports or letters to shareholders can be analysed in order to detect the impact of both sorts of uncertainty onto the causal relations that normally guide decision-making.  We propose structural measures of cognitive maps as a means 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#27010;&#24565;&#65292;&#25193;&#23637;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#20803;&#20998;&#31867;&#22120;&#32473;&#20986;&#30340;&#32452;&#25104;&#30693;&#35782;&#33258;&#28982;&#22320;&#23545;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#31034;&#20363;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#25104;&#23545;&#20851;&#31995;&#23398;&#20064;&#35753;&#32452;&#21512;&#23884;&#20837;&#25152;&#25552;&#20379;&#30340;&#34920;&#31034;&#26356;&#21152;&#24378;&#20581;&#12290;</title><link>http://arxiv.org/abs/2106.15278</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#23884;&#20837;&#36827;&#34892;&#24320;&#25918;&#38598;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#27010;&#24565;&#65292;&#25193;&#23637;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#20803;&#20998;&#31867;&#22120;&#32473;&#20986;&#30340;&#32452;&#25104;&#30693;&#35782;&#33258;&#28982;&#22320;&#23545;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#31034;&#20363;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#25104;&#23545;&#20851;&#31995;&#23398;&#20064;&#35753;&#32452;&#21512;&#23884;&#20837;&#25152;&#25552;&#20379;&#30340;&#34920;&#31034;&#26356;&#21152;&#24378;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21097;&#20313;&#31867;&#21035;&#30340;&#26631;&#31614;&#19981;&#21487;&#29992;&#65292;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#38480;&#20110;&#22788;&#29702;&#19968;&#23567;&#37096;&#20998;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#31034;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#35782;&#21035;&#25193;&#23637;&#21040;&#24050;&#30693;&#21644;&#26032;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#22810;&#20010;&#24322;&#36136;&#26631;&#31614;&#31354;&#38388;&#30340;&#30417;&#30563;&#20803;&#20998;&#31867;&#22120;&#32473;&#20986;&#30340;&#32452;&#25104;&#30693;&#35782;&#33258;&#28982;&#22320;&#23545;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#31034;&#20363;&#36827;&#34892;&#32858;&#31867;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#25104;&#23545;&#20851;&#31995;&#23398;&#20064;&#65292;&#32452;&#21512;&#23884;&#20837;&#25152;&#25552;&#20379;&#30340;&#34920;&#31034;&#26356;&#21152;&#24378;&#20581;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26469;&#21457;&#29616;&#26032;&#27010;&#24565;&#65292;&#20197;&#22686;&#24378;&#26410;&#35265;&#31867;&#21035;&#30340;&#21306;&#20998;&#24615;&#65292;&#24182;&#23398;&#20064;&#23545;&#26032;&#31867;&#21035;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#24050;&#30693;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24320;&#25918;&#38598;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#34920;&#26126;&#20102;&#22312;&#21457;&#29616;&#26032;&#27010;&#24565;&#21644;&#25552;&#39640;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on both labeled and unlabeled examples, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. The representations given by the combinatorial embedding are made more robust by unsupervised pairwise relation learning. The proposed algorithm discovers novel concepts via a joint optimization for enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26377;&#21521;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SDGNN&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26377;&#31526;&#21495;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#35813;&#27169;&#22411;&#37325;&#26500;&#20102;&#38142;&#25509;&#31526;&#21495;&#12289;&#38142;&#25509;&#26041;&#21521;&#21644;&#26377;&#21521;&#26377;&#31526;&#21495;&#19977;&#35282;&#24418;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2101.02390</link><description>&lt;p&gt;
SDGNN: &#23398;&#20064;&#26377;&#21521;&#26377;&#31526;&#21495;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDGNN: Learning Node Representation for Signed Directed Networks. (arXiv:2101.02390v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.02390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26377;&#21521;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SDGNN&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26377;&#31526;&#21495;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#35813;&#27169;&#22411;&#37325;&#26500;&#20102;&#38142;&#25509;&#31526;&#21495;&#12289;&#38142;&#25509;&#26041;&#21521;&#21644;&#26377;&#21521;&#26377;&#31526;&#21495;&#19977;&#35282;&#24418;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23884;&#20837;&#26088;&#22312;&#23558;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#26144;&#23556;&#20026;&#20302;&#32500;&#21521;&#37327;&#34920;&#31034;&#24418;&#24335;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#22312;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#26041;&#38754;&#24341;&#39046;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;GNN&#20165;&#36866;&#29992;&#20110;&#20165;&#23384;&#22312;&#27491;&#38142;&#25509;&#30340;&#26080;&#31526;&#21495;&#32593;&#32476;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#36716;&#31227;&#21040;&#26377;&#21521;&#26377;&#31526;&#21495;&#32593;&#32476;&#20013;&#24182;&#19981;&#23481;&#26131;&#65292;&#32780;&#36825;&#31181;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#20294;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20004;&#20010;&#22522;&#26412;&#30340;&#31038;&#20250;&#23398;&#29702;&#35770;&#65288;&#21363;&#22320;&#20301;&#29702;&#35770;&#21644;&#24179;&#34913;&#29702;&#35770;&#65289;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#20197;&#20998;&#26512;&#26377;&#21521;&#26377;&#31526;&#21495;&#32593;&#32476;&#20013;&#30340;&#31038;&#20250;&#26426;&#21046;&#12290;&#22312;&#30456;&#20851;&#30340;&#31038;&#20250;&#23398;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26377;&#21521;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SDGNN&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26377;&#31526;&#21495;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21516;&#26102;&#37325;&#26500;&#38142;&#25509;&#31526;&#21495;&#12289;&#38142;&#25509;&#26041;&#21521;&#21644;&#26377;&#21521;&#26377;&#31526;&#21495;&#19977;&#35282;&#24418;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network embedding is aimed at mapping nodes in a network into low-dimensional vector representations. Graph Neural Networks (GNNs) have received widespread attention and lead to state-of-the-art performance in learning node representations. However, most GNNs only work in unsigned networks, where only positive links exist. It is not trivial to transfer these models to signed directed networks, which are widely observed in the real world yet less studied. In this paper, we first review two fundamental sociological theories (i.e., status theory and balance theory) and conduct empirical studies on real-world datasets to analyze the social mechanism in signed directed networks. Guided by related sociological theories, we propose a novel Signed Directed Graph Neural Networks model named SDGNN to learn node embeddings for signed directed networks. The proposed model simultaneously reconstructs link signs, link directions, and signed directed triangles. We validate our model's effectiveness o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#20132;&#27969;&#35774;&#35745;&#20013;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#20998;&#26512;&#35884;&#35823;&#21644;&#20462;&#36766;&#33539;&#30068;&#30340;&#24515;&#29702;&#21407;&#22240;&#12289;&#20449;&#21495;&#26816;&#27979;&#31561;&#22240;&#32032;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#35884;&#35823;&#21644;&#20462;&#36766;&#33539;&#30068;&#30340;&#23433;&#20840;&#21327;&#35758;&#12290;&#35813;&#21327;&#35758;&#20351;&#29992;&#20102;&#25351;&#20195;&#21644;&#24847;&#20041;&#21306;&#20998;&#12289;&#20998;&#26512;&#34920;&#12289;&#20449;&#24687;&#26368;&#22823;&#31243;&#24230;&#21407;&#21017;&#21644;&#35748;&#35782;&#35770;&#32771;&#34385;&#31561;&#25216;&#26415;&#65292;&#20174;&#32780;&#24110;&#21161;&#26426;&#22120;&#20154;&#31036;&#35980;&#22320;&#29702;&#35299;&#29992;&#25143;&#26377;&#26102;&#26214;&#28073;&#38590;&#25026;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/1906.09689</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23545;&#20110;&#35884;&#35823;&#21644;&#20462;&#36766;&#33539;&#30068;&#30340;&#24863;&#30693;&#12290;&#21019;&#24314;&#20154;&#31867;&#23581;&#35797;&#25152;&#25351;&#30340;&#26412;&#20307;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot's sense-making of fallacies and rhetorical tropes. Creating ontologies of what humans try to say. (arXiv:1906.09689v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.09689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#20132;&#27969;&#35774;&#35745;&#20013;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#20998;&#26512;&#35884;&#35823;&#21644;&#20462;&#36766;&#33539;&#30068;&#30340;&#24515;&#29702;&#21407;&#22240;&#12289;&#20449;&#21495;&#26816;&#27979;&#31561;&#22240;&#32032;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#35884;&#35823;&#21644;&#20462;&#36766;&#33539;&#30068;&#30340;&#23433;&#20840;&#21327;&#35758;&#12290;&#35813;&#21327;&#35758;&#20351;&#29992;&#20102;&#25351;&#20195;&#21644;&#24847;&#20041;&#21306;&#20998;&#12289;&#20998;&#26512;&#34920;&#12289;&#20449;&#24687;&#26368;&#22823;&#31243;&#24230;&#21407;&#21017;&#21644;&#35748;&#35782;&#35770;&#32771;&#34385;&#31561;&#25216;&#26415;&#65292;&#20174;&#32780;&#24110;&#21161;&#26426;&#22120;&#20154;&#31036;&#35980;&#22320;&#29702;&#35299;&#29992;&#25143;&#26377;&#26102;&#26214;&#28073;&#38590;&#25026;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#29992;&#25143;&#21451;&#22909;&#26426;&#22120;&#20154;&#30340;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#20154;&#31995;&#32479;&#24212;&#35813;&#29702;&#35299;&#20154;&#31867;&#20132;&#27969;&#30340;&#24847;&#20041;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#36923;&#36753;&#21644;&#23383;&#38754;&#24847;&#24605;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#26426;&#22120;&#20154;&#20132;&#27969;&#35774;&#35745;&#24573;&#30053;&#20102;&#20132;&#27969;&#21644;&#31036;&#35980;&#35268;&#21017;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#36825;&#20123;&#35268;&#21017;&#26159;&#8220;&#23485;&#23481;&#30340;&#8221;&#21644;&#8220;&#20013;&#27490;&#19981;&#20449;&#20208;&#8221;&#30340;&#65292;&#26080;&#27861;&#22788;&#29702;&#20154;&#31867;&#35774;&#35745;&#35328;&#35821;&#30340;&#22522;&#26412;&#38544;&#21947;&#26041;&#24335;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21512;&#36923;&#36753;&#21644;&#38750;&#23383;&#38754;&#38472;&#36848;&#30340;&#24515;&#29702;&#21407;&#22240;&#12289;&#20449;&#21495;&#26816;&#27979;&#12289;&#22522;&#26412;&#24402;&#22240;&#38169;&#35823;&#21644;&#25311;&#20154;&#21270;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23433;&#20840;&#21327;&#35758;&#26469;&#38024;&#23545;&#35884;&#35823;&#21644;&#20462;&#36766;&#33539;&#30068;&#65292;&#22312;&#20351;&#29992;&#24343;&#38647;&#26684;&#30340;&#25351;&#20195;&#21644;&#24847;&#20041;&#21306;&#20998;&#12289;&#36125;&#19997;&#30340;&#20998;&#26512;&#34920;&#12289;&#26684;&#36182;&#26031;&#30340;&#20449;&#24687;&#26368;&#22823;&#31243;&#24230;&#21407;&#21017;&#21644;&#35748;&#35782;&#35770;&#32771;&#34385;&#31561;&#26041;&#38754;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#31036;&#35980;&#22320;&#29702;&#35299;&#29992;&#25143;&#26377;&#26102;&#26214;&#28073;&#38590;&#25026;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the design of user-friendly robots, human communication should be understood by the system beyond mere logics and literal meaning. Robot communication-design has long ignored the importance of communication and politeness rules that are 'forgiving' and 'suspending disbelief' and cannot handle the basically metaphorical way humans design their utterances. Through analysis of the psychological causes of illogical and non-literal statements, signal detection, fundamental attribution errors, and anthropomorphism, we developed a fail-safe protocol for fallacies and tropes that makes use of Frege's distinction between reference and sense, Beth's tableau analytics, Grice's maxim of quality, and epistemic considerations to have the robot politely make sense of a user's sometimes unintelligible demands. Keywords: social robots, logical fallacies, metaphors, reference, sense, maxim of quality, tableau reasoning, epistemics of the virtual
&lt;/p&gt;</description></item></channel></rss>